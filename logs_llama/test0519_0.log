nohup: ignoring input
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:0'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}
网络架构：{'159': ['225', '89', '106', '210', '66', '16', '148', '109', '222', '107', '78', '56', '47', '81', '169', '250', '34', '105', '13', '211', '7', '95', '183', '18', '137', '189', '102', '165', '167', '52', '140'], '52': ['95', '18', '102', '169', '225', '167', '189', '183', '165', '148', '16', '56', '222', '140', '7', '81', '211', '250', '47', '105', '78', '66', '159', '106', '107', '34', '137', '109', '13', '89', '210'], '7': ['250', '89', '211', '34', '16', '189', '66', '13', '102', '52', '109', '47', '78', '105', '81', '56', '148', '159', '137', '165', '95', '107', '18', '225', '210', '169', '140', '167', '106', '222', '183'], '47': ['18', '165', '159', '210', '13', '66', '7', '78', '89', '148', '56', '52', '137', '95', '211', '109', '183', '105', '106', '16', '189', '167', '169', '107', '250', '222', '225', '81', '34', '140', '102'], '89': ['7', '95', '225', '210', '18', '34', '81', '140', '167', '107', '183', '56', '13', '159', '169', '148', '16', '250', '66', '165', '211', '52', '102', '189', '78', '105', '106', '222', '47', '137', '109'], '222': ['107', '105', '159', '18', '7', '102', '167', '211', '66', '34', '137', '95', '47', '169', '165', '183', '148', '89', '52', '140', '81', '78', '189', '210', '56', '250', '16', '106', '13', '225', '109'], '81': ['56', '159', '211', '210', '13', '78', '34', '165', '106', '18', '52', '89', '109', '66', '183', '137', '102', '169', '148', '222', '250', '47', '105', '107', '95', '140', '167', '225', '7', '16', '189'], '105': ['169', '167', '18', '13', '78', '211', '7', '159', '210', '34', '165', '137', '16', '47', '189', '148', '56', '109', '225', '107', '222', '183', '95', '66', '102', '89', '106', '81', '140', '52', '250'], '140': ['169', '89', '18', '148', '66', '81', '56', '16', '222', '250', '47', '105', '165', '95', '225', '34', '137', '107', '211', '7', '52', '13', '210', '167', '106', '102', '109', '78', '183', '189', '159'], '16': ['105', '250', '148', '109', '52', '165', '106', '47', '89', '78', '102', '210', '222', '137', '167', '34', '18', '169', '95', '140', '189', '183', '66', '13', '159', '225', '211', '107', '7', '56', '81'], '167': ['7', '89', '106', '109', '165', '81', '137', '159', '105', '52', '13', '169', '95', '225', '18', '183', '189', '211', '78', '210', '250', '34', '140', '56', '107', '102', '16', '222', '148', '47', '66'], '250': ['225', '81', '78', '52', '47', '210', '105', '189', '140', '183', '16', '107', '7', '89', '95', '102', '106', '165', '211', '137', '13', '222', '148', '169', '18', '56', '66', '159', '109', '167', '34'], '102': ['250', '13', '34', '56', '148', '109', '16', '167', '7', '89', '140', '165', '211', '137', '18', '210', '52', '95', '81', '66', '222', '183', '105', '107', '225', '169', '106', '159', '78', '47', '189'], '34': ['169', '18', '140', '106', '78', '159', '95', '250', '56', '165', '16', '167', '81', '189', '137', '105', '107', '183', '102', '13', '47', '89', '210', '109', '211', '66', '7', '225', '222', '148', '52'], '165': ['7', '13', '81', '102', '167', '16', '225', '18', '148', '106', '78', '210', '47', '211', '189', '105', '222', '140', '34', '56', '169', '52', '183', '107', '159', '66', '95', '137', '109', '250', '89'], '183': ['210', '225', '159', '109', '89', '140', '105', '81', '189', '165', '250', '66', '137', '34', '16', '102', '106', '7', '148', '222', '95', '18', '52', '13', '211', '47', '56', '107', '78', '167', '169'], '56': ['78', '109', '148', '66', '107', '16', '95', '89', '81', '105', '225', '137', '165', '159', '47', '169', '52', '7', '18', '210', '167', '34', '183', '13', '222', '189', '140', '250', '106', '211', '102'], '148': ['16', '81', '189', '140', '56', '250', '78', '137', '159', '13', '183', '169', '89', '210', '167', '52', '165', '34', '102', '18', '225', '109', '222', '66', '95', '211', '105', '47', '107', '7', '106'], '225': ['102', '250', '34', '18', '13', '140', '52', '148', '106', '105', '211', '189', '222', '159', '107', '16', '66', '210', '47', '89', '183', '167', '7', '78', '169', '56', '165', '137', '81', '95', '109'], '211': ['165', '52', '169', '95', '7', '140', '13', '66', '107', '34', '81', '250', '105', '109', '18', '189', '222', '137', '106', '89', '225', '16', '183', '56', '167', '148', '102', '78', '47', '159', '210'], '95': ['105', '106', '183', '210', '66', '102', '34', '159', '148', '107', '140', '81', '167', '7', '222', '89', '225', '189', '137', '13', '52', '109', '16', '250', '18', '78', '169', '211', '47', '165', '56'], '78': ['169', '165', '16', '13', '107', '250', '66', '52', '225', '211', '210', '34', '7', '18', '137', '89', '167', '47', '81', '148', '183', '95', '102', '222', '106', '159', '56', '109', '105', '140', '189'], '13': ['95', '183', '102', '250', '140', '109', '211', '189', '16', '137', '34', '169', '159', '106', '165', '47', '7', '210', '89', '225', '78', '107', '148', '52', '56', '167', '18', '81', '222', '66', '105'], '107': ['89', '52', '225', '250', '102', '167', '106', '47', '148', '56', '183', '78', '137', '95', '189', '109', '159', '13', '34', '165', '18', '16', '7', '105', '140', '222', '211', '210', '81', '169', '66'], '106': ['47', '16', '52', '105', '66', '78', '140', '250', '225', '222', '189', '167', '95', '137', '107', '89', '165', '211', '109', '34', '102', '7', '148', '81', '13', '169', '18', '56', '210', '159', '183'], '137': ['148', '102', '105', '159', '81', '18', '189', '169', '107', '66', '89', '56', '140', '183', '250', '78', '52', '210', '109', '95', '222', '165', '106', '7', '211', '16', '13', '225', '47', '34', '167'], '169': ['66', '52', '13', '7', '159', '165', '189', '34', '78', '106', '222', '109', '107', '137', '183', '211', '16', '89', '250', '47', '210', '105', '81', '140', '148', '102', '167', '225', '18', '56', '95'], '66': ['137', '148', '81', '106', '183', '169', '7', '189', '165', '225', '56', '250', '140', '210', '47', '52', '109', '95', '18', '102', '159', '222', '105', '89', '16', '107', '13', '167', '211', '34', '78'], '18': ['107', '210', '47', '66', '211', '13', '52', '78', '89', '159', '56', '140', '167', '81', '148', '102', '165', '137', '109', '106', '16', '189', '105', '250', '169', '183', '34', '95', '225', '7', '222'], '189': ['225', '222', '7', '102', '16', '250', '159', '78', '169', '167', '148', '210', '47', '211', '56', '52', '66', '105', '140', '34', '13', '81', '165', '18', '137', '95', '109', '106', '183', '89', '107'], '210': ['102', '137', '169', '7', '165', '167', '78', '211', '225', '148', '140', '13', '81', '189', '95', '18', '159', '52', '105', '66', '250', '109', '34', '183', '47', '56', '222', '16', '106', '89', '107'], '109': ['211', '189', '66', '225', '107', '222', '78', '52', '18', '105', '159', '56', '165', '183', '89', '81', '7', '102', '34', '250', '148', '47', '137', '169', '210', '167', '140', '16', '13', '106', '95']}
159
cuda:0
copa
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:46<00:46, 46.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 27.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 30.55s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: copa] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: copa] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/revision/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/revision/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646?recursive=False&expand=False HTTP/1.1" 307 138
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646?recursive=False&expand=False HTTP/1.1" 200 501
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/axb?recursive=False&expand=False HTTP/1.1" 307 142
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/axb?recursive=False&expand=False HTTP/1.1" 200 232
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/revision/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/revision/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/copa?recursive=False&expand=False HTTP/1.1" 307 143
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/copa?recursive=False&expand=False HTTP/1.1" 200 348
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140239677976464 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_copa_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140239677976464 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_copa_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140239677976464 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_copa_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140239677976464 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_copa_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140242234502864 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140242234502864 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140242234502864 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140242234502864 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
DEBUG:lm_eval.api.task:Both target_delimiter " " and target choice: " the toilet filled with water." have whitespace
DEBUG:lm_eval.api.task:Both target_delimiter " " and target choice: " water flowed from the spout." have whitespace
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of copa from None to 0
INFO:lm_eval.api.task:Building contexts for copa on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 96067.43it/s]
DEBUG:lm_eval.evaluator:Task: copa; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:02<08:53,  2.68s/it]Running loglikelihood requests:   1%|          | 2/200 [00:03<04:33,  1.38s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:03<03:08,  1.04it/s]Running loglikelihood requests:   2%|▏         | 4/200 [00:04<02:28,  1.32it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:04<02:05,  1.55it/s]Running loglikelihood requests:   3%|▎         | 6/200 [00:04<01:52,  1.73it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:05<01:43,  1.87it/s]Running loglikelihood requests:   4%|▍         | 8/200 [00:05<01:37,  1.97it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:06<01:33,  2.05it/s]Running loglikelihood requests:   5%|▌         | 10/200 [00:06<01:29,  2.11it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:07<01:27,  2.16it/s]Running loglikelihood requests:   6%|▌         | 12/200 [00:07<01:25,  2.20it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:08<01:24,  2.22it/s]Running loglikelihood requests:   7%|▋         | 14/200 [00:08<01:22,  2.24it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:08<01:21,  2.26it/s]Running loglikelihood requests:   8%|▊         | 16/200 [00:09<01:20,  2.28it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:09<01:20,  2.28it/s]Running loglikelihood requests:   9%|▉         | 18/200 [00:10<01:19,  2.29it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:10<01:18,  2.30it/s]Running loglikelihood requests:  10%|█         | 20/200 [00:11<01:18,  2.30it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:11<01:17,  2.30it/s]Running loglikelihood requests:  11%|█         | 22/200 [00:11<01:17,  2.31it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:12<01:16,  2.31it/s]Running loglikelihood requests:  12%|█▏        | 24/200 [00:12<01:15,  2.32it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:13<01:15,  2.32it/s]Running loglikelihood requests:  13%|█▎        | 26/200 [00:13<01:14,  2.32it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:14<01:14,  2.32it/s]Running loglikelihood requests:  14%|█▍        | 28/200 [00:14<01:14,  2.32it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:14<01:13,  2.33it/s]Running loglikelihood requests:  15%|█▌        | 30/200 [00:15<01:12,  2.33it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:15<01:12,  2.34it/s]Running loglikelihood requests:  16%|█▌        | 32/200 [00:16<01:11,  2.34it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:16<01:11,  2.35it/s]Running loglikelihood requests:  17%|█▋        | 34/200 [00:17<01:10,  2.35it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:17<01:10,  2.35it/s]Running loglikelihood requests:  18%|█▊        | 36/200 [00:17<01:09,  2.35it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:18<01:09,  2.35it/s]Running loglikelihood requests:  19%|█▉        | 38/200 [00:18<01:08,  2.35it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:19<01:08,  2.35it/s]Running loglikelihood requests:  20%|██        | 40/200 [00:19<01:07,  2.36it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:20<01:07,  2.36it/s]Running loglikelihood requests:  21%|██        | 42/200 [00:20<01:07,  2.35it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:20<01:06,  2.36it/s]Running loglikelihood requests:  22%|██▏       | 44/200 [00:21<01:06,  2.36it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:21<01:05,  2.36it/s]Running loglikelihood requests:  23%|██▎       | 46/200 [00:22<01:05,  2.36it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:22<01:04,  2.36it/s]Running loglikelihood requests:  24%|██▍       | 48/200 [00:23<01:04,  2.36it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:23<01:04,  2.36it/s]Running loglikelihood requests:  25%|██▌       | 50/200 [00:23<01:03,  2.35it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:24<01:03,  2.35it/s]Running loglikelihood requests:  26%|██▌       | 52/200 [00:24<01:02,  2.36it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:25<01:02,  2.36it/s]Running loglikelihood requests:  27%|██▋       | 54/200 [00:25<01:01,  2.36it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:25<01:01,  2.37it/s]Running loglikelihood requests:  28%|██▊       | 56/200 [00:26<01:00,  2.37it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:26<01:00,  2.38it/s]Running loglikelihood requests:  29%|██▉       | 58/200 [00:27<00:59,  2.38it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:27<00:59,  2.38it/s]Running loglikelihood requests:  30%|███       | 60/200 [00:28<00:58,  2.38it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:28<00:58,  2.37it/s]Running loglikelihood requests:  31%|███       | 62/200 [00:28<00:58,  2.38it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:29<00:57,  2.38it/s]Running loglikelihood requests:  32%|███▏      | 64/200 [00:29<00:57,  2.38it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:30<00:56,  2.38it/s]Running loglikelihood requests:  33%|███▎      | 66/200 [00:30<00:56,  2.38it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:31<00:55,  2.38it/s]Running loglikelihood requests:  34%|███▍      | 68/200 [00:31<00:55,  2.39it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:31<00:54,  2.39it/s]Running loglikelihood requests:  35%|███▌      | 70/200 [00:32<00:54,  2.38it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:32<00:54,  2.38it/s]Running loglikelihood requests:  36%|███▌      | 72/200 [00:33<00:53,  2.39it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:33<00:53,  2.39it/s]Running loglikelihood requests:  37%|███▋      | 74/200 [00:33<00:52,  2.40it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:34<00:52,  2.40it/s]Running loglikelihood requests:  38%|███▊      | 76/200 [00:34<00:51,  2.40it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:35<00:51,  2.41it/s]Running loglikelihood requests:  39%|███▉      | 78/200 [00:35<00:50,  2.41it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:36<00:50,  2.41it/s]Running loglikelihood requests:  40%|████      | 80/200 [00:36<00:49,  2.41it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:36<00:49,  2.41it/s]Running loglikelihood requests:  41%|████      | 82/200 [00:37<00:49,  2.41it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:37<00:48,  2.40it/s]Running loglikelihood requests:  42%|████▏     | 84/200 [00:38<00:48,  2.41it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:38<00:47,  2.41it/s]Running loglikelihood requests:  43%|████▎     | 86/200 [00:38<00:47,  2.41it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:39<00:46,  2.41it/s]Running loglikelihood requests:  44%|████▍     | 88/200 [00:39<00:46,  2.41it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:40<00:45,  2.41it/s]Running loglikelihood requests:  45%|████▌     | 90/200 [00:40<00:45,  2.41it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:40<00:45,  2.41it/s]Running loglikelihood requests:  46%|████▌     | 92/200 [00:41<00:44,  2.40it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:41<00:44,  2.40it/s]Running loglikelihood requests:  47%|████▋     | 94/200 [00:42<00:44,  2.40it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:42<00:43,  2.40it/s]Running loglikelihood requests:  48%|████▊     | 96/200 [00:43<00:43,  2.40it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:43<00:42,  2.40it/s]Running loglikelihood requests:  49%|████▉     | 98/200 [00:43<00:42,  2.41it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:44<00:41,  2.43it/s]Running loglikelihood requests:  50%|█████     | 100/200 [00:44<00:41,  2.43it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:45<00:40,  2.43it/s]Running loglikelihood requests:  51%|█████     | 102/200 [00:45<00:40,  2.43it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:45<00:39,  2.43it/s]Running loglikelihood requests:  52%|█████▏    | 104/200 [00:46<00:39,  2.44it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:46<00:38,  2.44it/s]Running loglikelihood requests:  53%|█████▎    | 106/200 [00:47<00:38,  2.44it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:47<00:38,  2.44it/s]Running loglikelihood requests:  54%|█████▍    | 108/200 [00:48<00:37,  2.44it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:48<00:37,  2.44it/s]Running loglikelihood requests:  55%|█████▌    | 110/200 [00:48<00:37,  2.43it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:49<00:36,  2.43it/s]Running loglikelihood requests:  56%|█████▌    | 112/200 [00:49<00:36,  2.38it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:50<00:36,  2.37it/s]Running loglikelihood requests:  57%|█████▋    | 114/200 [00:50<00:36,  2.37it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:50<00:35,  2.40it/s]Running loglikelihood requests:  58%|█████▊    | 116/200 [00:51<00:34,  2.43it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:51<00:33,  2.45it/s]Running loglikelihood requests:  59%|█████▉    | 118/200 [00:52<00:33,  2.46it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:52<00:32,  2.47it/s]Running loglikelihood requests:  60%|██████    | 120/200 [00:52<00:32,  2.47it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:53<00:32,  2.47it/s]Running loglikelihood requests:  61%|██████    | 122/200 [00:53<00:31,  2.46it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:54<00:31,  2.46it/s]Running loglikelihood requests:  62%|██████▏   | 124/200 [00:54<00:30,  2.47it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:54<00:30,  2.46it/s]Running loglikelihood requests:  63%|██████▎   | 126/200 [00:55<00:30,  2.46it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:55<00:29,  2.46it/s]Running loglikelihood requests:  64%|██████▍   | 128/200 [00:56<00:29,  2.46it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:56<00:28,  2.46it/s]Running loglikelihood requests:  65%|██████▌   | 130/200 [00:57<00:28,  2.48it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:57<00:27,  2.47it/s]Running loglikelihood requests:  66%|██████▌   | 132/200 [00:57<00:27,  2.47it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:58<00:27,  2.47it/s]Running loglikelihood requests:  67%|██████▋   | 134/200 [00:58<00:26,  2.47it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:59<00:26,  2.47it/s]Running loglikelihood requests:  68%|██████▊   | 136/200 [00:59<00:25,  2.47it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:59<00:25,  2.48it/s]Running loglikelihood requests:  69%|██████▉   | 138/200 [01:00<00:24,  2.48it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [01:00<00:24,  2.47it/s]Running loglikelihood requests:  70%|███████   | 140/200 [01:01<00:24,  2.48it/s]Running loglikelihood requests:  70%|███████   | 141/200 [01:01<00:23,  2.48it/s]Running loglikelihood requests:  71%|███████   | 142/200 [01:01<00:23,  2.49it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [01:02<00:22,  2.49it/s]Running loglikelihood requests:  72%|███████▏  | 144/200 [01:02<00:22,  2.51it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:03<00:21,  2.50it/s]Running loglikelihood requests:  73%|███████▎  | 146/200 [01:03<00:21,  2.51it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [01:03<00:21,  2.52it/s]Running loglikelihood requests:  74%|███████▍  | 148/200 [01:04<00:20,  2.52it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:04<00:20,  2.52it/s]Running loglikelihood requests:  75%|███████▌  | 150/200 [01:05<00:19,  2.51it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:05<00:19,  2.51it/s]Running loglikelihood requests:  76%|███████▌  | 152/200 [01:05<00:19,  2.50it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:06<00:18,  2.50it/s]Running loglikelihood requests:  77%|███████▋  | 154/200 [01:06<00:18,  2.51it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:07<00:17,  2.51it/s]Running loglikelihood requests:  78%|███████▊  | 156/200 [01:07<00:17,  2.52it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:07<00:17,  2.53it/s]Running loglikelihood requests:  79%|███████▉  | 158/200 [01:08<00:16,  2.52it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:08<00:16,  2.52it/s]Running loglikelihood requests:  80%|████████  | 160/200 [01:08<00:15,  2.53it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:09<00:15,  2.54it/s]Running loglikelihood requests:  81%|████████  | 162/200 [01:09<00:14,  2.54it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:10<00:14,  2.55it/s]Running loglikelihood requests:  82%|████████▏ | 164/200 [01:10<00:14,  2.54it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:10<00:13,  2.53it/s]Running loglikelihood requests:  83%|████████▎ | 166/200 [01:11<00:13,  2.54it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:11<00:12,  2.55it/s]Running loglikelihood requests:  84%|████████▍ | 168/200 [01:12<00:12,  2.56it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:12<00:12,  2.55it/s]Running loglikelihood requests:  85%|████████▌ | 170/200 [01:12<00:11,  2.54it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:13<00:11,  2.54it/s]Running loglikelihood requests:  86%|████████▌ | 172/200 [01:13<00:10,  2.55it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:14<00:10,  2.56it/s]Running loglikelihood requests:  87%|████████▋ | 174/200 [01:14<00:10,  2.56it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:14<00:09,  2.57it/s]Running loglikelihood requests:  88%|████████▊ | 176/200 [01:15<00:09,  2.57it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:15<00:08,  2.57it/s]Running loglikelihood requests:  89%|████████▉ | 178/200 [01:16<00:08,  2.58it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:16<00:08,  2.58it/s]Running loglikelihood requests:  90%|█████████ | 180/200 [01:16<00:07,  2.58it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:17<00:07,  2.58it/s]Running loglikelihood requests:  91%|█████████ | 182/200 [01:17<00:06,  2.57it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:17<00:06,  2.60it/s]Running loglikelihood requests:  92%|█████████▏| 184/200 [01:18<00:06,  2.60it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:18<00:05,  2.60it/s]Running loglikelihood requests:  93%|█████████▎| 186/200 [01:19<00:05,  2.60it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:19<00:04,  2.62it/s]Running loglikelihood requests:  94%|█████████▍| 188/200 [01:19<00:04,  2.61it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:20<00:04,  2.62it/s]Running loglikelihood requests:  95%|█████████▌| 190/200 [01:20<00:03,  2.64it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:21<00:03,  2.63it/s]Running loglikelihood requests:  96%|█████████▌| 192/200 [01:21<00:03,  2.63it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:21<00:02,  2.63it/s]Running loglikelihood requests:  97%|█████████▋| 194/200 [01:22<00:02,  2.63it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:22<00:01,  2.67it/s]Running loglikelihood requests:  98%|█████████▊| 196/200 [01:22<00:01,  2.67it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:23<00:01,  2.69it/s]Running loglikelihood requests:  99%|█████████▉| 198/200 [01:23<00:00,  2.70it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:23<00:00,  2.70it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:24<00:00,  2.72it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:24<00:00,  2.37it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:1'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:1'}
full model:
{'copa': {'alias': 'copa', 'acc,none': 0.82, 'acc_stderr,none': 0.03861229196653691}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7827148355678417
0.8244609672079803
0.649635438690672
0.6067147398962018
0.806517251330611
0.9304450428937726
0.9492767057467371
0.8146777207922447
0.5943920512469448
0.6904992046065734
0.898578027138337
0.9772061768478973
0.9053772479641496
0.8020143483317842
0.5125553000556808
0.7018400400551881
0.8957865573767195
0.6076519427757794
0.975563476608663
0.9639212062070597
0.9296418237714587
0.9150145595079396
0.9197448761365332
0.7066536936990038
0.6639302750778917
0.912306872752127
0.7698141900267782
0.6230420491138425
0.8289959673107662
Total groups 68 exceeded the threshold, stopping comparison.
The group tensor is
[7, 3, 4, 1, 2, 6, 5, 0]
tensor([7, 3, 4, 1, 2, 6, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 4, 1, 0, 5, 7, 2]
tensor([6, 3, 4, 1, 0, 5, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 4, 3, 2, 0, 5, 6, 1]
tensor([7, 4, 3, 2, 0, 5, 6, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 6, 4, 2, 1, 3, 5, 0]
tensor([7, 6, 4, 2, 1, 3, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 3, 0, 1, 2, 3, 2]
tensor([0, 1, 3, 0, 1, 2, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 3, 1, 0, 2, 3, 2]
tensor([0, 1, 3, 1, 0, 2, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 0, 1.0, 1.0, 1.0, 1]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 2 to 3
done!
Normal merging for layer 4
tensor([4])
tensor(4)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 5 to 6
done!
Normal merging for layer 7
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 8 to 15
done!
Normal merging for layer 16
tensor([0, 3])
tensor(0)
tensor([1, 4])
tensor(1)
tensor([5, 7])
tensor(5)
tensor([2, 6])
tensor(2)
done!
Cross-layer merge completed for layers 17 to 21
done!
Normal merging for layer 22
tensor([0, 4])
tensor(0)
tensor([1, 3])
tensor(1)
tensor([5, 7])
tensor(5)
tensor([2, 6])
tensor(2)
done!
Cross-layer merge completed for layers 23 to 27
done!
Normal merging for layer 28
tensor([0, 3])
tensor(0)
tensor([1, 2, 4, 5, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 29 to 30
done!
Normal merging for layer 31
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
all done!
Model size: 12.1348 GB
52
cuda:1
logiqa
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:43<00:43, 43.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 25.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 28.39s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/logiqa HTTP/1.1" 200 743
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/EleutherAI/logiqa/EleutherAI/logiqa.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): datasets-server.hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://datasets-server.hf-mirror.com:443 "GET /parquet?dataset=EleutherAI/logiqa HTTP/1.1" 302 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET / HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/EleutherAI/logiqa/EleutherAI/logiqa.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/logiqa/resolve/main/logiqa.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/logiqa/resolve/main/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/logiqa/resolve/main/README.md HTTP/1.1" 200 0
DEBUG:filelock:Attempting to acquire lock 140239882247808 on /public/home/zouyifei001/.cache/huggingface/modules/datasets_modules/datasets/EleutherAI--logiqa.lock
DEBUG:filelock:Lock 140239882247808 acquired on /public/home/zouyifei001/.cache/huggingface/modules/datasets_modules/datasets/EleutherAI--logiqa.lock
DEBUG:filelock:Attempting to release lock 140239882247808 on /public/home/zouyifei001/.cache/huggingface/modules/datasets_modules/datasets/EleutherAI--logiqa.lock
DEBUG:filelock:Lock 140239882247808 released on /public/home/zouyifei001/.cache/huggingface/modules/datasets_modules/datasets/EleutherAI--logiqa.lock
DEBUG:filelock:Attempting to acquire lock 140239883743168 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___logiqa_logiqa_0.0.1_5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81.lock
DEBUG:filelock:Lock 140239883743168 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___logiqa_logiqa_0.0.1_5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81/dataset_info.json
DEBUG:filelock:Attempting to release lock 140239883743168 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___logiqa_logiqa_0.0.1_5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81.lock
DEBUG:filelock:Lock 140239883743168 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___logiqa_logiqa_0.0.1_5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81.lock
DEBUG:filelock:Attempting to acquire lock 140248123066656 on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81_builder.lock
DEBUG:filelock:Lock 140248123066656 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81/dataset_info.json
DEBUG:filelock:Attempting to release lock 140248123066656 on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81_builder.lock
DEBUG:filelock:Lock 140248123066656 released on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of logiqa from None to 0
INFO:lm_eval.api.task:Building contexts for logiqa on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 3033.35it/s]
DEBUG:lm_eval.evaluator:Task: logiqa; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:02<19:42,  2.96s/it]Running loglikelihood requests:   0%|          | 2/400 [00:05<16:55,  2.55s/it]Running loglikelihood requests:   1%|          | 3/400 [00:07<15:53,  2.40s/it]Running loglikelihood requests:   1%|          | 4/400 [00:09<15:20,  2.32s/it]Running loglikelihood requests:   1%|▏         | 5/400 [00:11<14:54,  2.27s/it]Running loglikelihood requests:   2%|▏         | 6/400 [00:13<14:37,  2.23s/it]Running loglikelihood requests:   2%|▏         | 7/400 [00:16<14:25,  2.20s/it]Running loglikelihood requests:   2%|▏         | 8/400 [00:18<14:16,  2.19s/it]Running loglikelihood requests:   2%|▏         | 9/400 [00:20<14:06,  2.17s/it]Running loglikelihood requests:   2%|▎         | 10/400 [00:22<13:57,  2.15s/it]Running loglikelihood requests:   3%|▎         | 11/400 [00:24<13:46,  2.12s/it]Running loglikelihood requests:   3%|▎         | 12/400 [00:26<13:33,  2.10s/it]Running loglikelihood requests:   3%|▎         | 13/400 [00:28<13:24,  2.08s/it]Running loglikelihood requests:   4%|▎         | 14/400 [00:30<13:15,  2.06s/it]Running loglikelihood requests:   4%|▍         | 15/400 [00:32<13:08,  2.05s/it]Running loglikelihood requests:   4%|▍         | 16/400 [00:34<13:03,  2.04s/it]Running loglikelihood requests:   4%|▍         | 17/400 [00:36<12:59,  2.03s/it]Running loglikelihood requests:   4%|▍         | 18/400 [00:38<12:54,  2.03s/it]Running loglikelihood requests:   5%|▍         | 19/400 [00:40<12:49,  2.02s/it]Running loglikelihood requests:   5%|▌         | 20/400 [00:42<12:45,  2.02s/it]Running loglikelihood requests:   5%|▌         | 21/400 [00:44<12:40,  2.01s/it]Running loglikelihood requests:   6%|▌         | 22/400 [00:46<12:35,  2.00s/it]Running loglikelihood requests:   6%|▌         | 23/400 [00:48<12:30,  1.99s/it]Running loglikelihood requests:   6%|▌         | 24/400 [00:50<12:26,  1.98s/it]Running loglikelihood requests:   6%|▋         | 25/400 [00:52<12:21,  1.98s/it]Running loglikelihood requests:   6%|▋         | 26/400 [00:54<12:14,  1.96s/it]Running loglikelihood requests:   7%|▋         | 27/400 [00:56<12:09,  1.95s/it]Running loglikelihood requests:   7%|▋         | 28/400 [00:58<12:05,  1.95s/it]Running loglikelihood requests:   7%|▋         | 29/400 [01:00<12:00,  1.94s/it]Running loglikelihood requests:   8%|▊         | 30/400 [01:02<11:55,  1.93s/it]Running loglikelihood requests:   8%|▊         | 31/400 [01:04<11:50,  1.92s/it]Running loglikelihood requests:   8%|▊         | 32/400 [01:06<11:46,  1.92s/it]Running loglikelihood requests:   8%|▊         | 33/400 [01:07<11:42,  1.91s/it]Running loglikelihood requests:   8%|▊         | 34/400 [01:09<11:38,  1.91s/it]Running loglikelihood requests:   9%|▉         | 35/400 [01:11<11:34,  1.90s/it]Running loglikelihood requests:   9%|▉         | 36/400 [01:13<11:32,  1.90s/it]Running loglikelihood requests:   9%|▉         | 37/400 [01:15<11:26,  1.89s/it]Running loglikelihood requests:  10%|▉         | 38/400 [01:17<11:20,  1.88s/it]Running loglikelihood requests:  10%|▉         | 39/400 [01:19<11:15,  1.87s/it]Running loglikelihood requests:  10%|█         | 40/400 [01:21<11:10,  1.86s/it]Running loglikelihood requests:  10%|█         | 41/400 [01:22<11:06,  1.86s/it]Running loglikelihood requests:  10%|█         | 42/400 [01:24<11:01,  1.85s/it]Running loglikelihood requests:  11%|█         | 43/400 [01:26<10:58,  1.84s/it]Running loglikelihood requests:  11%|█         | 44/400 [01:28<10:54,  1.84s/it]Running loglikelihood requests:  11%|█▏        | 45/400 [01:30<10:52,  1.84s/it]Running loglikelihood requests:  12%|█▏        | 46/400 [01:32<10:47,  1.83s/it]Running loglikelihood requests:  12%|█▏        | 47/400 [01:33<10:43,  1.82s/it]Running loglikelihood requests:  12%|█▏        | 48/400 [01:35<10:40,  1.82s/it]Running loglikelihood requests:  12%|█▏        | 49/400 [01:37<10:37,  1.82s/it]Running loglikelihood requests:  12%|█▎        | 50/400 [01:39<10:34,  1.81s/it]Running loglikelihood requests:  13%|█▎        | 51/400 [01:41<10:31,  1.81s/it]Running loglikelihood requests:  13%|█▎        | 52/400 [01:42<10:29,  1.81s/it]Running loglikelihood requests:  13%|█▎        | 53/400 [01:44<10:27,  1.81s/it]Running loglikelihood requests:  14%|█▎        | 54/400 [01:46<10:25,  1.81s/it]Running loglikelihood requests:  14%|█▍        | 55/400 [01:48<10:21,  1.80s/it]Running loglikelihood requests:  14%|█▍        | 56/400 [01:50<10:16,  1.79s/it]Running loglikelihood requests:  14%|█▍        | 57/400 [01:51<10:11,  1.78s/it]Running loglikelihood requests:  14%|█▍        | 58/400 [01:53<10:07,  1.78s/it]Running loglikelihood requests:  15%|█▍        | 59/400 [01:55<10:04,  1.77s/it]Running loglikelihood requests:  15%|█▌        | 60/400 [01:57<09:59,  1.76s/it]Running loglikelihood requests:  15%|█▌        | 61/400 [01:58<09:53,  1.75s/it]Running loglikelihood requests:  16%|█▌        | 62/400 [02:00<09:48,  1.74s/it]Running loglikelihood requests:  16%|█▌        | 63/400 [02:02<09:43,  1.73s/it]Running loglikelihood requests:  16%|█▌        | 64/400 [02:03<09:39,  1.72s/it]Running loglikelihood requests:  16%|█▋        | 65/400 [02:05<09:33,  1.71s/it]Running loglikelihood requests:  16%|█▋        | 66/400 [02:07<09:29,  1.71s/it]Running loglikelihood requests:  17%|█▋        | 67/400 [02:08<09:24,  1.69s/it]Running loglikelihood requests:  17%|█▋        | 68/400 [02:10<09:19,  1.69s/it]Running loglikelihood requests:  17%|█▋        | 69/400 [02:12<09:15,  1.68s/it]Running loglikelihood requests:  18%|█▊        | 70/400 [02:13<09:11,  1.67s/it]Running loglikelihood requests:  18%|█▊        | 71/400 [02:15<09:09,  1.67s/it]Running loglikelihood requests:  18%|█▊        | 72/400 [02:17<09:06,  1.67s/it]Running loglikelihood requests:  18%|█▊        | 73/400 [02:18<09:01,  1.66s/it]Running loglikelihood requests:  18%|█▊        | 74/400 [02:20<08:56,  1.65s/it]Running loglikelihood requests:  19%|█▉        | 75/400 [02:22<08:51,  1.64s/it]Running loglikelihood requests:  19%|█▉        | 76/400 [02:23<08:47,  1.63s/it]Running loglikelihood requests:  19%|█▉        | 77/400 [02:25<08:42,  1.62s/it]Running loglikelihood requests:  20%|█▉        | 78/400 [02:26<08:38,  1.61s/it]Running loglikelihood requests:  20%|█▉        | 79/400 [02:28<08:33,  1.60s/it]Running loglikelihood requests:  20%|██        | 80/400 [02:30<08:29,  1.59s/it]Running loglikelihood requests:  20%|██        | 81/400 [02:31<08:26,  1.59s/it]Running loglikelihood requests:  20%|██        | 82/400 [02:33<08:22,  1.58s/it]Running loglikelihood requests:  21%|██        | 83/400 [02:34<08:18,  1.57s/it]Running loglikelihood requests:  21%|██        | 84/400 [02:36<08:15,  1.57s/it]Running loglikelihood requests:  21%|██▏       | 85/400 [02:37<08:11,  1.56s/it]Running loglikelihood requests:  22%|██▏       | 86/400 [02:39<08:08,  1.56s/it]Running loglikelihood requests:  22%|██▏       | 87/400 [02:40<08:05,  1.55s/it]Running loglikelihood requests:  22%|██▏       | 88/400 [02:42<08:03,  1.55s/it]Running loglikelihood requests:  22%|██▏       | 89/400 [02:44<08:01,  1.55s/it]Running loglikelihood requests:  22%|██▎       | 90/400 [02:45<07:59,  1.55s/it]Running loglikelihood requests:  23%|██▎       | 91/400 [02:47<07:58,  1.55s/it]Running loglikelihood requests:  23%|██▎       | 92/400 [02:48<07:55,  1.54s/it]Running loglikelihood requests:  23%|██▎       | 93/400 [02:50<07:53,  1.54s/it]Running loglikelihood requests:  24%|██▎       | 94/400 [02:51<07:50,  1.54s/it]Running loglikelihood requests:  24%|██▍       | 95/400 [02:53<07:48,  1.54s/it]Running loglikelihood requests:  24%|██▍       | 96/400 [02:54<07:46,  1.53s/it]Running loglikelihood requests:  24%|██▍       | 97/400 [02:56<07:43,  1.53s/it]Running loglikelihood requests:  24%|██▍       | 98/400 [02:57<07:41,  1.53s/it]Running loglikelihood requests:  25%|██▍       | 99/400 [02:59<07:39,  1.53s/it]Running loglikelihood requests:  25%|██▌       | 100/400 [03:00<07:38,  1.53s/it]Running loglikelihood requests:  25%|██▌       | 101/400 [03:02<07:37,  1.53s/it]Running loglikelihood requests:  26%|██▌       | 102/400 [03:03<07:35,  1.53s/it]Running loglikelihood requests:  26%|██▌       | 103/400 [03:05<07:32,  1.52s/it]Running loglikelihood requests:  26%|██▌       | 104/400 [03:07<07:30,  1.52s/it]Running loglikelihood requests:  26%|██▋       | 105/400 [03:08<07:28,  1.52s/it]Running loglikelihood requests:  26%|██▋       | 106/400 [03:10<07:26,  1.52s/it]Running loglikelihood requests:  27%|██▋       | 107/400 [03:11<07:24,  1.52s/it]Running loglikelihood requests:  27%|██▋       | 108/400 [03:13<07:22,  1.51s/it]Running loglikelihood requests:  27%|██▋       | 109/400 [03:14<07:19,  1.51s/it]Running loglikelihood requests:  28%|██▊       | 110/400 [03:16<07:16,  1.51s/it]Running loglikelihood requests:  28%|██▊       | 111/400 [03:17<07:15,  1.51s/it]Running loglikelihood requests:  28%|██▊       | 112/400 [03:19<07:13,  1.50s/it]Running loglikelihood requests:  28%|██▊       | 113/400 [03:20<07:11,  1.50s/it]Running loglikelihood requests:  28%|██▊       | 114/400 [03:22<07:08,  1.50s/it]Running loglikelihood requests:  29%|██▉       | 115/400 [03:23<07:05,  1.49s/it]Running loglikelihood requests:  29%|██▉       | 116/400 [03:25<07:03,  1.49s/it]Running loglikelihood requests:  29%|██▉       | 117/400 [03:26<07:01,  1.49s/it]Running loglikelihood requests:  30%|██▉       | 118/400 [03:27<06:58,  1.48s/it]Running loglikelihood requests:  30%|██▉       | 119/400 [03:29<06:55,  1.48s/it]Running loglikelihood requests:  30%|███       | 120/400 [03:30<06:53,  1.48s/it]Running loglikelihood requests:  30%|███       | 121/400 [03:32<06:51,  1.48s/it]Running loglikelihood requests:  30%|███       | 122/400 [03:33<06:50,  1.48s/it]Running loglikelihood requests:  31%|███       | 123/400 [03:35<06:48,  1.47s/it]Running loglikelihood requests:  31%|███       | 124/400 [03:36<06:45,  1.47s/it]Running loglikelihood requests:  31%|███▏      | 125/400 [03:38<06:43,  1.47s/it]Running loglikelihood requests:  32%|███▏      | 126/400 [03:39<06:42,  1.47s/it]Running loglikelihood requests:  32%|███▏      | 127/400 [03:41<06:39,  1.46s/it]Running loglikelihood requests:  32%|███▏      | 128/400 [03:42<06:37,  1.46s/it]Running loglikelihood requests:  32%|███▏      | 129/400 [03:44<06:34,  1.46s/it]Running loglikelihood requests:  32%|███▎      | 130/400 [03:45<06:32,  1.45s/it]Running loglikelihood requests:  33%|███▎      | 131/400 [03:46<06:30,  1.45s/it]Running loglikelihood requests:  33%|███▎      | 132/400 [03:48<06:29,  1.45s/it]Running loglikelihood requests:  33%|███▎      | 133/400 [03:49<06:27,  1.45s/it]Running loglikelihood requests:  34%|███▎      | 134/400 [03:51<06:26,  1.45s/it]Running loglikelihood requests:  34%|███▍      | 135/400 [03:52<06:25,  1.45s/it]Running loglikelihood requests:  34%|███▍      | 136/400 [03:54<06:23,  1.45s/it]Running loglikelihood requests:  34%|███▍      | 137/400 [03:55<06:21,  1.45s/it]Running loglikelihood requests:  34%|███▍      | 138/400 [03:57<06:19,  1.45s/it]Running loglikelihood requests:  35%|███▍      | 139/400 [03:58<06:17,  1.45s/it]Running loglikelihood requests:  35%|███▌      | 140/400 [04:00<06:15,  1.45s/it]Running loglikelihood requests:  36%|███▌      | 142/400 [04:01<04:46,  1.11s/it]Running loglikelihood requests:  36%|███▌      | 143/400 [04:02<05:06,  1.19s/it]Running loglikelihood requests:  36%|███▌      | 144/400 [04:04<05:22,  1.26s/it]Running loglikelihood requests:  36%|███▋      | 145/400 [04:05<05:33,  1.31s/it]Running loglikelihood requests:  36%|███▋      | 146/400 [04:07<05:41,  1.34s/it]Running loglikelihood requests:  37%|███▋      | 147/400 [04:08<05:46,  1.37s/it]Running loglikelihood requests:  37%|███▋      | 148/400 [04:10<05:49,  1.39s/it]Running loglikelihood requests:  37%|███▋      | 149/400 [04:11<05:51,  1.40s/it]Running loglikelihood requests:  38%|███▊      | 150/400 [04:12<05:52,  1.41s/it]Running loglikelihood requests:  38%|███▊      | 151/400 [04:14<05:51,  1.41s/it]Running loglikelihood requests:  38%|███▊      | 152/400 [04:15<05:51,  1.42s/it]Running loglikelihood requests:  38%|███▊      | 153/400 [04:17<05:50,  1.42s/it]Running loglikelihood requests:  38%|███▊      | 154/400 [04:18<05:49,  1.42s/it]Running loglikelihood requests:  39%|███▉      | 155/400 [04:20<05:49,  1.43s/it]Running loglikelihood requests:  39%|███▉      | 156/400 [04:21<05:47,  1.42s/it]Running loglikelihood requests:  39%|███▉      | 157/400 [04:22<05:45,  1.42s/it]Running loglikelihood requests:  40%|███▉      | 158/400 [04:24<05:44,  1.42s/it]Running loglikelihood requests:  40%|███▉      | 159/400 [04:25<05:42,  1.42s/it]Running loglikelihood requests:  40%|████      | 160/400 [04:27<05:40,  1.42s/it]Running loglikelihood requests:  40%|████      | 161/400 [04:28<05:38,  1.42s/it]Running loglikelihood requests:  40%|████      | 162/400 [04:29<05:37,  1.42s/it]Running loglikelihood requests:  41%|████      | 163/400 [04:31<05:36,  1.42s/it]Running loglikelihood requests:  41%|████      | 164/400 [04:32<05:34,  1.42s/it]Running loglikelihood requests:  41%|████▏     | 165/400 [04:34<05:32,  1.41s/it]Running loglikelihood requests:  42%|████▏     | 166/400 [04:35<05:31,  1.41s/it]Running loglikelihood requests:  42%|████▏     | 167/400 [04:37<05:29,  1.41s/it]Running loglikelihood requests:  42%|████▏     | 168/400 [04:38<05:27,  1.41s/it]Running loglikelihood requests:  42%|████▏     | 169/400 [04:39<05:25,  1.41s/it]Running loglikelihood requests:  42%|████▎     | 170/400 [04:41<05:24,  1.41s/it]Running loglikelihood requests:  43%|████▎     | 171/400 [04:42<05:22,  1.41s/it]Running loglikelihood requests:  43%|████▎     | 172/400 [04:44<05:20,  1.41s/it]Running loglikelihood requests:  43%|████▎     | 173/400 [04:45<05:18,  1.40s/it]Running loglikelihood requests:  44%|████▎     | 174/400 [04:46<05:16,  1.40s/it]Running loglikelihood requests:  44%|████▍     | 175/400 [04:48<05:14,  1.40s/it]Running loglikelihood requests:  44%|████▍     | 176/400 [04:49<05:12,  1.40s/it]Running loglikelihood requests:  44%|████▍     | 177/400 [04:51<05:11,  1.40s/it]Running loglikelihood requests:  44%|████▍     | 178/400 [04:52<05:09,  1.39s/it]Running loglikelihood requests:  45%|████▍     | 179/400 [04:53<05:08,  1.39s/it]Running loglikelihood requests:  45%|████▌     | 180/400 [04:55<05:05,  1.39s/it]Running loglikelihood requests:  45%|████▌     | 181/400 [04:56<05:03,  1.38s/it]Running loglikelihood requests:  46%|████▌     | 182/400 [04:57<05:01,  1.38s/it]Running loglikelihood requests:  46%|████▌     | 183/400 [04:59<04:59,  1.38s/it]Running loglikelihood requests:  46%|████▌     | 184/400 [05:00<04:57,  1.38s/it]Running loglikelihood requests:  46%|████▋     | 185/400 [05:02<04:55,  1.37s/it]Running loglikelihood requests:  46%|████▋     | 186/400 [05:03<04:53,  1.37s/it]Running loglikelihood requests:  47%|████▋     | 187/400 [05:04<04:51,  1.37s/it]Running loglikelihood requests:  47%|████▋     | 188/400 [05:06<04:50,  1.37s/it]Running loglikelihood requests:  47%|████▋     | 189/400 [05:07<04:49,  1.37s/it]Running loglikelihood requests:  48%|████▊     | 190/400 [05:08<04:47,  1.37s/it]Running loglikelihood requests:  48%|████▊     | 191/400 [05:10<04:45,  1.37s/it]Running loglikelihood requests:  48%|████▊     | 192/400 [05:11<04:43,  1.37s/it]Running loglikelihood requests:  48%|████▊     | 193/400 [05:12<04:41,  1.36s/it]Running loglikelihood requests:  48%|████▊     | 194/400 [05:14<04:39,  1.36s/it]Running loglikelihood requests:  49%|████▉     | 195/400 [05:15<04:37,  1.36s/it]Running loglikelihood requests:  49%|████▉     | 196/400 [05:17<04:35,  1.35s/it]Running loglikelihood requests:  49%|████▉     | 197/400 [05:18<04:33,  1.35s/it]Running loglikelihood requests:  50%|████▉     | 198/400 [05:19<04:32,  1.35s/it]Running loglikelihood requests:  50%|████▉     | 199/400 [05:21<04:30,  1.35s/it]Running loglikelihood requests:  50%|█████     | 200/400 [05:22<04:29,  1.35s/it]Running loglikelihood requests:  50%|█████     | 201/400 [05:23<04:27,  1.34s/it]Running loglikelihood requests:  50%|█████     | 202/400 [05:25<04:25,  1.34s/it]Running loglikelihood requests:  51%|█████     | 203/400 [05:26<04:23,  1.34s/it]Running loglikelihood requests:  51%|█████     | 204/400 [05:27<04:21,  1.34s/it]Running loglikelihood requests:  51%|█████▏    | 205/400 [05:29<04:20,  1.34s/it]Running loglikelihood requests:  52%|█████▏    | 206/400 [05:30<04:18,  1.33s/it]Running loglikelihood requests:  52%|█████▏    | 207/400 [05:31<04:17,  1.33s/it]Running loglikelihood requests:  52%|█████▏    | 208/400 [05:33<04:15,  1.33s/it]Running loglikelihood requests:  52%|█████▏    | 209/400 [05:34<04:13,  1.33s/it]Running loglikelihood requests:  52%|█████▎    | 210/400 [05:35<04:12,  1.33s/it]Running loglikelihood requests:  53%|█████▎    | 211/400 [05:37<04:11,  1.33s/it]Running loglikelihood requests:  53%|█████▎    | 212/400 [05:38<04:10,  1.33s/it]Running loglikelihood requests:  53%|█████▎    | 213/400 [05:39<04:08,  1.33s/it]Running loglikelihood requests:  54%|█████▎    | 214/400 [05:41<04:07,  1.33s/it]Running loglikelihood requests:  54%|█████▍    | 215/400 [05:42<04:05,  1.33s/it]Running loglikelihood requests:  54%|█████▍    | 216/400 [05:43<04:04,  1.33s/it]Running loglikelihood requests:  54%|█████▍    | 217/400 [05:45<04:02,  1.33s/it]Running loglikelihood requests:  55%|█████▍    | 218/400 [05:46<04:01,  1.32s/it]Running loglikelihood requests:  55%|█████▍    | 219/400 [05:47<03:59,  1.32s/it]Running loglikelihood requests:  55%|█████▌    | 220/400 [05:48<03:57,  1.32s/it]Running loglikelihood requests:  55%|█████▌    | 221/400 [05:50<03:56,  1.32s/it]Running loglikelihood requests:  56%|█████▌    | 222/400 [05:51<03:54,  1.32s/it]Running loglikelihood requests:  56%|█████▌    | 223/400 [05:52<03:53,  1.32s/it]Running loglikelihood requests:  56%|█████▌    | 224/400 [05:54<03:52,  1.32s/it]Running loglikelihood requests:  56%|█████▋    | 225/400 [05:55<03:50,  1.32s/it]Running loglikelihood requests:  56%|█████▋    | 226/400 [05:56<03:48,  1.31s/it]Running loglikelihood requests:  57%|█████▋    | 227/400 [05:58<03:47,  1.31s/it]Running loglikelihood requests:  57%|█████▋    | 228/400 [05:59<03:45,  1.31s/it]Running loglikelihood requests:  57%|█████▋    | 229/400 [06:00<03:43,  1.31s/it]Running loglikelihood requests:  57%|█████▊    | 230/400 [06:02<03:42,  1.31s/it]Running loglikelihood requests:  58%|█████▊    | 231/400 [06:03<03:40,  1.31s/it]Running loglikelihood requests:  58%|█████▊    | 232/400 [06:04<03:39,  1.31s/it]Running loglikelihood requests:  58%|█████▊    | 233/400 [06:06<03:38,  1.31s/it]Running loglikelihood requests:  58%|█████▊    | 234/400 [06:07<03:36,  1.31s/it]Running loglikelihood requests:  59%|█████▉    | 235/400 [06:08<03:35,  1.30s/it]Running loglikelihood requests:  59%|█████▉    | 236/400 [06:09<03:33,  1.30s/it]Running loglikelihood requests:  59%|█████▉    | 237/400 [06:11<03:31,  1.30s/it]Running loglikelihood requests:  60%|█████▉    | 238/400 [06:12<03:30,  1.30s/it]Running loglikelihood requests:  60%|█████▉    | 239/400 [06:13<03:28,  1.30s/it]Running loglikelihood requests:  60%|██████    | 240/400 [06:15<03:27,  1.29s/it]Running loglikelihood requests:  60%|██████    | 241/400 [06:16<03:25,  1.29s/it]Running loglikelihood requests:  60%|██████    | 242/400 [06:17<03:24,  1.29s/it]Running loglikelihood requests:  61%|██████    | 243/400 [06:18<03:23,  1.29s/it]Running loglikelihood requests:  61%|██████    | 244/400 [06:20<03:21,  1.29s/it]Running loglikelihood requests:  61%|██████▏   | 245/400 [06:21<03:20,  1.29s/it]Running loglikelihood requests:  62%|██████▏   | 246/400 [06:22<03:19,  1.29s/it]Running loglikelihood requests:  62%|██████▏   | 247/400 [06:24<03:17,  1.29s/it]Running loglikelihood requests:  62%|██████▏   | 248/400 [06:25<03:16,  1.29s/it]Running loglikelihood requests:  62%|██████▏   | 249/400 [06:26<03:15,  1.29s/it]Running loglikelihood requests:  62%|██████▎   | 250/400 [06:28<03:13,  1.29s/it]Running loglikelihood requests:  63%|██████▎   | 251/400 [06:29<03:11,  1.29s/it]Running loglikelihood requests:  63%|██████▎   | 252/400 [06:30<03:10,  1.29s/it]Running loglikelihood requests:  63%|██████▎   | 253/400 [06:31<03:09,  1.29s/it]Running loglikelihood requests:  64%|██████▎   | 254/400 [06:33<03:07,  1.28s/it]Running loglikelihood requests:  64%|██████▍   | 255/400 [06:34<03:05,  1.28s/it]Running loglikelihood requests:  64%|██████▍   | 256/400 [06:35<03:04,  1.28s/it]Running loglikelihood requests:  64%|██████▍   | 257/400 [06:36<03:02,  1.28s/it]Running loglikelihood requests:  64%|██████▍   | 258/400 [06:38<03:01,  1.28s/it]Running loglikelihood requests:  65%|██████▍   | 259/400 [06:39<03:00,  1.28s/it]Running loglikelihood requests:  65%|██████▌   | 260/400 [06:40<02:58,  1.28s/it]Running loglikelihood requests:  65%|██████▌   | 261/400 [06:42<02:57,  1.28s/it]Running loglikelihood requests:  66%|██████▌   | 262/400 [06:43<02:56,  1.28s/it]Running loglikelihood requests:  66%|██████▌   | 263/400 [06:44<02:53,  1.27s/it]Running loglikelihood requests:  66%|██████▌   | 264/400 [06:45<02:52,  1.26s/it]Running loglikelihood requests:  66%|██████▋   | 265/400 [06:47<02:50,  1.26s/it]Running loglikelihood requests:  66%|██████▋   | 266/400 [06:48<02:48,  1.26s/it]Running loglikelihood requests:  67%|██████▋   | 267/400 [06:49<02:47,  1.26s/it]Running loglikelihood requests:  67%|██████▋   | 268/400 [06:50<02:45,  1.26s/it]Running loglikelihood requests:  67%|██████▋   | 269/400 [06:52<02:44,  1.25s/it]Running loglikelihood requests:  68%|██████▊   | 270/400 [06:53<02:43,  1.25s/it]Running loglikelihood requests:  68%|██████▊   | 271/400 [06:54<02:41,  1.25s/it]Running loglikelihood requests:  68%|██████▊   | 272/400 [06:55<02:40,  1.25s/it]Running loglikelihood requests:  68%|██████▊   | 273/400 [06:57<02:38,  1.25s/it]Running loglikelihood requests:  68%|██████▊   | 274/400 [06:58<02:37,  1.25s/it]Running loglikelihood requests:  69%|██████▉   | 275/400 [06:59<02:35,  1.25s/it]Running loglikelihood requests:  69%|██████▉   | 276/400 [07:00<02:34,  1.25s/it]Running loglikelihood requests:  69%|██████▉   | 277/400 [07:02<02:32,  1.24s/it]Running loglikelihood requests:  70%|██████▉   | 278/400 [07:03<02:31,  1.24s/it]Running loglikelihood requests:  70%|███████   | 280/400 [07:04<01:54,  1.05it/s]Running loglikelihood requests:  70%|███████   | 281/400 [07:05<02:01,  1.02s/it]Running loglikelihood requests:  70%|███████   | 282/400 [07:07<02:06,  1.08s/it]Running loglikelihood requests:  71%|███████   | 283/400 [07:08<02:10,  1.12s/it]Running loglikelihood requests:  71%|███████   | 284/400 [07:09<02:12,  1.15s/it]Running loglikelihood requests:  71%|███████▏  | 285/400 [07:10<02:14,  1.17s/it]Running loglikelihood requests:  72%|███████▏  | 286/400 [07:11<02:17,  1.20s/it]Running loglikelihood requests:  72%|███████▏  | 287/400 [07:13<02:16,  1.21s/it]Running loglikelihood requests:  72%|███████▏  | 288/400 [07:14<02:15,  1.21s/it]Running loglikelihood requests:  72%|███████▏  | 289/400 [07:15<02:14,  1.21s/it]Running loglikelihood requests:  72%|███████▎  | 290/400 [07:16<02:13,  1.21s/it]Running loglikelihood requests:  73%|███████▎  | 291/400 [07:18<02:11,  1.21s/it]Running loglikelihood requests:  73%|███████▎  | 292/400 [07:19<02:10,  1.21s/it]Running loglikelihood requests:  73%|███████▎  | 293/400 [07:20<02:08,  1.20s/it]Running loglikelihood requests:  74%|███████▎  | 294/400 [07:21<02:07,  1.20s/it]Running loglikelihood requests:  74%|███████▍  | 295/400 [07:22<02:05,  1.20s/it]Running loglikelihood requests:  74%|███████▍  | 296/400 [07:23<02:04,  1.19s/it]Running loglikelihood requests:  74%|███████▍  | 297/400 [07:25<02:02,  1.19s/it]Running loglikelihood requests:  74%|███████▍  | 298/400 [07:26<02:01,  1.19s/it]Running loglikelihood requests:  75%|███████▍  | 299/400 [07:27<01:59,  1.18s/it]Running loglikelihood requests:  75%|███████▌  | 300/400 [07:28<01:58,  1.18s/it]Running loglikelihood requests:  75%|███████▌  | 301/400 [07:29<01:56,  1.18s/it]Running loglikelihood requests:  76%|███████▌  | 302/400 [07:31<01:55,  1.18s/it]Running loglikelihood requests:  76%|███████▌  | 303/400 [07:32<01:54,  1.18s/it]Running loglikelihood requests:  76%|███████▌  | 304/400 [07:33<01:52,  1.18s/it]Running loglikelihood requests:  76%|███████▋  | 305/400 [07:34<01:51,  1.17s/it]Running loglikelihood requests:  76%|███████▋  | 306/400 [07:35<01:51,  1.19s/it]Running loglikelihood requests:  77%|███████▋  | 307/400 [07:37<01:52,  1.21s/it]Running loglikelihood requests:  77%|███████▋  | 308/400 [07:38<01:53,  1.23s/it]Running loglikelihood requests:  77%|███████▋  | 309/400 [07:39<01:53,  1.24s/it]Running loglikelihood requests:  78%|███████▊  | 310/400 [07:40<01:51,  1.24s/it]Running loglikelihood requests:  78%|███████▊  | 311/400 [07:42<01:48,  1.22s/it]Running loglikelihood requests:  78%|███████▊  | 312/400 [07:43<01:45,  1.20s/it]Running loglikelihood requests:  78%|███████▊  | 313/400 [07:44<01:43,  1.19s/it]Running loglikelihood requests:  78%|███████▊  | 314/400 [07:45<01:41,  1.18s/it]Running loglikelihood requests:  79%|███████▉  | 315/400 [07:46<01:40,  1.18s/it]Running loglikelihood requests:  79%|███████▉  | 316/400 [07:47<01:38,  1.17s/it]Running loglikelihood requests:  79%|███████▉  | 317/400 [07:48<01:36,  1.17s/it]Running loglikelihood requests:  80%|███████▉  | 318/400 [07:50<01:35,  1.16s/it]Running loglikelihood requests:  80%|███████▉  | 319/400 [07:51<01:33,  1.16s/it]Running loglikelihood requests:  80%|████████  | 320/400 [07:52<01:32,  1.16s/it]Running loglikelihood requests:  80%|████████  | 321/400 [07:53<01:31,  1.15s/it]Running loglikelihood requests:  80%|████████  | 322/400 [07:54<01:29,  1.15s/it]Running loglikelihood requests:  81%|████████  | 323/400 [07:55<01:28,  1.15s/it]Running loglikelihood requests:  81%|████████  | 324/400 [07:57<01:27,  1.15s/it]Running loglikelihood requests:  81%|████████▏ | 325/400 [07:58<01:25,  1.14s/it]Running loglikelihood requests:  82%|████████▏ | 326/400 [07:59<01:24,  1.14s/it]Running loglikelihood requests:  82%|████████▏ | 327/400 [08:00<01:23,  1.14s/it]Running loglikelihood requests:  82%|████████▏ | 328/400 [08:01<01:22,  1.14s/it]Running loglikelihood requests:  82%|████████▏ | 329/400 [08:02<01:20,  1.14s/it]Running loglikelihood requests:  82%|████████▎ | 330/400 [08:03<01:19,  1.14s/it]Running loglikelihood requests:  83%|████████▎ | 331/400 [08:04<01:18,  1.14s/it]Running loglikelihood requests:  83%|████████▎ | 332/400 [08:06<01:17,  1.13s/it]Running loglikelihood requests:  83%|████████▎ | 333/400 [08:07<01:15,  1.13s/it]Running loglikelihood requests:  84%|████████▎ | 334/400 [08:08<01:14,  1.13s/it]Running loglikelihood requests:  84%|████████▍ | 335/400 [08:09<01:13,  1.13s/it]Running loglikelihood requests:  84%|████████▍ | 336/400 [08:10<01:11,  1.12s/it]Running loglikelihood requests:  84%|████████▍ | 337/400 [08:11<01:10,  1.12s/it]Running loglikelihood requests:  84%|████████▍ | 338/400 [08:12<01:09,  1.12s/it]Running loglikelihood requests:  85%|████████▍ | 339/400 [08:14<01:10,  1.15s/it]Running loglikelihood requests:  85%|████████▌ | 340/400 [08:15<01:10,  1.18s/it]Running loglikelihood requests:  85%|████████▌ | 341/400 [08:16<01:10,  1.20s/it]Running loglikelihood requests:  86%|████████▌ | 342/400 [08:17<01:09,  1.21s/it]Running loglikelihood requests:  86%|████████▌ | 343/400 [08:18<01:09,  1.21s/it]Running loglikelihood requests:  86%|████████▌ | 344/400 [08:20<01:06,  1.18s/it]Running loglikelihood requests:  86%|████████▋ | 345/400 [08:21<01:05,  1.19s/it]Running loglikelihood requests:  86%|████████▋ | 346/400 [08:22<01:04,  1.19s/it]Running loglikelihood requests:  87%|████████▋ | 347/400 [08:23<01:03,  1.19s/it]Running loglikelihood requests:  87%|████████▋ | 348/400 [08:24<01:01,  1.18s/it]Running loglikelihood requests:  87%|████████▋ | 349/400 [08:26<00:59,  1.18s/it]Running loglikelihood requests:  88%|████████▊ | 350/400 [08:27<00:59,  1.19s/it]Running loglikelihood requests:  88%|████████▊ | 351/400 [08:28<00:58,  1.19s/it]Running loglikelihood requests:  88%|████████▊ | 352/400 [08:29<00:56,  1.19s/it]Running loglikelihood requests:  88%|████████▊ | 353/400 [08:30<00:55,  1.18s/it]Running loglikelihood requests:  88%|████████▊ | 354/400 [08:31<00:54,  1.18s/it]Running loglikelihood requests:  89%|████████▉ | 355/400 [08:33<00:52,  1.18s/it]Running loglikelihood requests:  89%|████████▉ | 356/400 [08:34<00:51,  1.17s/it]Running loglikelihood requests:  89%|████████▉ | 357/400 [08:35<00:49,  1.15s/it]Running loglikelihood requests:  90%|████████▉ | 358/400 [08:36<00:47,  1.13s/it]Running loglikelihood requests:  90%|████████▉ | 359/400 [08:37<00:45,  1.12s/it]Running loglikelihood requests:  90%|█████████ | 360/400 [08:38<00:44,  1.10s/it]Running loglikelihood requests:  90%|█████████ | 361/400 [08:39<00:42,  1.09s/it]Running loglikelihood requests:  90%|█████████ | 362/400 [08:40<00:41,  1.09s/it]Running loglikelihood requests:  91%|█████████ | 363/400 [08:41<00:39,  1.08s/it]Running loglikelihood requests:  91%|█████████ | 364/400 [08:42<00:38,  1.07s/it]Running loglikelihood requests:  91%|█████████▏| 365/400 [08:43<00:37,  1.07s/it]Running loglikelihood requests:  92%|█████████▏| 366/400 [08:44<00:36,  1.07s/it]Running loglikelihood requests:  92%|█████████▏| 367/400 [08:46<00:35,  1.07s/it]Running loglikelihood requests:  92%|█████████▏| 368/400 [08:47<00:34,  1.06s/it]Running loglikelihood requests:  92%|█████████▏| 369/400 [08:48<00:32,  1.06s/it]Running loglikelihood requests:  92%|█████████▎| 370/400 [08:49<00:31,  1.06s/it]Running loglikelihood requests:  93%|█████████▎| 371/400 [08:50<00:30,  1.05s/it]Running loglikelihood requests:  93%|█████████▎| 372/400 [08:51<00:29,  1.05s/it]Running loglikelihood requests:  93%|█████████▎| 373/400 [08:52<00:28,  1.05s/it]Running loglikelihood requests:  94%|█████████▎| 374/400 [08:53<00:27,  1.07s/it]Running loglikelihood requests:  94%|█████████▍| 375/400 [08:54<00:26,  1.05s/it]Running loglikelihood requests:  94%|█████████▍| 376/400 [08:55<00:24,  1.04s/it]Running loglikelihood requests:  94%|█████████▍| 377/400 [08:56<00:23,  1.03s/it]Running loglikelihood requests:  94%|█████████▍| 378/400 [08:57<00:22,  1.02s/it]Running loglikelihood requests:  95%|█████████▍| 379/400 [08:58<00:21,  1.01s/it]Running loglikelihood requests:  95%|█████████▌| 380/400 [08:59<00:19,  1.01it/s]Running loglikelihood requests:  95%|█████████▌| 381/400 [09:00<00:18,  1.02it/s]Running loglikelihood requests:  96%|█████████▌| 382/400 [09:01<00:17,  1.03it/s]Running loglikelihood requests:  96%|█████████▌| 383/400 [09:02<00:16,  1.03it/s]Running loglikelihood requests:  96%|█████████▌| 384/400 [09:03<00:15,  1.04it/s]Running loglikelihood requests:  96%|█████████▋| 385/400 [09:04<00:14,  1.04it/s]Running loglikelihood requests:  96%|█████████▋| 386/400 [09:05<00:13,  1.05it/s]Running loglikelihood requests:  97%|█████████▋| 387/400 [09:06<00:12,  1.05it/s]Running loglikelihood requests:  97%|█████████▋| 389/400 [09:06<00:07,  1.39it/s]Running loglikelihood requests:  98%|█████████▊| 390/400 [09:07<00:07,  1.31it/s]Running loglikelihood requests:  98%|█████████▊| 391/400 [09:08<00:07,  1.25it/s]Running loglikelihood requests:  98%|█████████▊| 393/400 [09:09<00:04,  1.57it/s]Running loglikelihood requests:  98%|█████████▊| 394/400 [09:10<00:04,  1.46it/s]Running loglikelihood requests:  99%|█████████▉| 395/400 [09:11<00:03,  1.37it/s]Running loglikelihood requests:  99%|█████████▉| 396/400 [09:12<00:03,  1.32it/s]Running loglikelihood requests:  99%|█████████▉| 397/400 [09:13<00:02,  1.28it/s]Running loglikelihood requests: 100%|█████████▉| 398/400 [09:13<00:01,  1.26it/s]Running loglikelihood requests: 100%|█████████▉| 399/400 [09:14<00:00,  1.24it/s]Running loglikelihood requests: 100%|██████████| 400/400 [09:15<00:00,  1.23it/s]Running loglikelihood requests: 100%|██████████| 400/400 [09:15<00:00,  1.39s/it]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:2'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:2'}
full model:
{'logiqa': {'alias': 'logiqa', 'acc,none': 0.29, 'acc_stderr,none': 0.045604802157206865, 'acc_norm,none': 0.33, 'acc_norm_stderr,none': 0.04725815626252609}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9179964803140478
0.7817057225882229
0.8413553272072316
0.9274797474668193
0.8768807463293081
0.9494139907523571
0.8960692461846443
0.9131107283061946
0.6329173647892901
0.8375042173336539
0.8817471801904351
0.8172295355829869
0.7824572665005357
0.9227400642857845
0.9246594853497696
0.8075911590072223
0.6900210787422486
0.599615993999193
0.9308030044211123
0.9504015361511146
0.8866807231108503
0.540104242930401
0.6701728801805507
0.9744992661822648
0.8193037468812308
0.840784693447352
0.9052511591891966
Total groups 70 exceeded the threshold, stopping comparison.
The group tensor is
[3, 6, 7, 1, 5, 2, 4, 0]
tensor([3, 6, 7, 1, 5, 2, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 2, 6, 3, 7, 1, 5, 0]
tensor([4, 2, 6, 3, 7, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 6, 1, 7, 2, 4, 0]
tensor([5, 3, 6, 1, 7, 2, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 1, 7, 2, 3, 0]
tensor([5, 4, 6, 1, 7, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 3, 0, 2, 1, 0, 4, 1]
tensor([5, 3, 0, 2, 1, 0, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 0, 5, 0, 1, 2, 3, 1]
tensor([4, 0, 5, 0, 1, 2, 3, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 4 to 8
done!
Normal merging for layer 9
tensor([2, 5])
tensor(2)
tensor([4, 7])
tensor(4)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 10 to 13
done!
Normal merging for layer 14
tensor([1, 3])
tensor(1)
tensor([4, 7])
tensor(4)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 15 to 31
done!
all done!
Model size: 11.9458 GB
7
cuda:2
mastermind_24_easy
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:43<00:43, 43.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:57<00:00, 25.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:57<00:00, 28.57s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_24_mcq_random HTTP/1.1" 200 772
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/flair/mastermind_24_mcq_random/flair/mastermind_24_mcq_random.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_24_mcq_random HTTP/1.1" 200 779
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/flair/mastermind_24_mcq_random/resolve/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_24_mcq_random/revision/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 779
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_24_mcq_random/tree/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5?recursive=False&expand=False HTTP/1.1" 200 290
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_24_mcq_random/tree/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5/data?recursive=False&expand=False HTTP/1.1" 200 358
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_24_mcq_random/revision/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 779
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/flair/mastermind_24_mcq_random/resolve/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:filelock:Attempting to acquire lock 140239887529072 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_24_mcq_random_default_0.0.0_cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5.lock
DEBUG:filelock:Lock 140239887529072 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_24_mcq_random_default_0.0.0_cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5/dataset_info.json
DEBUG:filelock:Attempting to release lock 140239887529072 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_24_mcq_random_default_0.0.0_cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5.lock
DEBUG:filelock:Lock 140239887529072 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_24_mcq_random_default_0.0.0_cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5.lock
DEBUG:filelock:Attempting to acquire lock 140239887529072 on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5_builder.lock
DEBUG:filelock:Lock 140239887529072 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5/dataset_info.json
DEBUG:filelock:Attempting to release lock 140239887529072 on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5_builder.lock
DEBUG:filelock:Lock 140239887529072 released on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of mastermind_24_easy from None to 0
INFO:lm_eval.api.task:Building contexts for mastermind_24_easy on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1496.70it/s]
DEBUG:lm_eval.evaluator:Task: mastermind_24_easy; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:01<11:57,  1.80s/it]Running loglikelihood requests:   0%|          | 2/400 [00:02<08:59,  1.36s/it]Running loglikelihood requests:   1%|          | 3/400 [00:03<08:00,  1.21s/it]Running loglikelihood requests:   1%|          | 4/400 [00:04<07:31,  1.14s/it]Running loglikelihood requests:   1%|▏         | 5/400 [00:05<07:14,  1.10s/it]Running loglikelihood requests:   2%|▏         | 6/400 [00:06<07:04,  1.08s/it]Running loglikelihood requests:   2%|▏         | 7/400 [00:08<06:56,  1.06s/it]Running loglikelihood requests:   2%|▏         | 8/400 [00:09<06:51,  1.05s/it]Running loglikelihood requests:   2%|▏         | 9/400 [00:10<06:47,  1.04s/it]Running loglikelihood requests:   2%|▎         | 10/400 [00:11<06:45,  1.04s/it]Running loglikelihood requests:   3%|▎         | 12/400 [00:12<05:07,  1.26it/s]Running loglikelihood requests:   3%|▎         | 13/400 [00:13<05:29,  1.18it/s]Running loglikelihood requests:   4%|▎         | 14/400 [00:14<05:45,  1.12it/s]Running loglikelihood requests:   4%|▍         | 16/400 [00:15<04:40,  1.37it/s]Running loglikelihood requests:   4%|▍         | 17/400 [00:16<05:06,  1.25it/s]Running loglikelihood requests:   4%|▍         | 18/400 [00:17<05:27,  1.17it/s]Running loglikelihood requests:   5%|▍         | 19/400 [00:18<05:43,  1.11it/s]Running loglikelihood requests:   5%|▌         | 20/400 [00:19<05:55,  1.07it/s]Running loglikelihood requests:   5%|▌         | 21/400 [00:20<06:04,  1.04it/s]Running loglikelihood requests:   6%|▌         | 22/400 [00:21<06:10,  1.02it/s]Running loglikelihood requests:   6%|▌         | 23/400 [00:22<06:14,  1.01it/s]Running loglikelihood requests:   6%|▋         | 25/400 [00:23<04:50,  1.29it/s]Running loglikelihood requests:   6%|▋         | 26/400 [00:24<05:12,  1.20it/s]Running loglikelihood requests:   7%|▋         | 27/400 [00:25<05:29,  1.13it/s]Running loglikelihood requests:   7%|▋         | 29/400 [00:26<04:28,  1.38it/s]Running loglikelihood requests:   8%|▊         | 30/400 [00:27<04:53,  1.26it/s]Running loglikelihood requests:   8%|▊         | 31/400 [00:28<05:12,  1.18it/s]Running loglikelihood requests:   8%|▊         | 32/400 [00:29<05:28,  1.12it/s]Running loglikelihood requests:   8%|▊         | 34/400 [00:30<04:26,  1.38it/s]Running loglikelihood requests:   9%|▉         | 35/400 [00:31<04:49,  1.26it/s]Running loglikelihood requests:  10%|▉         | 38/400 [00:32<03:25,  1.76it/s]Running loglikelihood requests:  10%|▉         | 39/400 [00:33<03:54,  1.54it/s]Running loglikelihood requests:  10%|█         | 40/400 [00:34<04:20,  1.38it/s]Running loglikelihood requests:  10%|█         | 41/400 [00:35<04:42,  1.27it/s]Running loglikelihood requests:  10%|█         | 42/400 [00:36<04:59,  1.19it/s]Running loglikelihood requests:  11%|█         | 43/400 [00:37<05:13,  1.14it/s]Running loglikelihood requests:  11%|█         | 44/400 [00:38<05:23,  1.10it/s]Running loglikelihood requests:  11%|█▏        | 45/400 [00:39<05:30,  1.07it/s]Running loglikelihood requests:  12%|█▏        | 47/400 [00:40<04:19,  1.36it/s]Running loglikelihood requests:  12%|█▏        | 48/400 [00:41<04:39,  1.26it/s]Running loglikelihood requests:  12%|█▏        | 49/400 [00:42<04:56,  1.18it/s]Running loglikelihood requests:  12%|█▎        | 50/400 [00:43<05:08,  1.13it/s]Running loglikelihood requests:  13%|█▎        | 52/400 [00:44<04:08,  1.40it/s]Running loglikelihood requests:  13%|█▎        | 53/400 [00:45<04:29,  1.29it/s]Running loglikelihood requests:  14%|█▎        | 54/400 [00:46<04:46,  1.21it/s]Running loglikelihood requests:  14%|█▍        | 56/400 [00:47<03:56,  1.45it/s]Running loglikelihood requests:  14%|█▍        | 57/400 [00:48<04:19,  1.32it/s]Running loglikelihood requests:  14%|█▍        | 58/400 [00:49<04:37,  1.23it/s]Running loglikelihood requests:  15%|█▍        | 59/400 [00:50<04:51,  1.17it/s]Running loglikelihood requests:  15%|█▌        | 60/400 [00:51<05:01,  1.13it/s]Running loglikelihood requests:  15%|█▌        | 61/400 [00:52<05:12,  1.08it/s]Running loglikelihood requests:  16%|█▌        | 62/400 [00:53<05:18,  1.06it/s]Running loglikelihood requests:  16%|█▌        | 63/400 [00:54<05:25,  1.03it/s]Running loglikelihood requests:  16%|█▌        | 64/400 [00:55<05:34,  1.00it/s]Running loglikelihood requests:  16%|█▋        | 65/400 [00:56<05:37,  1.01s/it]Running loglikelihood requests:  16%|█▋        | 66/400 [00:57<05:39,  1.02s/it]Running loglikelihood requests:  17%|█▋        | 67/400 [00:58<05:45,  1.04s/it]Running loglikelihood requests:  17%|█▋        | 68/400 [00:59<05:49,  1.05s/it]Running loglikelihood requests:  17%|█▋        | 69/400 [01:00<05:48,  1.05s/it]Running loglikelihood requests:  18%|█▊        | 70/400 [01:01<05:44,  1.04s/it]Running loglikelihood requests:  18%|█▊        | 71/400 [01:02<05:40,  1.04s/it]Running loglikelihood requests:  18%|█▊        | 72/400 [01:03<05:37,  1.03s/it]Running loglikelihood requests:  18%|█▊        | 73/400 [01:04<05:35,  1.03s/it]Running loglikelihood requests:  18%|█▊        | 74/400 [01:05<05:33,  1.02s/it]Running loglikelihood requests:  19%|█▉        | 76/400 [01:06<04:14,  1.27it/s]Running loglikelihood requests:  19%|█▉        | 77/400 [01:07<04:31,  1.19it/s]Running loglikelihood requests:  20%|█▉        | 78/400 [01:08<04:45,  1.13it/s]Running loglikelihood requests:  20%|█▉        | 79/400 [01:09<04:55,  1.09it/s]Running loglikelihood requests:  20%|██        | 80/400 [01:10<05:01,  1.06it/s]Running loglikelihood requests:  20%|██        | 81/400 [01:11<05:02,  1.05it/s]Running loglikelihood requests:  20%|██        | 82/400 [01:12<05:03,  1.05it/s]Running loglikelihood requests:  21%|██        | 83/400 [01:13<05:03,  1.04it/s]Running loglikelihood requests:  21%|██        | 84/400 [01:14<05:03,  1.04it/s]Running loglikelihood requests:  21%|██▏       | 85/400 [01:15<05:03,  1.04it/s]Running loglikelihood requests:  22%|██▏       | 87/400 [01:16<03:52,  1.35it/s]Running loglikelihood requests:  22%|██▏       | 88/400 [01:17<04:08,  1.25it/s]Running loglikelihood requests:  22%|██▏       | 89/400 [01:18<04:21,  1.19it/s]Running loglikelihood requests:  22%|██▎       | 90/400 [01:19<04:30,  1.15it/s]Running loglikelihood requests:  23%|██▎       | 91/400 [01:20<04:37,  1.11it/s]Running loglikelihood requests:  23%|██▎       | 92/400 [01:21<04:42,  1.09it/s]Running loglikelihood requests:  24%|██▎       | 94/400 [01:22<03:43,  1.37it/s]Running loglikelihood requests:  24%|██▍       | 95/400 [01:23<04:05,  1.24it/s]Running loglikelihood requests:  24%|██▍       | 96/400 [01:24<04:17,  1.18it/s]Running loglikelihood requests:  24%|██▍       | 98/400 [01:25<03:29,  1.44it/s]Running loglikelihood requests:  25%|██▌       | 100/400 [01:26<03:04,  1.63it/s]Running loglikelihood requests:  26%|██▌       | 102/400 [01:27<02:49,  1.76it/s]Running loglikelihood requests:  26%|██▌       | 103/400 [01:28<03:12,  1.54it/s]Running loglikelihood requests:  26%|██▌       | 104/400 [01:29<03:32,  1.39it/s]Running loglikelihood requests:  27%|██▋       | 107/400 [01:30<02:33,  1.91it/s]Running loglikelihood requests:  27%|██▋       | 108/400 [01:31<02:57,  1.65it/s]Running loglikelihood requests:  27%|██▋       | 109/400 [01:32<03:18,  1.47it/s]Running loglikelihood requests:  28%|██▊       | 110/400 [01:33<03:36,  1.34it/s]Running loglikelihood requests:  28%|██▊       | 111/400 [01:34<03:51,  1.25it/s]Running loglikelihood requests:  28%|██▊       | 112/400 [01:35<04:02,  1.19it/s]Running loglikelihood requests:  28%|██▊       | 113/400 [01:35<04:11,  1.14it/s]Running loglikelihood requests:  28%|██▊       | 114/400 [01:36<04:17,  1.11it/s]Running loglikelihood requests:  29%|██▉       | 116/400 [01:37<03:22,  1.40it/s]Running loglikelihood requests:  29%|██▉       | 117/400 [01:38<03:38,  1.30it/s]Running loglikelihood requests:  30%|██▉       | 118/400 [01:39<03:50,  1.22it/s]Running loglikelihood requests:  30%|███       | 121/400 [01:40<02:35,  1.80it/s]Running loglikelihood requests:  30%|███       | 122/400 [01:41<02:56,  1.57it/s]Running loglikelihood requests:  31%|███       | 123/400 [01:42<03:15,  1.42it/s]Running loglikelihood requests:  31%|███       | 124/400 [01:43<03:31,  1.31it/s]Running loglikelihood requests:  31%|███▏      | 125/400 [01:44<03:42,  1.24it/s]Running loglikelihood requests:  32%|███▏      | 126/400 [01:45<03:50,  1.19it/s]Running loglikelihood requests:  32%|███▏      | 127/400 [01:46<03:56,  1.15it/s]Running loglikelihood requests:  32%|███▏      | 128/400 [01:47<04:00,  1.13it/s]Running loglikelihood requests:  32%|███▎      | 130/400 [01:48<03:08,  1.43it/s]Running loglikelihood requests:  33%|███▎      | 132/400 [01:49<02:42,  1.65it/s]Running loglikelihood requests:  33%|███▎      | 133/400 [01:50<03:00,  1.48it/s]Running loglikelihood requests:  34%|███▎      | 134/400 [01:51<03:15,  1.36it/s]Running loglikelihood requests:  34%|███▍      | 135/400 [01:52<03:27,  1.27it/s]Running loglikelihood requests:  34%|███▍      | 136/400 [01:52<03:37,  1.22it/s]Running loglikelihood requests:  34%|███▍      | 137/400 [01:53<03:43,  1.18it/s]Running loglikelihood requests:  34%|███▍      | 138/400 [01:54<03:48,  1.15it/s]Running loglikelihood requests:  35%|███▍      | 139/400 [01:55<03:51,  1.13it/s]Running loglikelihood requests:  35%|███▌      | 140/400 [01:56<03:53,  1.11it/s]Running loglikelihood requests:  35%|███▌      | 141/400 [01:57<03:54,  1.11it/s]Running loglikelihood requests:  36%|███▌      | 142/400 [01:58<03:54,  1.10it/s]Running loglikelihood requests:  36%|███▌      | 143/400 [01:59<03:54,  1.10it/s]Running loglikelihood requests:  36%|███▋      | 145/400 [02:00<03:00,  1.42it/s]Running loglikelihood requests:  36%|███▋      | 146/400 [02:01<03:12,  1.32it/s]Running loglikelihood requests:  37%|███▋      | 147/400 [02:02<03:22,  1.25it/s]Running loglikelihood requests:  37%|███▋      | 148/400 [02:03<03:30,  1.20it/s]Running loglikelihood requests:  37%|███▋      | 149/400 [02:04<03:35,  1.16it/s]Running loglikelihood requests:  38%|███▊      | 150/400 [02:04<03:39,  1.14it/s]Running loglikelihood requests:  38%|███▊      | 152/400 [02:05<02:50,  1.45it/s]Running loglikelihood requests:  38%|███▊      | 153/400 [02:06<03:04,  1.34it/s]Running loglikelihood requests:  38%|███▊      | 154/400 [02:07<03:14,  1.26it/s]Running loglikelihood requests:  39%|███▉      | 155/400 [02:08<03:22,  1.21it/s]Running loglikelihood requests:  39%|███▉      | 157/400 [02:09<02:41,  1.50it/s]Running loglikelihood requests:  40%|███▉      | 158/400 [02:10<02:55,  1.38it/s]Running loglikelihood requests:  40%|████      | 160/400 [02:11<02:28,  1.62it/s]Running loglikelihood requests:  40%|████      | 161/400 [02:12<02:43,  1.46it/s]Running loglikelihood requests:  40%|████      | 162/400 [02:13<02:56,  1.35it/s]Running loglikelihood requests:  41%|████      | 163/400 [02:14<03:06,  1.27it/s]Running loglikelihood requests:  41%|████▏     | 165/400 [02:15<02:32,  1.54it/s]Running loglikelihood requests:  42%|████▏     | 166/400 [02:15<02:46,  1.41it/s]Running loglikelihood requests:  42%|████▏     | 167/400 [02:16<02:57,  1.31it/s]Running loglikelihood requests:  42%|████▏     | 168/400 [02:17<03:06,  1.25it/s]Running loglikelihood requests:  42%|████▎     | 170/400 [02:18<02:30,  1.53it/s]Running loglikelihood requests:  43%|████▎     | 171/400 [02:19<02:44,  1.40it/s]Running loglikelihood requests:  43%|████▎     | 172/400 [02:20<02:54,  1.31it/s]Running loglikelihood requests:  43%|████▎     | 173/400 [02:21<03:02,  1.24it/s]Running loglikelihood requests:  44%|████▎     | 174/400 [02:22<03:08,  1.20it/s]Running loglikelihood requests:  44%|████▍     | 175/400 [02:23<03:13,  1.16it/s]Running loglikelihood requests:  44%|████▍     | 176/400 [02:24<03:16,  1.14it/s]Running loglikelihood requests:  44%|████▍     | 178/400 [02:25<02:34,  1.44it/s]Running loglikelihood requests:  45%|████▍     | 179/400 [02:26<02:50,  1.29it/s]Running loglikelihood requests:  45%|████▌     | 180/400 [02:27<03:02,  1.21it/s]Running loglikelihood requests:  45%|████▌     | 181/400 [02:28<03:06,  1.17it/s]Running loglikelihood requests:  46%|████▌     | 183/400 [02:28<02:27,  1.47it/s]Running loglikelihood requests:  46%|████▌     | 184/400 [02:29<02:38,  1.36it/s]Running loglikelihood requests:  47%|████▋     | 187/400 [02:30<01:50,  1.93it/s]Running loglikelihood requests:  47%|████▋     | 188/400 [02:31<02:05,  1.68it/s]Running loglikelihood requests:  47%|████▋     | 189/400 [02:32<02:19,  1.51it/s]Running loglikelihood requests:  48%|████▊     | 190/400 [02:33<02:31,  1.39it/s]Running loglikelihood requests:  48%|████▊     | 191/400 [02:34<02:40,  1.30it/s]Running loglikelihood requests:  48%|████▊     | 192/400 [02:35<02:47,  1.24it/s]Running loglikelihood requests:  48%|████▊     | 193/400 [02:36<02:52,  1.20it/s]Running loglikelihood requests:  48%|████▊     | 194/400 [02:37<02:55,  1.17it/s]Running loglikelihood requests:  49%|████▉     | 196/400 [02:38<02:17,  1.48it/s]Running loglikelihood requests:  49%|████▉     | 197/400 [02:38<02:28,  1.37it/s]Running loglikelihood requests:  50%|████▉     | 198/400 [02:39<02:36,  1.29it/s]Running loglikelihood requests:  50%|████▉     | 199/400 [02:40<02:42,  1.23it/s]Running loglikelihood requests:  50%|█████     | 200/400 [02:41<02:47,  1.19it/s]Running loglikelihood requests:  50%|█████     | 202/400 [02:42<02:12,  1.50it/s]Running loglikelihood requests:  51%|█████     | 203/400 [02:43<02:22,  1.38it/s]Running loglikelihood requests:  51%|█████     | 204/400 [02:44<02:31,  1.30it/s]Running loglikelihood requests:  51%|█████▏    | 205/400 [02:45<02:37,  1.24it/s]Running loglikelihood requests:  52%|█████▏    | 206/400 [02:46<02:41,  1.20it/s]Running loglikelihood requests:  52%|█████▏    | 207/400 [02:47<02:45,  1.17it/s]Running loglikelihood requests:  52%|█████▎    | 210/400 [02:48<01:44,  1.81it/s]Running loglikelihood requests:  53%|█████▎    | 212/400 [02:48<01:37,  1.93it/s]Running loglikelihood requests:  53%|█████▎    | 213/400 [02:49<01:51,  1.68it/s]Running loglikelihood requests:  54%|█████▍    | 215/400 [02:50<01:40,  1.84it/s]Running loglikelihood requests:  54%|█████▍    | 216/400 [02:51<01:53,  1.62it/s]Running loglikelihood requests:  54%|█████▍    | 217/400 [02:52<02:04,  1.47it/s]Running loglikelihood requests:  55%|█████▍    | 219/400 [02:53<01:47,  1.69it/s]Running loglikelihood requests:  55%|█████▌    | 220/400 [02:54<01:59,  1.51it/s]Running loglikelihood requests:  55%|█████▌    | 221/400 [02:55<02:08,  1.39it/s]Running loglikelihood requests:  56%|█████▌    | 222/400 [02:56<02:16,  1.31it/s]Running loglikelihood requests:  56%|█████▌    | 223/400 [02:57<02:22,  1.25it/s]Running loglikelihood requests:  56%|█████▌    | 224/400 [02:57<02:26,  1.21it/s]Running loglikelihood requests:  56%|█████▋    | 225/400 [02:58<02:28,  1.18it/s]Running loglikelihood requests:  56%|█████▋    | 226/400 [02:59<02:30,  1.16it/s]Running loglikelihood requests:  57%|█████▋    | 227/400 [03:00<02:31,  1.14it/s]Running loglikelihood requests:  57%|█████▋    | 228/400 [03:01<02:31,  1.13it/s]Running loglikelihood requests:  57%|█████▋    | 229/400 [03:02<02:31,  1.13it/s]Running loglikelihood requests:  57%|█████▊    | 230/400 [03:03<02:31,  1.12it/s]Running loglikelihood requests:  58%|█████▊    | 231/400 [03:04<02:31,  1.12it/s]Running loglikelihood requests:  58%|█████▊    | 232/400 [03:05<02:30,  1.12it/s]Running loglikelihood requests:  58%|█████▊    | 233/400 [03:06<02:29,  1.11it/s]Running loglikelihood requests:  59%|█████▉    | 235/400 [03:06<01:54,  1.45it/s]Running loglikelihood requests:  59%|█████▉    | 236/400 [03:07<02:01,  1.35it/s]Running loglikelihood requests:  59%|█████▉    | 237/400 [03:08<02:07,  1.27it/s]Running loglikelihood requests:  60%|█████▉    | 238/400 [03:09<02:12,  1.23it/s]Running loglikelihood requests:  60%|█████▉    | 239/400 [03:10<02:15,  1.19it/s]Running loglikelihood requests:  60%|██████    | 240/400 [03:11<02:17,  1.17it/s]Running loglikelihood requests:  60%|██████    | 242/400 [03:12<01:46,  1.48it/s]Running loglikelihood requests:  61%|██████    | 243/400 [03:13<01:56,  1.34it/s]Running loglikelihood requests:  61%|██████    | 244/400 [03:14<02:06,  1.23it/s]Running loglikelihood requests:  61%|██████▏   | 245/400 [03:15<02:13,  1.16it/s]Running loglikelihood requests:  62%|██████▏   | 246/400 [03:16<02:19,  1.11it/s]Running loglikelihood requests:  62%|██████▏   | 247/400 [03:17<02:22,  1.07it/s]Running loglikelihood requests:  62%|██████▏   | 249/400 [03:18<01:51,  1.35it/s]Running loglikelihood requests:  62%|██████▎   | 250/400 [03:19<02:00,  1.25it/s]Running loglikelihood requests:  63%|██████▎   | 251/400 [03:20<02:03,  1.21it/s]Running loglikelihood requests:  63%|██████▎   | 252/400 [03:21<02:05,  1.18it/s]Running loglikelihood requests:  63%|██████▎   | 253/400 [03:22<02:11,  1.12it/s]Running loglikelihood requests:  64%|██████▎   | 254/400 [03:23<02:14,  1.09it/s]Running loglikelihood requests:  64%|██████▍   | 255/400 [03:24<02:15,  1.07it/s]Running loglikelihood requests:  64%|██████▍   | 256/400 [03:25<02:15,  1.06it/s]Running loglikelihood requests:  64%|██████▍   | 257/400 [03:26<02:16,  1.05it/s]Running loglikelihood requests:  64%|██████▍   | 258/400 [03:27<02:15,  1.05it/s]Running loglikelihood requests:  65%|██████▌   | 260/400 [03:28<01:43,  1.35it/s]Running loglikelihood requests:  65%|██████▌   | 261/400 [03:29<01:50,  1.26it/s]Running loglikelihood requests:  66%|██████▌   | 264/400 [03:29<01:15,  1.80it/s]Running loglikelihood requests:  66%|██████▋   | 265/400 [03:30<01:25,  1.59it/s]Running loglikelihood requests:  66%|██████▋   | 266/400 [03:31<01:33,  1.43it/s]Running loglikelihood requests:  67%|██████▋   | 267/400 [03:32<01:40,  1.32it/s]Running loglikelihood requests:  67%|██████▋   | 268/400 [03:33<01:45,  1.25it/s]Running loglikelihood requests:  67%|██████▋   | 269/400 [03:34<01:49,  1.19it/s]Running loglikelihood requests:  68%|██████▊   | 270/400 [03:35<01:52,  1.16it/s]Running loglikelihood requests:  68%|██████▊   | 271/400 [03:36<01:54,  1.12it/s]Running loglikelihood requests:  68%|██████▊   | 272/400 [03:37<01:54,  1.12it/s]Running loglikelihood requests:  68%|██████▊   | 273/400 [03:38<01:53,  1.12it/s]Running loglikelihood requests:  68%|██████▊   | 274/400 [03:39<01:53,  1.11it/s]Running loglikelihood requests:  69%|██████▉   | 277/400 [03:40<01:09,  1.77it/s]Running loglikelihood requests:  70%|██████▉   | 279/400 [03:41<01:03,  1.91it/s]Running loglikelihood requests:  70%|███████   | 281/400 [03:41<00:59,  2.00it/s]Running loglikelihood requests:  71%|███████   | 284/400 [03:42<00:48,  2.40it/s]Running loglikelihood requests:  71%|███████▏  | 285/400 [03:43<00:56,  2.02it/s]Running loglikelihood requests:  72%|███████▏  | 286/400 [03:44<01:05,  1.75it/s]Running loglikelihood requests:  72%|███████▏  | 287/400 [03:45<01:12,  1.56it/s]Running loglikelihood requests:  72%|███████▏  | 288/400 [03:46<01:18,  1.43it/s]Running loglikelihood requests:  72%|███████▏  | 289/400 [03:47<01:23,  1.34it/s]Running loglikelihood requests:  72%|███████▎  | 290/400 [03:48<01:26,  1.27it/s]Running loglikelihood requests:  73%|███████▎  | 291/400 [03:49<01:28,  1.23it/s]Running loglikelihood requests:  73%|███████▎  | 293/400 [03:50<01:09,  1.53it/s]Running loglikelihood requests:  74%|███████▎  | 294/400 [03:50<01:15,  1.41it/s]Running loglikelihood requests:  74%|███████▍  | 295/400 [03:51<01:19,  1.32it/s]Running loglikelihood requests:  74%|███████▍  | 296/400 [03:52<01:22,  1.27it/s]Running loglikelihood requests:  74%|███████▍  | 297/400 [03:53<01:24,  1.22it/s]Running loglikelihood requests:  75%|███████▍  | 299/400 [03:54<01:05,  1.53it/s]Running loglikelihood requests:  75%|███████▌  | 300/400 [03:55<01:10,  1.41it/s]Running loglikelihood requests:  75%|███████▌  | 301/400 [03:56<01:14,  1.32it/s]Running loglikelihood requests:  76%|███████▌  | 302/400 [03:57<01:17,  1.26it/s]Running loglikelihood requests:  76%|███████▌  | 304/400 [03:58<01:01,  1.56it/s]Running loglikelihood requests:  76%|███████▋  | 305/400 [03:58<01:06,  1.43it/s]Running loglikelihood requests:  77%|███████▋  | 307/400 [03:59<00:55,  1.68it/s]Running loglikelihood requests:  77%|███████▋  | 309/400 [04:00<00:49,  1.85it/s]Running loglikelihood requests:  78%|███████▊  | 310/400 [04:01<00:54,  1.64it/s]Running loglikelihood requests:  78%|███████▊  | 311/400 [04:02<00:59,  1.49it/s]Running loglikelihood requests:  78%|███████▊  | 312/400 [04:03<01:03,  1.38it/s]Running loglikelihood requests:  78%|███████▊  | 313/400 [04:04<01:06,  1.31it/s]Running loglikelihood requests:  79%|███████▉  | 315/400 [04:05<00:53,  1.60it/s]Running loglikelihood requests:  79%|███████▉  | 317/400 [04:05<00:46,  1.80it/s]Running loglikelihood requests:  80%|███████▉  | 318/400 [04:06<00:51,  1.60it/s]Running loglikelihood requests:  80%|███████▉  | 319/400 [04:07<00:55,  1.46it/s]Running loglikelihood requests:  80%|████████  | 321/400 [04:08<00:46,  1.70it/s]Running loglikelihood requests:  81%|████████  | 323/400 [04:09<00:41,  1.87it/s]Running loglikelihood requests:  82%|████████▏ | 326/400 [04:10<00:31,  2.33it/s]Running loglikelihood requests:  82%|████████▏ | 327/400 [04:11<00:37,  1.97it/s]Running loglikelihood requests:  82%|████████▏ | 328/400 [04:12<00:41,  1.72it/s]Running loglikelihood requests:  82%|████████▏ | 329/400 [04:13<00:46,  1.54it/s]Running loglikelihood requests:  82%|████████▎ | 330/400 [04:13<00:49,  1.42it/s]Running loglikelihood requests:  83%|████████▎ | 332/400 [04:14<00:40,  1.68it/s]Running loglikelihood requests:  83%|████████▎ | 333/400 [04:15<00:44,  1.52it/s]Running loglikelihood requests:  84%|████████▎ | 334/400 [04:16<00:46,  1.40it/s]Running loglikelihood requests:  84%|████████▍ | 335/400 [04:17<00:49,  1.32it/s]Running loglikelihood requests:  84%|████████▍ | 336/400 [04:18<00:50,  1.27it/s]Running loglikelihood requests:  84%|████████▍ | 337/400 [04:19<00:51,  1.23it/s]Running loglikelihood requests:  84%|████████▍ | 338/400 [04:20<00:51,  1.20it/s]Running loglikelihood requests:  85%|████████▍ | 339/400 [04:20<00:51,  1.18it/s]Running loglikelihood requests:  85%|████████▌ | 340/400 [04:21<00:51,  1.17it/s]Running loglikelihood requests:  85%|████████▌ | 341/400 [04:22<00:49,  1.19it/s]Running loglikelihood requests:  86%|████████▌ | 342/400 [04:23<00:48,  1.20it/s]Running loglikelihood requests:  86%|████████▌ | 343/400 [04:24<00:46,  1.21it/s]Running loglikelihood requests:  86%|████████▋ | 346/400 [04:25<00:27,  1.95it/s]Running loglikelihood requests:  87%|████████▋ | 348/400 [04:25<00:24,  2.11it/s]Running loglikelihood requests:  87%|████████▋ | 349/400 [04:26<00:27,  1.85it/s]Running loglikelihood requests:  88%|████████▊ | 350/400 [04:27<00:29,  1.67it/s]Running loglikelihood requests:  88%|████████▊ | 351/400 [04:28<00:31,  1.54it/s]Running loglikelihood requests:  88%|████████▊ | 352/400 [04:29<00:33,  1.45it/s]Running loglikelihood requests:  88%|████████▊ | 353/400 [04:29<00:33,  1.39it/s]Running loglikelihood requests:  88%|████████▊ | 354/400 [04:30<00:34,  1.35it/s]Running loglikelihood requests:  89%|████████▉ | 355/400 [04:31<00:34,  1.32it/s]Running loglikelihood requests:  89%|████████▉ | 356/400 [04:32<00:33,  1.30it/s]Running loglikelihood requests:  90%|████████▉ | 358/400 [04:33<00:25,  1.66it/s]Running loglikelihood requests:  90%|████████▉ | 359/400 [04:33<00:26,  1.54it/s]Running loglikelihood requests:  90%|█████████ | 360/400 [04:34<00:27,  1.45it/s]Running loglikelihood requests:  90%|█████████ | 361/400 [04:35<00:28,  1.39it/s]Running loglikelihood requests:  90%|█████████ | 362/400 [04:36<00:28,  1.35it/s]Running loglikelihood requests:  91%|█████████ | 364/400 [04:37<00:21,  1.69it/s]Running loglikelihood requests:  91%|█████████▏| 365/400 [04:37<00:22,  1.56it/s]Running loglikelihood requests:  92%|█████████▏| 367/400 [04:38<00:17,  1.84it/s]Running loglikelihood requests:  92%|█████████▏| 368/400 [04:39<00:19,  1.66it/s]Running loglikelihood requests:  92%|█████████▏| 369/400 [04:40<00:20,  1.54it/s]Running loglikelihood requests:  92%|█████████▎| 370/400 [04:41<00:20,  1.45it/s]Running loglikelihood requests:  93%|█████████▎| 371/400 [04:41<00:20,  1.39it/s]Running loglikelihood requests:  93%|█████████▎| 373/400 [04:42<00:15,  1.72it/s]Running loglikelihood requests:  94%|█████████▍| 375/400 [04:43<00:12,  1.96it/s]Running loglikelihood requests:  94%|█████████▍| 376/400 [04:44<00:13,  1.74it/s]Running loglikelihood requests:  95%|█████████▍| 379/400 [04:45<00:08,  2.35it/s]Running loglikelihood requests:  95%|█████████▌| 381/400 [04:45<00:07,  2.40it/s]Running loglikelihood requests:  96%|█████████▌| 382/400 [04:46<00:08,  2.05it/s]Running loglikelihood requests:  96%|█████████▌| 384/400 [04:47<00:07,  2.19it/s]Running loglikelihood requests:  96%|█████████▋| 386/400 [04:48<00:06,  2.29it/s]Running loglikelihood requests:  97%|█████████▋| 387/400 [04:49<00:06,  1.98it/s]Running loglikelihood requests:  97%|█████████▋| 389/400 [04:49<00:05,  2.14it/s]Running loglikelihood requests:  98%|█████████▊| 391/400 [04:50<00:03,  2.25it/s]Running loglikelihood requests:  98%|█████████▊| 393/400 [04:51<00:03,  2.33it/s]Running loglikelihood requests:  99%|█████████▉| 395/400 [04:52<00:02,  2.39it/s]Running loglikelihood requests:  99%|█████████▉| 397/400 [04:53<00:01,  2.43it/s]Running loglikelihood requests: 100%|█████████▉| 398/400 [04:53<00:00,  2.08it/s]Running loglikelihood requests: 100%|██████████| 400/400 [04:54<00:00,  2.21it/s]Running loglikelihood requests: 100%|██████████| 400/400 [04:54<00:00,  1.36it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:3'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:3'}
full model:
{'mastermind_24_easy': {'alias': 'mastermind_24_easy', 'acc,none': 0.33, 'acc_stderr,none': 0.04725815626252609}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9705388716727775
0.9880227828623074
0.982119607496995
0.9941469535767887
0.984917378949356
0.9883609317461776
0.9752442385272352
0.9858149677985593
0.9966001900863091
0.9954965780195978
0.9982374916142641
0.987839947084121
0.9740425263242771
0.9802324544963317
0.9965978314947238
0.9890806289155061
0.971808392501808
0.9767123038440666
0.9820534501654181
0.9672299205809463
0.9753579231968917
0.9973837437819523
0.9951994747100236
0.9465102818948206
0.9757933184251313
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[7, 0, 1, 3, 6, 5, 4, 2]
tensor([7, 0, 1, 3, 6, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 0, 1, 3, 7, 5, 4, 2]
tensor([6, 0, 1, 3, 7, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 0, 1, 3, 6, 5, 4, 2]
tensor([7, 0, 1, 3, 6, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 0, 1, 3, 7, 5, 4, 2]
tensor([6, 0, 1, 3, 7, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 0, 1, 3, 6, 5, 4, 2]
tensor([7, 0, 1, 3, 6, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 0, 2, 3, 6, 5, 4, 1]
tensor([7, 0, 2, 3, 6, 5, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 3
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Normal merging for layer 4
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 5
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 6 to 31
done!
all done!
Model size: 12.0718 GB
47
cuda:3
mrpc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 27.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 30.94s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: mrpc] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: mrpc] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
WARNING:lm_eval.api.task:[Task: mrpc] metric f1 is defined, but aggregation is not. using default aggregation=f1
WARNING:lm_eval.api.task:[Task: mrpc] metric f1 is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/revision/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 111
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/revision/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c?recursive=False&expand=False HTTP/1.1" 307 136
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c?recursive=False&expand=False HTTP/1.1" 200 530
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/ax?recursive=False&expand=False HTTP/1.1" 307 139
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/ax?recursive=False&expand=False HTTP/1.1" 200 231
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/revision/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 111
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/revision/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mrpc?recursive=False&expand=False HTTP/1.1" 307 141
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mrpc?recursive=False&expand=False HTTP/1.1" 200 356
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140239885811488 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140239885811488 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140239885811488 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140239885811488 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140237735512304 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140237735512304 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140237735512304 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140237735512304 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of mrpc from None to 0
INFO:lm_eval.api.task:Building contexts for mrpc on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2413.56it/s]
DEBUG:lm_eval.evaluator:Task: mrpc; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<05:36,  1.69s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:40,  1.22it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:03<02:00,  1.62it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:04<01:43,  1.86it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:05<01:34,  2.03it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:06<01:28,  2.15it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:06<01:23,  2.23it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:07<01:21,  2.28it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:08<01:18,  2.33it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:09<01:16,  2.36it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:10<01:15,  2.38it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:11<01:13,  2.40it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:11<01:12,  2.42it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:12<01:11,  2.43it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:13<01:09,  2.45it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:14<01:08,  2.46it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:15<01:07,  2.48it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:15<01:06,  2.49it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:16<01:05,  2.50it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:17<01:04,  2.51it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:18<01:03,  2.51it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:19<01:02,  2.52it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:19<01:01,  2.53it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:20<01:00,  2.54it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:21<00:59,  2.55it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:22<00:58,  2.56it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:22<00:57,  2.57it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:23<00:56,  2.58it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:24<00:55,  2.59it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:25<00:54,  2.60it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:25<00:53,  2.60it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:26<00:52,  2.61it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:27<00:51,  2.62it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:28<00:50,  2.63it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:29<00:49,  2.63it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:29<00:48,  2.65it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:30<00:47,  2.66it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:31<00:46,  2.67it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:31<00:45,  2.67it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:32<00:45,  2.68it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:33<00:44,  2.69it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:34<00:43,  2.69it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:34<00:42,  2.70it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:35<00:41,  2.70it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:36<00:40,  2.72it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:37<00:40,  2.72it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:37<00:39,  2.73it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:38<00:38,  2.74it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:39<00:37,  2.75it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:40<00:36,  2.77it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:40<00:35,  2.78it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:41<00:34,  2.78it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:42<00:34,  2.79it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:42<00:33,  2.79it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:43<00:32,  2.80it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:44<00:31,  2.80it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:45<00:30,  2.81it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:45<00:30,  2.81it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:46<00:29,  2.81it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:47<00:28,  2.82it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:47<00:28,  2.82it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:48<00:27,  2.83it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:49<00:26,  2.83it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:49<00:25,  2.83it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:50<00:25,  2.84it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:51<00:24,  2.85it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:52<00:23,  2.85it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:52<00:22,  2.86it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:53<00:22,  2.86it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:54<00:21,  2.87it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:54<00:20,  2.88it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:55<00:19,  2.89it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [00:56<00:18,  2.90it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [00:56<00:18,  2.92it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [00:57<00:17,  2.92it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [00:58<00:16,  2.93it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [00:58<00:16,  2.93it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [00:59<00:15,  2.93it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:00<00:14,  2.94it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:00<00:13,  2.96it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:01<00:13,  2.96it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:02<00:12,  2.98it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:02<00:11,  2.99it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:03<00:10,  3.01it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:04<00:10,  3.02it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:04<00:09,  3.04it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:05<00:08,  3.05it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:06<00:08,  3.07it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:06<00:07,  3.09it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:07<00:06,  3.11it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:08<00:06,  3.14it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:08<00:05,  3.16it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:09<00:04,  3.18it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:09<00:04,  3.20it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:10<00:03,  3.22it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:11<00:02,  3.24it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:11<00:02,  3.26it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:12<00:01,  3.28it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:12<00:00,  3.31it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:13<00:00,  3.35it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:13<00:00,  2.72it/s]
bootstrapping for stddev (sequential): f1_score
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:01<02:00,  1.22s/it]  2%|▏         | 2/100 [00:02<01:59,  1.22s/it]  3%|▎         | 3/100 [00:03<01:58,  1.22s/it]  4%|▍         | 4/100 [00:04<01:56,  1.22s/it]  5%|▌         | 5/100 [00:06<01:55,  1.22s/it]  6%|▌         | 6/100 [00:07<01:54,  1.22s/it]  7%|▋         | 7/100 [00:08<01:53,  1.22s/it]  8%|▊         | 8/100 [00:09<01:52,  1.22s/it]  9%|▉         | 9/100 [00:10<01:51,  1.23s/it] 10%|█         | 10/100 [00:12<01:50,  1.23s/it] 11%|█         | 11/100 [00:13<01:49,  1.23s/it] 12%|█▏        | 12/100 [00:14<01:47,  1.22s/it] 13%|█▎        | 13/100 [00:15<01:46,  1.22s/it] 14%|█▍        | 14/100 [00:17<01:45,  1.22s/it] 15%|█▌        | 15/100 [00:18<01:43,  1.22s/it] 16%|█▌        | 16/100 [00:19<01:42,  1.22s/it] 17%|█▋        | 17/100 [00:20<01:41,  1.22s/it] 18%|█▊        | 18/100 [00:21<01:40,  1.22s/it] 19%|█▉        | 19/100 [00:23<01:38,  1.22s/it] 20%|██        | 20/100 [00:24<01:37,  1.22s/it] 21%|██        | 21/100 [00:25<01:36,  1.22s/it] 22%|██▏       | 22/100 [00:26<01:35,  1.22s/it] 23%|██▎       | 23/100 [00:28<01:33,  1.22s/it] 24%|██▍       | 24/100 [00:29<01:32,  1.22s/it] 25%|██▌       | 25/100 [00:30<01:31,  1.22s/it] 26%|██▌       | 26/100 [00:31<01:30,  1.22s/it] 27%|██▋       | 27/100 [00:32<01:29,  1.22s/it] 28%|██▊       | 28/100 [00:34<01:27,  1.22s/it] 29%|██▉       | 29/100 [00:35<01:26,  1.22s/it] 30%|███       | 30/100 [00:36<01:25,  1.22s/it] 31%|███       | 31/100 [00:37<01:24,  1.22s/it] 32%|███▏      | 32/100 [00:39<01:22,  1.22s/it] 33%|███▎      | 33/100 [00:40<01:21,  1.22s/it] 34%|███▍      | 34/100 [00:41<01:20,  1.22s/it] 35%|███▌      | 35/100 [00:42<01:19,  1.22s/it] 36%|███▌      | 36/100 [00:43<01:18,  1.22s/it] 37%|███▋      | 37/100 [00:45<01:16,  1.22s/it] 38%|███▊      | 38/100 [00:46<01:15,  1.22s/it] 39%|███▉      | 39/100 [00:47<01:14,  1.22s/it] 40%|████      | 40/100 [00:48<01:13,  1.22s/it] 41%|████      | 41/100 [00:50<01:11,  1.22s/it] 42%|████▏     | 42/100 [00:51<01:10,  1.22s/it] 43%|████▎     | 43/100 [00:52<01:09,  1.22s/it] 44%|████▍     | 44/100 [00:53<01:08,  1.22s/it] 45%|████▌     | 45/100 [00:54<01:07,  1.22s/it] 46%|████▌     | 46/100 [00:56<01:05,  1.22s/it] 47%|████▋     | 47/100 [00:57<01:04,  1.22s/it] 48%|████▊     | 48/100 [00:58<01:03,  1.22s/it] 49%|████▉     | 49/100 [00:59<01:02,  1.22s/it] 50%|█████     | 50/100 [01:01<01:01,  1.22s/it] 51%|█████     | 51/100 [01:02<00:59,  1.22s/it] 52%|█████▏    | 52/100 [01:03<00:58,  1.22s/it] 53%|█████▎    | 53/100 [01:04<00:57,  1.22s/it] 54%|█████▍    | 54/100 [01:05<00:56,  1.22s/it] 55%|█████▌    | 55/100 [01:07<00:54,  1.22s/it] 56%|█████▌    | 56/100 [01:08<00:53,  1.22s/it] 57%|█████▋    | 57/100 [01:09<00:52,  1.22s/it] 58%|█████▊    | 58/100 [01:10<00:51,  1.22s/it] 59%|█████▉    | 59/100 [01:12<00:50,  1.22s/it] 60%|██████    | 60/100 [01:13<00:48,  1.22s/it] 61%|██████    | 61/100 [01:14<00:47,  1.22s/it] 62%|██████▏   | 62/100 [01:15<00:46,  1.22s/it] 63%|██████▎   | 63/100 [01:16<00:45,  1.22s/it] 64%|██████▍   | 64/100 [01:18<00:43,  1.22s/it] 65%|██████▌   | 65/100 [01:19<00:42,  1.22s/it] 66%|██████▌   | 66/100 [01:20<00:41,  1.22s/it] 67%|██████▋   | 67/100 [01:21<00:40,  1.22s/it] 68%|██████▊   | 68/100 [01:23<00:39,  1.22s/it] 69%|██████▉   | 69/100 [01:24<00:37,  1.22s/it] 70%|███████   | 70/100 [01:25<00:36,  1.22s/it] 71%|███████   | 71/100 [01:26<00:35,  1.22s/it] 72%|███████▏  | 72/100 [01:27<00:34,  1.22s/it] 73%|███████▎  | 73/100 [01:29<00:32,  1.22s/it] 74%|███████▍  | 74/100 [01:30<00:31,  1.22s/it] 75%|███████▌  | 75/100 [01:31<00:30,  1.22s/it] 76%|███████▌  | 76/100 [01:32<00:29,  1.22s/it] 77%|███████▋  | 77/100 [01:33<00:28,  1.22s/it] 78%|███████▊  | 78/100 [01:35<00:26,  1.22s/it] 79%|███████▉  | 79/100 [01:36<00:25,  1.22s/it] 80%|████████  | 80/100 [01:37<00:24,  1.22s/it] 81%|████████  | 81/100 [01:38<00:23,  1.22s/it] 82%|████████▏ | 82/100 [01:40<00:21,  1.22s/it] 83%|████████▎ | 83/100 [01:41<00:20,  1.22s/it] 84%|████████▍ | 84/100 [01:42<00:19,  1.22s/it] 85%|████████▌ | 85/100 [01:43<00:18,  1.22s/it] 86%|████████▌ | 86/100 [01:44<00:17,  1.22s/it] 87%|████████▋ | 87/100 [01:46<00:15,  1.22s/it] 88%|████████▊ | 88/100 [01:47<00:14,  1.22s/it] 89%|████████▉ | 89/100 [01:48<00:13,  1.22s/it] 90%|█████████ | 90/100 [01:49<00:12,  1.22s/it] 91%|█████████ | 91/100 [01:51<00:10,  1.22s/it] 92%|█████████▏| 92/100 [01:52<00:09,  1.22s/it] 93%|█████████▎| 93/100 [01:53<00:08,  1.22s/it] 94%|█████████▍| 94/100 [01:54<00:07,  1.22s/it] 95%|█████████▌| 95/100 [01:55<00:06,  1.22s/it] 96%|█████████▌| 96/100 [01:57<00:04,  1.22s/it] 97%|█████████▋| 97/100 [01:58<00:03,  1.22s/it] 98%|█████████▊| 98/100 [01:59<00:02,  1.22s/it] 99%|█████████▉| 99/100 [02:00<00:01,  1.23s/it]100%|██████████| 100/100 [02:02<00:00,  1.22s/it]100%|██████████| 100/100 [02:02<00:00,  1.22s/it]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:4'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:4'}
full model:
{'mrpc': {'alias': 'mrpc', 'acc,none': 0.7, 'acc_stderr,none': 0.04605661864718383, 'f1,none': np.float64(0.8214285714285714), 'f1_stderr,none': 0.0323693653386572}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.32375514242269293
0.9664141198776434
0.4263233992087649
0.29258192731768157
0.3497718342079178
0.5968480671674146
0.7870107004084098
0.7068508532201193
0.7836897379939763
0.2579819071737284
0.5774144903645764
0.7819348699339372
0.7404680063618143
0.651835611327285
0.6832243011482585
0.34680079188801705
0.6033476505145444
0.6299526106979766
0.6468468631829007
0.8458578364778718
0.39447753640727073
0.5291234791393744
0.944540808938638
0.8460909093145242
0.4915397062049404
0.5314721523201049
0.8670806018560002
0.6451914961665037
0.7380077491787214
0.32375514242269293
0.9664141198776434
0.4263233992087649
0.29258192731768157
0.3497718342079178
0.5968480671674146
0.7870107004084098
0.7068508532201193
0.7836897379939763
0.2579819071737284
0.5774144903645764
0.7819348699339372
0.7404680063618143
0.651835611327285
0.6832243011482585
0.34680079188801705
0.6033476505145444
0.6299526106979766
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[4, 5, 7, 1, 6, 2, 3, 0]
tensor([4, 5, 7, 1, 6, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 4, 5, 0, 1, 2]
tensor([6, 3, 7, 4, 5, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 1, 7, 2, 6, 0, 3, 5]
tensor([4, 1, 7, 2, 6, 0, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 5, 2, 7, 0, 4, 1]
tensor([6, 3, 5, 2, 7, 0, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 5, 6, 7, 3, 1, 0, 2]
tensor([4, 5, 6, 7, 3, 1, 0, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 2, 4, 1, 0, 5]
tensor([0, 3, 1, 2, 4, 1, 0, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1, 1.0]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 2
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 4 to 5
done!
Normal merging for layer 6
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
done!
Cross-layer merge completed for layers 7 to 9
done!
Normal merging for layer 10
tensor([0, 6])
tensor(0)
tensor([2, 5])
tensor(2)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([7])
tensor(7)
done!
Cross-layer merge completed for layers 11 to 26
done!
Normal merging for layer 27
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 28 to 30
done!
Normal merging for layer 31
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
all done!
Model size: 12.3867 GB
89
cuda:4
multirc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:42<00:42, 42.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 25.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 28.19s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: multirc] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: multirc] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/revision/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/revision/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/multirc?recursive=False&expand=False HTTP/1.1" 307 146
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/multirc?recursive=False&expand=False HTTP/1.1" 200 365
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:filelock:Attempting to acquire lock 140239882246032 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140239882246032 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140239882246032 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140239882246032 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140239887349952 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140239887349952 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140239887349952 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140239887349952 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of multirc from None to 0
INFO:lm_eval.api.task:Building contexts for multirc on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1282.49it/s]
DEBUG:lm_eval.evaluator:Task: multirc; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:03<11:15,  3.39s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:05<05:57,  1.81s/it]Running loglikelihood requests:   2%|▎         | 5/200 [00:08<04:56,  1.52s/it]Running loglikelihood requests:   4%|▎         | 7/200 [00:10<04:30,  1.40s/it]Running loglikelihood requests:   4%|▍         | 9/200 [00:13<04:14,  1.33s/it]Running loglikelihood requests:   6%|▌         | 11/200 [00:15<04:04,  1.30s/it]Running loglikelihood requests:   6%|▋         | 13/200 [00:18<03:57,  1.27s/it]Running loglikelihood requests:   8%|▊         | 15/200 [00:20<03:52,  1.26s/it]Running loglikelihood requests:   8%|▊         | 17/200 [00:23<03:51,  1.27s/it]Running loglikelihood requests:  10%|▉         | 19/200 [00:25<03:51,  1.28s/it]Running loglikelihood requests:  10%|█         | 21/200 [00:28<03:45,  1.26s/it]Running loglikelihood requests:  12%|█▏        | 23/200 [00:30<03:39,  1.24s/it]Running loglikelihood requests:  12%|█▎        | 25/200 [00:33<03:35,  1.23s/it]Running loglikelihood requests:  14%|█▎        | 27/200 [00:35<03:31,  1.22s/it]Running loglikelihood requests:  14%|█▍        | 29/200 [00:37<03:27,  1.21s/it]Running loglikelihood requests:  16%|█▌        | 31/200 [00:40<03:24,  1.21s/it]Running loglikelihood requests:  16%|█▋        | 33/200 [00:42<03:21,  1.20s/it]Running loglikelihood requests:  18%|█▊        | 35/200 [00:45<03:18,  1.20s/it]Running loglikelihood requests:  18%|█▊        | 37/200 [00:47<03:15,  1.20s/it]Running loglikelihood requests:  20%|█▉        | 39/200 [00:49<03:12,  1.20s/it]Running loglikelihood requests:  20%|██        | 41/200 [00:52<03:10,  1.20s/it]Running loglikelihood requests:  22%|██▏       | 43/200 [00:54<03:07,  1.20s/it]Running loglikelihood requests:  22%|██▎       | 45/200 [00:57<03:04,  1.19s/it]Running loglikelihood requests:  24%|██▎       | 47/200 [00:59<03:02,  1.19s/it]Running loglikelihood requests:  24%|██▍       | 49/200 [01:01<02:59,  1.19s/it]Running loglikelihood requests:  26%|██▌       | 51/200 [01:04<02:57,  1.19s/it]Running loglikelihood requests:  26%|██▋       | 53/200 [01:06<02:54,  1.19s/it]Running loglikelihood requests:  28%|██▊       | 55/200 [01:08<02:52,  1.19s/it]Running loglikelihood requests:  28%|██▊       | 57/200 [01:11<02:49,  1.19s/it]Running loglikelihood requests:  30%|██▉       | 59/200 [01:13<02:47,  1.19s/it]Running loglikelihood requests:  30%|███       | 61/200 [01:15<02:44,  1.18s/it]Running loglikelihood requests:  32%|███▏      | 63/200 [01:18<02:42,  1.18s/it]Running loglikelihood requests:  32%|███▎      | 65/200 [01:20<02:39,  1.18s/it]Running loglikelihood requests:  34%|███▎      | 67/200 [01:23<02:37,  1.18s/it]Running loglikelihood requests:  34%|███▍      | 69/200 [01:25<02:34,  1.18s/it]Running loglikelihood requests:  36%|███▌      | 71/200 [01:27<02:32,  1.18s/it]Running loglikelihood requests:  36%|███▋      | 73/200 [01:30<02:29,  1.18s/it]Running loglikelihood requests:  38%|███▊      | 75/200 [01:32<02:27,  1.18s/it]Running loglikelihood requests:  38%|███▊      | 77/200 [01:34<02:26,  1.19s/it]Running loglikelihood requests:  40%|███▉      | 79/200 [01:37<02:27,  1.22s/it]Running loglikelihood requests:  40%|████      | 81/200 [01:39<02:24,  1.22s/it]Running loglikelihood requests:  42%|████▏     | 83/200 [01:42<02:21,  1.21s/it]Running loglikelihood requests:  42%|████▎     | 85/200 [01:44<02:18,  1.21s/it]Running loglikelihood requests:  44%|████▎     | 87/200 [01:47<02:16,  1.21s/it]Running loglikelihood requests:  44%|████▍     | 89/200 [01:49<02:14,  1.21s/it]Running loglikelihood requests:  46%|████▌     | 91/200 [01:51<02:11,  1.21s/it]Running loglikelihood requests:  46%|████▋     | 93/200 [01:54<02:10,  1.22s/it]Running loglikelihood requests:  48%|████▊     | 95/200 [01:56<02:07,  1.21s/it]Running loglikelihood requests:  48%|████▊     | 97/200 [01:59<02:04,  1.21s/it]Running loglikelihood requests:  50%|████▉     | 99/200 [02:01<02:01,  1.20s/it]Running loglikelihood requests:  50%|█████     | 101/200 [02:03<01:58,  1.20s/it]Running loglikelihood requests:  52%|█████▏    | 103/200 [02:06<01:57,  1.21s/it]Running loglikelihood requests:  52%|█████▎    | 105/200 [02:08<01:55,  1.22s/it]Running loglikelihood requests:  54%|█████▎    | 107/200 [02:11<01:52,  1.21s/it]Running loglikelihood requests:  55%|█████▍    | 109/200 [02:13<01:50,  1.21s/it]Running loglikelihood requests:  56%|█████▌    | 111/200 [02:16<01:47,  1.21s/it]Running loglikelihood requests:  56%|█████▋    | 113/200 [02:18<01:44,  1.21s/it]Running loglikelihood requests:  57%|█████▊    | 115/200 [02:21<01:42,  1.21s/it]Running loglikelihood requests:  58%|█████▊    | 117/200 [02:23<01:41,  1.22s/it]Running loglikelihood requests:  60%|█████▉    | 119/200 [02:25<01:39,  1.23s/it]Running loglikelihood requests:  60%|██████    | 121/200 [02:28<01:37,  1.23s/it]Running loglikelihood requests:  62%|██████▏   | 123/200 [02:30<01:34,  1.23s/it]Running loglikelihood requests:  62%|██████▎   | 125/200 [02:33<01:32,  1.23s/it]Running loglikelihood requests:  64%|██████▎   | 127/200 [02:35<01:29,  1.23s/it]Running loglikelihood requests:  64%|██████▍   | 129/200 [02:38<01:27,  1.23s/it]Running loglikelihood requests:  66%|██████▌   | 131/200 [02:40<01:24,  1.23s/it]Running loglikelihood requests:  66%|██████▋   | 133/200 [02:43<01:21,  1.22s/it]Running loglikelihood requests:  68%|██████▊   | 135/200 [02:44<01:10,  1.09s/it]Running loglikelihood requests:  68%|██████▊   | 137/200 [02:46<01:02,  1.01it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [02:47<00:56,  1.08it/s]Running loglikelihood requests:  70%|███████   | 141/200 [02:49<00:51,  1.14it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [02:50<00:48,  1.19it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [02:52<00:44,  1.23it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [02:53<00:42,  1.26it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [02:55<00:39,  1.28it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [02:56<00:37,  1.30it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [02:58<00:35,  1.32it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [02:59<00:33,  1.33it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [03:01<00:31,  1.35it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [03:02<00:30,  1.36it/s]Running loglikelihood requests:  80%|████████  | 161/200 [03:04<00:28,  1.37it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [03:05<00:26,  1.40it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [03:06<00:24,  1.43it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [03:08<00:22,  1.45it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [03:09<00:21,  1.47it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [03:10<00:19,  1.48it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [03:12<00:18,  1.49it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [03:13<00:16,  1.50it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [03:14<00:15,  1.52it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [03:15<00:13,  1.52it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [03:17<00:12,  1.53it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [03:18<00:11,  1.54it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [03:19<00:09,  1.54it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [03:21<00:08,  1.55it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [03:22<00:07,  1.55it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [03:23<00:05,  1.56it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [03:24<00:04,  1.56it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [03:26<00:03,  1.56it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [03:27<00:01,  1.56it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [03:28<00:00,  1.56it/s]Running loglikelihood requests: 100%|██████████| 200/200 [03:28<00:00,  1.04s/it]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:5'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:5'}
full model:
{'multirc': {'alias': 'multirc', 'acc,none': 0.54, 'acc_stderr,none': 0.05009082659620331}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.6089030413152259
0.13723554564825818
0.32639298774801107
0.39554004962513223
0.17838079452737773
0.18172395498612623
0.4458149326815675
0.2599923469162762
0.44953845901781375
0.264228421159195
0.28453253935496087
0.5178234519727917
0.18549509579986273
0.3164231435223059
0.47622435667997154
0.5789910219470605
0.38414027299756903
0.6648306636715198
0.2288657593104162
0.30014630918632723
0.28954340172694804
0.7956189982678256
0.789982900191189
0.3644425339397572
0.37373995160413354
0.19965826892219962
0.3752330861186013
0.6279352732581062
0.3474029418882723
0.6089030413152259
0.13723554564825818
0.32639298774801107
0.39554004962513223
0.17838079452737773
0.18172395498612623
0.4458149326815675
0.2599923469162762
0.44953845901781375
0.264228421159195
0.28453253935496087
0.5178234519727917
0.18549509579986273
0.3164231435223059
0.47622435667997154
0.5789910219470605
0.38414027299756903
0.6648306636715198
0.2288657593104162
0.30014630918632723
0.28954340172694804
0.7956189982678256
0.789982900191189
0.3644425339397572
0.37373995160413354
0.19965826892219962
0.3752330861186013
0.6279352732581062
0.3474029418882723
0.6089030413152259
0.13723554564825818
0.32639298774801107
0.39554004962513223
0.17838079452737773
0.18172395498612623
0.4458149326815675
0.2599923469162762
0.44953845901781375
0.264228421159195
0.28453253935496087
0.5178234519727917
0.18549509579986273
0.3164231435223059
0.47622435667997154
0.5789910219470605
0.38414027299756903
0.6648306636715198
0.2288657593104162
0.30014630918632723
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 0, 6, 2, 4, 1]
tensor([5, 3, 7, 0, 6, 2, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 7, 4, 2, 1, 3, 6, 0]
tensor([5, 7, 4, 2, 1, 3, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 1, 3, 6, 0, 5, 7, 4]
tensor([2, 1, 3, 6, 0, 5, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 4, 1, 6, 0, 2, 3, 5]
tensor([7, 4, 1, 6, 0, 2, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 6, 3, 2, 5, 1, 7, 0]
tensor([4, 6, 3, 2, 5, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 0, 2, 1, 4, 7, 3, 6]
tensor([5, 0, 2, 1, 4, 7, 3, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 0, 1, 1.0, 1.0, 1.0]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
done!
Normal merging for layer 2
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 3 to 4
done!
Normal merging for layer 5
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
done!
Normal merging for layer 6
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
done!
Normal merging for layer 7
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
done!
Cross-layer merge completed for layers 8 to 25
done!
Normal merging for layer 26
tensor([0, 3])
tensor(0)
tensor([1, 2, 4, 5, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 27 to 31
done!
all done!
Model size: 12.3238 GB
222
cuda:5
rte
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:41<00:41, 41.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 24.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 27.01s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/rte?recursive=False&expand=False HTTP/1.1" 307 140
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/rte?recursive=False&expand=False HTTP/1.1" 200 354
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140215727071040 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140215727071040 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215727071040 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140215727071040 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140229742858544 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140229742858544 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140229742858544 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140229742858544 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of rte from None to 0
INFO:lm_eval.api.task:Building contexts for rte on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2538.37it/s]
DEBUG:lm_eval.evaluator:Task: rte; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:02<06:55,  2.09s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:03<03:36,  1.10s/it]Running loglikelihood requests:   2%|▎         | 5/200 [00:04<02:47,  1.16it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:06<02:25,  1.32it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:07<02:13,  1.43it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:08<02:04,  1.52it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:09<01:59,  1.57it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:10<01:54,  1.61it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:11<01:51,  1.65it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:13<01:47,  1.68it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:14<01:44,  1.72it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:15<01:41,  1.75it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:16<01:38,  1.78it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:17<01:35,  1.82it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:18<01:32,  1.85it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:21<02:19,  1.21it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:22<02:02,  1.37it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:23<01:49,  1.51it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:24<01:40,  1.62it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:25<01:33,  1.73it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:26<01:27,  1.82it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:27<01:22,  1.90it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:28<01:18,  1.98it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:29<01:14,  2.05it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:30<01:11,  2.11it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:30<01:08,  2.18it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:31<01:05,  2.24it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:32<01:02,  2.31it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:33<01:00,  2.37it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:34<00:58,  2.42it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:34<00:56,  2.46it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:35<00:54,  2.50it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:36<00:53,  2.52it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:37<00:52,  2.54it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:38<00:50,  2.57it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:38<00:49,  2.60it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:39<00:48,  2.61it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:40<00:47,  2.64it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:40<00:46,  2.67it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:41<00:44,  2.69it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:42<00:43,  2.71it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:43<00:42,  2.74it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:43<00:41,  2.76it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:44<00:40,  2.77it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:45<00:39,  2.80it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:45<00:38,  2.82it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:46<00:37,  2.83it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:47<00:36,  2.85it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:48<00:35,  2.87it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:48<00:35,  2.88it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:49<00:34,  2.89it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:50<00:33,  2.90it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:50<00:32,  2.92it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:51<00:31,  2.93it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:52<00:30,  2.94it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:52<00:30,  2.91it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:53<00:29,  2.91it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:54<00:28,  2.95it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:54<00:27,  2.97it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:55<00:27,  3.00it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:56<00:26,  3.02it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:56<00:25,  3.03it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:57<00:24,  3.04it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:58<00:23,  3.06it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:58<00:23,  3.06it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:59<00:22,  3.08it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [01:00<00:21,  3.10it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [01:00<00:20,  3.12it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [01:01<00:20,  3.14it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [01:01<00:19,  3.16it/s]Running loglikelihood requests:  70%|███████   | 141/200 [01:02<00:18,  3.18it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [01:03<00:17,  3.20it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:03<00:17,  3.21it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [01:04<00:16,  3.22it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:04<00:15,  3.24it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:05<00:15,  3.25it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:06<00:14,  3.25it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:06<00:13,  3.26it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:07<00:13,  3.28it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:08<00:12,  3.30it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:08<00:11,  3.32it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:09<00:11,  3.33it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:09<00:10,  3.34it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:10<00:09,  3.36it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:10<00:09,  3.37it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:11<00:08,  3.40it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:12<00:07,  3.42it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:12<00:07,  3.43it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:13<00:06,  3.45it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:13<00:06,  3.46it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:14<00:05,  3.48it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:14<00:04,  3.49it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:15<00:04,  3.50it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:16<00:03,  3.52it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:16<00:03,  3.45it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:17<00:02,  3.45it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:17<00:02,  3.45it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:18<00:01,  3.47it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:19<00:00,  3.54it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:19<00:00,  3.60it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:19<00:00,  2.51it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:6'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:6'}
full model:
{'rte': {'alias': 'rte', 'acc,none': 0.5, 'acc_stderr,none': 0.050251890762960605}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.34161229456626735
0.905233410256777
0.5205040718697735
0.4121994254524892
0.7398116665887099
0.6225415196831932
0.7923970242263771
0.7353888240887675
0.6535613357308766
0.7757058271862038
0.734046359122903
0.4471799126982846
0.773619360921301
0.7955347039939479
0.8672068064531693
0.8652880343596522
0.3302235467760883
0.6789268064017625
0.6072221471952108
0.9194446824778495
0.4812004589187253
0.5728915095234594
0.1682455054057436
0.93212414632396
0.9148362604533635
0.8268537756297094
0.7592245907029287
0.7256008379011685
0.7109756105942956
0.34161229456626735
0.905233410256777
0.5205040718697735
0.4121994254524892
0.7398116665887099
0.6225415196831932
0.7923970242263771
0.7353888240887675
0.6535613357308766
0.7757058271862038
0.734046359122903
0.4471799126982846
0.773619360921301
0.7955347039939479
0.8672068064531693
0.8652880343596522
0.3302235467760883
0.6789268064017625
0.6072221471952108
0.9194446824778495
0.4812004589187253
Total groups 73 exceeded the threshold, stopping comparison.
The group tensor is
[5, 2, 7, 1, 6, 4, 3, 0]
tensor([5, 2, 7, 1, 6, 4, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 6, 0, 7, 3, 4, 1]
tensor([5, 2, 6, 0, 7, 3, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 7, 2, 5, 4, 3, 0]
tensor([6, 1, 7, 2, 5, 4, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 2, 4, 1, 5, 0]
tensor([6, 3, 7, 2, 4, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 5, 6, 2, 3, 1, 4, 0]
tensor([7, 5, 6, 2, 3, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 5, 4, 1, 0, 1, 3]
tensor([0, 2, 5, 4, 1, 0, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1.0, 1]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 4
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 5 to 8
done!
Normal merging for layer 9
tensor([0, 5])
tensor(0)
tensor([4, 6])
tensor(4)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 10 to 30
done!
Normal merging for layer 31
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
all done!
Model size: 12.1348 GB
81
cuda:6
eq_bench
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:40<00:40, 40.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 23.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 26.24s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:eq_bench: No `until` specified in `generation_kwargs`! Defaulting to the fewshot_delimiter='\n\n'
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/pbevan11/EQ-Bench HTTP/1.1" 200 789
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/pbevan11/EQ-Bench/pbevan11/EQ-Bench.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/pbevan11/EQ-Bench HTTP/1.1" 200 796
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/pbevan11/EQ-Bench/resolve/9ce8e5ffc1a36be5f946b37610ec8c516871f0d3/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/pbevan11/EQ-Bench/revision/9ce8e5ffc1a36be5f946b37610ec8c516871f0d3 HTTP/1.1" 200 796
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/pbevan11/EQ-Bench/tree/9ce8e5ffc1a36be5f946b37610ec8c516871f0d3?recursive=False&expand=False HTTP/1.1" 200 301
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/pbevan11/EQ-Bench/tree/9ce8e5ffc1a36be5f946b37610ec8c516871f0d3/data?recursive=False&expand=False HTTP/1.1" 404 79
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/pbevan11/EQ-Bench/tree/9ce8e5ffc1a36be5f946b37610ec8c516871f0d3/data?recursive=False&expand=False HTTP/1.1" 404 79
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/pbevan11/EQ-Bench/revision/9ce8e5ffc1a36be5f946b37610ec8c516871f0d3 HTTP/1.1" 200 796
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/pbevan11/EQ-Bench/resolve/9ce8e5ffc1a36be5f946b37610ec8c516871f0d3/dataset_infos.json HTTP/1.1" 404 0
DEBUG:filelock:Attempting to acquire lock 140237735463440 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_pbevan11___eq-bench_default_0.0.0_9ce8e5ffc1a36be5f946b37610ec8c516871f0d3.lock
DEBUG:filelock:Lock 140237735463440 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_pbevan11___eq-bench_default_0.0.0_9ce8e5ffc1a36be5f946b37610ec8c516871f0d3.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/pbevan11___eq-bench/default/0.0.0/9ce8e5ffc1a36be5f946b37610ec8c516871f0d3/dataset_info.json
DEBUG:filelock:Attempting to release lock 140237735463440 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_pbevan11___eq-bench_default_0.0.0_9ce8e5ffc1a36be5f946b37610ec8c516871f0d3.lock
DEBUG:filelock:Lock 140237735463440 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_pbevan11___eq-bench_default_0.0.0_9ce8e5ffc1a36be5f946b37610ec8c516871f0d3.lock
DEBUG:filelock:Attempting to acquire lock 140238671042208 on /public/home/zouyifei001/.cache/huggingface/datasets/pbevan11___eq-bench/default/0.0.0/9ce8e5ffc1a36be5f946b37610ec8c516871f0d3_builder.lock
DEBUG:filelock:Lock 140238671042208 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/pbevan11___eq-bench/default/0.0.0/9ce8e5ffc1a36be5f946b37610ec8c516871f0d3_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/pbevan11___eq-bench/default/0.0.0/9ce8e5ffc1a36be5f946b37610ec8c516871f0d3/dataset_info.json
DEBUG:filelock:Attempting to release lock 140238671042208 on /public/home/zouyifei001/.cache/huggingface/datasets/pbevan11___eq-bench/default/0.0.0/9ce8e5ffc1a36be5f946b37610ec8c516871f0d3_builder.lock
DEBUG:filelock:Lock 140238671042208 released on /public/home/zouyifei001/.cache/huggingface/datasets/pbevan11___eq-bench/default/0.0.0/9ce8e5ffc1a36be5f946b37610ec8c516871f0d3_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
INFO:lm_eval.evaluator:eq_bench: Using gen_kwargs: {'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 80, 'until': ['\n\n']}
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of eq_bench from None to 0
INFO:lm_eval.api.task:Building contexts for eq_bench on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 85913.64it/s]
DEBUG:lm_eval.evaluator:Task: eq_bench; number of requests on this rank: 100
INFO:lm_eval.evaluator:Running generate_until requests
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/100 [00:08<14:18,  8.67s/it]Running generate_until requests:   2%|▏         | 2/100 [00:15<12:37,  7.73s/it]Running generate_until requests:   3%|▎         | 3/100 [00:22<11:48,  7.30s/it]Running generate_until requests:   4%|▍         | 4/100 [00:29<11:30,  7.19s/it]Running generate_until requests:   5%|▌         | 5/100 [00:36<11:14,  7.10s/it]Running generate_until requests:   6%|▌         | 6/100 [00:43<10:59,  7.02s/it]Running generate_until requests:   7%|▋         | 7/100 [00:49<10:38,  6.86s/it]Running generate_until requests:   8%|▊         | 8/100 [00:56<10:20,  6.74s/it]Running generate_until requests:   9%|▉         | 9/100 [01:03<10:27,  6.89s/it]Running generate_until requests:  10%|█         | 10/100 [01:10<10:07,  6.75s/it]Running generate_until requests:  11%|█         | 11/100 [01:16<09:55,  6.69s/it]Running generate_until requests:  12%|█▏        | 12/100 [01:23<10:06,  6.90s/it]Running generate_until requests:  13%|█▎        | 13/100 [01:30<09:58,  6.88s/it]Running generate_until requests:  14%|█▍        | 14/100 [01:37<09:46,  6.82s/it]Running generate_until requests:  15%|█▌        | 15/100 [01:44<09:38,  6.81s/it]Running generate_until requests:  16%|█▌        | 16/100 [01:50<09:20,  6.68s/it]Running generate_until requests:  17%|█▋        | 17/100 [01:56<09:05,  6.58s/it]Running generate_until requests:  18%|█▊        | 18/100 [02:03<08:52,  6.49s/it]Running generate_until requests:  19%|█▉        | 19/100 [02:09<08:44,  6.47s/it]Running generate_until requests:  20%|██        | 20/100 [02:16<08:33,  6.42s/it]Running generate_until requests:  21%|██        | 21/100 [02:22<08:29,  6.45s/it]Running generate_until requests:  22%|██▏       | 22/100 [02:29<08:24,  6.47s/it]Running generate_until requests:  23%|██▎       | 23/100 [02:35<08:18,  6.48s/it]Running generate_until requests:  24%|██▍       | 24/100 [02:42<08:12,  6.48s/it]Running generate_until requests:  25%|██▌       | 25/100 [02:48<08:17,  6.63s/it]Running generate_until requests:  26%|██▌       | 26/100 [02:55<08:00,  6.50s/it]Running generate_until requests:  27%|██▋       | 27/100 [03:01<07:47,  6.40s/it]Running generate_until requests:  28%|██▊       | 28/100 [03:07<07:43,  6.44s/it]Running generate_until requests:  29%|██▉       | 29/100 [03:14<07:36,  6.43s/it]Running generate_until requests:  30%|███       | 30/100 [03:20<07:29,  6.42s/it]Running generate_until requests:  31%|███       | 31/100 [03:26<07:17,  6.34s/it]Running generate_until requests:  32%|███▏      | 32/100 [03:33<07:22,  6.51s/it]Running generate_until requests:  33%|███▎      | 33/100 [03:39<07:08,  6.40s/it]Running generate_until requests:  34%|███▍      | 34/100 [03:46<06:56,  6.31s/it]Running generate_until requests:  35%|███▌      | 35/100 [03:52<06:46,  6.25s/it]Running generate_until requests:  36%|███▌      | 36/100 [03:58<06:34,  6.16s/it]Running generate_until requests:  37%|███▋      | 37/100 [04:04<06:33,  6.25s/it]Running generate_until requests:  38%|███▊      | 38/100 [04:10<06:23,  6.19s/it]Running generate_until requests:  39%|███▉      | 39/100 [04:16<06:15,  6.16s/it]Running generate_until requests:  40%|████      | 40/100 [04:22<06:03,  6.06s/it]Running generate_until requests:  41%|████      | 41/100 [04:29<06:06,  6.21s/it]Running generate_until requests:  42%|████▏     | 42/100 [04:35<05:59,  6.20s/it]Running generate_until requests:  43%|████▎     | 43/100 [04:40<05:45,  6.07s/it]Running generate_until requests:  44%|████▍     | 44/100 [04:46<05:34,  5.98s/it]Running generate_until requests:  45%|████▌     | 45/100 [04:52<05:25,  5.92s/it]Running generate_until requests:  46%|████▌     | 46/100 [04:58<05:25,  6.03s/it]Running generate_until requests:  47%|████▋     | 47/100 [05:05<05:26,  6.16s/it]Running generate_until requests:  48%|████▊     | 48/100 [05:11<05:14,  6.05s/it]Running generate_until requests:  49%|████▉     | 49/100 [05:16<05:06,  6.00s/it]Running generate_until requests:  50%|█████     | 50/100 [05:23<05:01,  6.04s/it]Running generate_until requests:  51%|█████     | 51/100 [05:28<04:51,  5.95s/it]Running generate_until requests:  52%|█████▏    | 52/100 [05:34<04:44,  5.92s/it]Running generate_until requests:  53%|█████▎    | 53/100 [05:40<04:33,  5.82s/it]Running generate_until requests:  54%|█████▍    | 54/100 [05:46<04:27,  5.82s/it]Running generate_until requests:  55%|█████▌    | 55/100 [05:51<04:16,  5.69s/it]Running generate_until requests:  56%|█████▌    | 56/100 [05:56<04:08,  5.64s/it]Running generate_until requests:  57%|█████▋    | 57/100 [06:02<04:05,  5.71s/it]Running generate_until requests:  58%|█████▊    | 58/100 [06:08<03:58,  5.68s/it]Running generate_until requests:  59%|█████▉    | 59/100 [06:14<03:53,  5.70s/it]Running generate_until requests:  60%|██████    | 60/100 [06:19<03:48,  5.72s/it]Running generate_until requests:  61%|██████    | 61/100 [06:25<03:44,  5.75s/it]Running generate_until requests:  62%|██████▏   | 62/100 [06:31<03:40,  5.80s/it]Running generate_until requests:  63%|██████▎   | 63/100 [06:37<03:30,  5.68s/it]Running generate_until requests:  64%|██████▍   | 64/100 [06:42<03:22,  5.63s/it]Running generate_until requests:  65%|██████▌   | 65/100 [06:48<03:16,  5.62s/it]Running generate_until requests:  66%|██████▌   | 66/100 [06:50<02:35,  4.56s/it]Running generate_until requests:  67%|██████▋   | 67/100 [06:55<02:39,  4.83s/it]Running generate_until requests:  68%|██████▊   | 68/100 [07:01<02:44,  5.14s/it]Running generate_until requests:  69%|██████▉   | 69/100 [07:07<02:43,  5.26s/it]Running generate_until requests:  70%|███████   | 70/100 [07:12<02:39,  5.30s/it]Running generate_until requests:  71%|███████   | 71/100 [07:14<02:05,  4.32s/it]Running generate_until requests:  72%|███████▏  | 72/100 [07:20<02:14,  4.79s/it]Running generate_until requests:  73%|███████▎  | 73/100 [07:22<01:46,  3.95s/it]Running generate_until requests:  74%|███████▍  | 74/100 [07:27<01:52,  4.34s/it]Running generate_until requests:  75%|███████▌  | 75/100 [07:32<01:53,  4.55s/it]Running generate_until requests:  76%|███████▌  | 76/100 [07:37<01:53,  4.75s/it]Running generate_until requests:  77%|███████▋  | 77/100 [07:43<01:53,  4.91s/it]Running generate_until requests:  78%|███████▊  | 78/100 [07:48<01:50,  5.03s/it]Running generate_until requests:  79%|███████▉  | 79/100 [07:54<01:48,  5.15s/it]Running generate_until requests:  80%|████████  | 80/100 [07:59<01:43,  5.19s/it]Running generate_until requests:  81%|████████  | 81/100 [08:04<01:39,  5.22s/it]Running generate_until requests:  82%|████████▏ | 82/100 [08:09<01:34,  5.24s/it]Running generate_until requests:  83%|████████▎ | 83/100 [08:15<01:28,  5.21s/it]Running generate_until requests:  84%|████████▍ | 84/100 [08:20<01:23,  5.23s/it]Running generate_until requests:  85%|████████▌ | 85/100 [08:25<01:19,  5.31s/it]Running generate_until requests:  86%|████████▌ | 86/100 [08:31<01:14,  5.29s/it]Running generate_until requests:  87%|████████▋ | 87/100 [08:32<00:55,  4.26s/it]Running generate_until requests:  88%|████████▊ | 88/100 [08:38<00:56,  4.67s/it]Running generate_until requests:  89%|████████▉ | 89/100 [08:44<00:54,  4.91s/it]Running generate_until requests:  90%|█████████ | 90/100 [08:49<00:51,  5.11s/it]Running generate_until requests:  91%|█████████ | 91/100 [08:54<00:46,  5.13s/it]Running generate_until requests:  92%|█████████▏| 92/100 [09:00<00:41,  5.19s/it]Running generate_until requests:  93%|█████████▎| 93/100 [09:05<00:36,  5.15s/it]Running generate_until requests:  94%|█████████▍| 94/100 [09:10<00:31,  5.18s/it]Running generate_until requests:  95%|█████████▌| 95/100 [09:15<00:25,  5.13s/it]Running generate_until requests:  96%|█████████▌| 96/100 [09:17<00:16,  4.11s/it]Running generate_until requests:  97%|█████████▋| 97/100 [09:18<00:10,  3.39s/it]Running generate_until requests:  98%|█████████▊| 98/100 [09:23<00:07,  3.90s/it]Running generate_until requests:  99%|█████████▉| 99/100 [09:29<00:04,  4.29s/it]Running generate_until requests: 100%|██████████| 100/100 [09:34<00:00,  4.57s/it]Running generate_until requests: 100%|██████████| 100/100 [09:34<00:00,  5.74s/it]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:7'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:7'}
full model:
{'eq_bench': {'alias': 'eq_bench', 'eqbench,none': -9.075457665929516, 'eqbench_stderr,none': 3.722206958756126, 'percent_parseable,none': 94.0, 'percent_parseable_stderr,none': 2.3868325657594203}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.6639018561214101
0.48429197946127234
0.9515491395508957
0.5799692129526803
0.8892832964569847
0.5583880670373075
0.6815933931575056
0.8296184325732159
0.8572432652494972
0.7515568516065031
0.7498285715344476
0.6640770908835784
0.7344737024966144
0.8863376660708159
0.7840365418395101
0.7895254515113318
0.915557892351788
0.8364869594903738
0.8262221986876632
0.8424210311799578
0.9059689119946415
0.938938460185247
0.9139805997979817
0.8791150946846779
0.9604433557463424
0.36429086085480555
0.6224935475213342
0.9367462541020114
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[2, 3, 7, 0, 6, 1, 5, 4]
tensor([2, 3, 7, 0, 6, 1, 5, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 5, 2, 1, 6, 0, 3, 7]
tensor([4, 5, 2, 1, 6, 0, 3, 7], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 4, 0, 7, 1, 2, 6]
tensor([5, 3, 4, 0, 7, 1, 2, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 6, 4, 0, 3, 1, 2, 7]
tensor([5, 6, 4, 0, 3, 1, 2, 7], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 3, 0, 1, 2, 2, 3]
tensor([0, 1, 3, 0, 1, 2, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[2, 3, 0, 0, 1, 1, 2, 3]
tensor([2, 3, 0, 0, 1, 1, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[2, 0, 1, 0, 3, 1, 2, 3]
tensor([2, 0, 1, 0, 3, 1, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[2, 0, 1, 0, 2, 1, 3, 3]
tensor([2, 0, 1, 0, 2, 1, 3, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 0, 1.0, 1, 1.0, 1.0]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 0, 1.0, 1, 1.0, 1.0]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 0, 1.0, 1, 1.0, 1.0]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([7])
tensor(7)
done!
Normal merging for layer 2
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 3 to 4
done!
Normal merging for layer 5
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
done!
Cross-layer merge completed for layers 6 to 15
done!
Normal merging for layer 16
tensor([0, 3])
tensor(0)
tensor([1, 4])
tensor(1)
tensor([5, 6])
tensor(5)
tensor([2, 7])
tensor(2)
done!
Cross-layer merge completed for layers 17 to 18
done!
Normal merging for layer 19
tensor([2, 3])
tensor(2)
tensor([4, 5])
tensor(4)
tensor([0, 6])
tensor(0)
tensor([1, 7])
tensor(1)
done!
Normal merging for layer 20
tensor([1, 3])
tensor(1)
tensor([2, 5])
tensor(2)
tensor([0, 6])
tensor(0)
tensor([4, 7])
tensor(4)
done!
Normal merging for layer 21
tensor([1, 3])
tensor(1)
tensor([2, 5])
tensor(2)
tensor([0, 4])
tensor(0)
tensor([6, 7])
tensor(6)
done!
Cross-layer merge completed for layers 22 to 24
done!
Normal merging for layer 25
tensor([0, 3])
tensor(0)
tensor([1, 2, 4, 5, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 26 to 29
done!
Normal merging for layer 30
tensor([0, 3])
tensor(0)
tensor([1, 2, 4, 5, 6, 7])
tensor(1)
done!
Normal merging for layer 31
tensor([0, 3])
tensor(0)
tensor([1, 2, 4, 5, 6, 7])
tensor(1)
done!
all done!
Model size: 12.7017 GB
105
cuda:7
winogrande
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:43<00:43, 43.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 25.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 27.93s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/winogrande HTTP/1.1" 307 67
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/winogrande HTTP/1.1" 200 1036
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/winogrande/winogrande.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): datasets-server.hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://datasets-server.hf-mirror.com:443 "GET /parquet?dataset=winogrande HTTP/1.1" 302 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET / HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/winogrande/winogrande.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/winogrande/resolve/main/winogrande.py HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/winogrande/resolve/main/winogrande.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/winogrande/resolve/main/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/winogrande/resolve/main/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/winogrande/resolve/main/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/winogrande/resolve/main/README.md HTTP/1.1" 200 0
DEBUG:filelock:Attempting to acquire lock 140238675085840 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_winogrande_winogrande_xl_1.1.0_a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2.lock
DEBUG:filelock:Lock 140238675085840 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_winogrande_winogrande_xl_1.1.0_a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2/dataset_info.json
DEBUG:filelock:Attempting to release lock 140238675085840 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_winogrande_winogrande_xl_1.1.0_a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2.lock
DEBUG:filelock:Lock 140238675085840 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_winogrande_winogrande_xl_1.1.0_a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2.lock
DEBUG:filelock:Attempting to acquire lock 140238670526032 on /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2_builder.lock
DEBUG:filelock:Lock 140238670526032 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2/dataset_info.json
DEBUG:filelock:Attempting to release lock 140238670526032 on /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2_builder.lock
DEBUG:filelock:Lock 140238670526032 released on /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
DEBUG:lm_eval.api.task:doc_to_text returned an int. Assuming multiple inputs.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of winogrande from None to 0
INFO:lm_eval.api.task:Building contexts for winogrande on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 144631.17it/s]
DEBUG:lm_eval.evaluator:Task: winogrande; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<04:13,  1.28s/it]Running loglikelihood requests:   1%|          | 2/200 [00:01<02:47,  1.18it/s]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:16,  1.44it/s]Running loglikelihood requests:   2%|▏         | 4/200 [00:02<02:01,  1.61it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:03<01:52,  1.73it/s]Running loglikelihood requests:   3%|▎         | 6/200 [00:03<01:46,  1.82it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:04<01:42,  1.88it/s]Running loglikelihood requests:   4%|▍         | 8/200 [00:04<01:40,  1.92it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:05<01:37,  1.95it/s]Running loglikelihood requests:   5%|▌         | 10/200 [00:05<01:36,  1.97it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:06<01:34,  1.99it/s]Running loglikelihood requests:   6%|▌         | 12/200 [00:06<01:34,  2.00it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:07<01:32,  2.01it/s]Running loglikelihood requests:   7%|▋         | 14/200 [00:07<01:31,  2.02it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:08<01:31,  2.03it/s]Running loglikelihood requests:   8%|▊         | 16/200 [00:08<01:30,  2.03it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:09<01:29,  2.04it/s]Running loglikelihood requests:   9%|▉         | 18/200 [00:09<01:29,  2.04it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:10<01:28,  2.05it/s]Running loglikelihood requests:  10%|█         | 20/200 [00:10<01:27,  2.05it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:11<01:27,  2.05it/s]Running loglikelihood requests:  11%|█         | 22/200 [00:11<01:26,  2.05it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:12<01:26,  2.06it/s]Running loglikelihood requests:  12%|█▏        | 24/200 [00:12<01:25,  2.06it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:13<01:24,  2.06it/s]Running loglikelihood requests:  13%|█▎        | 26/200 [00:13<01:24,  2.07it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:14<01:23,  2.07it/s]Running loglikelihood requests:  14%|█▍        | 28/200 [00:14<01:23,  2.07it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:15<01:22,  2.07it/s]Running loglikelihood requests:  15%|█▌        | 30/200 [00:15<01:21,  2.07it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:16<01:21,  2.08it/s]Running loglikelihood requests:  16%|█▌        | 32/200 [00:16<01:20,  2.08it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:16<01:20,  2.08it/s]Running loglikelihood requests:  17%|█▋        | 34/200 [00:17<01:19,  2.08it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:17<01:19,  2.09it/s]Running loglikelihood requests:  18%|█▊        | 36/200 [00:18<01:18,  2.09it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:18<01:17,  2.10it/s]Running loglikelihood requests:  19%|█▉        | 38/200 [00:19<01:17,  2.10it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:19<01:16,  2.10it/s]Running loglikelihood requests:  20%|██        | 40/200 [00:20<01:15,  2.11it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:20<01:15,  2.11it/s]Running loglikelihood requests:  21%|██        | 42/200 [00:21<01:14,  2.11it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:21<01:14,  2.11it/s]Running loglikelihood requests:  22%|██▏       | 44/200 [00:22<01:13,  2.11it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:22<01:13,  2.11it/s]Running loglikelihood requests:  23%|██▎       | 46/200 [00:23<01:12,  2.11it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:23<01:12,  2.11it/s]Running loglikelihood requests:  24%|██▍       | 48/200 [00:24<01:11,  2.11it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:24<01:11,  2.11it/s]Running loglikelihood requests:  25%|██▌       | 50/200 [00:25<01:11,  2.11it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:25<01:10,  2.11it/s]Running loglikelihood requests:  26%|██▌       | 52/200 [00:25<01:09,  2.11it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:26<01:09,  2.11it/s]Running loglikelihood requests:  27%|██▋       | 54/200 [00:26<01:09,  2.11it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:27<01:08,  2.12it/s]Running loglikelihood requests:  28%|██▊       | 56/200 [00:27<01:07,  2.12it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:28<01:07,  2.12it/s]Running loglikelihood requests:  29%|██▉       | 58/200 [00:28<01:06,  2.13it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:29<01:06,  2.13it/s]Running loglikelihood requests:  30%|███       | 60/200 [00:29<01:05,  2.13it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:30<01:05,  2.13it/s]Running loglikelihood requests:  31%|███       | 62/200 [00:30<01:04,  2.13it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:31<01:04,  2.13it/s]Running loglikelihood requests:  32%|███▏      | 64/200 [00:31<01:03,  2.13it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:32<01:03,  2.13it/s]Running loglikelihood requests:  33%|███▎      | 66/200 [00:32<01:02,  2.13it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:33<01:02,  2.13it/s]Running loglikelihood requests:  34%|███▍      | 68/200 [00:33<01:01,  2.13it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:33<01:01,  2.13it/s]Running loglikelihood requests:  35%|███▌      | 70/200 [00:34<01:01,  2.13it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:34<01:00,  2.13it/s]Running loglikelihood requests:  36%|███▌      | 72/200 [00:35<01:00,  2.13it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:35<00:59,  2.13it/s]Running loglikelihood requests:  37%|███▋      | 74/200 [00:36<00:59,  2.13it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:36<00:58,  2.13it/s]Running loglikelihood requests:  38%|███▊      | 76/200 [00:37<00:58,  2.13it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:37<00:57,  2.14it/s]Running loglikelihood requests:  39%|███▉      | 78/200 [00:38<00:57,  2.14it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:38<00:56,  2.14it/s]Running loglikelihood requests:  40%|████      | 80/200 [00:39<00:55,  2.14it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:39<00:55,  2.15it/s]Running loglikelihood requests:  41%|████      | 82/200 [00:40<00:54,  2.15it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:40<00:54,  2.15it/s]Running loglikelihood requests:  42%|████▏     | 84/200 [00:40<00:53,  2.16it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:41<00:53,  2.17it/s]Running loglikelihood requests:  43%|████▎     | 86/200 [00:41<00:52,  2.16it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:42<00:52,  2.17it/s]Running loglikelihood requests:  44%|████▍     | 88/200 [00:42<00:51,  2.17it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:43<00:51,  2.17it/s]Running loglikelihood requests:  45%|████▌     | 90/200 [00:43<00:50,  2.17it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:44<00:50,  2.17it/s]Running loglikelihood requests:  46%|████▌     | 92/200 [00:44<00:49,  2.17it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:45<00:49,  2.17it/s]Running loglikelihood requests:  47%|████▋     | 94/200 [00:45<00:48,  2.17it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:46<00:48,  2.17it/s]Running loglikelihood requests:  48%|████▊     | 96/200 [00:46<00:47,  2.17it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:46<00:47,  2.17it/s]Running loglikelihood requests:  49%|████▉     | 98/200 [00:47<00:46,  2.18it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:47<00:46,  2.18it/s]Running loglikelihood requests:  50%|█████     | 100/200 [00:48<00:45,  2.18it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:48<00:45,  2.18it/s]Running loglikelihood requests:  51%|█████     | 102/200 [00:49<00:44,  2.19it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:49<00:44,  2.19it/s]Running loglikelihood requests:  52%|█████▏    | 104/200 [00:50<00:43,  2.19it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:50<00:43,  2.20it/s]Running loglikelihood requests:  53%|█████▎    | 106/200 [00:51<00:42,  2.20it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:51<00:42,  2.20it/s]Running loglikelihood requests:  54%|█████▍    | 108/200 [00:51<00:41,  2.20it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:52<00:41,  2.20it/s]Running loglikelihood requests:  55%|█████▌    | 110/200 [00:52<00:40,  2.20it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:53<00:40,  2.20it/s]Running loglikelihood requests:  56%|█████▌    | 112/200 [00:53<00:40,  2.20it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:54<00:39,  2.20it/s]Running loglikelihood requests:  57%|█████▋    | 114/200 [00:54<00:38,  2.21it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:55<00:38,  2.21it/s]Running loglikelihood requests:  58%|█████▊    | 116/200 [00:55<00:37,  2.21it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:56<00:37,  2.21it/s]Running loglikelihood requests:  59%|█████▉    | 118/200 [00:56<00:37,  2.21it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:56<00:36,  2.21it/s]Running loglikelihood requests:  60%|██████    | 120/200 [00:57<00:36,  2.21it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:57<00:35,  2.21it/s]Running loglikelihood requests:  61%|██████    | 122/200 [00:58<00:35,  2.21it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:58<00:34,  2.21it/s]Running loglikelihood requests:  62%|██████▏   | 124/200 [00:59<00:34,  2.21it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:59<00:33,  2.21it/s]Running loglikelihood requests:  63%|██████▎   | 126/200 [01:00<00:33,  2.21it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [01:00<00:33,  2.21it/s]Running loglikelihood requests:  64%|██████▍   | 128/200 [01:01<00:32,  2.21it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [01:01<00:32,  2.21it/s]Running loglikelihood requests:  65%|██████▌   | 130/200 [01:01<00:31,  2.21it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [01:02<00:31,  2.22it/s]Running loglikelihood requests:  66%|██████▌   | 132/200 [01:02<00:30,  2.23it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [01:03<00:30,  2.23it/s]Running loglikelihood requests:  67%|██████▋   | 134/200 [01:03<00:29,  2.23it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [01:04<00:29,  2.23it/s]Running loglikelihood requests:  68%|██████▊   | 136/200 [01:04<00:28,  2.23it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [01:05<00:28,  2.23it/s]Running loglikelihood requests:  69%|██████▉   | 138/200 [01:05<00:27,  2.23it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [01:05<00:27,  2.24it/s]Running loglikelihood requests:  70%|███████   | 140/200 [01:06<00:26,  2.23it/s]Running loglikelihood requests:  70%|███████   | 141/200 [01:06<00:26,  2.23it/s]Running loglikelihood requests:  71%|███████   | 142/200 [01:07<00:25,  2.24it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [01:07<00:25,  2.24it/s]Running loglikelihood requests:  72%|███████▏  | 144/200 [01:08<00:25,  2.24it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:08<00:24,  2.24it/s]Running loglikelihood requests:  73%|███████▎  | 146/200 [01:09<00:24,  2.24it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [01:09<00:23,  2.24it/s]Running loglikelihood requests:  74%|███████▍  | 148/200 [01:09<00:23,  2.25it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:10<00:22,  2.25it/s]Running loglikelihood requests:  75%|███████▌  | 150/200 [01:10<00:22,  2.26it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:11<00:21,  2.26it/s]Running loglikelihood requests:  76%|███████▌  | 152/200 [01:11<00:21,  2.26it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:12<00:20,  2.26it/s]Running loglikelihood requests:  77%|███████▋  | 154/200 [01:12<00:20,  2.27it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:13<00:19,  2.27it/s]Running loglikelihood requests:  78%|███████▊  | 156/200 [01:13<00:19,  2.28it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:13<00:18,  2.27it/s]Running loglikelihood requests:  79%|███████▉  | 158/200 [01:14<00:18,  2.27it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:14<00:18,  2.27it/s]Running loglikelihood requests:  80%|████████  | 160/200 [01:15<00:17,  2.27it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:15<00:17,  2.27it/s]Running loglikelihood requests:  81%|████████  | 162/200 [01:16<00:16,  2.26it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:16<00:16,  2.27it/s]Running loglikelihood requests:  82%|████████▏ | 164/200 [01:17<00:15,  2.27it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:17<00:15,  2.28it/s]Running loglikelihood requests:  83%|████████▎ | 166/200 [01:17<00:14,  2.29it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:18<00:14,  2.29it/s]Running loglikelihood requests:  84%|████████▍ | 168/200 [01:18<00:13,  2.29it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:19<00:13,  2.29it/s]Running loglikelihood requests:  85%|████████▌ | 170/200 [01:19<00:13,  2.29it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:20<00:12,  2.29it/s]Running loglikelihood requests:  86%|████████▌ | 172/200 [01:20<00:12,  2.30it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:20<00:11,  2.30it/s]Running loglikelihood requests:  87%|████████▋ | 174/200 [01:21<00:11,  2.30it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:21<00:10,  2.30it/s]Running loglikelihood requests:  88%|████████▊ | 176/200 [01:22<00:10,  2.30it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:22<00:10,  2.30it/s]Running loglikelihood requests:  89%|████████▉ | 178/200 [01:23<00:09,  2.30it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:23<00:09,  2.30it/s]Running loglikelihood requests:  90%|█████████ | 180/200 [01:23<00:08,  2.32it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:24<00:08,  2.33it/s]Running loglikelihood requests:  91%|█████████ | 182/200 [01:24<00:07,  2.34it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:25<00:07,  2.34it/s]Running loglikelihood requests:  92%|█████████▏| 184/200 [01:25<00:06,  2.34it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:26<00:06,  2.34it/s]Running loglikelihood requests:  93%|█████████▎| 186/200 [01:26<00:05,  2.34it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:26<00:05,  2.33it/s]Running loglikelihood requests:  94%|█████████▍| 188/200 [01:27<00:05,  2.33it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:27<00:04,  2.34it/s]Running loglikelihood requests:  95%|█████████▌| 190/200 [01:28<00:04,  2.36it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:28<00:03,  2.38it/s]Running loglikelihood requests:  96%|█████████▌| 192/200 [01:29<00:03,  2.38it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:29<00:02,  2.39it/s]Running loglikelihood requests:  97%|█████████▋| 194/200 [01:29<00:02,  2.39it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:30<00:02,  2.39it/s]Running loglikelihood requests:  98%|█████████▊| 196/200 [01:30<00:01,  2.39it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:31<00:01,  2.39it/s]Running loglikelihood requests:  99%|█████████▉| 198/200 [01:31<00:00,  2.39it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:31<00:00,  2.39it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:32<00:00,  2.39it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:32<00:00,  2.16it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:0'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}
full model:
{'winogrande': {'alias': 'winogrande', 'acc,none': 0.69, 'acc_stderr,none': 0.046482319871173176}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9565474555896152
0.9256436871263655
0.919777800952949
0.8502516827829548
0.9788217808920475
0.7842184683361061
0.5847175538207384
0.7741211354638314
0.8734705119073022
0.9731317123565076
0.9046875380381674
0.8858535931048256
0.9314298098713261
0.8442403312485581
0.7166029053748131
0.7061450250233585
0.7563059483803782
0.6877563559653158
0.8738989590810852
0.784285727893607
0.8409178900131131
0.8425380927376759
0.6782655591765769
0.7228968829640015
0.8418686487797186
0.9141168065903919
0.8562481461255738
0.6139553840231665
0.934088538459943
Total groups 66 exceeded the threshold, stopping comparison.
The group tensor is
[6, 2, 4, 3, 5, 1, 7, 0]
tensor([6, 2, 4, 3, 5, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 3, 1, 5, 4, 7, 0]
tensor([6, 2, 3, 1, 5, 4, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 4, 1, 0, 3, 0, 1, 2]
tensor([5, 4, 1, 0, 3, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 3, 1, 4, 5, 1, 0]
tensor([0, 2, 3, 1, 4, 5, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[5, 1, 2, 3, 4, 0, 1, 0]
tensor([5, 1, 2, 3, 4, 0, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 2, 1, 1, 2, 3, 3, 0]
tensor([0, 2, 1, 1, 2, 3, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1, 1.0, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 0, 1.0, 1.0, 1.0, 1]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 2 to 7
done!
Normal merging for layer 8
tensor([3, 5])
tensor(3)
tensor([2, 6])
tensor(2)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 9 to 13
done!
Normal merging for layer 14
tensor([0, 7])
tensor(0)
tensor([3, 6])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
done!
Normal merging for layer 15
tensor([5, 7])
tensor(5)
tensor([1, 6])
tensor(1)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 16
tensor([0, 7])
tensor(0)
tensor([2, 3])
tensor(2)
tensor([1, 4])
tensor(1)
tensor([5, 6])
tensor(5)
done!
Cross-layer merge completed for layers 17 to 23
done!
Normal merging for layer 24
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
Normal merging for layer 25
tensor([0, 3])
tensor(0)
tensor([1, 2, 4, 5, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 26 to 31
done!
all done!
Model size: 11.8828 GB
140
cuda:0
qqp
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:40<00:40, 40.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 24.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.62s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: qqp] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: qqp] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
WARNING:lm_eval.api.task:[Task: qqp] metric f1 is defined, but aggregation is not. using default aggregation=f1
WARNING:lm_eval.api.task:[Task: qqp] metric f1 is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/qqp?recursive=False&expand=False HTTP/1.1" 307 140
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/qqp?recursive=False&expand=False HTTP/1.1" 200 355
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140229612652400 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qqp_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140229612652400 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qqp_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140229612652400 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qqp_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140229612652400 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qqp_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140237734733168 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140237734733168 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140237734733168 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140237734733168 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of qqp from None to 0
INFO:lm_eval.api.task:Building contexts for qqp on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2505.80it/s]
DEBUG:lm_eval.evaluator:Task: qqp; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<04:40,  1.41s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:12,  1.49it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:02<01:40,  1.93it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:03<01:27,  2.20it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:04<01:19,  2.41it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:05<01:14,  2.55it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:05<01:10,  2.65it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:06<01:07,  2.73it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:07<01:05,  2.79it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:07<01:03,  2.83it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:08<01:02,  2.88it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:09<01:00,  2.93it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:09<00:59,  2.97it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:10<00:57,  3.00it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:11<00:56,  3.03it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:11<00:55,  3.05it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:12<00:54,  3.07it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:13<00:53,  3.08it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:13<00:52,  3.09it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:14<00:51,  3.10it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:14<00:51,  3.10it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:15<00:50,  3.11it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:16<00:49,  3.12it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:16<00:48,  3.14it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:17<00:48,  3.14it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:18<00:47,  3.17it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:18<00:46,  3.18it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:19<00:45,  3.19it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:20<00:44,  3.20it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:20<00:43,  3.22it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:21<00:43,  3.23it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:21<00:42,  3.24it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:22<00:41,  3.26it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:23<00:40,  3.26it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:23<00:40,  3.27it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:24<00:39,  3.28it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:24<00:38,  3.28it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:25<00:38,  3.29it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:26<00:37,  3.30it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:26<00:36,  3.31it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:27<00:35,  3.32it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:27<00:35,  3.33it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:28<00:34,  3.34it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:29<00:33,  3.34it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:29<00:33,  3.35it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:30<00:32,  3.36it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:30<00:31,  3.37it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:31<00:31,  3.38it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:32<00:30,  3.39it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:32<00:29,  3.40it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:33<00:29,  3.41it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:33<00:28,  3.42it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:34<00:27,  3.43it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:34<00:27,  3.44it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:35<00:26,  3.45it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:36<00:25,  3.47it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:36<00:24,  3.48it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:37<00:24,  3.49it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:37<00:23,  3.49it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:38<00:23,  3.51it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:38<00:22,  3.51it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:39<00:21,  3.51it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:40<00:21,  3.51it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:40<00:20,  3.52it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:41<00:20,  3.53it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:41<00:19,  3.54it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:42<00:18,  3.55it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:42<00:18,  3.56it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:43<00:17,  3.57it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:43<00:17,  3.57it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:44<00:16,  3.52it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:45<00:16,  3.48it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [00:45<00:15,  3.45it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [00:46<00:15,  3.44it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [00:46<00:14,  3.43it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [00:47<00:14,  3.44it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [00:48<00:13,  3.44it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [00:48<00:13,  3.44it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [00:49<00:12,  3.45it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [00:49<00:11,  3.46it/s]Running loglikelihood requests:  80%|████████  | 161/200 [00:50<00:11,  3.46it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [00:50<00:10,  3.47it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [00:51<00:10,  3.47it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [00:52<00:09,  3.48it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [00:52<00:08,  3.49it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [00:53<00:08,  3.50it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [00:53<00:07,  3.50it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [00:54<00:07,  3.51it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [00:54<00:06,  3.52it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [00:55<00:05,  3.53it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [00:56<00:05,  3.53it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [00:56<00:04,  3.54it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [00:57<00:04,  3.54it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [00:57<00:03,  3.54it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [00:58<00:03,  3.55it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [00:58<00:02,  3.58it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [00:59<00:01,  3.60it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [00:59<00:01,  3.62it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:00<00:00,  3.64it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:01<00:00,  3.69it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:01<00:00,  3.28it/s]
bootstrapping for stddev (sequential): f1_score
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:01<01:58,  1.20s/it]  2%|▏         | 2/100 [00:02<01:57,  1.20s/it]  3%|▎         | 3/100 [00:03<01:56,  1.20s/it]  4%|▍         | 4/100 [00:04<01:55,  1.20s/it]  5%|▌         | 5/100 [00:06<01:54,  1.21s/it]  6%|▌         | 6/100 [00:07<01:53,  1.20s/it]  7%|▋         | 7/100 [00:08<01:52,  1.21s/it]  8%|▊         | 8/100 [00:09<01:50,  1.21s/it]  9%|▉         | 9/100 [00:10<01:49,  1.20s/it] 10%|█         | 10/100 [00:12<01:48,  1.20s/it] 11%|█         | 11/100 [00:13<01:46,  1.20s/it] 12%|█▏        | 12/100 [00:14<01:45,  1.20s/it] 13%|█▎        | 13/100 [00:15<01:44,  1.20s/it] 14%|█▍        | 14/100 [00:16<01:42,  1.20s/it] 15%|█▌        | 15/100 [00:18<01:41,  1.20s/it] 16%|█▌        | 16/100 [00:19<01:40,  1.20s/it] 17%|█▋        | 17/100 [00:20<01:39,  1.20s/it] 18%|█▊        | 18/100 [00:21<01:37,  1.19s/it] 19%|█▉        | 19/100 [00:22<01:36,  1.20s/it] 20%|██        | 20/100 [00:23<01:35,  1.20s/it] 21%|██        | 21/100 [00:25<01:34,  1.20s/it] 22%|██▏       | 22/100 [00:26<01:33,  1.20s/it] 23%|██▎       | 23/100 [00:27<01:32,  1.20s/it] 24%|██▍       | 24/100 [00:28<01:31,  1.20s/it] 25%|██▌       | 25/100 [00:29<01:29,  1.20s/it] 26%|██▌       | 26/100 [00:31<01:28,  1.20s/it] 27%|██▋       | 27/100 [00:32<01:27,  1.20s/it] 28%|██▊       | 28/100 [00:33<01:26,  1.20s/it] 29%|██▉       | 29/100 [00:34<01:25,  1.20s/it] 30%|███       | 30/100 [00:35<01:23,  1.20s/it] 31%|███       | 31/100 [00:37<01:22,  1.20s/it] 32%|███▏      | 32/100 [00:38<01:21,  1.20s/it] 33%|███▎      | 33/100 [00:39<01:20,  1.20s/it] 34%|███▍      | 34/100 [00:40<01:19,  1.20s/it] 35%|███▌      | 35/100 [00:41<01:18,  1.20s/it] 36%|███▌      | 36/100 [00:43<01:16,  1.20s/it] 37%|███▋      | 37/100 [00:44<01:15,  1.20s/it] 38%|███▊      | 38/100 [00:45<01:14,  1.20s/it] 39%|███▉      | 39/100 [00:46<01:13,  1.20s/it] 40%|████      | 40/100 [00:47<01:11,  1.20s/it] 41%|████      | 41/100 [00:49<01:10,  1.20s/it] 42%|████▏     | 42/100 [00:50<01:09,  1.20s/it] 43%|████▎     | 43/100 [00:51<01:08,  1.20s/it] 44%|████▍     | 44/100 [00:52<01:07,  1.20s/it] 45%|████▌     | 45/100 [00:53<01:05,  1.20s/it] 46%|████▌     | 46/100 [00:55<01:04,  1.20s/it] 47%|████▋     | 47/100 [00:56<01:03,  1.20s/it] 48%|████▊     | 48/100 [00:57<01:02,  1.20s/it] 49%|████▉     | 49/100 [00:58<01:01,  1.20s/it] 50%|█████     | 50/100 [00:59<00:59,  1.20s/it] 51%|█████     | 51/100 [01:01<00:58,  1.20s/it] 52%|█████▏    | 52/100 [01:02<00:57,  1.20s/it] 53%|█████▎    | 53/100 [01:03<00:56,  1.20s/it] 54%|█████▍    | 54/100 [01:04<00:55,  1.20s/it] 55%|█████▌    | 55/100 [01:05<00:53,  1.20s/it] 56%|█████▌    | 56/100 [01:07<00:52,  1.20s/it] 57%|█████▋    | 57/100 [01:08<00:51,  1.20s/it] 58%|█████▊    | 58/100 [01:09<00:50,  1.20s/it] 59%|█████▉    | 59/100 [01:10<00:49,  1.20s/it] 60%|██████    | 60/100 [01:11<00:47,  1.20s/it] 61%|██████    | 61/100 [01:13<00:46,  1.20s/it] 62%|██████▏   | 62/100 [01:14<00:45,  1.20s/it] 63%|██████▎   | 63/100 [01:15<00:44,  1.20s/it] 64%|██████▍   | 64/100 [01:16<00:43,  1.20s/it] 65%|██████▌   | 65/100 [01:17<00:41,  1.20s/it] 66%|██████▌   | 66/100 [01:19<00:40,  1.20s/it] 67%|██████▋   | 67/100 [01:20<00:39,  1.20s/it] 68%|██████▊   | 68/100 [01:21<00:38,  1.20s/it] 69%|██████▉   | 69/100 [01:22<00:37,  1.20s/it] 70%|███████   | 70/100 [01:23<00:35,  1.20s/it] 71%|███████   | 71/100 [01:25<00:34,  1.20s/it] 72%|███████▏  | 72/100 [01:26<00:33,  1.20s/it] 73%|███████▎  | 73/100 [01:27<00:32,  1.20s/it] 74%|███████▍  | 74/100 [01:28<00:31,  1.20s/it] 75%|███████▌  | 75/100 [01:29<00:29,  1.20s/it] 76%|███████▌  | 76/100 [01:31<00:28,  1.19s/it] 77%|███████▋  | 77/100 [01:32<00:27,  1.19s/it] 78%|███████▊  | 78/100 [01:33<00:26,  1.20s/it] 79%|███████▉  | 79/100 [01:34<00:25,  1.20s/it] 80%|████████  | 80/100 [01:35<00:23,  1.20s/it] 81%|████████  | 81/100 [01:37<00:22,  1.20s/it] 82%|████████▏ | 82/100 [01:38<00:21,  1.20s/it] 83%|████████▎ | 83/100 [01:39<00:20,  1.20s/it] 84%|████████▍ | 84/100 [01:40<00:19,  1.20s/it] 85%|████████▌ | 85/100 [01:41<00:17,  1.20s/it] 86%|████████▌ | 86/100 [01:43<00:16,  1.20s/it] 87%|████████▋ | 87/100 [01:44<00:15,  1.20s/it] 88%|████████▊ | 88/100 [01:45<00:14,  1.20s/it] 89%|████████▉ | 89/100 [01:46<00:13,  1.20s/it] 90%|█████████ | 90/100 [01:47<00:11,  1.20s/it] 91%|█████████ | 91/100 [01:49<00:10,  1.20s/it] 92%|█████████▏| 92/100 [01:50<00:09,  1.20s/it] 93%|█████████▎| 93/100 [01:51<00:08,  1.20s/it] 94%|█████████▍| 94/100 [01:52<00:07,  1.20s/it] 95%|█████████▌| 95/100 [01:53<00:05,  1.20s/it] 96%|█████████▌| 96/100 [01:55<00:04,  1.20s/it] 97%|█████████▋| 97/100 [01:56<00:03,  1.19s/it] 98%|█████████▊| 98/100 [01:57<00:02,  1.19s/it] 99%|█████████▉| 99/100 [01:58<00:01,  1.19s/it]100%|██████████| 100/100 [01:59<00:00,  1.19s/it]100%|██████████| 100/100 [01:59<00:00,  1.20s/it]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:1'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:1'}
full model:
{'qqp': {'alias': 'qqp', 'acc,none': 0.3, 'acc_stderr,none': 0.04605661864718382, 'f1,none': np.float64(0.46153846153846156), 'f1_stderr,none': 0.05409397997448054}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7956670939612598
0.9434665619842647
0.7711450328862919
0.8003111158987146
0.8660685706927677
0.6448594355779524
0.8450704433196731
0.7858702319299253
0.44571740342828275
0.8279983705061201
0.5649013775668057
0.5764430622159564
0.7600312367627909
0.4667905570890965
0.6441885742762831
0.5956490746171949
0.7561037067659027
0.6832036593513438
0.703034342774608
0.741950763385666
0.9094352412525417
0.3828457019607829
0.592762753179094
0.5983336442242687
0.5414593978662989
0.3299447012562784
0.4799857918750434
0.8524920468933548
0.713165518996795
0.7956670939612598
0.9434665619842647
0.7711450328862919
0.8003111158987146
0.8660685706927677
0.6448594355779524
0.8450704433196731
0.7858702319299253
0.44571740342828275
0.8279983705061201
0.5649013775668057
0.5764430622159564
0.7600312367627909
0.4667905570890965
0.6441885742762831
0.5956490746171949
0.7561037067659027
0.6832036593513438
0.703034342774608
0.741950763385666
0.9094352412525417
0.3828457019607829
0.592762753179094
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[5, 4, 6, 2, 7, 1, 3, 0]
tensor([5, 4, 6, 2, 7, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 4, 5, 2, 6, 1, 3, 0]
tensor([7, 4, 5, 2, 6, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 2, 7, 1, 3, 0]
tensor([5, 4, 6, 2, 7, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 4, 5, 2, 7, 3, 0, 1]
tensor([6, 4, 5, 2, 7, 3, 0, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 7, 4, 2, 5, 0, 1, 3]
tensor([6, 7, 4, 2, 5, 0, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 6, 4, 3, 7, 1, 0, 2]
tensor([5, 6, 4, 3, 7, 1, 0, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 3 to 4
done!
Normal merging for layer 5
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Normal merging for layer 6
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
done!
Normal merging for layer 7
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 8 to 31
done!
all done!
Model size: 12.1348 GB
16
cuda:1
mrpc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:49<00:49, 49.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 29.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 32.23s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: mrpc] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: mrpc] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
WARNING:lm_eval.api.task:[Task: mrpc] metric f1 is defined, but aggregation is not. using default aggregation=f1
WARNING:lm_eval.api.task:[Task: mrpc] metric f1 is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140215728668448 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140215728668448 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215728668448 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140215728668448 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140237193244752 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140237193244752 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140237193244752 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140237193244752 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of mrpc from None to 0
INFO:lm_eval.api.task:Building contexts for mrpc on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2503.20it/s]
DEBUG:lm_eval.evaluator:Task: mrpc; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<04:51,  1.47s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:22,  1.39it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:03<01:54,  1.70it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:04<01:42,  1.89it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:05<01:35,  2.00it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:06<01:31,  2.07it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:06<01:27,  2.13it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:07<01:25,  2.17it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:08<01:23,  2.20it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:09<01:21,  2.22it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:10<01:19,  2.24it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:11<01:18,  2.26it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:12<01:17,  2.27it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:13<01:15,  2.28it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:13<01:15,  2.28it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:14<01:13,  2.30it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:15<01:12,  2.31it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:16<01:10,  2.33it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:17<01:09,  2.34it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:18<01:08,  2.35it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:18<01:07,  2.36it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:19<01:06,  2.37it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:20<01:05,  2.38it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:21<01:04,  2.38it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:22<01:03,  2.37it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:23<01:03,  2.36it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:24<01:01,  2.39it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:24<01:00,  2.41it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:25<00:58,  2.42it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:26<00:57,  2.44it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:27<00:56,  2.45it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:28<00:55,  2.46it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:28<00:54,  2.47it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:29<00:53,  2.46it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:30<00:52,  2.48it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:31<00:51,  2.50it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:32<00:50,  2.52it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:32<00:49,  2.53it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:33<00:48,  2.54it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:34<00:47,  2.55it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:35<00:46,  2.56it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:35<00:45,  2.54it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:36<00:44,  2.56it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:37<00:44,  2.53it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:38<00:43,  2.54it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:39<00:42,  2.55it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:39<00:41,  2.56it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:40<00:40,  2.57it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:41<00:40,  2.57it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:42<00:38,  2.60it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:42<00:37,  2.63it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:43<00:36,  2.65it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:44<00:35,  2.67it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:45<00:34,  2.68it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:45<00:33,  2.68it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:46<00:33,  2.66it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:47<00:32,  2.65it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:48<00:32,  2.63it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:48<00:31,  2.65it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:49<00:30,  2.64it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:50<00:29,  2.65it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:51<00:28,  2.66it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:51<00:28,  2.67it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:52<00:27,  2.67it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:53<00:26,  2.68it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:54<00:25,  2.69it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:54<00:24,  2.70it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:55<00:24,  2.71it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:56<00:23,  2.71it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:57<00:22,  2.71it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:57<00:21,  2.72it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:58<00:20,  2.73it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [00:59<00:20,  2.74it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [00:59<00:19,  2.75it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:00<00:18,  2.75it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:01<00:17,  2.73it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:02<00:17,  2.71it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:02<00:16,  2.72it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:03<00:15,  2.74it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:04<00:14,  2.76it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:05<00:14,  2.77it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:05<00:13,  2.79it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:06<00:12,  2.81it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:07<00:11,  2.83it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:07<00:10,  2.82it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:08<00:10,  2.86it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:09<00:09,  2.90it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:09<00:08,  2.93it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:10<00:07,  2.97it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:11<00:06,  3.01it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:11<00:06,  3.03it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:12<00:05,  3.06it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:13<00:04,  3.08it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:13<00:04,  3.09it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:14<00:03,  3.13it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:15<00:02,  3.15it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:15<00:02,  3.18it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:16<00:01,  3.20it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:16<00:00,  3.22it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:17<00:00,  3.27it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:17<00:00,  2.58it/s]
bootstrapping for stddev (sequential): f1_score
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:01<01:58,  1.20s/it]  2%|▏         | 2/100 [00:02<01:57,  1.20s/it]  3%|▎         | 3/100 [00:03<01:56,  1.20s/it]  4%|▍         | 4/100 [00:04<01:56,  1.21s/it]  5%|▌         | 5/100 [00:06<01:54,  1.21s/it]  6%|▌         | 6/100 [00:07<01:53,  1.21s/it]  7%|▋         | 7/100 [00:08<01:51,  1.20s/it]  8%|▊         | 8/100 [00:09<01:50,  1.20s/it]  9%|▉         | 9/100 [00:10<01:49,  1.20s/it] 10%|█         | 10/100 [00:12<01:48,  1.20s/it] 11%|█         | 11/100 [00:13<01:46,  1.20s/it] 12%|█▏        | 12/100 [00:14<01:46,  1.21s/it] 13%|█▎        | 13/100 [00:15<01:44,  1.20s/it] 14%|█▍        | 14/100 [00:16<01:43,  1.20s/it] 15%|█▌        | 15/100 [00:18<01:41,  1.20s/it] 16%|█▌        | 16/100 [00:19<01:40,  1.20s/it] 17%|█▋        | 17/100 [00:20<01:40,  1.21s/it] 18%|█▊        | 18/100 [00:21<01:38,  1.21s/it] 19%|█▉        | 19/100 [00:22<01:37,  1.20s/it] 20%|██        | 20/100 [00:24<01:36,  1.20s/it] 21%|██        | 21/100 [00:25<01:35,  1.20s/it] 22%|██▏       | 22/100 [00:26<01:33,  1.20s/it] 23%|██▎       | 23/100 [00:27<01:32,  1.20s/it] 24%|██▍       | 24/100 [00:28<01:31,  1.20s/it] 25%|██▌       | 25/100 [00:30<01:30,  1.21s/it] 26%|██▌       | 26/100 [00:31<01:29,  1.21s/it] 27%|██▋       | 27/100 [00:32<01:27,  1.21s/it] 28%|██▊       | 28/100 [00:33<01:26,  1.20s/it] 29%|██▉       | 29/100 [00:34<01:26,  1.21s/it] 30%|███       | 30/100 [00:36<01:24,  1.21s/it] 31%|███       | 31/100 [00:37<01:23,  1.21s/it] 32%|███▏      | 32/100 [00:38<01:21,  1.21s/it] 33%|███▎      | 33/100 [00:39<01:20,  1.21s/it] 34%|███▍      | 34/100 [00:40<01:19,  1.20s/it] 35%|███▌      | 35/100 [00:42<01:18,  1.20s/it] 36%|███▌      | 36/100 [00:43<01:16,  1.20s/it] 37%|███▋      | 37/100 [00:44<01:15,  1.20s/it] 38%|███▊      | 38/100 [00:45<01:14,  1.20s/it] 39%|███▉      | 39/100 [00:46<01:13,  1.20s/it] 40%|████      | 40/100 [00:48<01:12,  1.21s/it] 41%|████      | 41/100 [00:49<01:11,  1.21s/it] 42%|████▏     | 42/100 [00:50<01:09,  1.20s/it] 43%|████▎     | 43/100 [00:51<01:08,  1.20s/it] 44%|████▍     | 44/100 [00:52<01:07,  1.20s/it] 45%|████▌     | 45/100 [00:54<01:06,  1.20s/it] 46%|████▌     | 46/100 [00:55<01:05,  1.21s/it] 47%|████▋     | 47/100 [00:56<01:03,  1.21s/it] 48%|████▊     | 48/100 [00:57<01:03,  1.21s/it] 49%|████▉     | 49/100 [00:59<01:01,  1.21s/it] 50%|█████     | 50/100 [01:00<01:00,  1.21s/it] 51%|█████     | 51/100 [01:01<00:59,  1.21s/it] 52%|█████▏    | 52/100 [01:02<00:57,  1.21s/it] 53%|█████▎    | 53/100 [01:03<00:56,  1.21s/it] 54%|█████▍    | 54/100 [01:05<00:55,  1.21s/it] 55%|█████▌    | 55/100 [01:06<00:54,  1.21s/it] 56%|█████▌    | 56/100 [01:07<00:53,  1.21s/it] 57%|█████▋    | 57/100 [01:08<00:52,  1.21s/it] 58%|█████▊    | 58/100 [01:09<00:51,  1.23s/it] 59%|█████▉    | 59/100 [01:11<00:50,  1.22s/it] 60%|██████    | 60/100 [01:12<00:48,  1.22s/it] 61%|██████    | 61/100 [01:13<00:47,  1.21s/it] 62%|██████▏   | 62/100 [01:14<00:46,  1.21s/it] 63%|██████▎   | 63/100 [01:16<00:44,  1.21s/it] 64%|██████▍   | 64/100 [01:17<00:43,  1.21s/it] 65%|██████▌   | 65/100 [01:18<00:42,  1.21s/it] 66%|██████▌   | 66/100 [01:19<00:41,  1.21s/it] 67%|██████▋   | 67/100 [01:20<00:39,  1.21s/it] 68%|██████▊   | 68/100 [01:22<00:38,  1.21s/it] 69%|██████▉   | 69/100 [01:23<00:37,  1.21s/it] 70%|███████   | 70/100 [01:24<00:36,  1.21s/it] 71%|███████   | 71/100 [01:25<00:34,  1.21s/it] 72%|███████▏  | 72/100 [01:26<00:33,  1.20s/it] 73%|███████▎  | 73/100 [01:28<00:32,  1.20s/it] 74%|███████▍  | 74/100 [01:29<00:31,  1.20s/it] 75%|███████▌  | 75/100 [01:30<00:30,  1.21s/it] 76%|███████▌  | 76/100 [01:31<00:28,  1.20s/it] 77%|███████▋  | 77/100 [01:32<00:27,  1.21s/it] 78%|███████▊  | 78/100 [01:34<00:26,  1.21s/it] 79%|███████▉  | 79/100 [01:35<00:25,  1.21s/it] 80%|████████  | 80/100 [01:36<00:24,  1.21s/it] 81%|████████  | 81/100 [01:37<00:22,  1.21s/it] 82%|████████▏ | 82/100 [01:38<00:21,  1.21s/it] 83%|████████▎ | 83/100 [01:40<00:20,  1.20s/it] 84%|████████▍ | 84/100 [01:41<00:19,  1.20s/it] 85%|████████▌ | 85/100 [01:42<00:18,  1.20s/it] 86%|████████▌ | 86/100 [01:43<00:16,  1.20s/it] 87%|████████▋ | 87/100 [01:44<00:15,  1.20s/it] 88%|████████▊ | 88/100 [01:46<00:14,  1.20s/it] 89%|████████▉ | 89/100 [01:47<00:13,  1.21s/it] 90%|█████████ | 90/100 [01:48<00:12,  1.21s/it] 91%|█████████ | 91/100 [01:49<00:10,  1.21s/it] 92%|█████████▏| 92/100 [01:50<00:09,  1.21s/it] 93%|█████████▎| 93/100 [01:52<00:08,  1.21s/it] 94%|█████████▍| 94/100 [01:53<00:07,  1.20s/it] 95%|█████████▌| 95/100 [01:54<00:06,  1.20s/it] 96%|█████████▌| 96/100 [01:55<00:04,  1.20s/it] 97%|█████████▋| 97/100 [01:56<00:03,  1.20s/it] 98%|█████████▊| 98/100 [01:58<00:02,  1.20s/it] 99%|█████████▉| 99/100 [01:59<00:01,  1.20s/it]100%|██████████| 100/100 [02:00<00:00,  1.20s/it]100%|██████████| 100/100 [02:00<00:00,  1.21s/it]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:2'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:2'}
full model:
{'mrpc': {'alias': 'mrpc', 'acc,none': 0.7, 'acc_stderr,none': 0.04605661864718383, 'f1,none': np.float64(0.8214285714285714), 'f1_stderr,none': 0.0323693653386572}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.32375514242269293
0.9664141198776434
0.4263233992087649
0.29258192731768157
0.3497718342079178
0.5968480671674146
0.7870107004084098
0.7068508532201193
0.7836897379939763
0.2579819071737284
0.5774144903645764
0.7819348699339372
0.7404680063618143
0.651835611327285
0.6832243011482585
0.34680079188801705
0.6033476505145444
0.6299526106979766
0.6468468631829007
0.8458578364778718
0.39447753640727073
0.5291234791393744
0.944540808938638
0.8460909093145242
0.4915397062049404
0.5314721523201049
0.8670806018560002
0.6451914961665037
0.7380077491787214
0.32375514242269293
0.9664141198776434
0.4263233992087649
0.29258192731768157
0.3497718342079178
0.5968480671674146
0.7870107004084098
0.7068508532201193
0.7836897379939763
0.2579819071737284
0.5774144903645764
0.7819348699339372
0.7404680063618143
0.651835611327285
0.6832243011482585
0.34680079188801705
0.6033476505145444
0.6299526106979766
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[4, 5, 7, 1, 6, 2, 3, 0]
tensor([4, 5, 7, 1, 6, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 4, 5, 0, 1, 2]
tensor([6, 3, 7, 4, 5, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 1, 7, 2, 6, 0, 3, 5]
tensor([4, 1, 7, 2, 6, 0, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 5, 2, 7, 0, 4, 1]
tensor([6, 3, 5, 2, 7, 0, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 5, 6, 7, 3, 1, 0, 2]
tensor([4, 5, 6, 7, 3, 1, 0, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 2, 4, 1, 0, 5]
tensor([0, 3, 1, 2, 4, 1, 0, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1, 1.0]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 2
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 4 to 5
done!
Normal merging for layer 6
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
done!
Cross-layer merge completed for layers 7 to 9
done!
Normal merging for layer 10
tensor([0, 6])
tensor(0)
tensor([2, 5])
tensor(2)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([7])
tensor(7)
done!
Cross-layer merge completed for layers 11 to 26
done!
Normal merging for layer 27
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 28 to 30
done!
Normal merging for layer 31
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
all done!
Model size: 12.3867 GB
167
cuda:2
boolq
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:02<00:00, 28.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:02<00:00, 31.36s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/boolq?recursive=False&expand=False HTTP/1.1" 307 144
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/boolq?recursive=False&expand=False HTTP/1.1" 200 355
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:filelock:Attempting to acquire lock 140237735260192 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_boolq_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140237735260192 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_boolq_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140237735260192 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_boolq_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140237735260192 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_boolq_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140238669121008 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140238669121008 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140238669121008 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140238669121008 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of boolq from None to 0
INFO:lm_eval.api.task:Building contexts for boolq on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2588.80it/s]
DEBUG:lm_eval.evaluator:Task: boolq; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:04<15:07,  4.56s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:06<06:13,  1.90s/it]Running loglikelihood requests:   2%|▎         | 5/200 [00:08<04:34,  1.41s/it]Running loglikelihood requests:   4%|▎         | 7/200 [00:10<03:51,  1.20s/it]Running loglikelihood requests:   4%|▍         | 9/200 [00:11<03:25,  1.08s/it]Running loglikelihood requests:   6%|▌         | 11/200 [00:13<03:06,  1.01it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:15<02:52,  1.09it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:16<02:42,  1.14it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:18<02:33,  1.19it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:19<02:27,  1.22it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:21<02:24,  1.24it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:22<02:19,  1.26it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:24<02:15,  1.29it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:25<02:12,  1.31it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:27<02:10,  1.31it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:28<02:06,  1.34it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:30<02:02,  1.36it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:31<01:59,  1.38it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:33<01:57,  1.39it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:34<01:55,  1.39it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:35<01:53,  1.40it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:37<01:51,  1.40it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:38<01:49,  1.42it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:39<01:46,  1.44it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:41<01:43,  1.45it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:42<01:41,  1.47it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:43<01:39,  1.48it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:45<01:37,  1.48it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:46<01:35,  1.50it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:47<01:33,  1.51it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:49<01:31,  1.51it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:50<01:29,  1.53it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:51<01:27,  1.53it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:53<01:26,  1.54it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:54<01:23,  1.56it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:55<01:21,  1.59it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:56<01:18,  1.61it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:57<01:16,  1.64it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:59<01:14,  1.66it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [01:00<01:12,  1.68it/s]Running loglikelihood requests:  40%|████      | 81/200 [01:01<01:10,  1.69it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [01:02<01:08,  1.70it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [01:03<01:07,  1.71it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [01:04<01:05,  1.72it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [01:06<01:04,  1.73it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [01:07<01:02,  1.74it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [01:08<01:01,  1.74it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [01:09<00:59,  1.75it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [01:10<00:58,  1.76it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [01:11<00:57,  1.77it/s]Running loglikelihood requests:  50%|█████     | 101/200 [01:12<00:55,  1.78it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [01:13<00:54,  1.79it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [01:14<00:52,  1.80it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [01:16<00:51,  1.81it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [01:17<00:50,  1.80it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [01:18<00:49,  1.81it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [01:19<00:47,  1.82it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [01:20<00:46,  1.83it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [01:21<00:45,  1.84it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [01:22<00:43,  1.85it/s]Running loglikelihood requests:  60%|██████    | 121/200 [01:23<00:42,  1.87it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [01:24<00:40,  1.90it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [01:25<00:39,  1.92it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [01:26<00:37,  1.93it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [01:27<00:35,  1.99it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [01:28<00:33,  2.03it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [01:29<00:32,  2.07it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [01:30<00:30,  2.10it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [01:31<00:29,  2.12it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [01:32<00:28,  2.12it/s]Running loglikelihood requests:  70%|███████   | 141/200 [01:33<00:27,  2.13it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [01:34<00:26,  2.17it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:34<00:25,  2.19it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [01:35<00:23,  2.21it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:36<00:22,  2.23it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:37<00:21,  2.24it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:38<00:20,  2.25it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:39<00:20,  2.23it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:40<00:19,  2.26it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:41<00:17,  2.29it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:41<00:16,  2.31it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:42<00:15,  2.32it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:43<00:14,  2.35it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:44<00:13,  2.38it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:45<00:12,  2.39it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:46<00:11,  2.43it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:46<00:10,  2.48it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:47<00:10,  2.50it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:48<00:09,  2.52it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:49<00:08,  2.54it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:49<00:07,  2.59it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:50<00:06,  2.61it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:51<00:05,  2.65it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:52<00:04,  2.70it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:52<00:04,  2.72it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:53<00:03,  2.77it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:54<00:02,  2.81it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:54<00:01,  2.87it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:55<00:01,  3.00it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:55<00:00,  3.17it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:55<00:00,  1.72it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:3'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:3'}
full model:
{'boolq': {'alias': 'boolq', 'acc,none': 0.67, 'acc_stderr,none': 0.04725815626252609}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9782993007407264
0.4824830207482711
0.6583529368570694
0.8822475410601947
0.3059812833421515
0.7645942683746021
0.5411564675926485
0.6399758236302138
0.752913626333875
0.9170504110771489
0.8755679311709376
0.9130694495301263
0.6399088029710222
0.5907791688883935
0.8704540476128766
0.484807489124531
0.7579322019225017
0.8465026175931075
0.8104840653949666
0.671147278193032
0.7709951349967222
0.532915988335396
0.6066099270395096
0.5511989097245372
0.4671998655475952
0.6078287002452507
0.3992240879306912
0.5299030614769079
0.5709371890677749
0.9782993007407264
0.4824830207482711
0.6583529368570694
0.8822475410601947
0.3059812833421515
0.7645942683746021
0.5411564675926485
0.6399758236302138
0.752913626333875
0.9170504110771489
0.8755679311709376
0.9130694495301263
0.6399088029710222
0.5907791688883935
0.8704540476128766
0.484807489124531
0.7579322019225017
0.8465026175931075
0.8104840653949666
0.671147278193032
0.7709951349967222
0.532915988335396
0.6066099270395096
0.5511989097245372
0.4671998655475952
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[6, 2, 7, 0, 5, 3, 4, 1]
tensor([6, 2, 7, 0, 5, 3, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 2, 3, 0, 6, 1, 4, 5]
tensor([7, 2, 3, 0, 6, 1, 4, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 1, 7, 0, 6, 4, 2, 3]
tensor([5, 1, 7, 0, 6, 4, 2, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 3, 5, 1, 7, 2, 6, 0]
tensor([4, 3, 5, 1, 7, 2, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 6, 7, 3, 4, 5, 0, 2]
tensor([1, 6, 7, 3, 4, 5, 0, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 4, 1, 7, 2, 5, 0]
tensor([6, 3, 4, 1, 7, 2, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 2
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
done!
Normal merging for layer 4
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
done!
Normal merging for layer 5
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 6 to 31
done!
all done!
Model size: 12.0718 GB
250
cuda:3
openbookqa
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:44<00:44, 44.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:57<00:00, 26.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:57<00:00, 28.81s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa HTTP/1.1" 307 67
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa HTTP/1.1" 200 1408
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/openbookqa/openbookqa.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa HTTP/1.1" 307 67
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa HTTP/1.1" 200 1408
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/openbookqa/resolve/388097ea7776314e93a529163e0fea805b8a6454/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/openbookqa/resolve/388097ea7776314e93a529163e0fea805b8a6454/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa/revision/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 117
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa/revision/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 1408
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa/tree/388097ea7776314e93a529163e0fea805b8a6454?recursive=False&expand=False HTTP/1.1" 307 142
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa/tree/388097ea7776314e93a529163e0fea805b8a6454?recursive=False&expand=False HTTP/1.1" 200 390
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa/tree/388097ea7776314e93a529163e0fea805b8a6454/additional?recursive=False&expand=False HTTP/1.1" 307 153
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa/tree/388097ea7776314e93a529163e0fea805b8a6454/additional?recursive=False&expand=False HTTP/1.1" 200 363
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa/revision/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 117
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa/revision/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 1408
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/openbookqa/resolve/388097ea7776314e93a529163e0fea805b8a6454/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/openbookqa/resolve/388097ea7776314e93a529163e0fea805b8a6454/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa/tree/388097ea7776314e93a529163e0fea805b8a6454/main?recursive=False&expand=False HTTP/1.1" 307 147
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa/tree/388097ea7776314e93a529163e0fea805b8a6454/main?recursive=False&expand=False HTTP/1.1" 200 359
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140237735831520 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_openbookqa_main_0.0.0_388097ea7776314e93a529163e0fea805b8a6454.lock
DEBUG:filelock:Lock 140237735831520 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_openbookqa_main_0.0.0_388097ea7776314e93a529163e0fea805b8a6454.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454/dataset_info.json
DEBUG:filelock:Attempting to release lock 140237735831520 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_openbookqa_main_0.0.0_388097ea7776314e93a529163e0fea805b8a6454.lock
DEBUG:filelock:Lock 140237735831520 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_openbookqa_main_0.0.0_388097ea7776314e93a529163e0fea805b8a6454.lock
DEBUG:filelock:Attempting to acquire lock 140229612268976 on /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454_builder.lock
DEBUG:filelock:Lock 140229612268976 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454/dataset_info.json
DEBUG:filelock:Attempting to release lock 140229612268976 on /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454_builder.lock
DEBUG:filelock:Lock 140229612268976 released on /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of openbookqa from None to 0
INFO:lm_eval.api.task:Building contexts for openbookqa on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2734.16it/s]
DEBUG:lm_eval.evaluator:Task: openbookqa; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:01<08:01,  1.21s/it]Running loglikelihood requests:   0%|          | 2/400 [00:01<06:06,  1.09it/s]Running loglikelihood requests:   1%|          | 3/400 [00:02<05:26,  1.22it/s]Running loglikelihood requests:   1%|          | 4/400 [00:03<05:05,  1.29it/s]Running loglikelihood requests:   1%|▏         | 5/400 [00:03<04:36,  1.43it/s]Running loglikelihood requests:   2%|▏         | 6/400 [00:04<04:17,  1.53it/s]Running loglikelihood requests:   2%|▏         | 7/400 [00:05<04:05,  1.60it/s]Running loglikelihood requests:   2%|▏         | 8/400 [00:05<03:55,  1.66it/s]Running loglikelihood requests:   2%|▏         | 9/400 [00:06<03:48,  1.71it/s]Running loglikelihood requests:   2%|▎         | 10/400 [00:06<03:43,  1.75it/s]Running loglikelihood requests:   3%|▎         | 11/400 [00:07<03:39,  1.77it/s]Running loglikelihood requests:   3%|▎         | 12/400 [00:07<03:36,  1.79it/s]Running loglikelihood requests:   3%|▎         | 13/400 [00:08<03:34,  1.81it/s]Running loglikelihood requests:   4%|▍         | 15/400 [00:08<02:42,  2.37it/s]Running loglikelihood requests:   4%|▍         | 16/400 [00:09<02:53,  2.21it/s]Running loglikelihood requests:   4%|▍         | 17/400 [00:09<03:01,  2.11it/s]Running loglikelihood requests:   4%|▍         | 18/400 [00:10<03:12,  1.98it/s]Running loglikelihood requests:   5%|▍         | 19/400 [00:11<03:14,  1.96it/s]Running loglikelihood requests:   5%|▌         | 20/400 [00:11<03:15,  1.95it/s]Running loglikelihood requests:   5%|▌         | 21/400 [00:12<03:15,  1.94it/s]Running loglikelihood requests:   6%|▌         | 22/400 [00:12<03:14,  1.94it/s]Running loglikelihood requests:   6%|▌         | 23/400 [00:13<03:14,  1.94it/s]Running loglikelihood requests:   6%|▌         | 24/400 [00:13<03:13,  1.94it/s]Running loglikelihood requests:   6%|▋         | 25/400 [00:14<03:12,  1.94it/s]Running loglikelihood requests:   6%|▋         | 26/400 [00:14<03:11,  1.95it/s]Running loglikelihood requests:   7%|▋         | 27/400 [00:15<03:10,  1.96it/s]Running loglikelihood requests:   7%|▋         | 28/400 [00:15<03:09,  1.97it/s]Running loglikelihood requests:   7%|▋         | 29/400 [00:16<03:07,  1.98it/s]Running loglikelihood requests:   8%|▊         | 33/400 [00:16<01:36,  3.80it/s]Running loglikelihood requests:   8%|▊         | 34/400 [00:17<01:51,  3.30it/s]Running loglikelihood requests:   9%|▉         | 35/400 [00:17<02:03,  2.96it/s]Running loglikelihood requests:   9%|▉         | 36/400 [00:18<02:14,  2.71it/s]Running loglikelihood requests:   9%|▉         | 37/400 [00:18<02:23,  2.54it/s]Running loglikelihood requests:  10%|▉         | 38/400 [00:19<02:30,  2.41it/s]Running loglikelihood requests:  10%|▉         | 39/400 [00:19<02:34,  2.33it/s]Running loglikelihood requests:  10%|█         | 40/400 [00:19<02:38,  2.27it/s]Running loglikelihood requests:  10%|█         | 41/400 [00:20<02:40,  2.24it/s]Running loglikelihood requests:  10%|█         | 42/400 [00:20<02:41,  2.21it/s]Running loglikelihood requests:  11%|█         | 43/400 [00:21<02:42,  2.19it/s]Running loglikelihood requests:  11%|█         | 44/400 [00:21<02:43,  2.18it/s]Running loglikelihood requests:  11%|█▏        | 45/400 [00:22<02:43,  2.17it/s]Running loglikelihood requests:  12%|█▏        | 46/400 [00:22<02:43,  2.16it/s]Running loglikelihood requests:  12%|█▏        | 47/400 [00:23<02:43,  2.16it/s]Running loglikelihood requests:  12%|█▏        | 48/400 [00:23<02:42,  2.17it/s]Running loglikelihood requests:  12%|█▏        | 49/400 [00:24<02:41,  2.17it/s]Running loglikelihood requests:  12%|█▎        | 50/400 [00:24<02:40,  2.18it/s]Running loglikelihood requests:  13%|█▎        | 51/400 [00:25<02:40,  2.18it/s]Running loglikelihood requests:  13%|█▎        | 52/400 [00:25<02:39,  2.18it/s]Running loglikelihood requests:  13%|█▎        | 53/400 [00:25<02:38,  2.19it/s]Running loglikelihood requests:  14%|█▎        | 54/400 [00:26<02:37,  2.19it/s]Running loglikelihood requests:  14%|█▍        | 55/400 [00:26<02:37,  2.19it/s]Running loglikelihood requests:  14%|█▍        | 57/400 [00:27<02:00,  2.86it/s]Running loglikelihood requests:  15%|█▌        | 60/400 [00:27<01:25,  3.99it/s]Running loglikelihood requests:  15%|█▌        | 61/400 [00:28<01:37,  3.47it/s]Running loglikelihood requests:  16%|█▌        | 62/400 [00:28<01:49,  3.10it/s]Running loglikelihood requests:  16%|█▌        | 63/400 [00:29<01:58,  2.84it/s]Running loglikelihood requests:  16%|█▌        | 64/400 [00:29<02:06,  2.66it/s]Running loglikelihood requests:  16%|█▋        | 65/400 [00:29<02:12,  2.53it/s]Running loglikelihood requests:  16%|█▋        | 66/400 [00:30<02:17,  2.44it/s]Running loglikelihood requests:  17%|█▋        | 67/400 [00:30<02:21,  2.35it/s]Running loglikelihood requests:  17%|█▋        | 68/400 [00:31<02:23,  2.31it/s]Running loglikelihood requests:  17%|█▋        | 69/400 [00:31<02:24,  2.29it/s]Running loglikelihood requests:  18%|█▊        | 70/400 [00:32<02:25,  2.27it/s]Running loglikelihood requests:  18%|█▊        | 71/400 [00:32<02:25,  2.26it/s]Running loglikelihood requests:  18%|█▊        | 72/400 [00:33<02:25,  2.25it/s]Running loglikelihood requests:  18%|█▊        | 73/400 [00:33<02:25,  2.25it/s]Running loglikelihood requests:  18%|█▊        | 74/400 [00:34<02:24,  2.26it/s]Running loglikelihood requests:  19%|█▉        | 75/400 [00:34<02:24,  2.25it/s]Running loglikelihood requests:  19%|█▉        | 76/400 [00:34<02:23,  2.25it/s]Running loglikelihood requests:  19%|█▉        | 77/400 [00:35<02:23,  2.25it/s]Running loglikelihood requests:  20%|█▉        | 78/400 [00:35<02:23,  2.25it/s]Running loglikelihood requests:  20%|█▉        | 79/400 [00:36<02:22,  2.26it/s]Running loglikelihood requests:  20%|██        | 81/400 [00:36<01:48,  2.94it/s]Running loglikelihood requests:  20%|██        | 82/400 [00:37<01:56,  2.74it/s]Running loglikelihood requests:  21%|██        | 83/400 [00:37<02:01,  2.60it/s]Running loglikelihood requests:  21%|██        | 84/400 [00:38<02:06,  2.50it/s]Running loglikelihood requests:  22%|██▏       | 86/400 [00:38<01:40,  3.12it/s]Running loglikelihood requests:  22%|██▏       | 87/400 [00:38<01:48,  2.87it/s]Running loglikelihood requests:  22%|██▏       | 88/400 [00:39<01:55,  2.69it/s]Running loglikelihood requests:  22%|██▏       | 89/400 [00:39<02:01,  2.57it/s]Running loglikelihood requests:  22%|██▎       | 90/400 [00:40<02:04,  2.48it/s]Running loglikelihood requests:  23%|██▎       | 91/400 [00:40<02:07,  2.42it/s]Running loglikelihood requests:  23%|██▎       | 92/400 [00:41<02:09,  2.38it/s]Running loglikelihood requests:  23%|██▎       | 93/400 [00:41<02:10,  2.35it/s]Running loglikelihood requests:  24%|██▎       | 94/400 [00:41<02:11,  2.33it/s]Running loglikelihood requests:  24%|██▍       | 95/400 [00:42<02:11,  2.31it/s]Running loglikelihood requests:  24%|██▍       | 96/400 [00:42<02:12,  2.30it/s]Running loglikelihood requests:  24%|██▍       | 97/400 [00:43<02:12,  2.29it/s]Running loglikelihood requests:  24%|██▍       | 98/400 [00:43<02:12,  2.29it/s]Running loglikelihood requests:  25%|██▍       | 99/400 [00:44<02:11,  2.29it/s]Running loglikelihood requests:  25%|██▌       | 100/400 [00:44<02:11,  2.28it/s]Running loglikelihood requests:  25%|██▌       | 101/400 [00:45<02:10,  2.29it/s]Running loglikelihood requests:  26%|██▌       | 102/400 [00:45<02:10,  2.29it/s]Running loglikelihood requests:  26%|██▌       | 103/400 [00:45<02:09,  2.30it/s]Running loglikelihood requests:  26%|██▋       | 106/400 [00:46<01:19,  3.69it/s]Running loglikelihood requests:  27%|██▋       | 107/400 [00:46<01:29,  3.28it/s]Running loglikelihood requests:  27%|██▋       | 108/400 [00:47<01:38,  2.98it/s]Running loglikelihood requests:  27%|██▋       | 109/400 [00:47<01:45,  2.77it/s]Running loglikelihood requests:  28%|██▊       | 110/400 [00:48<01:50,  2.63it/s]Running loglikelihood requests:  28%|██▊       | 111/400 [00:48<01:54,  2.53it/s]Running loglikelihood requests:  28%|██▊       | 112/400 [00:48<01:57,  2.46it/s]Running loglikelihood requests:  28%|██▊       | 113/400 [00:49<01:59,  2.41it/s]Running loglikelihood requests:  28%|██▊       | 114/400 [00:49<02:00,  2.37it/s]Running loglikelihood requests:  29%|██▉       | 115/400 [00:50<02:01,  2.35it/s]Running loglikelihood requests:  29%|██▉       | 116/400 [00:50<02:02,  2.31it/s]Running loglikelihood requests:  29%|██▉       | 117/400 [00:51<02:03,  2.28it/s]Running loglikelihood requests:  30%|██▉       | 118/400 [00:51<02:02,  2.30it/s]Running loglikelihood requests:  30%|██▉       | 119/400 [00:52<02:01,  2.31it/s]Running loglikelihood requests:  30%|███       | 120/400 [00:52<02:01,  2.30it/s]Running loglikelihood requests:  30%|███       | 121/400 [00:52<02:00,  2.31it/s]Running loglikelihood requests:  30%|███       | 122/400 [00:53<02:00,  2.31it/s]Running loglikelihood requests:  31%|███       | 123/400 [00:53<02:00,  2.30it/s]Running loglikelihood requests:  31%|███       | 124/400 [00:54<01:59,  2.31it/s]Running loglikelihood requests:  31%|███▏      | 125/400 [00:54<01:59,  2.31it/s]Running loglikelihood requests:  32%|███▏      | 126/400 [00:55<01:58,  2.31it/s]Running loglikelihood requests:  32%|███▏      | 127/400 [00:55<01:57,  2.32it/s]Running loglikelihood requests:  32%|███▏      | 128/400 [00:55<01:57,  2.31it/s]Running loglikelihood requests:  32%|███▏      | 129/400 [00:56<01:57,  2.30it/s]Running loglikelihood requests:  32%|███▎      | 130/400 [00:56<01:57,  2.30it/s]Running loglikelihood requests:  33%|███▎      | 131/400 [00:57<01:56,  2.31it/s]Running loglikelihood requests:  33%|███▎      | 132/400 [00:57<01:55,  2.31it/s]Running loglikelihood requests:  33%|███▎      | 133/400 [00:58<01:55,  2.32it/s]Running loglikelihood requests:  34%|███▎      | 134/400 [00:58<01:53,  2.33it/s]Running loglikelihood requests:  34%|███▍      | 138/400 [00:58<00:59,  4.44it/s]Running loglikelihood requests:  35%|███▍      | 139/400 [00:59<01:08,  3.81it/s]Running loglikelihood requests:  35%|███▌      | 140/400 [00:59<01:17,  3.37it/s]Running loglikelihood requests:  35%|███▌      | 141/400 [01:00<01:24,  3.06it/s]Running loglikelihood requests:  36%|███▌      | 142/400 [01:00<01:30,  2.85it/s]Running loglikelihood requests:  36%|███▌      | 143/400 [01:01<01:35,  2.70it/s]Running loglikelihood requests:  36%|███▌      | 144/400 [01:01<01:38,  2.60it/s]Running loglikelihood requests:  36%|███▋      | 145/400 [01:01<01:40,  2.53it/s]Running loglikelihood requests:  37%|███▋      | 147/400 [01:02<01:19,  3.19it/s]Running loglikelihood requests:  37%|███▋      | 148/400 [01:02<01:25,  2.95it/s]Running loglikelihood requests:  37%|███▋      | 149/400 [01:03<01:29,  2.79it/s]Running loglikelihood requests:  38%|███▊      | 151/400 [01:03<01:13,  3.39it/s]Running loglikelihood requests:  38%|███▊      | 152/400 [01:03<01:20,  3.08it/s]Running loglikelihood requests:  38%|███▊      | 153/400 [01:04<01:25,  2.88it/s]Running loglikelihood requests:  38%|███▊      | 154/400 [01:04<01:30,  2.73it/s]Running loglikelihood requests:  39%|███▉      | 155/400 [01:05<01:33,  2.63it/s]Running loglikelihood requests:  39%|███▉      | 156/400 [01:05<01:35,  2.56it/s]Running loglikelihood requests:  39%|███▉      | 157/400 [01:06<01:36,  2.51it/s]Running loglikelihood requests:  40%|███▉      | 158/400 [01:06<01:37,  2.47it/s]Running loglikelihood requests:  40%|███▉      | 159/400 [01:06<01:38,  2.45it/s]Running loglikelihood requests:  40%|████      | 160/400 [01:07<01:38,  2.44it/s]Running loglikelihood requests:  40%|████      | 161/400 [01:07<01:38,  2.42it/s]Running loglikelihood requests:  40%|████      | 162/400 [01:08<01:38,  2.41it/s]Running loglikelihood requests:  41%|████      | 163/400 [01:08<01:38,  2.42it/s]Running loglikelihood requests:  42%|████▏     | 166/400 [01:08<01:00,  3.86it/s]Running loglikelihood requests:  42%|████▏     | 167/400 [01:09<01:07,  3.43it/s]Running loglikelihood requests:  43%|████▎     | 171/400 [01:09<00:43,  5.29it/s]Running loglikelihood requests:  43%|████▎     | 172/400 [01:10<00:51,  4.44it/s]Running loglikelihood requests:  43%|████▎     | 173/400 [01:10<00:59,  3.82it/s]Running loglikelihood requests:  44%|████▎     | 174/400 [01:11<01:06,  3.40it/s]Running loglikelihood requests:  44%|████▍     | 175/400 [01:11<01:12,  3.11it/s]Running loglikelihood requests:  44%|████▍     | 176/400 [01:11<01:16,  2.91it/s]Running loglikelihood requests:  44%|████▍     | 177/400 [01:12<01:20,  2.77it/s]Running loglikelihood requests:  44%|████▍     | 178/400 [01:12<01:23,  2.67it/s]Running loglikelihood requests:  45%|████▍     | 179/400 [01:13<01:25,  2.58it/s]Running loglikelihood requests:  45%|████▌     | 180/400 [01:13<01:26,  2.54it/s]Running loglikelihood requests:  45%|████▌     | 181/400 [01:13<01:27,  2.50it/s]Running loglikelihood requests:  46%|████▌     | 182/400 [01:14<01:28,  2.47it/s]Running loglikelihood requests:  46%|████▌     | 183/400 [01:14<01:28,  2.45it/s]Running loglikelihood requests:  46%|████▌     | 184/400 [01:15<01:27,  2.46it/s]Running loglikelihood requests:  46%|████▋     | 185/400 [01:15<01:27,  2.45it/s]Running loglikelihood requests:  46%|████▋     | 186/400 [01:16<01:27,  2.45it/s]Running loglikelihood requests:  47%|████▋     | 187/400 [01:16<01:26,  2.45it/s]Running loglikelihood requests:  47%|████▋     | 188/400 [01:16<01:26,  2.45it/s]Running loglikelihood requests:  48%|████▊     | 190/400 [01:17<01:05,  3.19it/s]Running loglikelihood requests:  48%|████▊     | 192/400 [01:17<00:56,  3.71it/s]Running loglikelihood requests:  48%|████▊     | 193/400 [01:18<01:02,  3.33it/s]Running loglikelihood requests:  48%|████▊     | 194/400 [01:18<01:07,  3.06it/s]Running loglikelihood requests:  49%|████▉     | 195/400 [01:18<01:11,  2.88it/s]Running loglikelihood requests:  49%|████▉     | 196/400 [01:19<01:14,  2.75it/s]Running loglikelihood requests:  49%|████▉     | 197/400 [01:19<01:16,  2.66it/s]Running loglikelihood requests:  50%|████▉     | 198/400 [01:20<01:18,  2.58it/s]Running loglikelihood requests:  50%|█████     | 202/400 [01:20<00:41,  4.75it/s]Running loglikelihood requests:  51%|█████     | 203/400 [01:20<00:48,  4.08it/s]Running loglikelihood requests:  51%|█████     | 204/400 [01:21<00:54,  3.62it/s]Running loglikelihood requests:  51%|█████▏    | 205/400 [01:21<00:59,  3.30it/s]Running loglikelihood requests:  52%|█████▏    | 206/400 [01:22<01:03,  3.06it/s]Running loglikelihood requests:  52%|█████▏    | 207/400 [01:22<01:06,  2.89it/s]Running loglikelihood requests:  52%|█████▏    | 209/400 [01:22<00:54,  3.53it/s]Running loglikelihood requests:  52%|█████▎    | 210/400 [01:23<00:58,  3.23it/s]Running loglikelihood requests:  53%|█████▎    | 211/400 [01:23<01:03,  3.00it/s]Running loglikelihood requests:  53%|█████▎    | 212/400 [01:24<01:06,  2.83it/s]Running loglikelihood requests:  53%|█████▎    | 213/400 [01:24<01:08,  2.72it/s]Running loglikelihood requests:  54%|█████▎    | 214/400 [01:24<01:10,  2.64it/s]Running loglikelihood requests:  54%|█████▍    | 215/400 [01:25<01:11,  2.59it/s]Running loglikelihood requests:  54%|█████▍    | 216/400 [01:25<01:11,  2.57it/s]Running loglikelihood requests:  54%|█████▍    | 217/400 [01:26<01:11,  2.55it/s]Running loglikelihood requests:  55%|█████▍    | 218/400 [01:26<01:11,  2.54it/s]Running loglikelihood requests:  55%|█████▍    | 219/400 [01:26<01:11,  2.53it/s]Running loglikelihood requests:  55%|█████▌    | 220/400 [01:27<01:11,  2.51it/s]Running loglikelihood requests:  55%|█████▌    | 221/400 [01:27<01:11,  2.51it/s]Running loglikelihood requests:  56%|█████▌    | 222/400 [01:28<01:10,  2.51it/s]Running loglikelihood requests:  56%|█████▌    | 223/400 [01:28<01:10,  2.51it/s]Running loglikelihood requests:  56%|█████▌    | 224/400 [01:28<01:09,  2.53it/s]Running loglikelihood requests:  56%|█████▋    | 225/400 [01:29<01:09,  2.53it/s]Running loglikelihood requests:  56%|█████▋    | 226/400 [01:29<01:08,  2.53it/s]Running loglikelihood requests:  57%|█████▋    | 227/400 [01:30<01:08,  2.52it/s]Running loglikelihood requests:  57%|█████▋    | 228/400 [01:30<01:07,  2.54it/s]Running loglikelihood requests:  57%|█████▋    | 229/400 [01:30<01:07,  2.55it/s]Running loglikelihood requests:  57%|█████▊    | 230/400 [01:31<01:06,  2.54it/s]Running loglikelihood requests:  58%|█████▊    | 231/400 [01:31<01:06,  2.53it/s]Running loglikelihood requests:  58%|█████▊    | 232/400 [01:32<01:06,  2.54it/s]Running loglikelihood requests:  58%|█████▊    | 233/400 [01:32<01:05,  2.55it/s]Running loglikelihood requests:  58%|█████▊    | 234/400 [01:32<01:05,  2.54it/s]Running loglikelihood requests:  59%|█████▉    | 235/400 [01:33<01:04,  2.55it/s]Running loglikelihood requests:  59%|█████▉    | 237/400 [01:33<00:49,  3.32it/s]Running loglikelihood requests:  60%|█████▉    | 239/400 [01:34<00:41,  3.87it/s]Running loglikelihood requests:  60%|██████    | 240/400 [01:34<00:46,  3.47it/s]Running loglikelihood requests:  60%|██████    | 241/400 [01:34<00:49,  3.20it/s]Running loglikelihood requests:  60%|██████    | 242/400 [01:35<00:52,  3.00it/s]Running loglikelihood requests:  61%|██████    | 243/400 [01:35<00:54,  2.86it/s]Running loglikelihood requests:  61%|██████    | 244/400 [01:35<00:56,  2.76it/s]Running loglikelihood requests:  61%|██████▏   | 245/400 [01:36<00:57,  2.69it/s]Running loglikelihood requests:  62%|██████▏   | 246/400 [01:36<00:58,  2.64it/s]Running loglikelihood requests:  62%|██████▏   | 247/400 [01:37<00:58,  2.61it/s]Running loglikelihood requests:  62%|██████▏   | 248/400 [01:37<00:58,  2.58it/s]Running loglikelihood requests:  62%|██████▏   | 249/400 [01:37<00:58,  2.57it/s]Running loglikelihood requests:  62%|██████▎   | 250/400 [01:38<00:58,  2.56it/s]Running loglikelihood requests:  63%|██████▎   | 251/400 [01:38<00:58,  2.54it/s]Running loglikelihood requests:  63%|██████▎   | 252/400 [01:39<00:58,  2.54it/s]Running loglikelihood requests:  63%|██████▎   | 253/400 [01:39<00:58,  2.53it/s]Running loglikelihood requests:  64%|██████▎   | 254/400 [01:39<00:57,  2.54it/s]Running loglikelihood requests:  64%|██████▍   | 255/400 [01:40<00:56,  2.55it/s]Running loglikelihood requests:  64%|██████▍   | 256/400 [01:40<00:56,  2.57it/s]Running loglikelihood requests:  64%|██████▍   | 257/400 [01:41<00:55,  2.56it/s]Running loglikelihood requests:  64%|██████▍   | 258/400 [01:41<00:55,  2.56it/s]Running loglikelihood requests:  65%|██████▍   | 259/400 [01:41<00:55,  2.56it/s]Running loglikelihood requests:  65%|██████▌   | 260/400 [01:42<00:54,  2.56it/s]Running loglikelihood requests:  65%|██████▌   | 261/400 [01:42<00:54,  2.56it/s]Running loglikelihood requests:  66%|██████▌   | 262/400 [01:43<00:53,  2.56it/s]Running loglikelihood requests:  66%|██████▌   | 263/400 [01:43<00:53,  2.58it/s]Running loglikelihood requests:  66%|██████▋   | 265/400 [01:43<00:39,  3.38it/s]Running loglikelihood requests:  66%|██████▋   | 266/400 [01:44<00:42,  3.16it/s]Running loglikelihood requests:  67%|██████▋   | 267/400 [01:44<00:44,  2.98it/s]Running loglikelihood requests:  67%|██████▋   | 268/400 [01:44<00:46,  2.86it/s]Running loglikelihood requests:  67%|██████▋   | 269/400 [01:45<00:47,  2.78it/s]Running loglikelihood requests:  68%|██████▊   | 270/400 [01:45<00:47,  2.75it/s]Running loglikelihood requests:  68%|██████▊   | 271/400 [01:46<00:47,  2.71it/s]Running loglikelihood requests:  68%|██████▊   | 272/400 [01:46<00:47,  2.68it/s]Running loglikelihood requests:  68%|██████▊   | 273/400 [01:46<00:47,  2.68it/s]Running loglikelihood requests:  68%|██████▊   | 274/400 [01:47<00:47,  2.64it/s]Running loglikelihood requests:  69%|██████▉   | 275/400 [01:47<00:47,  2.62it/s]Running loglikelihood requests:  69%|██████▉   | 276/400 [01:48<00:47,  2.60it/s]Running loglikelihood requests:  69%|██████▉   | 277/400 [01:48<00:47,  2.60it/s]Running loglikelihood requests:  70%|██████▉   | 278/400 [01:48<00:46,  2.60it/s]Running loglikelihood requests:  70%|██████▉   | 279/400 [01:49<00:46,  2.59it/s]Running loglikelihood requests:  70%|███████   | 280/400 [01:49<00:46,  2.58it/s]Running loglikelihood requests:  70%|███████   | 281/400 [01:49<00:45,  2.61it/s]Running loglikelihood requests:  70%|███████   | 282/400 [01:50<00:45,  2.60it/s]Running loglikelihood requests:  71%|███████   | 283/400 [01:50<00:44,  2.63it/s]Running loglikelihood requests:  71%|███████   | 284/400 [01:51<00:44,  2.63it/s]Running loglikelihood requests:  71%|███████▏  | 285/400 [01:51<00:43,  2.64it/s]Running loglikelihood requests:  72%|███████▏  | 286/400 [01:51<00:43,  2.65it/s]Running loglikelihood requests:  72%|███████▏  | 287/400 [01:52<00:42,  2.64it/s]Running loglikelihood requests:  72%|███████▏  | 288/400 [01:52<00:42,  2.65it/s]Running loglikelihood requests:  72%|███████▏  | 289/400 [01:52<00:41,  2.66it/s]Running loglikelihood requests:  72%|███████▎  | 290/400 [01:53<00:40,  2.68it/s]Running loglikelihood requests:  73%|███████▎  | 291/400 [01:53<00:40,  2.69it/s]Running loglikelihood requests:  73%|███████▎  | 292/400 [01:54<00:40,  2.68it/s]Running loglikelihood requests:  73%|███████▎  | 293/400 [01:54<00:40,  2.67it/s]Running loglikelihood requests:  74%|███████▎  | 294/400 [01:54<00:39,  2.66it/s]Running loglikelihood requests:  74%|███████▍  | 297/400 [01:55<00:23,  4.29it/s]Running loglikelihood requests:  74%|███████▍  | 298/400 [01:55<00:27,  3.77it/s]Running loglikelihood requests:  75%|███████▍  | 299/400 [01:55<00:29,  3.43it/s]Running loglikelihood requests:  75%|███████▌  | 300/400 [01:56<00:31,  3.19it/s]Running loglikelihood requests:  75%|███████▌  | 301/400 [01:56<00:32,  3.03it/s]Running loglikelihood requests:  76%|███████▌  | 302/400 [01:57<00:33,  2.91it/s]Running loglikelihood requests:  76%|███████▌  | 303/400 [01:57<00:34,  2.80it/s]Running loglikelihood requests:  77%|███████▋  | 307/400 [01:57<00:18,  5.11it/s]Running loglikelihood requests:  77%|███████▋  | 308/400 [01:58<00:21,  4.26it/s]Running loglikelihood requests:  77%|███████▋  | 309/400 [01:58<00:24,  3.76it/s]Running loglikelihood requests:  78%|███████▊  | 310/400 [01:59<00:26,  3.40it/s]Running loglikelihood requests:  78%|███████▊  | 311/400 [01:59<00:28,  3.15it/s]Running loglikelihood requests:  78%|███████▊  | 312/400 [01:59<00:29,  2.95it/s]Running loglikelihood requests:  78%|███████▊  | 313/400 [02:00<00:30,  2.81it/s]Running loglikelihood requests:  78%|███████▊  | 314/400 [02:00<00:31,  2.76it/s]Running loglikelihood requests:  79%|███████▉  | 315/400 [02:01<00:30,  2.74it/s]Running loglikelihood requests:  79%|███████▉  | 316/400 [02:01<00:30,  2.74it/s]Running loglikelihood requests:  79%|███████▉  | 317/400 [02:01<00:30,  2.75it/s]Running loglikelihood requests:  80%|███████▉  | 318/400 [02:02<00:30,  2.72it/s]Running loglikelihood requests:  80%|████████  | 321/400 [02:02<00:18,  4.35it/s]Running loglikelihood requests:  80%|████████  | 322/400 [02:02<00:20,  3.86it/s]Running loglikelihood requests:  81%|████████  | 323/400 [02:03<00:21,  3.53it/s]Running loglikelihood requests:  81%|████████  | 324/400 [02:03<00:22,  3.31it/s]Running loglikelihood requests:  81%|████████▏ | 325/400 [02:03<00:24,  3.12it/s]Running loglikelihood requests:  82%|████████▏ | 326/400 [02:04<00:24,  3.00it/s]Running loglikelihood requests:  82%|████████▏ | 327/400 [02:04<00:25,  2.91it/s]Running loglikelihood requests:  82%|████████▏ | 328/400 [02:05<00:25,  2.87it/s]Running loglikelihood requests:  82%|████████▏ | 329/400 [02:05<00:25,  2.81it/s]Running loglikelihood requests:  82%|████████▎ | 330/400 [02:05<00:25,  2.77it/s]Running loglikelihood requests:  83%|████████▎ | 331/400 [02:06<00:24,  2.78it/s]Running loglikelihood requests:  83%|████████▎ | 332/400 [02:06<00:24,  2.75it/s]Running loglikelihood requests:  83%|████████▎ | 333/400 [02:06<00:23,  2.79it/s]Running loglikelihood requests:  84%|████████▎ | 334/400 [02:07<00:23,  2.76it/s]Running loglikelihood requests:  84%|████████▍ | 336/400 [02:07<00:17,  3.59it/s]Running loglikelihood requests:  84%|████████▍ | 337/400 [02:07<00:18,  3.33it/s]Running loglikelihood requests:  84%|████████▍ | 338/400 [02:08<00:19,  3.17it/s]Running loglikelihood requests:  85%|████████▍ | 339/400 [02:08<00:20,  3.03it/s]Running loglikelihood requests:  85%|████████▌ | 340/400 [02:09<00:20,  2.94it/s]Running loglikelihood requests:  85%|████████▌ | 341/400 [02:09<00:20,  2.88it/s]Running loglikelihood requests:  86%|████████▌ | 342/400 [02:09<00:20,  2.82it/s]Running loglikelihood requests:  86%|████████▌ | 343/400 [02:10<00:20,  2.77it/s]Running loglikelihood requests:  86%|████████▌ | 344/400 [02:10<00:20,  2.76it/s]Running loglikelihood requests:  86%|████████▋ | 345/400 [02:10<00:19,  2.76it/s]Running loglikelihood requests:  86%|████████▋ | 346/400 [02:11<00:19,  2.84it/s]Running loglikelihood requests:  87%|████████▋ | 347/400 [02:11<00:18,  2.81it/s]Running loglikelihood requests:  87%|████████▋ | 348/400 [02:11<00:18,  2.78it/s]Running loglikelihood requests:  87%|████████▋ | 349/400 [02:12<00:18,  2.79it/s]Running loglikelihood requests:  88%|████████▊ | 350/400 [02:12<00:18,  2.78it/s]Running loglikelihood requests:  88%|████████▊ | 351/400 [02:13<00:17,  2.78it/s]Running loglikelihood requests:  88%|████████▊ | 352/400 [02:13<00:16,  2.84it/s]Running loglikelihood requests:  88%|████████▊ | 353/400 [02:13<00:16,  2.88it/s]Running loglikelihood requests:  88%|████████▊ | 354/400 [02:14<00:16,  2.84it/s]Running loglikelihood requests:  89%|████████▉ | 355/400 [02:14<00:15,  2.82it/s]Running loglikelihood requests:  89%|████████▉ | 356/400 [02:14<00:15,  2.85it/s]Running loglikelihood requests:  89%|████████▉ | 357/400 [02:15<00:15,  2.83it/s]Running loglikelihood requests:  90%|████████▉ | 358/400 [02:15<00:14,  2.82it/s]Running loglikelihood requests:  90%|████████▉ | 359/400 [02:15<00:14,  2.82it/s]Running loglikelihood requests:  90%|█████████ | 360/400 [02:16<00:14,  2.86it/s]Running loglikelihood requests:  91%|█████████ | 363/400 [02:16<00:08,  4.56it/s]Running loglikelihood requests:  91%|█████████ | 364/400 [02:16<00:08,  4.03it/s]Running loglikelihood requests:  91%|█████████▏| 365/400 [02:17<00:09,  3.70it/s]Running loglikelihood requests:  92%|█████████▏| 366/400 [02:17<00:09,  3.49it/s]Running loglikelihood requests:  92%|█████████▏| 367/400 [02:17<00:09,  3.33it/s]Running loglikelihood requests:  92%|█████████▏| 368/400 [02:18<00:10,  3.18it/s]Running loglikelihood requests:  92%|█████████▏| 369/400 [02:18<00:10,  3.07it/s]Running loglikelihood requests:  92%|█████████▎| 370/400 [02:18<00:09,  3.01it/s]Running loglikelihood requests:  93%|█████████▎| 371/400 [02:19<00:09,  3.00it/s]Running loglikelihood requests:  93%|█████████▎| 372/400 [02:19<00:09,  3.01it/s]Running loglikelihood requests:  93%|█████████▎| 373/400 [02:19<00:08,  3.00it/s]Running loglikelihood requests:  94%|█████████▎| 374/400 [02:20<00:08,  2.97it/s]Running loglikelihood requests:  94%|█████████▍| 377/400 [02:20<00:04,  4.81it/s]Running loglikelihood requests:  94%|█████████▍| 378/400 [02:20<00:05,  4.28it/s]Running loglikelihood requests:  95%|█████████▍| 379/400 [02:21<00:05,  3.89it/s]Running loglikelihood requests:  95%|█████████▌| 380/400 [02:21<00:05,  3.66it/s]Running loglikelihood requests:  95%|█████████▌| 381/400 [02:21<00:05,  3.51it/s]Running loglikelihood requests:  96%|█████████▌| 382/400 [02:22<00:05,  3.39it/s]Running loglikelihood requests:  96%|█████████▌| 383/400 [02:22<00:05,  3.28it/s]Running loglikelihood requests:  96%|█████████▌| 384/400 [02:22<00:04,  3.22it/s]Running loglikelihood requests:  97%|█████████▋| 387/400 [02:23<00:02,  5.02it/s]Running loglikelihood requests:  97%|█████████▋| 388/400 [02:23<00:02,  4.44it/s]Running loglikelihood requests:  98%|█████████▊| 390/400 [02:23<00:02,  4.90it/s]Running loglikelihood requests:  98%|█████████▊| 391/400 [02:24<00:02,  4.41it/s]Running loglikelihood requests:  98%|█████████▊| 392/400 [02:24<00:01,  4.06it/s]Running loglikelihood requests:  98%|█████████▊| 393/400 [02:24<00:01,  3.89it/s]Running loglikelihood requests:  98%|█████████▊| 394/400 [02:25<00:01,  3.65it/s]Running loglikelihood requests:  99%|█████████▉| 395/400 [02:25<00:01,  3.53it/s]Running loglikelihood requests:  99%|█████████▉| 396/400 [02:25<00:01,  3.48it/s]Running loglikelihood requests: 100%|█████████▉| 398/400 [02:26<00:00,  4.46it/s]Running loglikelihood requests: 100%|█████████▉| 399/400 [02:26<00:00,  4.38it/s]Running loglikelihood requests: 100%|██████████| 400/400 [02:26<00:00,  2.73it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:4'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:4'}
full model:
{'openbookqa': {'alias': 'openbookqa', 'acc,none': 0.25, 'acc_stderr,none': 0.04351941398892446, 'acc_norm,none': 0.39, 'acc_norm_stderr,none': 0.04902071300001973}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7187319796021299
0.7542307644255121
0.9218102409329888
0.9507846064925026
0.9244473932111009
0.9697851265705995
0.9088369054586506
0.9555908644563589
0.6065244449438403
0.8380982140772407
0.9227293008726757
0.940339967265193
0.8763319017557598
0.8956221731459367
0.8537768995934096
0.9486125995141542
0.9673324054839537
0.9137514724447725
0.9379677722073356
0.9822722776537973
0.9198472199854921
0.886666948473494
0.9000260607563779
0.9759633945879138
0.9869744580755335
Total groups 76 exceeded the threshold, stopping comparison.
The group tensor is
[6, 2, 3, 1, 5, 4, 7, 0]
tensor([6, 2, 3, 1, 5, 4, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 3, 1, 4, 5, 7, 0]
tensor([6, 2, 3, 1, 4, 5, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 3, 1, 4, 5, 7, 0]
tensor([6, 2, 3, 1, 4, 5, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 5, 2, 4, 3, 7, 0]
tensor([6, 1, 5, 2, 4, 3, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 1, 3, 2, 5, 4, 6, 0]
tensor([7, 1, 3, 2, 5, 4, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 3, 1, 5, 4, 7, 0]
tensor([6, 2, 3, 1, 5, 4, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1, 1.0, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 1, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Normal merging for layer 4
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
done!
Normal merging for layer 5
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 6 to 29
done!
Normal merging for layer 30
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
Normal merging for layer 31
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
all done!
Model size: 12.3238 GB
102
cuda:4
mnli
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:41<00:41, 41.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 24.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 27.06s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: mnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: mnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mnli?recursive=False&expand=False HTTP/1.1" 307 141
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mnli?recursive=False&expand=False HTTP/1.1" 200 512
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140215727066768 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140215727066768 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215727066768 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140215727066768 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140215727066768 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140215727066768 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215727066768 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140215727066768 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of mnli from None to 0
INFO:lm_eval.api.task:Building contexts for mnli on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 132479.60it/s]
DEBUG:lm_eval.evaluator:Task: mnli; number of requests on this rank: 300
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/300 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/300 [00:01<08:32,  1.72s/it]Running loglikelihood requests:   1%|          | 2/300 [00:02<07:04,  1.42s/it]Running loglikelihood requests:   1%|▏         | 4/300 [00:03<04:04,  1.21it/s]Running loglikelihood requests:   2%|▏         | 5/300 [00:04<04:14,  1.16it/s]Running loglikelihood requests:   2%|▏         | 7/300 [00:05<03:18,  1.47it/s]Running loglikelihood requests:   3%|▎         | 8/300 [00:06<03:32,  1.38it/s]Running loglikelihood requests:   3%|▎         | 10/300 [00:07<02:52,  1.68it/s]Running loglikelihood requests:   4%|▎         | 11/300 [00:08<03:06,  1.55it/s]Running loglikelihood requests:   4%|▍         | 13/300 [00:09<02:36,  1.83it/s]Running loglikelihood requests:   5%|▍         | 14/300 [00:09<02:51,  1.67it/s]Running loglikelihood requests:   5%|▌         | 16/300 [00:10<02:25,  1.96it/s]Running loglikelihood requests:   6%|▌         | 17/300 [00:11<02:38,  1.78it/s]Running loglikelihood requests:   6%|▌         | 18/300 [00:12<02:50,  1.66it/s]Running loglikelihood requests:   7%|▋         | 20/300 [00:12<02:22,  1.96it/s]Running loglikelihood requests:   7%|▋         | 21/300 [00:13<02:36,  1.79it/s]Running loglikelihood requests:   8%|▊         | 23/300 [00:14<02:14,  2.06it/s]Running loglikelihood requests:   8%|▊         | 25/300 [00:15<02:01,  2.27it/s]Running loglikelihood requests:   9%|▊         | 26/300 [00:15<02:16,  2.01it/s]Running loglikelihood requests:   9%|▉         | 28/300 [00:16<02:01,  2.24it/s]Running loglikelihood requests:  10%|▉         | 29/300 [00:17<02:16,  1.99it/s]Running loglikelihood requests:  10%|█         | 30/300 [00:17<02:28,  1.81it/s]Running loglikelihood requests:  10%|█         | 31/300 [00:18<02:38,  1.69it/s]Running loglikelihood requests:  11%|█         | 32/300 [00:19<02:46,  1.61it/s]Running loglikelihood requests:  11%|█▏        | 34/300 [00:20<02:15,  1.97it/s]Running loglikelihood requests:  12%|█▏        | 36/300 [00:20<01:59,  2.22it/s]Running loglikelihood requests:  13%|█▎        | 38/300 [00:21<01:49,  2.40it/s]Running loglikelihood requests:  13%|█▎        | 40/300 [00:22<01:42,  2.55it/s]Running loglikelihood requests:  14%|█▎        | 41/300 [00:22<01:56,  2.22it/s]Running loglikelihood requests:  14%|█▍        | 42/300 [00:23<02:09,  1.99it/s]Running loglikelihood requests:  14%|█▍        | 43/300 [00:24<02:20,  1.83it/s]Running loglikelihood requests:  15%|█▌        | 45/300 [00:24<01:58,  2.15it/s]Running loglikelihood requests:  16%|█▌        | 47/300 [00:25<01:46,  2.38it/s]Running loglikelihood requests:  16%|█▌        | 48/300 [00:26<01:59,  2.10it/s]Running loglikelihood requests:  17%|█▋        | 50/300 [00:26<01:46,  2.35it/s]Running loglikelihood requests:  17%|█▋        | 52/300 [00:27<01:38,  2.53it/s]Running loglikelihood requests:  18%|█▊        | 53/300 [00:28<01:51,  2.21it/s]Running loglikelihood requests:  18%|█▊        | 54/300 [00:28<02:03,  2.00it/s]Running loglikelihood requests:  18%|█▊        | 55/300 [00:29<02:12,  1.85it/s]Running loglikelihood requests:  19%|█▉        | 57/300 [00:30<01:51,  2.18it/s]Running loglikelihood requests:  20%|█▉        | 59/300 [00:30<01:39,  2.42it/s]Running loglikelihood requests:  20%|██        | 61/300 [00:31<01:32,  2.60it/s]Running loglikelihood requests:  21%|██        | 62/300 [00:32<01:44,  2.27it/s]Running loglikelihood requests:  21%|██▏       | 64/300 [00:32<01:34,  2.49it/s]Running loglikelihood requests:  22%|██▏       | 65/300 [00:33<01:46,  2.20it/s]Running loglikelihood requests:  22%|██▏       | 66/300 [00:34<01:57,  2.00it/s]Running loglikelihood requests:  22%|██▏       | 67/300 [00:34<02:05,  1.85it/s]Running loglikelihood requests:  23%|██▎       | 69/300 [00:35<01:44,  2.21it/s]Running loglikelihood requests:  24%|██▎       | 71/300 [00:36<01:32,  2.46it/s]Running loglikelihood requests:  24%|██▍       | 72/300 [00:36<01:44,  2.18it/s]Running loglikelihood requests:  24%|██▍       | 73/300 [00:37<01:54,  1.99it/s]Running loglikelihood requests:  25%|██▌       | 75/300 [00:38<01:37,  2.31it/s]Running loglikelihood requests:  26%|██▌       | 77/300 [00:38<01:27,  2.54it/s]Running loglikelihood requests:  26%|██▋       | 79/300 [00:39<01:21,  2.71it/s]Running loglikelihood requests:  27%|██▋       | 80/300 [00:40<01:33,  2.36it/s]Running loglikelihood requests:  27%|██▋       | 81/300 [00:40<01:43,  2.12it/s]Running loglikelihood requests:  27%|██▋       | 82/300 [00:41<01:51,  1.95it/s]Running loglikelihood requests:  28%|██▊       | 83/300 [00:42<01:58,  1.83it/s]Running loglikelihood requests:  28%|██▊       | 85/300 [00:42<01:36,  2.22it/s]Running loglikelihood requests:  29%|██▊       | 86/300 [00:43<01:45,  2.02it/s]Running loglikelihood requests:  29%|██▉       | 88/300 [00:44<01:30,  2.35it/s]Running loglikelihood requests:  30%|██▉       | 89/300 [00:44<01:39,  2.11it/s]Running loglikelihood requests:  30%|███       | 91/300 [00:45<01:26,  2.42it/s]Running loglikelihood requests:  31%|███       | 93/300 [00:45<01:18,  2.63it/s]Running loglikelihood requests:  32%|███▏      | 95/300 [00:46<01:13,  2.78it/s]Running loglikelihood requests:  32%|███▏      | 96/300 [00:47<01:24,  2.42it/s]Running loglikelihood requests:  33%|███▎      | 98/300 [00:47<01:16,  2.64it/s]Running loglikelihood requests:  33%|███▎      | 99/300 [00:48<01:26,  2.33it/s]Running loglikelihood requests:  34%|███▎      | 101/300 [00:49<01:17,  2.58it/s]Running loglikelihood requests:  34%|███▍      | 102/300 [00:49<01:26,  2.28it/s]Running loglikelihood requests:  34%|███▍      | 103/300 [00:50<01:34,  2.07it/s]Running loglikelihood requests:  35%|███▍      | 104/300 [00:50<01:41,  1.93it/s]Running loglikelihood requests:  35%|███▌      | 105/300 [00:51<01:46,  1.83it/s]Running loglikelihood requests:  36%|███▌      | 107/300 [00:52<01:26,  2.24it/s]Running loglikelihood requests:  36%|███▌      | 108/300 [00:52<01:33,  2.04it/s]Running loglikelihood requests:  36%|███▋      | 109/300 [00:53<01:39,  1.91it/s]Running loglikelihood requests:  37%|███▋      | 111/300 [00:54<01:22,  2.30it/s]Running loglikelihood requests:  38%|███▊      | 113/300 [00:54<01:12,  2.56it/s]Running loglikelihood requests:  38%|███▊      | 115/300 [00:55<01:07,  2.76it/s]Running loglikelihood requests:  39%|███▉      | 117/300 [00:55<01:03,  2.90it/s]Running loglikelihood requests:  40%|███▉      | 119/300 [00:56<01:00,  2.99it/s]Running loglikelihood requests:  40%|████      | 121/300 [00:57<00:58,  3.06it/s]Running loglikelihood requests:  41%|████      | 122/300 [00:57<01:07,  2.63it/s]Running loglikelihood requests:  41%|████▏     | 124/300 [00:58<01:02,  2.82it/s]Running loglikelihood requests:  42%|████▏     | 125/300 [00:59<01:10,  2.47it/s]Running loglikelihood requests:  42%|████▏     | 126/300 [00:59<01:18,  2.22it/s]Running loglikelihood requests:  42%|████▏     | 127/300 [01:00<01:24,  2.05it/s]Running loglikelihood requests:  43%|████▎     | 129/300 [01:00<01:10,  2.42it/s]Running loglikelihood requests:  44%|████▎     | 131/300 [01:01<01:02,  2.68it/s]Running loglikelihood requests:  44%|████▍     | 133/300 [01:02<00:57,  2.89it/s]Running loglikelihood requests:  45%|████▍     | 134/300 [01:02<01:05,  2.53it/s]Running loglikelihood requests:  45%|████▌     | 135/300 [01:03<01:12,  2.29it/s]Running loglikelihood requests:  45%|████▌     | 136/300 [01:03<01:17,  2.11it/s]Running loglikelihood requests:  46%|████▌     | 137/300 [01:04<01:22,  1.99it/s]Running loglikelihood requests:  46%|████▌     | 138/300 [01:05<01:25,  1.90it/s]Running loglikelihood requests:  47%|████▋     | 140/300 [01:05<01:07,  2.35it/s]Running loglikelihood requests:  47%|████▋     | 142/300 [01:06<00:59,  2.67it/s]Running loglikelihood requests:  48%|████▊     | 143/300 [01:06<01:05,  2.39it/s]Running loglikelihood requests:  48%|████▊     | 145/300 [01:07<00:57,  2.70it/s]Running loglikelihood requests:  49%|████▊     | 146/300 [01:07<01:04,  2.40it/s]Running loglikelihood requests:  49%|████▉     | 147/300 [01:08<01:09,  2.20it/s]Running loglikelihood requests:  49%|████▉     | 148/300 [01:09<01:14,  2.05it/s]Running loglikelihood requests:  50%|█████     | 150/300 [01:09<01:00,  2.46it/s]Running loglikelihood requests:  51%|█████     | 152/300 [01:10<00:53,  2.76it/s]Running loglikelihood requests:  51%|█████▏    | 154/300 [01:10<00:49,  2.96it/s]Running loglikelihood requests:  52%|█████▏    | 155/300 [01:11<00:55,  2.60it/s]Running loglikelihood requests:  52%|█████▏    | 157/300 [01:12<00:50,  2.85it/s]Running loglikelihood requests:  53%|█████▎    | 159/300 [01:12<00:46,  3.03it/s]Running loglikelihood requests:  54%|█████▎    | 161/300 [01:13<00:44,  3.16it/s]Running loglikelihood requests:  54%|█████▍    | 162/300 [01:13<00:50,  2.74it/s]Running loglikelihood requests:  55%|█████▍    | 164/300 [01:14<00:46,  2.96it/s]Running loglikelihood requests:  55%|█████▌    | 165/300 [01:14<00:51,  2.60it/s]Running loglikelihood requests:  56%|█████▌    | 167/300 [01:15<00:46,  2.87it/s]Running loglikelihood requests:  56%|█████▋    | 169/300 [01:16<00:42,  3.05it/s]Running loglikelihood requests:  57%|█████▋    | 170/300 [01:16<00:48,  2.66it/s]Running loglikelihood requests:  57%|█████▋    | 171/300 [01:17<00:53,  2.40it/s]Running loglikelihood requests:  57%|█████▋    | 172/300 [01:17<00:58,  2.20it/s]Running loglikelihood requests:  58%|█████▊    | 173/300 [01:18<01:01,  2.07it/s]Running loglikelihood requests:  58%|█████▊    | 175/300 [01:18<00:49,  2.51it/s]Running loglikelihood requests:  59%|█████▉    | 177/300 [01:19<00:43,  2.82it/s]Running loglikelihood requests:  60%|█████▉    | 179/300 [01:20<00:39,  3.04it/s]Running loglikelihood requests:  60%|██████    | 180/300 [01:20<00:45,  2.66it/s]Running loglikelihood requests:  60%|██████    | 181/300 [01:21<00:49,  2.40it/s]Running loglikelihood requests:  61%|██████    | 183/300 [01:21<00:42,  2.75it/s]Running loglikelihood requests:  62%|██████▏   | 185/300 [01:22<00:38,  3.00it/s]Running loglikelihood requests:  62%|██████▏   | 186/300 [01:22<00:43,  2.64it/s]Running loglikelihood requests:  63%|██████▎   | 188/300 [01:23<00:38,  2.93it/s]Running loglikelihood requests:  63%|██████▎   | 190/300 [01:23<00:35,  3.14it/s]Running loglikelihood requests:  64%|██████▎   | 191/300 [01:24<00:39,  2.74it/s]Running loglikelihood requests:  64%|██████▍   | 192/300 [01:25<00:43,  2.47it/s]Running loglikelihood requests:  65%|██████▍   | 194/300 [01:25<00:37,  2.82it/s]Running loglikelihood requests:  65%|██████▌   | 195/300 [01:26<00:41,  2.53it/s]Running loglikelihood requests:  65%|██████▌   | 196/300 [01:26<00:44,  2.32it/s]Running loglikelihood requests:  66%|██████▌   | 198/300 [01:27<00:37,  2.73it/s]Running loglikelihood requests:  66%|██████▋   | 199/300 [01:27<00:41,  2.46it/s]Running loglikelihood requests:  67%|██████▋   | 200/300 [01:28<00:43,  2.28it/s]Running loglikelihood requests:  67%|██████▋   | 201/300 [01:28<00:46,  2.15it/s]Running loglikelihood requests:  68%|██████▊   | 203/300 [01:29<00:37,  2.61it/s]Running loglikelihood requests:  68%|██████▊   | 205/300 [01:29<00:32,  2.94it/s]Running loglikelihood requests:  69%|██████▊   | 206/300 [01:30<00:35,  2.62it/s]Running loglikelihood requests:  69%|██████▉   | 208/300 [01:30<00:31,  2.94it/s]Running loglikelihood requests:  70%|███████   | 210/300 [01:31<00:28,  3.17it/s]Running loglikelihood requests:  71%|███████   | 212/300 [01:32<00:26,  3.33it/s]Running loglikelihood requests:  71%|███████   | 213/300 [01:32<00:30,  2.90it/s]Running loglikelihood requests:  72%|███████▏  | 215/300 [01:33<00:26,  3.15it/s]Running loglikelihood requests:  72%|███████▏  | 216/300 [01:33<00:30,  2.77it/s]Running loglikelihood requests:  72%|███████▏  | 217/300 [01:34<00:33,  2.50it/s]Running loglikelihood requests:  73%|███████▎  | 219/300 [01:34<00:28,  2.88it/s]Running loglikelihood requests:  74%|███████▎  | 221/300 [01:35<00:25,  3.15it/s]Running loglikelihood requests:  74%|███████▍  | 222/300 [01:35<00:28,  2.77it/s]Running loglikelihood requests:  75%|███████▍  | 224/300 [01:36<00:24,  3.07it/s]Running loglikelihood requests:  75%|███████▌  | 225/300 [01:36<00:27,  2.73it/s]Running loglikelihood requests:  76%|███████▌  | 227/300 [01:37<00:23,  3.06it/s]Running loglikelihood requests:  76%|███████▌  | 228/300 [01:37<00:26,  2.71it/s]Running loglikelihood requests:  76%|███████▋  | 229/300 [01:38<00:28,  2.47it/s]Running loglikelihood requests:  77%|███████▋  | 231/300 [01:38<00:23,  2.88it/s]Running loglikelihood requests:  77%|███████▋  | 232/300 [01:39<00:26,  2.59it/s]Running loglikelihood requests:  78%|███████▊  | 233/300 [01:39<00:27,  2.39it/s]Running loglikelihood requests:  78%|███████▊  | 234/300 [01:40<00:29,  2.26it/s]Running loglikelihood requests:  78%|███████▊  | 235/300 [01:41<00:30,  2.16it/s]Running loglikelihood requests:  79%|███████▊  | 236/300 [01:41<00:30,  2.09it/s]Running loglikelihood requests:  79%|███████▉  | 237/300 [01:42<00:30,  2.04it/s]Running loglikelihood requests:  79%|███████▉  | 238/300 [01:42<00:30,  2.00it/s]Running loglikelihood requests:  80%|████████  | 240/300 [01:43<00:23,  2.55it/s]Running loglikelihood requests:  81%|████████  | 242/300 [01:43<00:19,  2.94it/s]Running loglikelihood requests:  81%|████████▏ | 244/300 [01:44<00:17,  3.22it/s]Running loglikelihood requests:  82%|████████▏ | 246/300 [01:44<00:15,  3.42it/s]Running loglikelihood requests:  83%|████████▎ | 248/300 [01:45<00:14,  3.55it/s]Running loglikelihood requests:  83%|████████▎ | 250/300 [01:45<00:13,  3.65it/s]Running loglikelihood requests:  84%|████████▍ | 252/300 [01:46<00:12,  3.71it/s]Running loglikelihood requests:  85%|████████▍ | 254/300 [01:46<00:12,  3.76it/s]Running loglikelihood requests:  85%|████████▌ | 256/300 [01:47<00:11,  3.80it/s]Running loglikelihood requests:  86%|████████▌ | 257/300 [01:47<00:13,  3.26it/s]Running loglikelihood requests:  86%|████████▌ | 258/300 [01:48<00:14,  2.88it/s]Running loglikelihood requests:  87%|████████▋ | 260/300 [01:48<00:12,  3.20it/s]Running loglikelihood requests:  87%|████████▋ | 261/300 [01:49<00:13,  2.84it/s]Running loglikelihood requests:  88%|████████▊ | 263/300 [01:49<00:11,  3.18it/s]Running loglikelihood requests:  88%|████████▊ | 264/300 [01:50<00:12,  2.83it/s]Running loglikelihood requests:  88%|████████▊ | 265/300 [01:50<00:13,  2.58it/s]Running loglikelihood requests:  89%|████████▊ | 266/300 [01:51<00:14,  2.41it/s]Running loglikelihood requests:  89%|████████▉ | 268/300 [01:51<00:11,  2.89it/s]Running loglikelihood requests:  90%|█████████ | 270/300 [01:52<00:09,  3.23it/s]Running loglikelihood requests:  91%|█████████ | 272/300 [01:52<00:08,  3.48it/s]Running loglikelihood requests:  91%|█████████▏| 274/300 [01:53<00:07,  3.66it/s]Running loglikelihood requests:  92%|█████████▏| 275/300 [01:53<00:07,  3.19it/s]Running loglikelihood requests:  92%|█████████▏| 277/300 [01:54<00:06,  3.48it/s]Running loglikelihood requests:  93%|█████████▎| 278/300 [01:54<00:07,  3.06it/s]Running loglikelihood requests:  93%|█████████▎| 279/300 [01:55<00:07,  2.77it/s]Running loglikelihood requests:  94%|█████████▎| 281/300 [01:55<00:05,  3.19it/s]Running loglikelihood requests:  94%|█████████▍| 282/300 [01:56<00:06,  2.86it/s]Running loglikelihood requests:  94%|█████████▍| 283/300 [01:56<00:06,  2.64it/s]Running loglikelihood requests:  95%|█████████▍| 284/300 [01:57<00:06,  2.47it/s]Running loglikelihood requests:  95%|█████████▌| 286/300 [01:57<00:04,  2.99it/s]Running loglikelihood requests:  96%|█████████▌| 288/300 [01:58<00:03,  3.36it/s]Running loglikelihood requests:  97%|█████████▋| 290/300 [01:58<00:02,  3.63it/s]Running loglikelihood requests:  97%|█████████▋| 292/300 [01:58<00:02,  3.84it/s]Running loglikelihood requests:  98%|█████████▊| 293/300 [01:59<00:02,  3.35it/s]Running loglikelihood requests:  98%|█████████▊| 294/300 [01:59<00:01,  3.01it/s]Running loglikelihood requests:  99%|█████████▊| 296/300 [02:00<00:01,  3.42it/s]Running loglikelihood requests:  99%|█████████▉| 298/300 [02:00<00:00,  3.75it/s]Running loglikelihood requests: 100%|█████████▉| 299/300 [02:01<00:00,  3.32it/s]Running loglikelihood requests: 100%|██████████| 300/300 [02:01<00:00,  2.48it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:5'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:5'}
full model:
{'mnli': {'alias': 'mnli', 'acc,none': 0.4, 'acc_stderr,none': 0.0492365963917331}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8171409999433525
0.8739893758684915
0.8333413990263023
0.8782828649633461
0.7602933913725962
0.7047335470608914
0.9813668198386444
0.7850949920891616
0.748983595161589
0.6557873189175887
0.7043210867322975
0.9420132312730142
0.9548348739467002
0.8459964595909506
0.6932219150662173
0.8804855383939617
0.868566987197908
0.8437399449827557
0.9153450822718411
0.8433082326287293
0.910397090611448
0.7626694638419581
0.752151649412453
0.8346986395359313
0.9530736745854924
0.8635918585764104
0.8647192950921815
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[4, 5, 6, 1, 7, 2, 3, 0]
tensor([4, 5, 6, 1, 7, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 4, 6, 2, 5, 1, 3, 0]
tensor([7, 4, 6, 2, 5, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 7, 3, 5, 1, 4, 0]
tensor([6, 2, 7, 3, 5, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 2, 5, 4, 6, 1, 3, 0]
tensor([7, 2, 5, 4, 6, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 5, 3, 1, 2, 4, 0]
tensor([0, 1, 5, 3, 1, 2, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 1, 2, 2, 3, 0]
tensor([0, 3, 1, 1, 2, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 2, 3, 1, 2, 0, 3, 1]
tensor([0, 2, 3, 1, 2, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 3, 1, 2, 2, 1, 3, 0]
tensor([0, 3, 1, 2, 2, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1.0, 1]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 4 to 8
done!
Normal merging for layer 9
tensor([0, 7])
tensor(0)
tensor([1, 4])
tensor(1)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 10 to 20
done!
Normal merging for layer 21
tensor([0, 7])
tensor(0)
tensor([2, 3])
tensor(2)
tensor([4, 5])
tensor(4)
tensor([1, 6])
tensor(1)
done!
Normal merging for layer 22
tensor([0, 5])
tensor(0)
tensor([3, 7])
tensor(3)
tensor([1, 4])
tensor(1)
tensor([2, 6])
tensor(2)
done!
Normal merging for layer 23
tensor([0, 7])
tensor(0)
tensor([2, 5])
tensor(2)
tensor([3, 4])
tensor(3)
tensor([1, 6])
tensor(1)
done!
Cross-layer merge completed for layers 24 to 25
done!
Normal merging for layer 26
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 27 to 31
done!
all done!
Model size: 12.5127 GB
34
cuda:5
mastermind_46_easy
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:42<00:42, 42.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 24.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 27.28s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_46_mcq_random HTTP/1.1" 200 778
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/flair/mastermind_46_mcq_random/flair/mastermind_46_mcq_random.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_46_mcq_random HTTP/1.1" 200 786
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/flair/mastermind_46_mcq_random/resolve/544d077942975b1664c0bc4fd54df026050329a4/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_46_mcq_random/revision/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 786
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_46_mcq_random/tree/544d077942975b1664c0bc4fd54df026050329a4?recursive=False&expand=False HTTP/1.1" 200 290
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_46_mcq_random/tree/544d077942975b1664c0bc4fd54df026050329a4/data?recursive=False&expand=False HTTP/1.1" 200 361
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_46_mcq_random/revision/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 786
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/flair/mastermind_46_mcq_random/resolve/544d077942975b1664c0bc4fd54df026050329a4/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:filelock:Attempting to acquire lock 140237735259040 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_46_mcq_random_default_0.0.0_544d077942975b1664c0bc4fd54df026050329a4.lock
DEBUG:filelock:Lock 140237735259040 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_46_mcq_random_default_0.0.0_544d077942975b1664c0bc4fd54df026050329a4.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_46_mcq_random/default/0.0.0/544d077942975b1664c0bc4fd54df026050329a4/dataset_info.json
DEBUG:filelock:Attempting to release lock 140237735259040 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_46_mcq_random_default_0.0.0_544d077942975b1664c0bc4fd54df026050329a4.lock
DEBUG:filelock:Lock 140237735259040 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_46_mcq_random_default_0.0.0_544d077942975b1664c0bc4fd54df026050329a4.lock
DEBUG:filelock:Attempting to acquire lock 140212902830896 on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_46_mcq_random/default/0.0.0/544d077942975b1664c0bc4fd54df026050329a4_builder.lock
DEBUG:filelock:Lock 140212902830896 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_46_mcq_random/default/0.0.0/544d077942975b1664c0bc4fd54df026050329a4_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_46_mcq_random/default/0.0.0/544d077942975b1664c0bc4fd54df026050329a4/dataset_info.json
DEBUG:filelock:Attempting to release lock 140212902830896 on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_46_mcq_random/default/0.0.0/544d077942975b1664c0bc4fd54df026050329a4_builder.lock
DEBUG:filelock:Lock 140212902830896 released on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_46_mcq_random/default/0.0.0/544d077942975b1664c0bc4fd54df026050329a4_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of mastermind_46_easy from None to 0
INFO:lm_eval.api.task:Building contexts for mastermind_46_easy on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1486.28it/s]
DEBUG:lm_eval.evaluator:Task: mastermind_46_easy; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:01<12:50,  1.93s/it]Running loglikelihood requests:   0%|          | 2/400 [00:03<10:56,  1.65s/it]Running loglikelihood requests:   1%|          | 3/400 [00:04<10:18,  1.56s/it]Running loglikelihood requests:   1%|          | 4/400 [00:06<10:00,  1.52s/it]Running loglikelihood requests:   1%|▏         | 5/400 [00:07<09:49,  1.49s/it]Running loglikelihood requests:   2%|▏         | 6/400 [00:09<09:48,  1.49s/it]Running loglikelihood requests:   2%|▏         | 7/400 [00:10<09:40,  1.48s/it]Running loglikelihood requests:   2%|▏         | 8/400 [00:12<09:34,  1.46s/it]Running loglikelihood requests:   2%|▏         | 9/400 [00:13<09:29,  1.46s/it]Running loglikelihood requests:   2%|▎         | 10/400 [00:14<09:25,  1.45s/it]Running loglikelihood requests:   3%|▎         | 11/400 [00:16<09:22,  1.44s/it]Running loglikelihood requests:   3%|▎         | 12/400 [00:17<09:18,  1.44s/it]Running loglikelihood requests:   3%|▎         | 13/400 [00:19<09:16,  1.44s/it]Running loglikelihood requests:   4%|▎         | 14/400 [00:20<09:14,  1.44s/it]Running loglikelihood requests:   4%|▍         | 15/400 [00:22<09:13,  1.44s/it]Running loglikelihood requests:   4%|▍         | 16/400 [00:23<09:11,  1.44s/it]Running loglikelihood requests:   4%|▍         | 17/400 [00:25<09:09,  1.43s/it]Running loglikelihood requests:   4%|▍         | 18/400 [00:26<09:07,  1.43s/it]Running loglikelihood requests:   5%|▍         | 19/400 [00:27<09:05,  1.43s/it]Running loglikelihood requests:   5%|▌         | 20/400 [00:29<09:07,  1.44s/it]Running loglikelihood requests:   5%|▌         | 21/400 [00:30<09:04,  1.44s/it]Running loglikelihood requests:   6%|▌         | 22/400 [00:32<09:01,  1.43s/it]Running loglikelihood requests:   6%|▌         | 23/400 [00:33<08:59,  1.43s/it]Running loglikelihood requests:   6%|▌         | 24/400 [00:35<08:56,  1.43s/it]Running loglikelihood requests:   6%|▋         | 25/400 [00:36<08:54,  1.43s/it]Running loglikelihood requests:   6%|▋         | 26/400 [00:37<08:52,  1.42s/it]Running loglikelihood requests:   7%|▋         | 27/400 [00:39<08:50,  1.42s/it]Running loglikelihood requests:   7%|▋         | 28/400 [00:40<08:49,  1.42s/it]Running loglikelihood requests:   7%|▋         | 29/400 [00:42<08:48,  1.42s/it]Running loglikelihood requests:   8%|▊         | 30/400 [00:43<08:46,  1.42s/it]Running loglikelihood requests:   8%|▊         | 31/400 [00:44<08:44,  1.42s/it]Running loglikelihood requests:   8%|▊         | 32/400 [00:46<08:42,  1.42s/it]Running loglikelihood requests:   8%|▊         | 33/400 [00:47<08:40,  1.42s/it]Running loglikelihood requests:   8%|▊         | 34/400 [00:49<08:38,  1.42s/it]Running loglikelihood requests:   9%|▉         | 35/400 [00:50<08:36,  1.41s/it]Running loglikelihood requests:   9%|▉         | 36/400 [00:52<08:34,  1.41s/it]Running loglikelihood requests:   9%|▉         | 37/400 [00:53<08:32,  1.41s/it]Running loglikelihood requests:  10%|▉         | 38/400 [00:54<08:31,  1.41s/it]Running loglikelihood requests:  10%|▉         | 39/400 [00:56<08:29,  1.41s/it]Running loglikelihood requests:  10%|█         | 40/400 [00:57<08:28,  1.41s/it]Running loglikelihood requests:  10%|█         | 41/400 [00:59<08:26,  1.41s/it]Running loglikelihood requests:  10%|█         | 42/400 [01:00<08:24,  1.41s/it]Running loglikelihood requests:  11%|█         | 43/400 [01:01<08:22,  1.41s/it]Running loglikelihood requests:  11%|█         | 44/400 [01:03<08:21,  1.41s/it]Running loglikelihood requests:  11%|█▏        | 45/400 [01:04<08:19,  1.41s/it]Running loglikelihood requests:  12%|█▏        | 46/400 [01:06<08:18,  1.41s/it]Running loglikelihood requests:  12%|█▏        | 47/400 [01:07<08:16,  1.41s/it]Running loglikelihood requests:  12%|█▏        | 48/400 [01:08<08:14,  1.41s/it]Running loglikelihood requests:  12%|█▏        | 49/400 [01:10<08:13,  1.41s/it]Running loglikelihood requests:  12%|█▎        | 50/400 [01:11<08:12,  1.41s/it]Running loglikelihood requests:  13%|█▎        | 51/400 [01:13<08:10,  1.41s/it]Running loglikelihood requests:  13%|█▎        | 52/400 [01:14<08:10,  1.41s/it]Running loglikelihood requests:  13%|█▎        | 53/400 [01:15<08:08,  1.41s/it]Running loglikelihood requests:  14%|█▎        | 54/400 [01:17<08:06,  1.41s/it]Running loglikelihood requests:  14%|█▍        | 55/400 [01:18<08:05,  1.41s/it]Running loglikelihood requests:  14%|█▍        | 56/400 [01:20<08:03,  1.41s/it]Running loglikelihood requests:  14%|█▍        | 57/400 [01:21<08:02,  1.41s/it]Running loglikelihood requests:  14%|█▍        | 58/400 [01:23<07:59,  1.40s/it]Running loglikelihood requests:  15%|█▍        | 59/400 [01:24<07:58,  1.40s/it]Running loglikelihood requests:  15%|█▌        | 60/400 [01:25<07:56,  1.40s/it]Running loglikelihood requests:  15%|█▌        | 61/400 [01:27<07:54,  1.40s/it]Running loglikelihood requests:  16%|█▌        | 62/400 [01:28<07:53,  1.40s/it]Running loglikelihood requests:  16%|█▌        | 63/400 [01:29<07:51,  1.40s/it]Running loglikelihood requests:  16%|█▌        | 64/400 [01:31<07:50,  1.40s/it]Running loglikelihood requests:  16%|█▋        | 65/400 [01:32<07:48,  1.40s/it]Running loglikelihood requests:  16%|█▋        | 66/400 [01:34<07:47,  1.40s/it]Running loglikelihood requests:  17%|█▋        | 67/400 [01:35<07:45,  1.40s/it]Running loglikelihood requests:  17%|█▋        | 68/400 [01:36<07:44,  1.40s/it]Running loglikelihood requests:  17%|█▋        | 69/400 [01:38<07:42,  1.40s/it]Running loglikelihood requests:  18%|█▊        | 71/400 [01:39<05:53,  1.08s/it]Running loglikelihood requests:  18%|█▊        | 72/400 [01:41<06:18,  1.16s/it]Running loglikelihood requests:  18%|█▊        | 73/400 [01:42<06:38,  1.22s/it]Running loglikelihood requests:  18%|█▊        | 74/400 [01:43<06:52,  1.27s/it]Running loglikelihood requests:  19%|█▉        | 75/400 [01:45<07:03,  1.30s/it]Running loglikelihood requests:  19%|█▉        | 76/400 [01:46<07:10,  1.33s/it]Running loglikelihood requests:  19%|█▉        | 77/400 [01:48<07:15,  1.35s/it]Running loglikelihood requests:  20%|█▉        | 78/400 [01:49<07:18,  1.36s/it]Running loglikelihood requests:  20%|█▉        | 79/400 [01:50<07:19,  1.37s/it]Running loglikelihood requests:  20%|██        | 80/400 [01:52<07:19,  1.37s/it]Running loglikelihood requests:  20%|██        | 81/400 [01:53<07:19,  1.38s/it]Running loglikelihood requests:  20%|██        | 82/400 [01:55<07:19,  1.38s/it]Running loglikelihood requests:  21%|██        | 83/400 [01:56<07:18,  1.38s/it]Running loglikelihood requests:  21%|██        | 84/400 [01:57<07:16,  1.38s/it]Running loglikelihood requests:  21%|██▏       | 85/400 [01:59<07:15,  1.38s/it]Running loglikelihood requests:  22%|██▏       | 86/400 [02:00<07:14,  1.38s/it]Running loglikelihood requests:  22%|██▏       | 87/400 [02:02<07:13,  1.39s/it]Running loglikelihood requests:  22%|██▏       | 88/400 [02:03<07:11,  1.38s/it]Running loglikelihood requests:  22%|██▏       | 89/400 [02:04<07:10,  1.38s/it]Running loglikelihood requests:  22%|██▎       | 90/400 [02:06<07:08,  1.38s/it]Running loglikelihood requests:  23%|██▎       | 91/400 [02:07<07:07,  1.38s/it]Running loglikelihood requests:  23%|██▎       | 92/400 [02:08<07:05,  1.38s/it]Running loglikelihood requests:  23%|██▎       | 93/400 [02:10<07:03,  1.38s/it]Running loglikelihood requests:  24%|██▎       | 94/400 [02:11<07:02,  1.38s/it]Running loglikelihood requests:  24%|██▍       | 95/400 [02:13<07:00,  1.38s/it]Running loglikelihood requests:  24%|██▍       | 96/400 [02:14<06:58,  1.38s/it]Running loglikelihood requests:  24%|██▍       | 97/400 [02:15<06:57,  1.38s/it]Running loglikelihood requests:  24%|██▍       | 98/400 [02:17<06:56,  1.38s/it]Running loglikelihood requests:  25%|██▍       | 99/400 [02:18<06:54,  1.38s/it]Running loglikelihood requests:  25%|██▌       | 100/400 [02:19<06:53,  1.38s/it]Running loglikelihood requests:  25%|██▌       | 101/400 [02:21<06:52,  1.38s/it]Running loglikelihood requests:  26%|██▌       | 102/400 [02:22<06:51,  1.38s/it]Running loglikelihood requests:  26%|██▌       | 103/400 [02:24<06:50,  1.38s/it]Running loglikelihood requests:  26%|██▌       | 104/400 [02:25<06:48,  1.38s/it]Running loglikelihood requests:  26%|██▋       | 105/400 [02:26<06:46,  1.38s/it]Running loglikelihood requests:  26%|██▋       | 106/400 [02:28<06:45,  1.38s/it]Running loglikelihood requests:  27%|██▋       | 107/400 [02:29<06:43,  1.38s/it]Running loglikelihood requests:  27%|██▋       | 108/400 [02:30<06:41,  1.38s/it]Running loglikelihood requests:  27%|██▋       | 109/400 [02:32<06:40,  1.38s/it]Running loglikelihood requests:  28%|██▊       | 110/400 [02:33<06:38,  1.38s/it]Running loglikelihood requests:  28%|██▊       | 111/400 [02:35<06:37,  1.37s/it]Running loglikelihood requests:  28%|██▊       | 112/400 [02:36<06:35,  1.37s/it]Running loglikelihood requests:  28%|██▊       | 113/400 [02:37<06:33,  1.37s/it]Running loglikelihood requests:  28%|██▊       | 114/400 [02:39<06:32,  1.37s/it]Running loglikelihood requests:  29%|██▉       | 115/400 [02:40<06:30,  1.37s/it]Running loglikelihood requests:  29%|██▉       | 116/400 [02:41<06:29,  1.37s/it]Running loglikelihood requests:  29%|██▉       | 117/400 [02:43<06:27,  1.37s/it]Running loglikelihood requests:  30%|██▉       | 118/400 [02:44<06:26,  1.37s/it]Running loglikelihood requests:  30%|██▉       | 119/400 [02:46<06:24,  1.37s/it]Running loglikelihood requests:  30%|███       | 120/400 [02:47<06:23,  1.37s/it]Running loglikelihood requests:  30%|███       | 121/400 [02:48<06:21,  1.37s/it]Running loglikelihood requests:  30%|███       | 122/400 [02:50<06:20,  1.37s/it]Running loglikelihood requests:  31%|███       | 123/400 [02:51<06:18,  1.37s/it]Running loglikelihood requests:  31%|███       | 124/400 [02:52<06:17,  1.37s/it]Running loglikelihood requests:  31%|███▏      | 125/400 [02:54<06:16,  1.37s/it]Running loglikelihood requests:  32%|███▏      | 126/400 [02:55<06:14,  1.37s/it]Running loglikelihood requests:  32%|███▏      | 127/400 [02:56<06:13,  1.37s/it]Running loglikelihood requests:  32%|███▏      | 128/400 [02:58<06:12,  1.37s/it]Running loglikelihood requests:  32%|███▏      | 129/400 [02:59<06:10,  1.37s/it]Running loglikelihood requests:  32%|███▎      | 130/400 [03:01<06:08,  1.37s/it]Running loglikelihood requests:  33%|███▎      | 131/400 [03:02<06:07,  1.36s/it]Running loglikelihood requests:  33%|███▎      | 132/400 [03:03<06:05,  1.36s/it]Running loglikelihood requests:  33%|███▎      | 133/400 [03:05<06:03,  1.36s/it]Running loglikelihood requests:  34%|███▎      | 134/400 [03:06<06:02,  1.36s/it]Running loglikelihood requests:  34%|███▍      | 135/400 [03:07<06:00,  1.36s/it]Running loglikelihood requests:  34%|███▍      | 136/400 [03:09<05:59,  1.36s/it]Running loglikelihood requests:  34%|███▍      | 137/400 [03:10<06:02,  1.38s/it]Running loglikelihood requests:  34%|███▍      | 138/400 [03:12<05:59,  1.37s/it]Running loglikelihood requests:  35%|███▍      | 139/400 [03:13<05:56,  1.37s/it]Running loglikelihood requests:  35%|███▌      | 140/400 [03:14<05:54,  1.36s/it]Running loglikelihood requests:  35%|███▌      | 141/400 [03:16<05:53,  1.36s/it]Running loglikelihood requests:  36%|███▌      | 142/400 [03:17<05:51,  1.36s/it]Running loglikelihood requests:  36%|███▌      | 143/400 [03:18<05:49,  1.36s/it]Running loglikelihood requests:  36%|███▌      | 144/400 [03:20<05:47,  1.36s/it]Running loglikelihood requests:  36%|███▋      | 145/400 [03:21<05:45,  1.36s/it]Running loglikelihood requests:  36%|███▋      | 146/400 [03:22<05:44,  1.36s/it]Running loglikelihood requests:  37%|███▋      | 147/400 [03:24<05:42,  1.35s/it]Running loglikelihood requests:  37%|███▋      | 148/400 [03:25<05:40,  1.35s/it]Running loglikelihood requests:  37%|███▋      | 149/400 [03:26<05:39,  1.35s/it]Running loglikelihood requests:  38%|███▊      | 150/400 [03:28<05:37,  1.35s/it]Running loglikelihood requests:  38%|███▊      | 151/400 [03:29<05:36,  1.35s/it]Running loglikelihood requests:  38%|███▊      | 152/400 [03:30<05:35,  1.35s/it]Running loglikelihood requests:  38%|███▊      | 153/400 [03:32<05:34,  1.35s/it]Running loglikelihood requests:  38%|███▊      | 154/400 [03:33<05:32,  1.35s/it]Running loglikelihood requests:  39%|███▉      | 155/400 [03:35<05:31,  1.35s/it]Running loglikelihood requests:  39%|███▉      | 156/400 [03:36<05:30,  1.35s/it]Running loglikelihood requests:  39%|███▉      | 157/400 [03:37<05:28,  1.35s/it]Running loglikelihood requests:  40%|███▉      | 158/400 [03:39<05:26,  1.35s/it]Running loglikelihood requests:  40%|███▉      | 159/400 [03:40<05:24,  1.35s/it]Running loglikelihood requests:  40%|████      | 160/400 [03:41<05:23,  1.35s/it]Running loglikelihood requests:  40%|████      | 161/400 [03:43<05:21,  1.35s/it]Running loglikelihood requests:  40%|████      | 162/400 [03:44<05:19,  1.34s/it]Running loglikelihood requests:  41%|████      | 163/400 [03:45<05:18,  1.34s/it]Running loglikelihood requests:  41%|████      | 164/400 [03:47<05:16,  1.34s/it]Running loglikelihood requests:  41%|████▏     | 165/400 [03:48<05:14,  1.34s/it]Running loglikelihood requests:  42%|████▏     | 166/400 [03:49<05:13,  1.34s/it]Running loglikelihood requests:  42%|████▏     | 167/400 [03:51<05:12,  1.34s/it]Running loglikelihood requests:  42%|████▏     | 168/400 [03:52<05:10,  1.34s/it]Running loglikelihood requests:  42%|████▏     | 169/400 [03:53<05:09,  1.34s/it]Running loglikelihood requests:  42%|████▎     | 170/400 [03:55<05:07,  1.34s/it]Running loglikelihood requests:  43%|████▎     | 171/400 [03:56<05:06,  1.34s/it]Running loglikelihood requests:  43%|████▎     | 172/400 [03:57<05:05,  1.34s/it]Running loglikelihood requests:  43%|████▎     | 173/400 [03:59<05:03,  1.34s/it]Running loglikelihood requests:  44%|████▎     | 174/400 [04:00<05:02,  1.34s/it]Running loglikelihood requests:  44%|████▍     | 175/400 [04:01<05:00,  1.34s/it]Running loglikelihood requests:  44%|████▍     | 176/400 [04:03<04:59,  1.34s/it]Running loglikelihood requests:  44%|████▍     | 177/400 [04:04<04:58,  1.34s/it]Running loglikelihood requests:  44%|████▍     | 178/400 [04:05<04:57,  1.34s/it]Running loglikelihood requests:  45%|████▍     | 179/400 [04:07<04:55,  1.34s/it]Running loglikelihood requests:  45%|████▌     | 180/400 [04:08<04:54,  1.34s/it]Running loglikelihood requests:  45%|████▌     | 181/400 [04:09<04:52,  1.34s/it]Running loglikelihood requests:  46%|████▌     | 182/400 [04:11<04:57,  1.36s/it]Running loglikelihood requests:  46%|████▌     | 183/400 [04:12<04:58,  1.38s/it]Running loglikelihood requests:  46%|████▌     | 184/400 [04:14<04:54,  1.36s/it]Running loglikelihood requests:  46%|████▋     | 185/400 [04:15<04:51,  1.36s/it]Running loglikelihood requests:  46%|████▋     | 186/400 [04:16<04:49,  1.35s/it]Running loglikelihood requests:  47%|████▋     | 187/400 [04:18<04:46,  1.35s/it]Running loglikelihood requests:  47%|████▋     | 188/400 [04:19<04:44,  1.34s/it]Running loglikelihood requests:  47%|████▋     | 189/400 [04:20<04:42,  1.34s/it]Running loglikelihood requests:  48%|████▊     | 190/400 [04:22<04:41,  1.34s/it]Running loglikelihood requests:  48%|████▊     | 191/400 [04:23<04:39,  1.34s/it]Running loglikelihood requests:  48%|████▊     | 192/400 [04:24<04:37,  1.34s/it]Running loglikelihood requests:  48%|████▊     | 193/400 [04:26<04:36,  1.34s/it]Running loglikelihood requests:  48%|████▊     | 194/400 [04:27<04:34,  1.33s/it]Running loglikelihood requests:  49%|████▉     | 195/400 [04:28<04:33,  1.33s/it]Running loglikelihood requests:  49%|████▉     | 196/400 [04:30<04:31,  1.33s/it]Running loglikelihood requests:  49%|████▉     | 197/400 [04:31<04:30,  1.33s/it]Running loglikelihood requests:  50%|████▉     | 198/400 [04:32<04:28,  1.33s/it]Running loglikelihood requests:  50%|████▉     | 199/400 [04:34<04:27,  1.33s/it]Running loglikelihood requests:  50%|█████     | 200/400 [04:35<04:26,  1.33s/it]Running loglikelihood requests:  50%|█████     | 201/400 [04:36<04:24,  1.33s/it]Running loglikelihood requests:  50%|█████     | 202/400 [04:38<04:23,  1.33s/it]Running loglikelihood requests:  51%|█████     | 203/400 [04:39<04:22,  1.33s/it]Running loglikelihood requests:  51%|█████     | 204/400 [04:40<04:20,  1.33s/it]Running loglikelihood requests:  51%|█████▏    | 205/400 [04:42<04:19,  1.33s/it]Running loglikelihood requests:  52%|█████▏    | 206/400 [04:43<04:17,  1.33s/it]Running loglikelihood requests:  52%|█████▏    | 207/400 [04:44<04:16,  1.33s/it]Running loglikelihood requests:  52%|█████▏    | 208/400 [04:46<04:15,  1.33s/it]Running loglikelihood requests:  52%|█████▏    | 209/400 [04:47<04:13,  1.33s/it]Running loglikelihood requests:  52%|█████▎    | 210/400 [04:48<04:12,  1.33s/it]Running loglikelihood requests:  53%|█████▎    | 211/400 [04:50<04:10,  1.33s/it]Running loglikelihood requests:  53%|█████▎    | 212/400 [04:51<04:09,  1.33s/it]Running loglikelihood requests:  53%|█████▎    | 213/400 [04:52<04:07,  1.32s/it]Running loglikelihood requests:  54%|█████▎    | 214/400 [04:53<04:05,  1.32s/it]Running loglikelihood requests:  54%|█████▍    | 215/400 [04:55<04:03,  1.32s/it]Running loglikelihood requests:  54%|█████▍    | 216/400 [04:56<04:02,  1.32s/it]Running loglikelihood requests:  54%|█████▍    | 217/400 [04:57<04:00,  1.31s/it]Running loglikelihood requests:  55%|█████▍    | 218/400 [04:59<03:59,  1.31s/it]Running loglikelihood requests:  55%|█████▍    | 219/400 [05:00<03:57,  1.31s/it]Running loglikelihood requests:  55%|█████▌    | 220/400 [05:01<03:56,  1.31s/it]Running loglikelihood requests:  55%|█████▌    | 221/400 [05:03<03:54,  1.31s/it]Running loglikelihood requests:  56%|█████▌    | 222/400 [05:04<03:53,  1.31s/it]Running loglikelihood requests:  56%|█████▌    | 223/400 [05:05<03:51,  1.31s/it]Running loglikelihood requests:  56%|█████▌    | 224/400 [05:07<03:50,  1.31s/it]Running loglikelihood requests:  56%|█████▋    | 225/400 [05:08<03:48,  1.31s/it]Running loglikelihood requests:  56%|█████▋    | 226/400 [05:09<03:47,  1.31s/it]Running loglikelihood requests:  57%|█████▋    | 227/400 [05:10<03:46,  1.31s/it]Running loglikelihood requests:  57%|█████▋    | 228/400 [05:12<03:45,  1.31s/it]Running loglikelihood requests:  57%|█████▋    | 229/400 [05:13<03:46,  1.32s/it]Running loglikelihood requests:  57%|█████▊    | 230/400 [05:14<03:43,  1.32s/it]Running loglikelihood requests:  58%|█████▊    | 231/400 [05:16<03:41,  1.31s/it]Running loglikelihood requests:  58%|█████▊    | 232/400 [05:17<03:40,  1.31s/it]Running loglikelihood requests:  58%|█████▊    | 233/400 [05:18<03:38,  1.31s/it]Running loglikelihood requests:  58%|█████▊    | 234/400 [05:20<03:36,  1.31s/it]Running loglikelihood requests:  59%|█████▉    | 235/400 [05:21<03:35,  1.31s/it]Running loglikelihood requests:  59%|█████▉    | 236/400 [05:22<03:33,  1.30s/it]Running loglikelihood requests:  59%|█████▉    | 237/400 [05:24<03:32,  1.30s/it]Running loglikelihood requests:  60%|█████▉    | 238/400 [05:25<03:30,  1.30s/it]Running loglikelihood requests:  60%|█████▉    | 239/400 [05:26<03:28,  1.30s/it]Running loglikelihood requests:  60%|██████    | 240/400 [05:27<03:27,  1.30s/it]Running loglikelihood requests:  60%|██████    | 241/400 [05:29<03:25,  1.29s/it]Running loglikelihood requests:  60%|██████    | 242/400 [05:30<03:24,  1.29s/it]Running loglikelihood requests:  61%|██████    | 243/400 [05:31<03:22,  1.29s/it]Running loglikelihood requests:  61%|██████    | 244/400 [05:33<03:21,  1.29s/it]Running loglikelihood requests:  61%|██████▏   | 245/400 [05:34<03:19,  1.29s/it]Running loglikelihood requests:  62%|██████▏   | 246/400 [05:35<03:18,  1.29s/it]Running loglikelihood requests:  62%|██████▏   | 247/400 [05:36<03:16,  1.28s/it]Running loglikelihood requests:  62%|██████▏   | 248/400 [05:38<03:15,  1.28s/it]Running loglikelihood requests:  62%|██████▏   | 249/400 [05:39<03:13,  1.28s/it]Running loglikelihood requests:  62%|██████▎   | 250/400 [05:40<03:12,  1.28s/it]Running loglikelihood requests:  63%|██████▎   | 251/400 [05:42<03:10,  1.28s/it]Running loglikelihood requests:  63%|██████▎   | 252/400 [05:43<03:09,  1.28s/it]Running loglikelihood requests:  63%|██████▎   | 253/400 [05:44<03:08,  1.28s/it]Running loglikelihood requests:  64%|██████▎   | 254/400 [05:45<03:06,  1.28s/it]Running loglikelihood requests:  64%|██████▍   | 255/400 [05:47<03:05,  1.28s/it]Running loglikelihood requests:  64%|██████▍   | 256/400 [05:48<03:04,  1.28s/it]Running loglikelihood requests:  64%|██████▍   | 257/400 [05:49<03:02,  1.28s/it]Running loglikelihood requests:  64%|██████▍   | 258/400 [05:50<03:00,  1.27s/it]Running loglikelihood requests:  65%|██████▍   | 259/400 [05:52<02:59,  1.27s/it]Running loglikelihood requests:  65%|██████▌   | 260/400 [05:53<02:57,  1.27s/it]Running loglikelihood requests:  65%|██████▌   | 261/400 [05:54<02:56,  1.27s/it]Running loglikelihood requests:  66%|██████▌   | 262/400 [05:56<02:54,  1.27s/it]Running loglikelihood requests:  66%|██████▌   | 263/400 [05:57<02:53,  1.27s/it]Running loglikelihood requests:  66%|██████▌   | 264/400 [05:58<02:52,  1.27s/it]Running loglikelihood requests:  66%|██████▋   | 265/400 [05:59<02:50,  1.27s/it]Running loglikelihood requests:  66%|██████▋   | 266/400 [06:01<02:49,  1.26s/it]Running loglikelihood requests:  67%|██████▋   | 267/400 [06:02<02:47,  1.26s/it]Running loglikelihood requests:  67%|██████▋   | 268/400 [06:03<02:46,  1.26s/it]Running loglikelihood requests:  67%|██████▋   | 269/400 [06:04<02:45,  1.26s/it]Running loglikelihood requests:  68%|██████▊   | 270/400 [06:06<02:43,  1.26s/it]Running loglikelihood requests:  68%|██████▊   | 271/400 [06:07<02:42,  1.26s/it]Running loglikelihood requests:  68%|██████▊   | 272/400 [06:08<02:41,  1.26s/it]Running loglikelihood requests:  68%|██████▊   | 273/400 [06:09<02:40,  1.26s/it]Running loglikelihood requests:  68%|██████▊   | 274/400 [06:11<02:38,  1.26s/it]Running loglikelihood requests:  69%|██████▉   | 275/400 [06:12<02:37,  1.26s/it]Running loglikelihood requests:  69%|██████▉   | 276/400 [06:13<02:35,  1.26s/it]Running loglikelihood requests:  69%|██████▉   | 277/400 [06:14<02:34,  1.26s/it]Running loglikelihood requests:  70%|██████▉   | 278/400 [06:16<02:33,  1.26s/it]Running loglikelihood requests:  70%|██████▉   | 279/400 [06:17<02:32,  1.26s/it]Running loglikelihood requests:  70%|███████   | 280/400 [06:18<02:30,  1.26s/it]Running loglikelihood requests:  70%|███████   | 281/400 [06:19<02:29,  1.26s/it]Running loglikelihood requests:  70%|███████   | 282/400 [06:21<02:28,  1.26s/it]Running loglikelihood requests:  71%|███████   | 283/400 [06:22<02:27,  1.26s/it]Running loglikelihood requests:  71%|███████   | 284/400 [06:23<02:25,  1.26s/it]Running loglikelihood requests:  71%|███████▏  | 285/400 [06:25<02:24,  1.26s/it]Running loglikelihood requests:  72%|███████▏  | 286/400 [06:26<02:23,  1.26s/it]Running loglikelihood requests:  72%|███████▏  | 287/400 [06:27<02:21,  1.25s/it]Running loglikelihood requests:  72%|███████▏  | 288/400 [06:28<02:20,  1.25s/it]Running loglikelihood requests:  72%|███████▏  | 289/400 [06:30<02:19,  1.25s/it]Running loglikelihood requests:  72%|███████▎  | 290/400 [06:31<02:17,  1.25s/it]Running loglikelihood requests:  73%|███████▎  | 291/400 [06:32<02:16,  1.25s/it]Running loglikelihood requests:  73%|███████▎  | 292/400 [06:33<02:15,  1.25s/it]Running loglikelihood requests:  73%|███████▎  | 293/400 [06:35<02:13,  1.25s/it]Running loglikelihood requests:  74%|███████▎  | 294/400 [06:36<02:12,  1.25s/it]Running loglikelihood requests:  74%|███████▍  | 295/400 [06:37<02:11,  1.25s/it]Running loglikelihood requests:  74%|███████▍  | 296/400 [06:38<02:09,  1.25s/it]Running loglikelihood requests:  74%|███████▍  | 297/400 [06:40<02:08,  1.25s/it]Running loglikelihood requests:  74%|███████▍  | 298/400 [06:41<02:06,  1.24s/it]Running loglikelihood requests:  75%|███████▍  | 299/400 [06:42<02:05,  1.24s/it]Running loglikelihood requests:  75%|███████▌  | 300/400 [06:43<02:04,  1.24s/it]Running loglikelihood requests:  75%|███████▌  | 301/400 [06:44<02:02,  1.24s/it]Running loglikelihood requests:  76%|███████▌  | 302/400 [06:46<02:01,  1.24s/it]Running loglikelihood requests:  76%|███████▌  | 303/400 [06:47<01:59,  1.24s/it]Running loglikelihood requests:  76%|███████▌  | 304/400 [06:48<01:58,  1.24s/it]Running loglikelihood requests:  76%|███████▋  | 305/400 [06:49<01:57,  1.24s/it]Running loglikelihood requests:  76%|███████▋  | 306/400 [06:51<01:55,  1.23s/it]Running loglikelihood requests:  77%|███████▋  | 307/400 [06:52<01:54,  1.23s/it]Running loglikelihood requests:  77%|███████▋  | 308/400 [06:53<01:53,  1.23s/it]Running loglikelihood requests:  77%|███████▋  | 309/400 [06:54<01:52,  1.23s/it]Running loglikelihood requests:  78%|███████▊  | 310/400 [06:56<01:51,  1.23s/it]Running loglikelihood requests:  78%|███████▊  | 311/400 [06:57<01:49,  1.23s/it]Running loglikelihood requests:  78%|███████▊  | 312/400 [06:58<01:48,  1.23s/it]Running loglikelihood requests:  78%|███████▊  | 313/400 [06:59<01:46,  1.23s/it]Running loglikelihood requests:  78%|███████▊  | 314/400 [07:00<01:45,  1.23s/it]Running loglikelihood requests:  79%|███████▉  | 315/400 [07:02<01:44,  1.23s/it]Running loglikelihood requests:  79%|███████▉  | 316/400 [07:03<01:43,  1.23s/it]Running loglikelihood requests:  79%|███████▉  | 317/400 [07:04<01:41,  1.23s/it]Running loglikelihood requests:  80%|███████▉  | 318/400 [07:05<01:40,  1.23s/it]Running loglikelihood requests:  80%|███████▉  | 319/400 [07:07<01:39,  1.22s/it]Running loglikelihood requests:  80%|████████  | 320/400 [07:08<01:37,  1.22s/it]Running loglikelihood requests:  80%|████████  | 321/400 [07:09<01:36,  1.22s/it]Running loglikelihood requests:  80%|████████  | 322/400 [07:10<01:35,  1.22s/it]Running loglikelihood requests:  81%|████████  | 323/400 [07:11<01:34,  1.22s/it]Running loglikelihood requests:  81%|████████  | 324/400 [07:13<01:32,  1.22s/it]Running loglikelihood requests:  81%|████████▏ | 325/400 [07:14<01:31,  1.22s/it]Running loglikelihood requests:  82%|████████▏ | 326/400 [07:15<01:30,  1.22s/it]Running loglikelihood requests:  82%|████████▏ | 327/400 [07:16<01:28,  1.22s/it]Running loglikelihood requests:  82%|████████▏ | 328/400 [07:18<01:27,  1.22s/it]Running loglikelihood requests:  82%|████████▏ | 329/400 [07:19<01:26,  1.22s/it]Running loglikelihood requests:  82%|████████▎ | 330/400 [07:20<01:25,  1.22s/it]Running loglikelihood requests:  83%|████████▎ | 331/400 [07:21<01:23,  1.22s/it]Running loglikelihood requests:  83%|████████▎ | 332/400 [07:22<01:22,  1.22s/it]Running loglikelihood requests:  83%|████████▎ | 333/400 [07:24<01:21,  1.22s/it]Running loglikelihood requests:  84%|████████▎ | 334/400 [07:25<01:20,  1.21s/it]Running loglikelihood requests:  84%|████████▍ | 335/400 [07:26<01:18,  1.21s/it]Running loglikelihood requests:  84%|████████▍ | 336/400 [07:27<01:17,  1.21s/it]Running loglikelihood requests:  84%|████████▍ | 337/400 [07:28<01:16,  1.22s/it]Running loglikelihood requests:  84%|████████▍ | 338/400 [07:30<01:15,  1.22s/it]Running loglikelihood requests:  85%|████████▍ | 339/400 [07:31<01:14,  1.21s/it]Running loglikelihood requests:  85%|████████▌ | 340/400 [07:32<01:12,  1.21s/it]Running loglikelihood requests:  85%|████████▌ | 341/400 [07:33<01:11,  1.21s/it]Running loglikelihood requests:  86%|████████▌ | 342/400 [07:35<01:10,  1.21s/it]Running loglikelihood requests:  86%|████████▌ | 343/400 [07:36<01:09,  1.21s/it]Running loglikelihood requests:  86%|████████▌ | 344/400 [07:37<01:07,  1.21s/it]Running loglikelihood requests:  86%|████████▋ | 345/400 [07:38<01:06,  1.21s/it]Running loglikelihood requests:  86%|████████▋ | 346/400 [07:39<01:05,  1.21s/it]Running loglikelihood requests:  87%|████████▋ | 347/400 [07:41<01:04,  1.21s/it]Running loglikelihood requests:  87%|████████▋ | 348/400 [07:42<01:02,  1.21s/it]Running loglikelihood requests:  87%|████████▋ | 349/400 [07:43<01:01,  1.21s/it]Running loglikelihood requests:  88%|████████▊ | 350/400 [07:44<01:00,  1.21s/it]Running loglikelihood requests:  88%|████████▊ | 351/400 [07:45<00:59,  1.21s/it]Running loglikelihood requests:  88%|████████▊ | 352/400 [07:47<00:57,  1.20s/it]Running loglikelihood requests:  88%|████████▊ | 353/400 [07:48<00:56,  1.20s/it]Running loglikelihood requests:  88%|████████▊ | 354/400 [07:49<00:55,  1.20s/it]Running loglikelihood requests:  89%|████████▉ | 355/400 [07:50<00:54,  1.20s/it]Running loglikelihood requests:  89%|████████▉ | 356/400 [07:51<00:52,  1.20s/it]Running loglikelihood requests:  89%|████████▉ | 357/400 [07:53<00:51,  1.20s/it]Running loglikelihood requests:  90%|████████▉ | 358/400 [07:54<00:50,  1.20s/it]Running loglikelihood requests:  90%|████████▉ | 359/400 [07:55<00:49,  1.20s/it]Running loglikelihood requests:  90%|█████████ | 360/400 [07:56<00:48,  1.20s/it]Running loglikelihood requests:  90%|█████████ | 361/400 [07:57<00:46,  1.20s/it]Running loglikelihood requests:  90%|█████████ | 362/400 [07:59<00:45,  1.20s/it]Running loglikelihood requests:  91%|█████████ | 363/400 [08:00<00:44,  1.20s/it]Running loglikelihood requests:  91%|█████████ | 364/400 [08:01<00:43,  1.20s/it]Running loglikelihood requests:  91%|█████████▏| 365/400 [08:02<00:42,  1.20s/it]Running loglikelihood requests:  92%|█████████▏| 366/400 [08:03<00:40,  1.20s/it]Running loglikelihood requests:  92%|█████████▏| 367/400 [08:05<00:39,  1.20s/it]Running loglikelihood requests:  92%|█████████▏| 368/400 [08:06<00:38,  1.20s/it]Running loglikelihood requests:  92%|█████████▏| 369/400 [08:07<00:37,  1.20s/it]Running loglikelihood requests:  92%|█████████▎| 370/400 [08:08<00:35,  1.20s/it]Running loglikelihood requests:  93%|█████████▎| 371/400 [08:09<00:34,  1.20s/it]Running loglikelihood requests:  93%|█████████▎| 372/400 [08:11<00:33,  1.19s/it]Running loglikelihood requests:  93%|█████████▎| 373/400 [08:12<00:32,  1.19s/it]Running loglikelihood requests:  94%|█████████▎| 374/400 [08:13<00:30,  1.19s/it]Running loglikelihood requests:  94%|█████████▍| 375/400 [08:14<00:29,  1.19s/it]Running loglikelihood requests:  94%|█████████▍| 376/400 [08:15<00:28,  1.19s/it]Running loglikelihood requests:  94%|█████████▍| 377/400 [08:17<00:27,  1.18s/it]Running loglikelihood requests:  94%|█████████▍| 378/400 [08:18<00:25,  1.18s/it]Running loglikelihood requests:  95%|█████████▍| 379/400 [08:19<00:24,  1.18s/it]Running loglikelihood requests:  95%|█████████▌| 380/400 [08:20<00:23,  1.18s/it]Running loglikelihood requests:  95%|█████████▌| 381/400 [08:21<00:22,  1.17s/it]Running loglikelihood requests:  96%|█████████▌| 382/400 [08:22<00:21,  1.17s/it]Running loglikelihood requests:  96%|█████████▌| 383/400 [08:24<00:19,  1.17s/it]Running loglikelihood requests:  96%|█████████▌| 384/400 [08:25<00:18,  1.17s/it]Running loglikelihood requests:  96%|█████████▋| 385/400 [08:26<00:17,  1.16s/it]Running loglikelihood requests:  96%|█████████▋| 386/400 [08:27<00:16,  1.15s/it]Running loglikelihood requests:  97%|█████████▋| 387/400 [08:28<00:14,  1.14s/it]Running loglikelihood requests:  97%|█████████▋| 388/400 [08:29<00:13,  1.14s/it]Running loglikelihood requests:  97%|█████████▋| 389/400 [08:30<00:12,  1.11s/it]Running loglikelihood requests:  98%|█████████▊| 390/400 [08:31<00:10,  1.10s/it]Running loglikelihood requests:  98%|█████████▊| 391/400 [08:32<00:09,  1.08s/it]Running loglikelihood requests:  98%|█████████▊| 392/400 [08:33<00:08,  1.07s/it]Running loglikelihood requests:  98%|█████████▊| 393/400 [08:34<00:07,  1.06s/it]Running loglikelihood requests:  98%|█████████▊| 394/400 [08:35<00:06,  1.05s/it]Running loglikelihood requests:  99%|█████████▉| 395/400 [08:37<00:05,  1.05s/it]Running loglikelihood requests:  99%|█████████▉| 396/400 [08:38<00:04,  1.04s/it]Running loglikelihood requests:  99%|█████████▉| 397/400 [08:39<00:03,  1.02s/it]Running loglikelihood requests: 100%|█████████▉| 398/400 [08:40<00:02,  1.01s/it]Running loglikelihood requests: 100%|█████████▉| 399/400 [08:40<00:01,  1.00s/it]Running loglikelihood requests: 100%|██████████| 400/400 [08:41<00:00,  1.01it/s]Running loglikelihood requests: 100%|██████████| 400/400 [08:41<00:00,  1.30s/it]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:6'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:6'}
full model:
{'mastermind_46_easy': {'alias': 'mastermind_46_easy', 'acc,none': 0.75, 'acc_stderr,none': 0.04351941398892446}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9900730655060652
0.9975621416797388
0.9971758007187725
0.9980230222224056
0.9889479987910904
0.9764864248764976
0.9903944102306983
0.9944743493554844
0.9959595842537624
0.9872350810596702
0.9630442697748985
0.9836939129473328
0.9680042833072414
0.9739989664157891
0.9945965755000291
0.9874567967365809
0.9910190996776087
0.9888895436517939
0.9820307305549418
0.9881803990776891
0.9859944271811626
0.9931460182904469
0.984259193892523
0.99673879588459
0.9800909214289261
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[5, 2, 0, 3, 7, 6, 1, 4]
tensor([5, 2, 0, 3, 7, 6, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 0, 3, 7, 6, 1, 4]
tensor([5, 2, 0, 3, 7, 6, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 0, 3, 7, 6, 1, 4]
tensor([5, 2, 0, 3, 7, 6, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 0, 3, 7, 6, 1, 4]
tensor([5, 2, 0, 3, 7, 6, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 0, 3, 7, 6, 1, 4]
tensor([5, 2, 0, 3, 7, 6, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 0, 3, 7, 6, 1, 4]
tensor([5, 2, 0, 3, 7, 6, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
done!
Normal merging for layer 3
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
done!
Normal merging for layer 4
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
done!
Normal merging for layer 5
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 6 to 31
done!
all done!
Model size: 12.0718 GB
165
cuda:6
logiqa
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:42<00:42, 42.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 25.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 27.59s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/logiqa HTTP/1.1" 200 743
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/EleutherAI/logiqa/EleutherAI/logiqa.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): datasets-server.hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://datasets-server.hf-mirror.com:443 "GET /parquet?dataset=EleutherAI/logiqa HTTP/1.1" 302 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET / HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/EleutherAI/logiqa/EleutherAI/logiqa.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/logiqa/resolve/main/logiqa.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/logiqa/resolve/main/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/logiqa/resolve/main/README.md HTTP/1.1" 200 0
DEBUG:filelock:Attempting to acquire lock 140238671042064 on /public/home/zouyifei001/.cache/huggingface/modules/datasets_modules/datasets/EleutherAI--logiqa.lock
DEBUG:filelock:Lock 140238671042064 acquired on /public/home/zouyifei001/.cache/huggingface/modules/datasets_modules/datasets/EleutherAI--logiqa.lock
DEBUG:filelock:Attempting to release lock 140238671042064 on /public/home/zouyifei001/.cache/huggingface/modules/datasets_modules/datasets/EleutherAI--logiqa.lock
DEBUG:filelock:Lock 140238671042064 released on /public/home/zouyifei001/.cache/huggingface/modules/datasets_modules/datasets/EleutherAI--logiqa.lock
DEBUG:filelock:Attempting to acquire lock 140215738731360 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___logiqa_logiqa_0.0.1_5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81.lock
DEBUG:filelock:Lock 140215738731360 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___logiqa_logiqa_0.0.1_5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215738731360 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___logiqa_logiqa_0.0.1_5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81.lock
DEBUG:filelock:Lock 140215738731360 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___logiqa_logiqa_0.0.1_5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81.lock
DEBUG:filelock:Attempting to acquire lock 140229881632368 on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81_builder.lock
DEBUG:filelock:Lock 140229881632368 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81/dataset_info.json
DEBUG:filelock:Attempting to release lock 140229881632368 on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81_builder.lock
DEBUG:filelock:Lock 140229881632368 released on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of logiqa from None to 0
INFO:lm_eval.api.task:Building contexts for logiqa on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 3186.10it/s]
DEBUG:lm_eval.evaluator:Task: logiqa; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:02<18:38,  2.80s/it]Running loglikelihood requests:   0%|          | 2/400 [00:05<16:35,  2.50s/it]Running loglikelihood requests:   1%|          | 3/400 [00:07<15:48,  2.39s/it]Running loglikelihood requests:   1%|          | 4/400 [00:09<15:20,  2.32s/it]Running loglikelihood requests:   1%|▏         | 5/400 [00:11<14:58,  2.28s/it]Running loglikelihood requests:   2%|▏         | 6/400 [00:13<14:44,  2.24s/it]Running loglikelihood requests:   2%|▏         | 7/400 [00:16<14:33,  2.22s/it]Running loglikelihood requests:   2%|▏         | 8/400 [00:18<14:24,  2.20s/it]Running loglikelihood requests:   2%|▏         | 9/400 [00:20<14:14,  2.19s/it]Running loglikelihood requests:   2%|▎         | 10/400 [00:22<14:06,  2.17s/it]Running loglikelihood requests:   3%|▎         | 11/400 [00:24<13:54,  2.14s/it]Running loglikelihood requests:   3%|▎         | 12/400 [00:26<13:40,  2.12s/it]Running loglikelihood requests:   3%|▎         | 13/400 [00:28<13:29,  2.09s/it]Running loglikelihood requests:   4%|▎         | 14/400 [00:30<13:20,  2.07s/it]Running loglikelihood requests:   4%|▍         | 15/400 [00:32<13:13,  2.06s/it]Running loglikelihood requests:   4%|▍         | 16/400 [00:34<13:08,  2.05s/it]Running loglikelihood requests:   4%|▍         | 17/400 [00:36<13:04,  2.05s/it]Running loglikelihood requests:   4%|▍         | 18/400 [00:38<12:59,  2.04s/it]Running loglikelihood requests:   5%|▍         | 19/400 [00:40<12:57,  2.04s/it]Running loglikelihood requests:   5%|▌         | 20/400 [00:42<12:56,  2.04s/it]Running loglikelihood requests:   5%|▌         | 21/400 [00:44<12:49,  2.03s/it]Running loglikelihood requests:   6%|▌         | 22/400 [00:46<12:43,  2.02s/it]Running loglikelihood requests:   6%|▌         | 23/400 [00:48<12:38,  2.01s/it]Running loglikelihood requests:   6%|▌         | 24/400 [00:50<12:34,  2.01s/it]Running loglikelihood requests:   6%|▋         | 25/400 [00:52<12:29,  2.00s/it]Running loglikelihood requests:   6%|▋         | 26/400 [00:54<12:23,  1.99s/it]Running loglikelihood requests:   7%|▋         | 27/400 [00:56<12:18,  1.98s/it]Running loglikelihood requests:   7%|▋         | 28/400 [00:58<12:13,  1.97s/it]Running loglikelihood requests:   7%|▋         | 29/400 [01:00<12:07,  1.96s/it]Running loglikelihood requests:   8%|▊         | 30/400 [01:02<12:02,  1.95s/it]Running loglikelihood requests:   8%|▊         | 31/400 [01:04<11:58,  1.95s/it]Running loglikelihood requests:   8%|▊         | 32/400 [01:06<11:53,  1.94s/it]Running loglikelihood requests:   8%|▊         | 33/400 [01:08<11:50,  1.93s/it]Running loglikelihood requests:   8%|▊         | 34/400 [01:10<11:46,  1.93s/it]Running loglikelihood requests:   9%|▉         | 35/400 [01:12<11:43,  1.93s/it]Running loglikelihood requests:   9%|▉         | 36/400 [01:14<11:40,  1.93s/it]Running loglikelihood requests:   9%|▉         | 37/400 [01:16<11:35,  1.92s/it]Running loglikelihood requests:  10%|▉         | 38/400 [01:18<11:30,  1.91s/it]Running loglikelihood requests:  10%|▉         | 39/400 [01:19<11:24,  1.90s/it]Running loglikelihood requests:  10%|█         | 40/400 [01:21<11:19,  1.89s/it]Running loglikelihood requests:  10%|█         | 41/400 [01:23<11:16,  1.88s/it]Running loglikelihood requests:  10%|█         | 42/400 [01:25<11:12,  1.88s/it]Running loglikelihood requests:  11%|█         | 43/400 [01:27<11:07,  1.87s/it]Running loglikelihood requests:  11%|█         | 44/400 [01:29<11:04,  1.87s/it]Running loglikelihood requests:  11%|█▏        | 45/400 [01:31<11:00,  1.86s/it]Running loglikelihood requests:  12%|█▏        | 46/400 [01:32<10:56,  1.85s/it]Running loglikelihood requests:  12%|█▏        | 47/400 [01:34<10:52,  1.85s/it]Running loglikelihood requests:  12%|█▏        | 48/400 [01:36<10:48,  1.84s/it]Running loglikelihood requests:  12%|█▏        | 49/400 [01:38<10:45,  1.84s/it]Running loglikelihood requests:  12%|█▎        | 50/400 [01:40<10:42,  1.84s/it]Running loglikelihood requests:  13%|█▎        | 51/400 [01:42<10:43,  1.84s/it]Running loglikelihood requests:  13%|█▎        | 52/400 [01:43<10:38,  1.83s/it]Running loglikelihood requests:  13%|█▎        | 53/400 [01:45<10:33,  1.83s/it]Running loglikelihood requests:  14%|█▎        | 54/400 [01:47<10:29,  1.82s/it]Running loglikelihood requests:  14%|█▍        | 55/400 [01:49<10:24,  1.81s/it]Running loglikelihood requests:  14%|█▍        | 56/400 [01:51<10:19,  1.80s/it]Running loglikelihood requests:  14%|█▍        | 57/400 [01:52<10:14,  1.79s/it]Running loglikelihood requests:  14%|█▍        | 58/400 [01:54<10:10,  1.79s/it]Running loglikelihood requests:  15%|█▍        | 59/400 [01:56<10:06,  1.78s/it]Running loglikelihood requests:  15%|█▌        | 60/400 [01:58<10:01,  1.77s/it]Running loglikelihood requests:  15%|█▌        | 61/400 [01:59<09:55,  1.76s/it]Running loglikelihood requests:  16%|█▌        | 62/400 [02:01<09:49,  1.75s/it]Running loglikelihood requests:  16%|█▌        | 63/400 [02:03<09:45,  1.74s/it]Running loglikelihood requests:  16%|█▌        | 64/400 [02:05<09:41,  1.73s/it]Running loglikelihood requests:  16%|█▋        | 65/400 [02:06<09:35,  1.72s/it]Running loglikelihood requests:  16%|█▋        | 66/400 [02:08<09:31,  1.71s/it]Running loglikelihood requests:  17%|█▋        | 67/400 [02:10<09:26,  1.70s/it]Running loglikelihood requests:  17%|█▋        | 68/400 [02:11<09:21,  1.69s/it]Running loglikelihood requests:  17%|█▋        | 69/400 [02:13<09:17,  1.69s/it]Running loglikelihood requests:  18%|█▊        | 70/400 [02:15<09:14,  1.68s/it]Running loglikelihood requests:  18%|█▊        | 71/400 [02:16<09:10,  1.67s/it]Running loglikelihood requests:  18%|█▊        | 72/400 [02:18<09:06,  1.67s/it]Running loglikelihood requests:  18%|█▊        | 73/400 [02:20<09:02,  1.66s/it]Running loglikelihood requests:  18%|█▊        | 74/400 [02:21<08:57,  1.65s/it]Running loglikelihood requests:  19%|█▉        | 75/400 [02:23<08:53,  1.64s/it]Running loglikelihood requests:  19%|█▉        | 76/400 [02:24<08:48,  1.63s/it]Running loglikelihood requests:  19%|█▉        | 77/400 [02:26<08:43,  1.62s/it]Running loglikelihood requests:  20%|█▉        | 78/400 [02:28<08:39,  1.61s/it]Running loglikelihood requests:  20%|█▉        | 79/400 [02:29<08:34,  1.60s/it]Running loglikelihood requests:  20%|██        | 80/400 [02:31<08:31,  1.60s/it]Running loglikelihood requests:  20%|██        | 81/400 [02:32<08:27,  1.59s/it]Running loglikelihood requests:  20%|██        | 82/400 [02:34<08:23,  1.58s/it]Running loglikelihood requests:  21%|██        | 83/400 [02:35<08:19,  1.58s/it]Running loglikelihood requests:  21%|██        | 84/400 [02:37<08:16,  1.57s/it]Running loglikelihood requests:  21%|██▏       | 85/400 [02:39<08:13,  1.57s/it]Running loglikelihood requests:  22%|██▏       | 86/400 [02:40<08:09,  1.56s/it]Running loglikelihood requests:  22%|██▏       | 87/400 [02:42<08:06,  1.56s/it]Running loglikelihood requests:  22%|██▏       | 88/400 [02:43<08:04,  1.55s/it]Running loglikelihood requests:  22%|██▏       | 89/400 [02:45<08:02,  1.55s/it]Running loglikelihood requests:  22%|██▎       | 90/400 [02:46<08:00,  1.55s/it]Running loglikelihood requests:  23%|██▎       | 91/400 [02:48<07:58,  1.55s/it]Running loglikelihood requests:  23%|██▎       | 92/400 [02:49<07:56,  1.55s/it]Running loglikelihood requests:  23%|██▎       | 93/400 [02:51<07:53,  1.54s/it]Running loglikelihood requests:  24%|██▎       | 94/400 [02:52<07:51,  1.54s/it]Running loglikelihood requests:  24%|██▍       | 95/400 [02:54<07:49,  1.54s/it]Running loglikelihood requests:  24%|██▍       | 96/400 [02:56<07:46,  1.54s/it]Running loglikelihood requests:  24%|██▍       | 97/400 [02:57<07:44,  1.53s/it]Running loglikelihood requests:  24%|██▍       | 98/400 [02:59<07:42,  1.53s/it]Running loglikelihood requests:  25%|██▍       | 99/400 [03:00<07:40,  1.53s/it]Running loglikelihood requests:  25%|██▌       | 100/400 [03:02<07:39,  1.53s/it]Running loglikelihood requests:  25%|██▌       | 101/400 [03:03<07:37,  1.53s/it]Running loglikelihood requests:  26%|██▌       | 102/400 [03:05<07:34,  1.53s/it]Running loglikelihood requests:  26%|██▌       | 103/400 [03:06<07:32,  1.52s/it]Running loglikelihood requests:  26%|██▌       | 104/400 [03:08<07:31,  1.53s/it]Running loglikelihood requests:  26%|██▋       | 105/400 [03:09<07:30,  1.53s/it]Running loglikelihood requests:  26%|██▋       | 106/400 [03:11<07:28,  1.53s/it]Running loglikelihood requests:  27%|██▋       | 107/400 [03:12<07:26,  1.52s/it]Running loglikelihood requests:  27%|██▋       | 108/400 [03:14<07:23,  1.52s/it]Running loglikelihood requests:  27%|██▋       | 109/400 [03:15<07:21,  1.52s/it]Running loglikelihood requests:  28%|██▊       | 110/400 [03:17<07:18,  1.51s/it]Running loglikelihood requests:  28%|██▊       | 111/400 [03:18<07:16,  1.51s/it]Running loglikelihood requests:  28%|██▊       | 112/400 [03:20<07:13,  1.51s/it]Running loglikelihood requests:  28%|██▊       | 113/400 [03:21<07:11,  1.50s/it]Running loglikelihood requests:  28%|██▊       | 114/400 [03:23<07:08,  1.50s/it]Running loglikelihood requests:  29%|██▉       | 115/400 [03:24<07:06,  1.50s/it]Running loglikelihood requests:  29%|██▉       | 116/400 [03:26<07:04,  1.49s/it]Running loglikelihood requests:  29%|██▉       | 117/400 [03:27<07:02,  1.49s/it]Running loglikelihood requests:  30%|██▉       | 118/400 [03:29<07:00,  1.49s/it]Running loglikelihood requests:  30%|██▉       | 119/400 [03:30<06:57,  1.48s/it]Running loglikelihood requests:  30%|███       | 120/400 [03:32<06:54,  1.48s/it]Running loglikelihood requests:  30%|███       | 121/400 [03:33<06:52,  1.48s/it]Running loglikelihood requests:  30%|███       | 122/400 [03:35<06:50,  1.48s/it]Running loglikelihood requests:  31%|███       | 123/400 [03:36<06:48,  1.47s/it]Running loglikelihood requests:  31%|███       | 124/400 [03:38<06:45,  1.47s/it]Running loglikelihood requests:  31%|███▏      | 125/400 [03:39<06:43,  1.47s/it]Running loglikelihood requests:  32%|███▏      | 126/400 [03:40<06:41,  1.47s/it]Running loglikelihood requests:  32%|███▏      | 127/400 [03:42<06:39,  1.46s/it]Running loglikelihood requests:  32%|███▏      | 128/400 [03:43<06:37,  1.46s/it]Running loglikelihood requests:  32%|███▏      | 129/400 [03:45<06:35,  1.46s/it]Running loglikelihood requests:  32%|███▎      | 130/400 [03:46<06:33,  1.46s/it]Running loglikelihood requests:  33%|███▎      | 131/400 [03:48<06:32,  1.46s/it]Running loglikelihood requests:  33%|███▎      | 132/400 [03:49<06:30,  1.46s/it]Running loglikelihood requests:  33%|███▎      | 133/400 [03:51<06:28,  1.46s/it]Running loglikelihood requests:  34%|███▎      | 134/400 [03:52<06:27,  1.45s/it]Running loglikelihood requests:  34%|███▍      | 135/400 [03:54<06:25,  1.46s/it]Running loglikelihood requests:  34%|███▍      | 136/400 [03:55<06:23,  1.45s/it]Running loglikelihood requests:  34%|███▍      | 137/400 [03:56<06:21,  1.45s/it]Running loglikelihood requests:  34%|███▍      | 138/400 [03:58<06:20,  1.45s/it]Running loglikelihood requests:  35%|███▍      | 139/400 [03:59<06:18,  1.45s/it]Running loglikelihood requests:  35%|███▌      | 140/400 [04:01<06:16,  1.45s/it]Running loglikelihood requests:  36%|███▌      | 142/400 [04:02<04:47,  1.11s/it]Running loglikelihood requests:  36%|███▌      | 143/400 [04:04<05:07,  1.19s/it]Running loglikelihood requests:  36%|███▌      | 144/400 [04:05<05:22,  1.26s/it]Running loglikelihood requests:  36%|███▋      | 145/400 [04:07<05:33,  1.31s/it]Running loglikelihood requests:  36%|███▋      | 146/400 [04:08<05:41,  1.34s/it]Running loglikelihood requests:  37%|███▋      | 147/400 [04:09<05:46,  1.37s/it]Running loglikelihood requests:  37%|███▋      | 148/400 [04:11<05:50,  1.39s/it]Running loglikelihood requests:  37%|███▋      | 149/400 [04:12<05:51,  1.40s/it]Running loglikelihood requests:  38%|███▊      | 150/400 [04:14<05:52,  1.41s/it]Running loglikelihood requests:  38%|███▊      | 151/400 [04:15<05:52,  1.41s/it]Running loglikelihood requests:  38%|███▊      | 152/400 [04:17<05:51,  1.42s/it]Running loglikelihood requests:  38%|███▊      | 153/400 [04:18<05:50,  1.42s/it]Running loglikelihood requests:  38%|███▊      | 154/400 [04:19<05:50,  1.42s/it]Running loglikelihood requests:  39%|███▉      | 155/400 [04:21<05:49,  1.42s/it]Running loglikelihood requests:  39%|███▉      | 156/400 [04:22<05:47,  1.42s/it]Running loglikelihood requests:  39%|███▉      | 157/400 [04:24<05:46,  1.43s/it]Running loglikelihood requests:  40%|███▉      | 158/400 [04:25<05:45,  1.43s/it]Running loglikelihood requests:  40%|███▉      | 159/400 [04:27<05:43,  1.43s/it]Running loglikelihood requests:  40%|████      | 160/400 [04:28<05:42,  1.43s/it]Running loglikelihood requests:  40%|████      | 161/400 [04:29<05:40,  1.43s/it]Running loglikelihood requests:  40%|████      | 162/400 [04:31<05:38,  1.42s/it]Running loglikelihood requests:  41%|████      | 163/400 [04:32<05:37,  1.42s/it]Running loglikelihood requests:  41%|████      | 164/400 [04:34<05:35,  1.42s/it]Running loglikelihood requests:  41%|████▏     | 165/400 [04:35<05:33,  1.42s/it]Running loglikelihood requests:  42%|████▏     | 166/400 [04:37<05:31,  1.42s/it]Running loglikelihood requests:  42%|████▏     | 167/400 [04:38<05:29,  1.41s/it]Running loglikelihood requests:  42%|████▏     | 168/400 [04:39<05:27,  1.41s/it]Running loglikelihood requests:  42%|████▏     | 169/400 [04:41<05:26,  1.41s/it]Running loglikelihood requests:  42%|████▎     | 170/400 [04:42<05:24,  1.41s/it]Running loglikelihood requests:  43%|████▎     | 171/400 [04:44<05:22,  1.41s/it]Running loglikelihood requests:  43%|████▎     | 172/400 [04:45<05:20,  1.41s/it]Running loglikelihood requests:  43%|████▎     | 173/400 [04:46<05:19,  1.41s/it]Running loglikelihood requests:  44%|████▎     | 174/400 [04:48<05:17,  1.40s/it]Running loglikelihood requests:  44%|████▍     | 175/400 [04:49<05:15,  1.40s/it]Running loglikelihood requests:  44%|████▍     | 176/400 [04:51<05:13,  1.40s/it]Running loglikelihood requests:  44%|████▍     | 177/400 [04:52<05:11,  1.40s/it]Running loglikelihood requests:  44%|████▍     | 178/400 [04:53<05:09,  1.39s/it]Running loglikelihood requests:  45%|████▍     | 179/400 [04:55<05:07,  1.39s/it]Running loglikelihood requests:  45%|████▌     | 180/400 [04:56<05:05,  1.39s/it]Running loglikelihood requests:  45%|████▌     | 181/400 [04:58<05:03,  1.39s/it]Running loglikelihood requests:  46%|████▌     | 182/400 [04:59<05:01,  1.38s/it]Running loglikelihood requests:  46%|████▌     | 183/400 [05:00<04:59,  1.38s/it]Running loglikelihood requests:  46%|████▌     | 184/400 [05:02<04:57,  1.38s/it]Running loglikelihood requests:  46%|████▋     | 185/400 [05:03<04:56,  1.38s/it]Running loglikelihood requests:  46%|████▋     | 186/400 [05:04<04:54,  1.38s/it]Running loglikelihood requests:  47%|████▋     | 187/400 [05:06<04:52,  1.38s/it]Running loglikelihood requests:  47%|████▋     | 188/400 [05:07<04:51,  1.37s/it]Running loglikelihood requests:  47%|████▋     | 189/400 [05:08<04:49,  1.37s/it]Running loglikelihood requests:  48%|████▊     | 190/400 [05:10<04:47,  1.37s/it]Running loglikelihood requests:  48%|████▊     | 191/400 [05:11<04:46,  1.37s/it]Running loglikelihood requests:  48%|████▊     | 192/400 [05:13<04:44,  1.37s/it]Running loglikelihood requests:  48%|████▊     | 193/400 [05:14<04:42,  1.37s/it]Running loglikelihood requests:  48%|████▊     | 194/400 [05:15<04:40,  1.36s/it]Running loglikelihood requests:  49%|████▉     | 195/400 [05:17<04:38,  1.36s/it]Running loglikelihood requests:  49%|████▉     | 196/400 [05:18<04:36,  1.36s/it]Running loglikelihood requests:  49%|████▉     | 197/400 [05:19<04:34,  1.35s/it]Running loglikelihood requests:  50%|████▉     | 198/400 [05:21<04:32,  1.35s/it]Running loglikelihood requests:  50%|████▉     | 199/400 [05:22<04:31,  1.35s/it]Running loglikelihood requests:  50%|█████     | 200/400 [05:23<04:29,  1.35s/it]Running loglikelihood requests:  50%|█████     | 201/400 [05:25<04:27,  1.34s/it]Running loglikelihood requests:  50%|█████     | 202/400 [05:26<04:25,  1.34s/it]Running loglikelihood requests:  51%|█████     | 203/400 [05:27<04:23,  1.34s/it]Running loglikelihood requests:  51%|█████     | 204/400 [05:29<04:22,  1.34s/it]Running loglikelihood requests:  51%|█████▏    | 205/400 [05:30<04:20,  1.34s/it]Running loglikelihood requests:  52%|█████▏    | 206/400 [05:31<04:19,  1.34s/it]Running loglikelihood requests:  52%|█████▏    | 207/400 [05:33<04:17,  1.33s/it]Running loglikelihood requests:  52%|█████▏    | 208/400 [05:34<04:15,  1.33s/it]Running loglikelihood requests:  52%|█████▏    | 209/400 [05:35<04:14,  1.33s/it]Running loglikelihood requests:  52%|█████▎    | 210/400 [05:37<04:12,  1.33s/it]Running loglikelihood requests:  53%|█████▎    | 211/400 [05:38<04:11,  1.33s/it]Running loglikelihood requests:  53%|█████▎    | 212/400 [05:39<04:09,  1.33s/it]Running loglikelihood requests:  53%|█████▎    | 213/400 [05:41<04:08,  1.33s/it]Running loglikelihood requests:  54%|█████▎    | 214/400 [05:42<04:08,  1.34s/it]Running loglikelihood requests:  54%|█████▍    | 215/400 [05:43<04:07,  1.34s/it]Running loglikelihood requests:  54%|█████▍    | 216/400 [05:45<04:06,  1.34s/it]Running loglikelihood requests:  54%|█████▍    | 217/400 [05:46<04:04,  1.34s/it]Running loglikelihood requests:  55%|█████▍    | 218/400 [05:47<04:03,  1.34s/it]Running loglikelihood requests:  55%|█████▍    | 219/400 [05:49<04:01,  1.34s/it]Running loglikelihood requests:  55%|█████▌    | 220/400 [05:50<04:00,  1.33s/it]Running loglikelihood requests:  55%|█████▌    | 221/400 [05:51<03:58,  1.33s/it]Running loglikelihood requests:  56%|█████▌    | 222/400 [05:53<03:56,  1.33s/it]Running loglikelihood requests:  56%|█████▌    | 223/400 [05:54<03:55,  1.33s/it]Running loglikelihood requests:  56%|█████▌    | 224/400 [05:55<03:53,  1.33s/it]Running loglikelihood requests:  56%|█████▋    | 225/400 [05:57<03:52,  1.33s/it]Running loglikelihood requests:  56%|█████▋    | 226/400 [05:58<03:50,  1.32s/it]Running loglikelihood requests:  57%|█████▋    | 227/400 [05:59<03:48,  1.32s/it]Running loglikelihood requests:  57%|█████▋    | 228/400 [06:01<03:47,  1.32s/it]Running loglikelihood requests:  57%|█████▋    | 229/400 [06:02<03:45,  1.32s/it]Running loglikelihood requests:  57%|█████▊    | 230/400 [06:03<03:44,  1.32s/it]Running loglikelihood requests:  58%|█████▊    | 231/400 [06:05<03:42,  1.32s/it]Running loglikelihood requests:  58%|█████▊    | 232/400 [06:06<03:41,  1.32s/it]Running loglikelihood requests:  58%|█████▊    | 233/400 [06:07<03:39,  1.32s/it]Running loglikelihood requests:  58%|█████▊    | 234/400 [06:09<03:38,  1.31s/it]Running loglikelihood requests:  59%|█████▉    | 235/400 [06:10<03:36,  1.31s/it]Running loglikelihood requests:  59%|█████▉    | 236/400 [06:11<03:34,  1.31s/it]Running loglikelihood requests:  59%|█████▉    | 237/400 [06:12<03:32,  1.31s/it]Running loglikelihood requests:  60%|█████▉    | 238/400 [06:14<03:31,  1.30s/it]Running loglikelihood requests:  60%|█████▉    | 239/400 [06:15<03:29,  1.30s/it]Running loglikelihood requests:  60%|██████    | 240/400 [06:16<03:28,  1.30s/it]Running loglikelihood requests:  60%|██████    | 241/400 [06:18<03:27,  1.30s/it]Running loglikelihood requests:  60%|██████    | 242/400 [06:19<03:25,  1.30s/it]Running loglikelihood requests:  61%|██████    | 243/400 [06:20<03:24,  1.30s/it]Running loglikelihood requests:  61%|██████    | 244/400 [06:22<03:23,  1.30s/it]Running loglikelihood requests:  61%|██████▏   | 245/400 [06:23<03:22,  1.30s/it]Running loglikelihood requests:  62%|██████▏   | 246/400 [06:24<03:21,  1.31s/it]Running loglikelihood requests:  62%|██████▏   | 247/400 [06:25<03:19,  1.30s/it]Running loglikelihood requests:  62%|██████▏   | 248/400 [06:27<03:18,  1.30s/it]Running loglikelihood requests:  62%|██████▏   | 249/400 [06:28<03:16,  1.30s/it]Running loglikelihood requests:  62%|██████▎   | 250/400 [06:29<03:15,  1.30s/it]Running loglikelihood requests:  63%|██████▎   | 251/400 [06:31<03:13,  1.30s/it]Running loglikelihood requests:  63%|██████▎   | 252/400 [06:32<03:12,  1.30s/it]Running loglikelihood requests:  63%|██████▎   | 253/400 [06:33<03:10,  1.30s/it]Running loglikelihood requests:  64%|██████▎   | 254/400 [06:35<03:08,  1.29s/it]Running loglikelihood requests:  64%|██████▍   | 255/400 [06:36<03:07,  1.29s/it]Running loglikelihood requests:  64%|██████▍   | 256/400 [06:37<03:05,  1.29s/it]Running loglikelihood requests:  64%|██████▍   | 257/400 [06:38<03:04,  1.29s/it]Running loglikelihood requests:  64%|██████▍   | 258/400 [06:40<03:02,  1.29s/it]Running loglikelihood requests:  65%|██████▍   | 259/400 [06:41<03:01,  1.29s/it]Running loglikelihood requests:  65%|██████▌   | 260/400 [06:42<02:59,  1.28s/it]Running loglikelihood requests:  65%|██████▌   | 261/400 [06:44<02:58,  1.28s/it]Running loglikelihood requests:  66%|██████▌   | 262/400 [06:45<02:56,  1.28s/it]Running loglikelihood requests:  66%|██████▌   | 263/400 [06:46<02:55,  1.28s/it]Running loglikelihood requests:  66%|██████▌   | 264/400 [06:47<02:53,  1.27s/it]Running loglikelihood requests:  66%|██████▋   | 265/400 [06:49<02:51,  1.27s/it]Running loglikelihood requests:  66%|██████▋   | 266/400 [06:50<02:49,  1.27s/it]Running loglikelihood requests:  67%|██████▋   | 267/400 [06:51<02:48,  1.27s/it]Running loglikelihood requests:  67%|██████▋   | 268/400 [06:52<02:46,  1.26s/it]Running loglikelihood requests:  67%|██████▋   | 269/400 [06:54<02:45,  1.26s/it]Running loglikelihood requests:  68%|██████▊   | 270/400 [06:55<02:44,  1.26s/it]Running loglikelihood requests:  68%|██████▊   | 271/400 [06:56<02:42,  1.26s/it]Running loglikelihood requests:  68%|██████▊   | 272/400 [06:57<02:41,  1.26s/it]Running loglikelihood requests:  68%|██████▊   | 273/400 [06:59<02:39,  1.26s/it]Running loglikelihood requests:  68%|██████▊   | 274/400 [07:00<02:38,  1.26s/it]Running loglikelihood requests:  69%|██████▉   | 275/400 [07:01<02:37,  1.26s/it]Running loglikelihood requests:  69%|██████▉   | 276/400 [07:02<02:35,  1.26s/it]Running loglikelihood requests:  69%|██████▉   | 277/400 [07:04<02:34,  1.25s/it]Running loglikelihood requests:  70%|██████▉   | 278/400 [07:05<02:32,  1.25s/it]Running loglikelihood requests:  70%|███████   | 280/400 [07:06<01:55,  1.04it/s]Running loglikelihood requests:  70%|███████   | 281/400 [07:07<02:02,  1.03s/it]Running loglikelihood requests:  70%|███████   | 282/400 [07:09<02:08,  1.09s/it]Running loglikelihood requests:  71%|███████   | 283/400 [07:10<02:11,  1.13s/it]Running loglikelihood requests:  71%|███████   | 284/400 [07:11<02:13,  1.15s/it]Running loglikelihood requests:  71%|███████▏  | 285/400 [07:12<02:17,  1.20s/it]Running loglikelihood requests:  72%|███████▏  | 286/400 [07:14<02:18,  1.22s/it]Running loglikelihood requests:  72%|███████▏  | 287/400 [07:15<02:19,  1.24s/it]Running loglikelihood requests:  72%|███████▏  | 288/400 [07:16<02:20,  1.25s/it]Running loglikelihood requests:  72%|███████▏  | 289/400 [07:18<02:19,  1.26s/it]Running loglikelihood requests:  72%|███████▎  | 290/400 [07:19<02:18,  1.26s/it]Running loglikelihood requests:  73%|███████▎  | 291/400 [07:20<02:17,  1.26s/it]Running loglikelihood requests:  73%|███████▎  | 292/400 [07:21<02:13,  1.24s/it]Running loglikelihood requests:  73%|███████▎  | 293/400 [07:22<02:11,  1.22s/it]Running loglikelihood requests:  74%|███████▎  | 294/400 [07:24<02:08,  1.21s/it]Running loglikelihood requests:  74%|███████▍  | 295/400 [07:25<02:06,  1.21s/it]Running loglikelihood requests:  74%|███████▍  | 296/400 [07:26<02:04,  1.20s/it]Running loglikelihood requests:  74%|███████▍  | 297/400 [07:27<02:02,  1.19s/it]Running loglikelihood requests:  74%|███████▍  | 298/400 [07:28<02:01,  1.19s/it]Running loglikelihood requests:  75%|███████▍  | 299/400 [07:30<01:59,  1.18s/it]Running loglikelihood requests:  75%|███████▌  | 300/400 [07:31<01:58,  1.18s/it]Running loglikelihood requests:  75%|███████▌  | 301/400 [07:32<01:56,  1.18s/it]Running loglikelihood requests:  76%|███████▌  | 302/400 [07:33<01:55,  1.18s/it]Running loglikelihood requests:  76%|███████▌  | 303/400 [07:34<01:53,  1.17s/it]Running loglikelihood requests:  76%|███████▌  | 304/400 [07:35<01:52,  1.17s/it]Running loglikelihood requests:  76%|███████▋  | 305/400 [07:37<01:51,  1.17s/it]Running loglikelihood requests:  76%|███████▋  | 306/400 [07:38<01:50,  1.17s/it]Running loglikelihood requests:  77%|███████▋  | 307/400 [07:39<01:48,  1.17s/it]Running loglikelihood requests:  77%|███████▋  | 308/400 [07:40<01:47,  1.17s/it]Running loglikelihood requests:  77%|███████▋  | 309/400 [07:41<01:46,  1.17s/it]Running loglikelihood requests:  78%|███████▊  | 310/400 [07:42<01:44,  1.17s/it]Running loglikelihood requests:  78%|███████▊  | 311/400 [07:44<01:43,  1.16s/it]Running loglikelihood requests:  78%|███████▊  | 312/400 [07:45<01:42,  1.16s/it]Running loglikelihood requests:  78%|███████▊  | 313/400 [07:46<01:41,  1.16s/it]Running loglikelihood requests:  78%|███████▊  | 314/400 [07:47<01:39,  1.16s/it]Running loglikelihood requests:  79%|███████▉  | 315/400 [07:48<01:38,  1.16s/it]Running loglikelihood requests:  79%|███████▉  | 316/400 [07:49<01:37,  1.16s/it]Running loglikelihood requests:  79%|███████▉  | 317/400 [07:50<01:36,  1.16s/it]Running loglikelihood requests:  80%|███████▉  | 318/400 [07:52<01:34,  1.15s/it]Running loglikelihood requests:  80%|███████▉  | 319/400 [07:53<01:33,  1.15s/it]Running loglikelihood requests:  80%|████████  | 320/400 [07:54<01:31,  1.15s/it]Running loglikelihood requests:  80%|████████  | 321/400 [07:55<01:30,  1.15s/it]Running loglikelihood requests:  80%|████████  | 322/400 [07:56<01:29,  1.15s/it]Running loglikelihood requests:  81%|████████  | 323/400 [07:57<01:28,  1.15s/it]Running loglikelihood requests:  81%|████████  | 324/400 [07:58<01:26,  1.14s/it]Running loglikelihood requests:  81%|████████▏ | 325/400 [08:00<01:25,  1.14s/it]Running loglikelihood requests:  82%|████████▏ | 326/400 [08:01<01:24,  1.14s/it]Running loglikelihood requests:  82%|████████▏ | 327/400 [08:02<01:23,  1.14s/it]Running loglikelihood requests:  82%|████████▏ | 328/400 [08:03<01:21,  1.14s/it]Running loglikelihood requests:  82%|████████▏ | 329/400 [08:04<01:20,  1.14s/it]Running loglikelihood requests:  82%|████████▎ | 330/400 [08:05<01:19,  1.13s/it]Running loglikelihood requests:  83%|████████▎ | 331/400 [08:06<01:18,  1.13s/it]Running loglikelihood requests:  83%|████████▎ | 332/400 [08:08<01:16,  1.13s/it]Running loglikelihood requests:  83%|████████▎ | 333/400 [08:09<01:15,  1.13s/it]Running loglikelihood requests:  84%|████████▎ | 334/400 [08:10<01:14,  1.13s/it]Running loglikelihood requests:  84%|████████▍ | 335/400 [08:11<01:13,  1.13s/it]Running loglikelihood requests:  84%|████████▍ | 336/400 [08:12<01:11,  1.12s/it]Running loglikelihood requests:  84%|████████▍ | 337/400 [08:13<01:10,  1.12s/it]Running loglikelihood requests:  84%|████████▍ | 338/400 [08:14<01:09,  1.12s/it]Running loglikelihood requests:  85%|████████▍ | 339/400 [08:15<01:08,  1.12s/it]Running loglikelihood requests:  85%|████████▌ | 340/400 [08:16<01:06,  1.12s/it]Running loglikelihood requests:  85%|████████▌ | 341/400 [08:18<01:05,  1.11s/it]Running loglikelihood requests:  86%|████████▌ | 342/400 [08:19<01:04,  1.11s/it]Running loglikelihood requests:  86%|████████▌ | 343/400 [08:20<01:03,  1.11s/it]Running loglikelihood requests:  86%|████████▌ | 344/400 [08:21<01:01,  1.11s/it]Running loglikelihood requests:  86%|████████▋ | 345/400 [08:22<01:00,  1.10s/it]Running loglikelihood requests:  86%|████████▋ | 346/400 [08:23<00:59,  1.10s/it]Running loglikelihood requests:  87%|████████▋ | 347/400 [08:24<00:58,  1.10s/it]Running loglikelihood requests:  87%|████████▋ | 348/400 [08:25<00:57,  1.10s/it]Running loglikelihood requests:  87%|████████▋ | 349/400 [08:26<00:55,  1.10s/it]Running loglikelihood requests:  88%|████████▊ | 350/400 [08:27<00:54,  1.09s/it]Running loglikelihood requests:  88%|████████▊ | 351/400 [08:29<00:53,  1.09s/it]Running loglikelihood requests:  88%|████████▊ | 352/400 [08:30<00:52,  1.09s/it]Running loglikelihood requests:  88%|████████▊ | 353/400 [08:31<00:51,  1.09s/it]Running loglikelihood requests:  88%|████████▊ | 354/400 [08:32<00:49,  1.09s/it]Running loglikelihood requests:  89%|████████▉ | 355/400 [08:33<00:48,  1.08s/it]Running loglikelihood requests:  89%|████████▉ | 356/400 [08:34<00:47,  1.08s/it]Running loglikelihood requests:  89%|████████▉ | 357/400 [08:35<00:46,  1.08s/it]Running loglikelihood requests:  90%|████████▉ | 358/400 [08:36<00:45,  1.08s/it]Running loglikelihood requests:  90%|████████▉ | 359/400 [08:37<00:44,  1.08s/it]Running loglikelihood requests:  90%|█████████ | 360/400 [08:38<00:42,  1.07s/it]Running loglikelihood requests:  90%|█████████ | 361/400 [08:39<00:41,  1.07s/it]Running loglikelihood requests:  90%|█████████ | 362/400 [08:40<00:40,  1.07s/it]Running loglikelihood requests:  91%|█████████ | 363/400 [08:41<00:39,  1.07s/it]Running loglikelihood requests:  91%|█████████ | 364/400 [08:43<00:38,  1.07s/it]Running loglikelihood requests:  91%|█████████▏| 365/400 [08:44<00:37,  1.06s/it]Running loglikelihood requests:  92%|█████████▏| 366/400 [08:45<00:36,  1.06s/it]Running loglikelihood requests:  92%|█████████▏| 367/400 [08:46<00:35,  1.06s/it]Running loglikelihood requests:  92%|█████████▏| 368/400 [08:47<00:33,  1.06s/it]Running loglikelihood requests:  92%|█████████▏| 369/400 [08:48<00:32,  1.06s/it]Running loglikelihood requests:  92%|█████████▎| 370/400 [08:49<00:31,  1.06s/it]Running loglikelihood requests:  93%|█████████▎| 371/400 [08:50<00:30,  1.05s/it]Running loglikelihood requests:  93%|█████████▎| 372/400 [08:51<00:29,  1.05s/it]Running loglikelihood requests:  93%|█████████▎| 373/400 [08:52<00:28,  1.05s/it]Running loglikelihood requests:  94%|█████████▎| 374/400 [08:53<00:27,  1.04s/it]Running loglikelihood requests:  94%|█████████▍| 375/400 [08:54<00:25,  1.04s/it]Running loglikelihood requests:  94%|█████████▍| 376/400 [08:55<00:24,  1.04s/it]Running loglikelihood requests:  94%|█████████▍| 377/400 [08:56<00:23,  1.03s/it]Running loglikelihood requests:  94%|█████████▍| 378/400 [08:57<00:22,  1.02s/it]Running loglikelihood requests:  95%|█████████▍| 379/400 [08:58<00:21,  1.01s/it]Running loglikelihood requests:  95%|█████████▌| 380/400 [08:59<00:20,  1.00s/it]Running loglikelihood requests:  95%|█████████▌| 381/400 [09:00<00:18,  1.01it/s]Running loglikelihood requests:  96%|█████████▌| 382/400 [09:01<00:17,  1.02it/s]Running loglikelihood requests:  96%|█████████▌| 383/400 [09:02<00:16,  1.02it/s]Running loglikelihood requests:  96%|█████████▌| 384/400 [09:03<00:15,  1.03it/s]Running loglikelihood requests:  96%|█████████▋| 385/400 [09:04<00:14,  1.03it/s]Running loglikelihood requests:  96%|█████████▋| 386/400 [09:05<00:13,  1.04it/s]Running loglikelihood requests:  97%|█████████▋| 387/400 [09:06<00:12,  1.04it/s]Running loglikelihood requests:  97%|█████████▋| 389/400 [09:07<00:08,  1.37it/s]Running loglikelihood requests:  98%|█████████▊| 390/400 [09:08<00:07,  1.29it/s]Running loglikelihood requests:  98%|█████████▊| 391/400 [09:09<00:07,  1.24it/s]Running loglikelihood requests:  98%|█████████▊| 393/400 [09:09<00:04,  1.55it/s]Running loglikelihood requests:  98%|█████████▊| 394/400 [09:10<00:04,  1.44it/s]Running loglikelihood requests:  99%|█████████▉| 395/400 [09:11<00:03,  1.36it/s]Running loglikelihood requests:  99%|█████████▉| 396/400 [09:12<00:03,  1.31it/s]Running loglikelihood requests:  99%|█████████▉| 397/400 [09:13<00:02,  1.27it/s]Running loglikelihood requests: 100%|█████████▉| 398/400 [09:14<00:01,  1.25it/s]Running loglikelihood requests: 100%|█████████▉| 399/400 [09:14<00:00,  1.23it/s]Running loglikelihood requests: 100%|██████████| 400/400 [09:15<00:00,  1.22it/s]Running loglikelihood requests: 100%|██████████| 400/400 [09:15<00:00,  1.39s/it]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:7'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:7'}
full model:
{'logiqa': {'alias': 'logiqa', 'acc,none': 0.29, 'acc_stderr,none': 0.045604802157206865, 'acc_norm,none': 0.33, 'acc_norm_stderr,none': 0.04725815626252609}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9179964803140478
0.7817057225882229
0.8413553272072316
0.9274797474668193
0.8768807463293081
0.9494139907523571
0.8960692461846443
0.9131107283061946
0.6329173647892901
0.8375042173336539
0.8817471801904351
0.8172295355829869
0.7824572665005357
0.9227400642857845
0.9246594853497696
0.8075911590072223
0.6900210787422486
0.599615993999193
0.9308030044211123
0.9504015361511146
0.8866807231108503
0.540104242930401
0.6701728801805507
0.9744992661822648
0.8193037468812308
0.840784693447352
0.9052511591891966
Total groups 70 exceeded the threshold, stopping comparison.
The group tensor is
[3, 6, 7, 1, 5, 2, 4, 0]
tensor([3, 6, 7, 1, 5, 2, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 2, 6, 3, 7, 1, 5, 0]
tensor([4, 2, 6, 3, 7, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 6, 1, 7, 2, 4, 0]
tensor([5, 3, 6, 1, 7, 2, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 1, 7, 2, 3, 0]
tensor([5, 4, 6, 1, 7, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 3, 0, 2, 1, 0, 4, 1]
tensor([5, 3, 0, 2, 1, 0, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 0, 5, 0, 1, 2, 3, 1]
tensor([4, 0, 5, 0, 1, 2, 3, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 4 to 8
done!
Normal merging for layer 9
tensor([2, 5])
tensor(2)
tensor([4, 7])
tensor(4)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 10 to 13
done!
Normal merging for layer 14
tensor([1, 3])
tensor(1)
tensor([4, 7])
tensor(4)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 15 to 31
done!
all done!
Model size: 11.9458 GB
183
cuda:7
rte
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:40<00:40, 40.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 23.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 26.28s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140237734699680 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140237734699680 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140237734699680 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140237734699680 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140229612273008 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140229612273008 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140229612273008 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140229612273008 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of rte from None to 0
INFO:lm_eval.api.task:Building contexts for rte on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2616.04it/s]
DEBUG:lm_eval.evaluator:Task: rte; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<06:17,  1.90s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:03<03:19,  1.01s/it]Running loglikelihood requests:   2%|▎         | 5/200 [00:04<02:39,  1.22it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:05<02:20,  1.37it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:06<02:09,  1.47it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:08<02:02,  1.54it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:09<01:57,  1.59it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:10<01:53,  1.63it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:11<01:49,  1.67it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:12<01:46,  1.70it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:13<01:43,  1.74it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:14<01:40,  1.76it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:16<01:37,  1.79it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:17<01:34,  1.83it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:18<01:31,  1.86it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:19<01:29,  1.89it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:20<01:27,  1.91it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:21<01:25,  1.93it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:22<01:23,  1.94it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:23<01:21,  1.97it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:24<01:19,  2.00it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:25<01:17,  2.04it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:25<01:14,  2.07it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:26<01:12,  2.12it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:27<01:09,  2.16it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:28<01:07,  2.22it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:29<01:04,  2.27it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:30<01:02,  2.33it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:31<01:00,  2.38it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:31<00:58,  2.43it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:32<00:56,  2.47it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:33<00:54,  2.50it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:34<00:53,  2.52it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:34<00:52,  2.55it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:35<00:50,  2.57it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:36<00:49,  2.59it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:37<00:48,  2.61it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:37<00:47,  2.63it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:38<00:46,  2.66it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:39<00:45,  2.67it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:40<00:44,  2.68it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:40<00:43,  2.71it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:41<00:42,  2.73it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:42<00:40,  2.76it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:43<00:39,  2.78it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:43<00:38,  2.81it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:44<00:37,  2.83it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:45<00:36,  2.85it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:45<00:35,  2.87it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:46<00:34,  2.89it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:47<00:34,  2.90it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:47<00:33,  2.91it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:48<00:32,  2.93it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:49<00:31,  2.95it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:49<00:30,  2.96it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:50<00:29,  2.97it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:51<00:29,  2.99it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:51<00:28,  2.99it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:52<00:27,  3.00it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:53<00:26,  3.01it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:53<00:26,  3.02it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:54<00:25,  3.03it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:55<00:24,  3.03it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:55<00:24,  3.04it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:56<00:23,  3.04it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:57<00:22,  3.05it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:57<00:21,  3.07it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:58<00:20,  3.10it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:58<00:20,  3.12it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:59<00:19,  3.14it/s]Running loglikelihood requests:  70%|███████   | 141/200 [01:00<00:18,  3.15it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [01:00<00:17,  3.17it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:01<00:17,  3.19it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [01:02<00:16,  3.20it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:02<00:15,  3.21it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:03<00:15,  3.22it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:03<00:14,  3.23it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:04<00:13,  3.24it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:05<00:13,  3.25it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:05<00:12,  3.28it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:06<00:11,  3.30it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:06<00:11,  3.31it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:07<00:10,  3.32it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:08<00:09,  3.34it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:08<00:09,  3.36it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:09<00:08,  3.33it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:09<00:08,  3.33it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:10<00:07,  3.35it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:11<00:06,  3.38it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:11<00:06,  3.40it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:12<00:05,  3.42it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:12<00:04,  3.44it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:13<00:04,  3.45it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:14<00:03,  3.47it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:14<00:03,  3.48it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:15<00:02,  3.50it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:15<00:01,  3.52it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:16<00:01,  3.53it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:16<00:00,  3.61it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:17<00:00,  3.67it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:17<00:00,  2.59it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:0'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}
full model:
{'rte': {'alias': 'rte', 'acc,none': 0.5, 'acc_stderr,none': 0.050251890762960605}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.34161229456626735
0.905233410256777
0.5205040718697735
0.4121994254524892
0.7398116665887099
0.6225415196831932
0.7923970242263771
0.7353888240887675
0.6535613357308766
0.7757058271862038
0.734046359122903
0.4471799126982846
0.773619360921301
0.7955347039939479
0.8672068064531693
0.8652880343596522
0.3302235467760883
0.6789268064017625
0.6072221471952108
0.9194446824778495
0.4812004589187253
0.5728915095234594
0.1682455054057436
0.93212414632396
0.9148362604533635
0.8268537756297094
0.7592245907029287
0.7256008379011685
0.7109756105942956
0.34161229456626735
0.905233410256777
0.5205040718697735
0.4121994254524892
0.7398116665887099
0.6225415196831932
0.7923970242263771
0.7353888240887675
0.6535613357308766
0.7757058271862038
0.734046359122903
0.4471799126982846
0.773619360921301
0.7955347039939479
0.8672068064531693
0.8652880343596522
0.3302235467760883
0.6789268064017625
0.6072221471952108
0.9194446824778495
0.4812004589187253
Total groups 73 exceeded the threshold, stopping comparison.
The group tensor is
[5, 2, 7, 1, 6, 4, 3, 0]
tensor([5, 2, 7, 1, 6, 4, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 6, 0, 7, 3, 4, 1]
tensor([5, 2, 6, 0, 7, 3, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 7, 2, 5, 4, 3, 0]
tensor([6, 1, 7, 2, 5, 4, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 2, 4, 1, 5, 0]
tensor([6, 3, 7, 2, 4, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 5, 6, 2, 3, 1, 4, 0]
tensor([7, 5, 6, 2, 3, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 5, 4, 1, 0, 1, 3]
tensor([0, 2, 5, 4, 1, 0, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1.0, 1]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 4
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 5 to 8
done!
Normal merging for layer 9
tensor([0, 5])
tensor(0)
tensor([4, 6])
tensor(4)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 10 to 30
done!
Normal merging for layer 31
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
all done!
Model size: 12.1348 GB
56
cuda:0
fda
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:40<00:40, 40.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 23.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 26.24s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:None: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/hazyresearch/based-fda HTTP/1.1" 200 1036
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/hazyresearch/based-fda/hazyresearch/based-fda.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/hazyresearch/based-fda HTTP/1.1" 200 1043
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/hazyresearch/based-fda/resolve/42569d301e12fbcf8d5a69e04e892aa013e20314/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/hazyresearch/based-fda/revision/42569d301e12fbcf8d5a69e04e892aa013e20314 HTTP/1.1" 200 1043
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/hazyresearch/based-fda/tree/42569d301e12fbcf8d5a69e04e892aa013e20314?recursive=False&expand=False HTTP/1.1" 200 291
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/hazyresearch/based-fda/paths-info/42569d301e12fbcf8d5a69e04e892aa013e20314 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/hazyresearch/based-fda/tree/42569d301e12fbcf8d5a69e04e892aa013e20314/data?recursive=False&expand=False HTTP/1.1" 200 484
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/hazyresearch/based-fda/paths-info/42569d301e12fbcf8d5a69e04e892aa013e20314 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/hazyresearch/based-fda/revision/42569d301e12fbcf8d5a69e04e892aa013e20314 HTTP/1.1" 200 1043
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/hazyresearch/based-fda/resolve/42569d301e12fbcf8d5a69e04e892aa013e20314/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/hazyresearch/based-fda/paths-info/42569d301e12fbcf8d5a69e04e892aa013e20314 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/hazyresearch/based-fda/paths-info/42569d301e12fbcf8d5a69e04e892aa013e20314 HTTP/1.1" 200 218
DEBUG:filelock:Attempting to acquire lock 140237735055616 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_hazyresearch___based-fda_default_0.0.0_42569d301e12fbcf8d5a69e04e892aa013e20314.lock
DEBUG:filelock:Lock 140237735055616 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_hazyresearch___based-fda_default_0.0.0_42569d301e12fbcf8d5a69e04e892aa013e20314.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/hazyresearch___based-fda/default/0.0.0/42569d301e12fbcf8d5a69e04e892aa013e20314/dataset_info.json
DEBUG:filelock:Attempting to release lock 140237735055616 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_hazyresearch___based-fda_default_0.0.0_42569d301e12fbcf8d5a69e04e892aa013e20314.lock
DEBUG:filelock:Lock 140237735055616 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_hazyresearch___based-fda_default_0.0.0_42569d301e12fbcf8d5a69e04e892aa013e20314.lock
DEBUG:filelock:Attempting to acquire lock 140237735863664 on /public/home/zouyifei001/.cache/huggingface/datasets/hazyresearch___based-fda/default/0.0.0/42569d301e12fbcf8d5a69e04e892aa013e20314_builder.lock
DEBUG:filelock:Lock 140237735863664 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/hazyresearch___based-fda/default/0.0.0/42569d301e12fbcf8d5a69e04e892aa013e20314_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/hazyresearch___based-fda/default/0.0.0/42569d301e12fbcf8d5a69e04e892aa013e20314/dataset_info.json
DEBUG:filelock:Attempting to release lock 140237735863664 on /public/home/zouyifei001/.cache/huggingface/datasets/hazyresearch___based-fda/default/0.0.0/42569d301e12fbcf8d5a69e04e892aa013e20314_builder.lock
DEBUG:filelock:Lock 140237735863664 released on /public/home/zouyifei001/.cache/huggingface/datasets/hazyresearch___based-fda/default/0.0.0/42569d301e12fbcf8d5a69e04e892aa013e20314_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
INFO:lm_eval.evaluator:fda: Using gen_kwargs: {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of fda from None to 0
INFO:lm_eval.api.task:Building contexts for fda on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 281875.27it/s]
DEBUG:lm_eval.evaluator:Task: fda; number of requests on this rank: 100
INFO:lm_eval.evaluator:Running generate_until requests
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/100 [00:18<29:52, 18.10s/it]Running generate_until requests:   2%|▏         | 2/100 [00:30<24:23, 14.93s/it]Running generate_until requests:   3%|▎         | 3/100 [00:42<22:04, 13.66s/it]Running generate_until requests:   4%|▍         | 4/100 [00:59<23:44, 14.84s/it]Running generate_until requests:   5%|▌         | 5/100 [01:13<23:12, 14.66s/it]Running generate_until requests:   6%|▌         | 6/100 [01:27<22:13, 14.19s/it]Running generate_until requests:   7%|▋         | 7/100 [01:43<23:13, 14.98s/it]Running generate_until requests:   8%|▊         | 8/100 [02:00<23:44, 15.48s/it]Running generate_until requests:   9%|▉         | 9/100 [02:12<21:40, 14.29s/it]Running generate_until requests:  10%|█         | 10/100 [02:22<19:44, 13.16s/it]Running generate_until requests:  11%|█         | 11/100 [02:33<18:23, 12.40s/it]Running generate_until requests:  12%|█▏        | 12/100 [02:49<19:59, 13.63s/it]Running generate_until requests:  13%|█▎        | 13/100 [03:00<18:40, 12.88s/it]Running generate_until requests:  14%|█▍        | 14/100 [03:17<20:00, 13.96s/it]Running generate_until requests:  15%|█▌        | 15/100 [03:28<18:31, 13.08s/it]Running generate_until requests:  16%|█▌        | 16/100 [03:40<17:40, 12.62s/it]Running generate_until requests:  17%|█▋        | 17/100 [03:51<16:55, 12.23s/it]Running generate_until requests:  18%|█▊        | 18/100 [04:07<18:26, 13.49s/it]Running generate_until requests:  19%|█▉        | 19/100 [04:18<17:12, 12.75s/it]Running generate_until requests:  20%|██        | 20/100 [04:35<18:29, 13.87s/it]Running generate_until requests:  21%|██        | 21/100 [04:51<19:16, 14.64s/it]Running generate_until requests:  22%|██▏       | 22/100 [05:05<18:30, 14.24s/it]Running generate_until requests:  23%|██▎       | 23/100 [05:21<19:07, 14.90s/it]Running generate_until requests:  24%|██▍       | 24/100 [05:32<17:35, 13.89s/it]Running generate_until requests:  25%|██▌       | 25/100 [05:49<18:21, 14.68s/it]Running generate_until requests:  26%|██▌       | 26/100 [06:05<18:42, 15.17s/it]Running generate_until requests:  27%|██▋       | 27/100 [06:22<18:55, 15.56s/it]Running generate_until requests:  28%|██▊       | 28/100 [06:34<17:21, 14.47s/it]Running generate_until requests:  29%|██▉       | 29/100 [06:50<17:47, 15.04s/it]Running generate_until requests:  30%|███       | 30/100 [07:03<16:43, 14.34s/it]Running generate_until requests:  31%|███       | 31/100 [07:19<17:11, 14.95s/it]Running generate_until requests:  32%|███▏      | 32/100 [07:31<16:00, 14.12s/it]Running generate_until requests:  33%|███▎      | 33/100 [07:48<16:30, 14.78s/it]Running generate_until requests:  34%|███▍      | 34/100 [08:04<16:47, 15.26s/it]Running generate_until requests:  35%|███▌      | 35/100 [08:16<15:36, 14.41s/it]Running generate_until requests:  36%|███▌      | 36/100 [08:28<14:26, 13.54s/it]Running generate_until requests:  37%|███▋      | 37/100 [08:39<13:25, 12.78s/it]Running generate_until requests:  38%|███▊      | 38/100 [08:51<13:05, 12.67s/it]Running generate_until requests:  39%|███▉      | 39/100 [09:08<13:59, 13.76s/it]Running generate_until requests:  40%|████      | 40/100 [09:20<13:13, 13.22s/it]Running generate_until requests:  41%|████      | 41/100 [09:36<13:54, 14.15s/it]Running generate_until requests:  42%|████▏     | 42/100 [09:47<12:47, 13.24s/it]Running generate_until requests:  43%|████▎     | 43/100 [09:58<11:59, 12.62s/it]Running generate_until requests:  44%|████▍     | 44/100 [10:10<11:24, 12.22s/it]Running generate_until requests:  45%|████▌     | 45/100 [10:20<10:41, 11.67s/it]Running generate_until requests:  46%|████▌     | 46/100 [10:36<11:44, 13.04s/it]Running generate_until requests:  47%|████▋     | 47/100 [10:52<12:22, 14.00s/it]Running generate_until requests:  48%|████▊     | 48/100 [11:04<11:29, 13.27s/it]Running generate_until requests:  49%|████▉     | 49/100 [11:15<10:36, 12.48s/it]Running generate_until requests:  50%|█████     | 50/100 [11:27<10:22, 12.45s/it]Running generate_until requests:  51%|█████     | 51/100 [11:38<09:53, 12.10s/it]Running generate_until requests:  52%|█████▏    | 52/100 [11:56<10:57, 13.69s/it]Running generate_until requests:  53%|█████▎    | 53/100 [12:07<10:11, 13.00s/it]Running generate_until requests:  54%|█████▍    | 54/100 [12:18<09:34, 12.48s/it]Running generate_until requests:  55%|█████▌    | 55/100 [12:35<10:11, 13.59s/it]Running generate_until requests:  56%|█████▌    | 56/100 [12:46<09:24, 12.84s/it]Running generate_until requests:  57%|█████▋    | 57/100 [13:00<09:34, 13.35s/it]Running generate_until requests:  58%|█████▊    | 58/100 [13:13<09:17, 13.28s/it]Running generate_until requests:  59%|█████▉    | 59/100 [13:29<09:39, 14.13s/it]Running generate_until requests:  60%|██████    | 60/100 [13:45<09:48, 14.72s/it]Running generate_until requests:  61%|██████    | 61/100 [14:01<09:41, 14.90s/it]Running generate_until requests:  62%|██████▏   | 62/100 [14:12<08:47, 13.87s/it]Running generate_until requests:  63%|██████▎   | 63/100 [14:23<07:56, 12.87s/it]Running generate_until requests:  64%|██████▍   | 64/100 [14:34<07:19, 12.21s/it]Running generate_until requests:  65%|██████▌   | 65/100 [14:50<07:48, 13.37s/it]Running generate_until requests:  66%|██████▌   | 66/100 [15:06<08:02, 14.18s/it]Running generate_until requests:  67%|██████▋   | 67/100 [15:22<08:06, 14.76s/it]Running generate_until requests:  68%|██████▊   | 68/100 [15:38<08:04, 15.15s/it]Running generate_until requests:  69%|██████▉   | 69/100 [15:54<07:58, 15.42s/it]Running generate_until requests:  70%|███████   | 70/100 [16:07<07:19, 14.65s/it]Running generate_until requests:  71%|███████   | 71/100 [16:23<07:17, 15.08s/it]Running generate_until requests:  72%|███████▏  | 72/100 [16:39<07:10, 15.38s/it]Running generate_until requests:  73%|███████▎  | 73/100 [16:51<06:28, 14.41s/it]Running generate_until requests:  74%|███████▍  | 74/100 [17:07<06:29, 14.97s/it]Running generate_until requests:  75%|███████▌  | 75/100 [17:19<05:46, 13.88s/it]Running generate_until requests:  76%|███████▌  | 76/100 [17:29<05:10, 12.93s/it]Running generate_until requests:  77%|███████▋  | 77/100 [17:40<04:41, 12.23s/it]Running generate_until requests:  78%|███████▊  | 78/100 [17:51<04:18, 11.74s/it]Running generate_until requests:  79%|███████▉  | 79/100 [18:06<04:32, 12.99s/it]Running generate_until requests:  80%|████████  | 80/100 [18:19<04:15, 12.78s/it]Running generate_until requests:  81%|████████  | 81/100 [18:35<04:20, 13.72s/it]Running generate_until requests:  82%|████████▏ | 82/100 [18:51<04:21, 14.51s/it]Running generate_until requests:  83%|████████▎ | 83/100 [19:03<03:55, 13.84s/it]Running generate_until requests:  84%|████████▍ | 84/100 [19:16<03:34, 13.41s/it]Running generate_until requests:  85%|████████▌ | 85/100 [19:26<03:07, 12.52s/it]Running generate_until requests:  86%|████████▌ | 86/100 [19:41<03:05, 13.26s/it]Running generate_until requests:  87%|████████▋ | 87/100 [19:52<02:42, 12.53s/it]Running generate_until requests:  88%|████████▊ | 88/100 [20:03<02:24, 12.02s/it]Running generate_until requests:  89%|████████▉ | 89/100 [20:17<02:19, 12.70s/it]Running generate_until requests:  90%|█████████ | 90/100 [20:27<01:57, 11.76s/it]Running generate_until requests:  91%|█████████ | 91/100 [20:33<01:30, 10.03s/it]Running generate_until requests:  92%|█████████▏| 92/100 [20:40<01:12,  9.12s/it]Running generate_until requests:  93%|█████████▎| 93/100 [20:49<01:05,  9.31s/it]Running generate_until requests:  94%|█████████▍| 94/100 [20:57<00:52,  8.75s/it]Running generate_until requests:  95%|█████████▌| 95/100 [21:03<00:39,  7.85s/it]Running generate_until requests:  96%|█████████▌| 96/100 [21:12<00:33,  8.39s/it]Running generate_until requests:  97%|█████████▋| 97/100 [21:16<00:21,  7.07s/it]Running generate_until requests:  98%|█████████▊| 98/100 [21:24<00:14,  7.31s/it]Running generate_until requests:  99%|█████████▉| 99/100 [21:32<00:07,  7.43s/it]Running generate_until requests: 100%|██████████| 100/100 [21:36<00:00,  6.39s/it]Running generate_until requests: 100%|██████████| 100/100 [21:36<00:00, 12.96s/it]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:1'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:1'}
full model:
{'fda': {'alias': 'fda', 'contains,none': np.float64(0.87), 'contains_stderr,none': 'N/A'}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.21659225770002863
0.3676799923890141
0.7718276709674655
0.684178265870135
0.5285482451872934
0.273816514287302
0.7566340923804737
0.4195302615712732
0.7048918175177445
0.8024335694665047
0.2596571673087474
0.2906758673048459
0.6559956854150746
0.2288937672529506
0.2941846927212091
0.14545644431597915
0.27237013713290037
0.1954083722558289
0.191889756656264
0.3231769309584375
0.763391177198558
0.7591908569872912
0.47699609290977873
0.24147181909203896
0.6276241833035655
0.5039903698909594
0.506775271104323
0.17806387173399968
0.3609346134453604
0.21659225770002863
0.3676799923890141
0.7718276709674655
0.684178265870135
0.5285482451872934
0.273816514287302
0.7566340923804737
0.4195302615712732
0.7048918175177445
0.8024335694665047
0.2596571673087474
0.2906758673048459
0.6559956854150746
0.2288937672529506
0.2941846927212091
0.14545644431597915
0.27237013713290037
0.1954083722558289
0.191889756656264
0.3231769309584375
0.763391177198558
0.7591908569872912
0.47699609290977873
0.24147181909203896
0.6276241833035655
0.5039903698909594
0.506775271104323
0.17806387173399968
0.3609346134453604
0.21659225770002863
0.3676799923890141
0.7718276709674655
0.684178265870135
0.5285482451872934
0.273816514287302
0.7566340923804737
0.4195302615712732
0.7048918175177445
0.8024335694665047
0.2596571673087474
0.2906758673048459
0.6559956854150746
0.2288937672529506
0.2941846927212091
0.14545644431597915
0.27237013713290037
Total groups 70 exceeded the threshold, stopping comparison.
The group tensor is
[6, 2, 7, 0, 5, 1, 4, 3]
tensor([6, 2, 7, 0, 5, 1, 4, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 0, 1, 5, 2, 7, 6, 3]
tensor([4, 0, 1, 5, 2, 7, 6, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 0, 4, 2, 6, 3, 7, 5]
tensor([1, 0, 4, 2, 6, 3, 7, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 0, 6, 3, 5, 1, 4, 2]
tensor([7, 0, 6, 3, 5, 1, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 0, 2, 5, 1, 1, 4]
tensor([0, 3, 0, 2, 5, 1, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 3, 1, 4, 1, 5, 2, 0]
tensor([0, 3, 1, 4, 1, 5, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
done!
Normal merging for layer 2
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
done!
Normal merging for layer 3
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 4 to 11
done!
Normal merging for layer 12
tensor([0, 2])
tensor(0)
tensor([5, 6])
tensor(5)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
done!
Normal merging for layer 13
tensor([0, 7])
tensor(0)
tensor([2, 4])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
done!
Cross-layer merge completed for layers 14 to 31
done!
all done!
Model size: 11.8828 GB
148
cuda:1
rte
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:02<00:00, 28.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:02<00:00, 31.30s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140212903046672 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140212903046672 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140212903046672 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140212903046672 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140229612264176 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140229612264176 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140229612264176 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140229612264176 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of rte from None to 0
INFO:lm_eval.api.task:Building contexts for rte on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2619.64it/s]
DEBUG:lm_eval.evaluator:Task: rte; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<06:16,  1.89s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:03<03:16,  1.00it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:04<02:36,  1.25it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:05<02:17,  1.40it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:06<02:06,  1.51it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:07<01:59,  1.58it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:09<01:54,  1.64it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:10<01:50,  1.68it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:11<01:46,  1.72it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:12<01:43,  1.75it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:13<01:40,  1.79it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:14<01:37,  1.82it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:15<01:35,  1.84it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:16<01:31,  1.88it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:17<01:29,  1.92it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:18<01:26,  1.94it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:19<01:24,  1.97it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:20<01:23,  1.98it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:21<01:21,  2.00it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:22<01:19,  2.02it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:23<01:17,  2.06it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:24<01:14,  2.09it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:25<01:12,  2.14it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:26<01:10,  2.18it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:27<01:07,  2.22it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:27<01:05,  2.28it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:28<01:02,  2.34it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:29<01:00,  2.39it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:30<00:58,  2.45it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:30<00:56,  2.50it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:31<00:54,  2.53it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:32<00:53,  2.57it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:33<00:52,  2.59it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:34<00:50,  2.61it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:34<00:49,  2.64it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:35<00:48,  2.66it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:36<00:47,  2.68it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:36<00:46,  2.70it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:37<00:44,  2.73it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:38<00:43,  2.76it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:39<00:42,  2.78it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:39<00:41,  2.80it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:40<00:40,  2.82it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:41<00:39,  2.84it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:41<00:38,  2.86it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:42<00:37,  2.88it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:43<00:36,  2.90it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:43<00:35,  2.92it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:44<00:35,  2.93it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:45<00:34,  2.95it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:45<00:33,  2.96it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:46<00:32,  2.97it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:47<00:31,  2.98it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:47<00:31,  3.00it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:48<00:30,  3.01it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:49<00:29,  3.02it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:49<00:28,  3.03it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:50<00:27,  3.04it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:51<00:27,  3.05it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:51<00:26,  3.06it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:52<00:25,  3.07it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:53<00:24,  3.08it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:53<00:24,  3.09it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:54<00:23,  3.09it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:55<00:22,  3.10it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:55<00:22,  3.11it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:56<00:21,  3.13it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:56<00:20,  3.16it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:57<00:19,  3.18it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:58<00:19,  3.19it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:58<00:18,  3.20it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:59<00:17,  3.22it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:00<00:16,  3.24it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [01:00<00:16,  3.25it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:01<00:15,  3.26it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:01<00:14,  3.27it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:02<00:14,  3.28it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:03<00:13,  3.29it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:03<00:13,  3.30it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:04<00:12,  3.33it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:04<00:11,  3.35it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:05<00:11,  3.36it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:06<00:10,  3.37it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:06<00:09,  3.39it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:07<00:09,  3.41it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:07<00:08,  3.44it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:08<00:07,  3.45it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:08<00:07,  3.47it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:09<00:06,  3.48it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:10<00:06,  3.49it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:10<00:05,  3.50it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:11<00:04,  3.52it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:11<00:04,  3.53it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:12<00:03,  3.54it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:12<00:03,  3.55it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:13<00:02,  3.57it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:13<00:01,  3.59it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:14<00:01,  3.61it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:14<00:00,  3.69it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:15<00:00,  3.76it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:15<00:00,  2.65it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:2'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:2'}
full model:
{'rte': {'alias': 'rte', 'acc,none': 0.5, 'acc_stderr,none': 0.050251890762960605}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.34161229456626735
0.905233410256777
0.5205040718697735
0.4121994254524892
0.7398116665887099
0.6225415196831932
0.7923970242263771
0.7353888240887675
0.6535613357308766
0.7757058271862038
0.734046359122903
0.4471799126982846
0.773619360921301
0.7955347039939479
0.8672068064531693
0.8652880343596522
0.3302235467760883
0.6789268064017625
0.6072221471952108
0.9194446824778495
0.4812004589187253
0.5728915095234594
0.1682455054057436
0.93212414632396
0.9148362604533635
0.8268537756297094
0.7592245907029287
0.7256008379011685
0.7109756105942956
0.34161229456626735
0.905233410256777
0.5205040718697735
0.4121994254524892
0.7398116665887099
0.6225415196831932
0.7923970242263771
0.7353888240887675
0.6535613357308766
0.7757058271862038
0.734046359122903
0.4471799126982846
0.773619360921301
0.7955347039939479
0.8672068064531693
0.8652880343596522
0.3302235467760883
0.6789268064017625
0.6072221471952108
0.9194446824778495
0.4812004589187253
Total groups 73 exceeded the threshold, stopping comparison.
The group tensor is
[5, 2, 7, 1, 6, 4, 3, 0]
tensor([5, 2, 7, 1, 6, 4, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 6, 0, 7, 3, 4, 1]
tensor([5, 2, 6, 0, 7, 3, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 7, 2, 5, 4, 3, 0]
tensor([6, 1, 7, 2, 5, 4, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 2, 4, 1, 5, 0]
tensor([6, 3, 7, 2, 4, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 5, 6, 2, 3, 1, 4, 0]
tensor([7, 5, 6, 2, 3, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 5, 4, 1, 0, 1, 3]
tensor([0, 2, 5, 4, 1, 0, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1.0, 1]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 4
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 5 to 8
done!
Normal merging for layer 9
tensor([0, 5])
tensor(0)
tensor([4, 6])
tensor(4)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 10 to 30
done!
Normal merging for layer 31
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
all done!
Model size: 12.1348 GB
225
cuda:2
mastermind_24_easy
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:43<00:43, 43.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 25.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 28.01s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_24_mcq_random HTTP/1.1" 200 772
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/flair/mastermind_24_mcq_random/flair/mastermind_24_mcq_random.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_24_mcq_random HTTP/1.1" 200 779
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/flair/mastermind_24_mcq_random/resolve/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/flair/mastermind_24_mcq_random/resolve/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:filelock:Attempting to acquire lock 140237198322640 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_24_mcq_random_default_0.0.0_cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5.lock
DEBUG:filelock:Lock 140237198322640 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_24_mcq_random_default_0.0.0_cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5/dataset_info.json
DEBUG:filelock:Attempting to release lock 140237198322640 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_24_mcq_random_default_0.0.0_cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5.lock
DEBUG:filelock:Lock 140237198322640 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_24_mcq_random_default_0.0.0_cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5.lock
DEBUG:filelock:Attempting to acquire lock 140215182750784 on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5_builder.lock
DEBUG:filelock:Lock 140215182750784 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215182750784 on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5_builder.lock
DEBUG:filelock:Lock 140215182750784 released on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of mastermind_24_easy from None to 0
INFO:lm_eval.api.task:Building contexts for mastermind_24_easy on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1495.50it/s]
DEBUG:lm_eval.evaluator:Task: mastermind_24_easy; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:01<10:47,  1.62s/it]Running loglikelihood requests:   0%|          | 2/400 [00:02<08:31,  1.29s/it]Running loglikelihood requests:   1%|          | 3/400 [00:03<07:47,  1.18s/it]Running loglikelihood requests:   1%|          | 4/400 [00:04<07:26,  1.13s/it]Running loglikelihood requests:   1%|▏         | 5/400 [00:05<07:12,  1.10s/it]Running loglikelihood requests:   2%|▏         | 6/400 [00:06<07:03,  1.08s/it]Running loglikelihood requests:   2%|▏         | 7/400 [00:07<06:57,  1.06s/it]Running loglikelihood requests:   2%|▏         | 8/400 [00:08<06:53,  1.05s/it]Running loglikelihood requests:   2%|▏         | 9/400 [00:09<06:49,  1.05s/it]Running loglikelihood requests:   2%|▎         | 10/400 [00:10<06:47,  1.04s/it]Running loglikelihood requests:   3%|▎         | 12/400 [00:12<05:08,  1.26it/s]Running loglikelihood requests:   3%|▎         | 13/400 [00:13<05:30,  1.17it/s]Running loglikelihood requests:   4%|▎         | 14/400 [00:14<05:47,  1.11it/s]Running loglikelihood requests:   4%|▍         | 16/400 [00:15<04:42,  1.36it/s]Running loglikelihood requests:   4%|▍         | 17/400 [00:16<05:08,  1.24it/s]Running loglikelihood requests:   4%|▍         | 18/400 [00:17<05:29,  1.16it/s]Running loglikelihood requests:   5%|▍         | 19/400 [00:18<05:45,  1.10it/s]Running loglikelihood requests:   5%|▌         | 20/400 [00:19<05:57,  1.06it/s]Running loglikelihood requests:   5%|▌         | 21/400 [00:20<06:05,  1.04it/s]Running loglikelihood requests:   6%|▌         | 22/400 [00:21<06:11,  1.02it/s]Running loglikelihood requests:   6%|▌         | 23/400 [00:22<06:15,  1.00it/s]Running loglikelihood requests:   6%|▋         | 25/400 [00:23<04:51,  1.29it/s]Running loglikelihood requests:   6%|▋         | 26/400 [00:24<05:13,  1.19it/s]Running loglikelihood requests:   7%|▋         | 27/400 [00:25<05:30,  1.13it/s]Running loglikelihood requests:   7%|▋         | 29/400 [00:26<04:30,  1.37it/s]Running loglikelihood requests:   8%|▊         | 30/400 [00:27<04:54,  1.25it/s]Running loglikelihood requests:   8%|▊         | 31/400 [00:28<05:14,  1.17it/s]Running loglikelihood requests:   8%|▊         | 32/400 [00:29<05:30,  1.11it/s]Running loglikelihood requests:   8%|▊         | 34/400 [00:30<04:27,  1.37it/s]Running loglikelihood requests:   9%|▉         | 35/400 [00:31<04:52,  1.25it/s]Running loglikelihood requests:  10%|▉         | 38/400 [00:32<03:26,  1.75it/s]Running loglikelihood requests:  10%|▉         | 39/400 [00:33<03:56,  1.53it/s]Running loglikelihood requests:  10%|█         | 40/400 [00:34<04:24,  1.36it/s]Running loglikelihood requests:  10%|█         | 41/400 [00:35<04:47,  1.25it/s]Running loglikelihood requests:  10%|█         | 42/400 [00:36<05:06,  1.17it/s]Running loglikelihood requests:  11%|█         | 43/400 [00:37<05:20,  1.11it/s]Running loglikelihood requests:  11%|█         | 44/400 [00:38<05:31,  1.08it/s]Running loglikelihood requests:  11%|█▏        | 45/400 [00:39<05:38,  1.05it/s]Running loglikelihood requests:  12%|█▏        | 47/400 [00:40<04:26,  1.33it/s]Running loglikelihood requests:  12%|█▏        | 48/400 [00:41<04:47,  1.22it/s]Running loglikelihood requests:  12%|█▏        | 49/400 [00:42<05:05,  1.15it/s]Running loglikelihood requests:  12%|█▎        | 50/400 [00:43<05:18,  1.10it/s]Running loglikelihood requests:  13%|█▎        | 52/400 [00:44<04:15,  1.36it/s]Running loglikelihood requests:  13%|█▎        | 53/400 [00:45<04:37,  1.25it/s]Running loglikelihood requests:  14%|█▎        | 54/400 [00:46<04:55,  1.17it/s]Running loglikelihood requests:  14%|█▍        | 56/400 [00:47<04:03,  1.41it/s]Running loglikelihood requests:  14%|█▍        | 57/400 [00:48<04:26,  1.29it/s]Running loglikelihood requests:  14%|█▍        | 58/400 [00:49<04:45,  1.20it/s]Running loglikelihood requests:  15%|█▍        | 59/400 [00:50<04:59,  1.14it/s]Running loglikelihood requests:  15%|█▌        | 60/400 [00:51<05:10,  1.10it/s]Running loglikelihood requests:  15%|█▌        | 61/400 [00:52<05:18,  1.06it/s]Running loglikelihood requests:  16%|█▌        | 62/400 [00:53<05:23,  1.04it/s]Running loglikelihood requests:  16%|█▌        | 63/400 [00:54<05:27,  1.03it/s]Running loglikelihood requests:  16%|█▌        | 64/400 [00:55<05:29,  1.02it/s]Running loglikelihood requests:  16%|█▋        | 65/400 [00:56<05:30,  1.01it/s]Running loglikelihood requests:  16%|█▋        | 66/400 [00:57<05:30,  1.01it/s]Running loglikelihood requests:  17%|█▋        | 67/400 [00:58<05:30,  1.01it/s]Running loglikelihood requests:  17%|█▋        | 68/400 [00:59<05:30,  1.01it/s]Running loglikelihood requests:  17%|█▋        | 69/400 [01:00<05:29,  1.00it/s]Running loglikelihood requests:  18%|█▊        | 70/400 [01:01<05:28,  1.00it/s]Running loglikelihood requests:  18%|█▊        | 71/400 [01:02<05:27,  1.00it/s]Running loglikelihood requests:  18%|█▊        | 72/400 [01:03<05:26,  1.00it/s]Running loglikelihood requests:  18%|█▊        | 73/400 [01:04<05:25,  1.00it/s]Running loglikelihood requests:  18%|█▊        | 74/400 [01:05<05:24,  1.00it/s]Running loglikelihood requests:  19%|█▉        | 76/400 [01:06<04:08,  1.30it/s]Running loglikelihood requests:  19%|█▉        | 77/400 [01:07<04:25,  1.21it/s]Running loglikelihood requests:  20%|█▉        | 78/400 [01:08<04:39,  1.15it/s]Running loglikelihood requests:  20%|█▉        | 79/400 [01:09<04:49,  1.11it/s]Running loglikelihood requests:  20%|██        | 80/400 [01:10<04:56,  1.08it/s]Running loglikelihood requests:  20%|██        | 81/400 [01:11<05:01,  1.06it/s]Running loglikelihood requests:  20%|██        | 82/400 [01:12<05:04,  1.04it/s]Running loglikelihood requests:  21%|██        | 83/400 [01:13<05:06,  1.03it/s]Running loglikelihood requests:  21%|██        | 84/400 [01:14<05:07,  1.03it/s]Running loglikelihood requests:  21%|██▏       | 85/400 [01:15<05:08,  1.02it/s]Running loglikelihood requests:  22%|██▏       | 87/400 [01:16<03:57,  1.32it/s]Running loglikelihood requests:  22%|██▏       | 88/400 [01:17<04:14,  1.23it/s]Running loglikelihood requests:  22%|██▏       | 89/400 [01:18<04:27,  1.16it/s]Running loglikelihood requests:  22%|██▎       | 90/400 [01:19<04:36,  1.12it/s]Running loglikelihood requests:  23%|██▎       | 91/400 [01:20<04:43,  1.09it/s]Running loglikelihood requests:  23%|██▎       | 92/400 [01:21<04:48,  1.07it/s]Running loglikelihood requests:  24%|██▎       | 94/400 [01:22<03:45,  1.35it/s]Running loglikelihood requests:  24%|██▍       | 95/400 [01:23<04:03,  1.25it/s]Running loglikelihood requests:  24%|██▍       | 96/400 [01:24<04:17,  1.18it/s]Running loglikelihood requests:  24%|██▍       | 98/400 [01:25<03:30,  1.43it/s]Running loglikelihood requests:  25%|██▌       | 100/400 [01:26<03:06,  1.61it/s]Running loglikelihood requests:  26%|██▌       | 102/400 [01:27<02:51,  1.74it/s]Running loglikelihood requests:  26%|██▌       | 103/400 [01:28<03:15,  1.52it/s]Running loglikelihood requests:  26%|██▌       | 104/400 [01:29<03:36,  1.37it/s]Running loglikelihood requests:  27%|██▋       | 107/400 [01:30<02:36,  1.87it/s]Running loglikelihood requests:  27%|██▋       | 108/400 [01:31<03:01,  1.61it/s]Running loglikelihood requests:  27%|██▋       | 109/400 [01:32<03:22,  1.43it/s]Running loglikelihood requests:  28%|██▊       | 110/400 [01:33<03:41,  1.31it/s]Running loglikelihood requests:  28%|██▊       | 111/400 [01:34<03:56,  1.22it/s]Running loglikelihood requests:  28%|██▊       | 112/400 [01:35<04:08,  1.16it/s]Running loglikelihood requests:  28%|██▊       | 113/400 [01:36<04:17,  1.12it/s]Running loglikelihood requests:  28%|██▊       | 114/400 [01:37<04:22,  1.09it/s]Running loglikelihood requests:  29%|██▉       | 116/400 [01:38<03:26,  1.37it/s]Running loglikelihood requests:  29%|██▉       | 117/400 [01:39<03:42,  1.27it/s]Running loglikelihood requests:  30%|██▉       | 118/400 [01:40<03:55,  1.20it/s]Running loglikelihood requests:  30%|███       | 121/400 [01:41<02:38,  1.76it/s]Running loglikelihood requests:  30%|███       | 122/400 [01:42<03:00,  1.54it/s]Running loglikelihood requests:  31%|███       | 123/400 [01:43<03:19,  1.39it/s]Running loglikelihood requests:  31%|███       | 124/400 [01:44<03:35,  1.28it/s]Running loglikelihood requests:  31%|███▏      | 125/400 [01:45<03:46,  1.21it/s]Running loglikelihood requests:  32%|███▏      | 126/400 [01:46<03:55,  1.17it/s]Running loglikelihood requests:  32%|███▏      | 127/400 [01:47<04:00,  1.13it/s]Running loglikelihood requests:  32%|███▏      | 128/400 [01:48<04:05,  1.11it/s]Running loglikelihood requests:  32%|███▎      | 130/400 [01:48<03:11,  1.41it/s]Running loglikelihood requests:  33%|███▎      | 132/400 [01:49<02:45,  1.62it/s]Running loglikelihood requests:  33%|███▎      | 133/400 [01:50<03:03,  1.45it/s]Running loglikelihood requests:  34%|███▎      | 134/400 [01:51<03:19,  1.33it/s]Running loglikelihood requests:  34%|███▍      | 135/400 [01:52<03:37,  1.22it/s]Running loglikelihood requests:  34%|███▍      | 136/400 [01:53<03:46,  1.16it/s]Running loglikelihood requests:  34%|███▍      | 137/400 [01:54<03:51,  1.13it/s]Running loglikelihood requests:  34%|███▍      | 138/400 [01:55<03:55,  1.11it/s]Running loglikelihood requests:  35%|███▍      | 139/400 [01:56<03:57,  1.10it/s]Running loglikelihood requests:  35%|███▌      | 140/400 [01:57<03:58,  1.09it/s]Running loglikelihood requests:  35%|███▌      | 141/400 [01:58<03:58,  1.09it/s]Running loglikelihood requests:  36%|███▌      | 142/400 [01:59<03:58,  1.08it/s]Running loglikelihood requests:  36%|███▌      | 143/400 [02:00<03:58,  1.08it/s]Running loglikelihood requests:  36%|███▋      | 145/400 [02:01<03:02,  1.40it/s]Running loglikelihood requests:  36%|███▋      | 146/400 [02:02<03:15,  1.30it/s]Running loglikelihood requests:  37%|███▋      | 147/400 [02:03<03:25,  1.23it/s]Running loglikelihood requests:  37%|███▋      | 148/400 [02:04<03:33,  1.18it/s]Running loglikelihood requests:  37%|███▋      | 149/400 [02:05<03:38,  1.15it/s]Running loglikelihood requests:  38%|███▊      | 150/400 [02:05<03:42,  1.13it/s]Running loglikelihood requests:  38%|███▊      | 152/400 [02:06<02:53,  1.43it/s]Running loglikelihood requests:  38%|███▊      | 153/400 [02:07<03:06,  1.32it/s]Running loglikelihood requests:  38%|███▊      | 154/400 [02:08<03:17,  1.25it/s]Running loglikelihood requests:  39%|███▉      | 155/400 [02:09<03:25,  1.19it/s]Running loglikelihood requests:  39%|███▉      | 157/400 [02:10<02:44,  1.48it/s]Running loglikelihood requests:  40%|███▉      | 158/400 [02:11<02:58,  1.36it/s]Running loglikelihood requests:  40%|████      | 160/400 [02:12<02:30,  1.59it/s]Running loglikelihood requests:  40%|████      | 161/400 [02:13<02:46,  1.44it/s]Running loglikelihood requests:  40%|████      | 162/400 [02:14<02:58,  1.33it/s]Running loglikelihood requests:  41%|████      | 163/400 [02:15<03:09,  1.25it/s]Running loglikelihood requests:  41%|████▏     | 165/400 [02:16<02:34,  1.52it/s]Running loglikelihood requests:  42%|████▏     | 166/400 [02:17<02:48,  1.38it/s]Running loglikelihood requests:  42%|████▏     | 167/400 [02:18<03:00,  1.29it/s]Running loglikelihood requests:  42%|████▏     | 168/400 [02:19<03:09,  1.23it/s]Running loglikelihood requests:  42%|████▎     | 170/400 [02:19<02:33,  1.50it/s]Running loglikelihood requests:  43%|████▎     | 171/400 [02:20<02:46,  1.37it/s]Running loglikelihood requests:  43%|████▎     | 172/400 [02:21<02:57,  1.28it/s]Running loglikelihood requests:  43%|████▎     | 173/400 [02:22<03:05,  1.22it/s]Running loglikelihood requests:  44%|████▎     | 174/400 [02:23<03:12,  1.18it/s]Running loglikelihood requests:  44%|████▍     | 175/400 [02:24<03:16,  1.15it/s]Running loglikelihood requests:  44%|████▍     | 176/400 [02:25<03:19,  1.13it/s]Running loglikelihood requests:  44%|████▍     | 178/400 [02:26<02:35,  1.43it/s]Running loglikelihood requests:  45%|████▍     | 179/400 [02:27<02:46,  1.33it/s]Running loglikelihood requests:  45%|████▌     | 180/400 [02:28<02:55,  1.25it/s]Running loglikelihood requests:  45%|████▌     | 181/400 [02:29<03:02,  1.20it/s]Running loglikelihood requests:  46%|████▌     | 183/400 [02:30<02:26,  1.48it/s]Running loglikelihood requests:  46%|████▌     | 184/400 [02:31<02:38,  1.36it/s]Running loglikelihood requests:  47%|████▋     | 187/400 [02:32<01:50,  1.92it/s]Running loglikelihood requests:  47%|████▋     | 188/400 [02:32<02:06,  1.67it/s]Running loglikelihood requests:  47%|████▋     | 189/400 [02:33<02:21,  1.49it/s]Running loglikelihood requests:  48%|████▊     | 190/400 [02:34<02:33,  1.37it/s]Running loglikelihood requests:  48%|████▊     | 191/400 [02:35<02:42,  1.28it/s]Running loglikelihood requests:  48%|████▊     | 192/400 [02:36<02:50,  1.22it/s]Running loglikelihood requests:  48%|████▊     | 193/400 [02:37<02:55,  1.18it/s]Running loglikelihood requests:  48%|████▊     | 194/400 [02:38<02:58,  1.15it/s]Running loglikelihood requests:  49%|████▉     | 196/400 [02:39<02:20,  1.45it/s]Running loglikelihood requests:  49%|████▉     | 197/400 [02:40<02:31,  1.34it/s]Running loglikelihood requests:  50%|████▉     | 198/400 [02:41<02:39,  1.26it/s]Running loglikelihood requests:  50%|████▉     | 199/400 [02:42<02:46,  1.21it/s]Running loglikelihood requests:  50%|█████     | 200/400 [02:43<02:50,  1.17it/s]Running loglikelihood requests:  50%|█████     | 202/400 [02:44<02:14,  1.47it/s]Running loglikelihood requests:  51%|█████     | 203/400 [02:44<02:25,  1.36it/s]Running loglikelihood requests:  51%|█████     | 204/400 [02:45<02:33,  1.27it/s]Running loglikelihood requests:  51%|█████▏    | 205/400 [02:46<02:40,  1.22it/s]Running loglikelihood requests:  52%|█████▏    | 206/400 [02:47<02:45,  1.18it/s]Running loglikelihood requests:  52%|█████▏    | 207/400 [02:48<02:48,  1.15it/s]Running loglikelihood requests:  52%|█████▎    | 210/400 [02:49<01:46,  1.78it/s]Running loglikelihood requests:  53%|█████▎    | 212/400 [02:50<01:39,  1.89it/s]Running loglikelihood requests:  53%|█████▎    | 213/400 [02:51<01:53,  1.65it/s]Running loglikelihood requests:  54%|█████▍    | 215/400 [02:52<01:42,  1.80it/s]Running loglikelihood requests:  54%|█████▍    | 216/400 [02:53<01:55,  1.59it/s]Running loglikelihood requests:  54%|█████▍    | 217/400 [02:54<02:07,  1.44it/s]Running loglikelihood requests:  55%|█████▍    | 219/400 [02:55<01:49,  1.66it/s]Running loglikelihood requests:  55%|█████▌    | 220/400 [02:56<02:01,  1.48it/s]Running loglikelihood requests:  55%|█████▌    | 221/400 [02:56<02:11,  1.37it/s]Running loglikelihood requests:  56%|█████▌    | 222/400 [02:57<02:18,  1.28it/s]Running loglikelihood requests:  56%|█████▌    | 223/400 [02:58<02:24,  1.22it/s]Running loglikelihood requests:  56%|█████▌    | 224/400 [02:59<02:28,  1.18it/s]Running loglikelihood requests:  56%|█████▋    | 225/400 [03:00<02:31,  1.15it/s]Running loglikelihood requests:  56%|█████▋    | 226/400 [03:01<02:34,  1.13it/s]Running loglikelihood requests:  57%|█████▋    | 227/400 [03:02<02:35,  1.11it/s]Running loglikelihood requests:  57%|█████▋    | 228/400 [03:03<02:35,  1.10it/s]Running loglikelihood requests:  57%|█████▋    | 229/400 [03:04<02:35,  1.10it/s]Running loglikelihood requests:  57%|█████▊    | 230/400 [03:05<02:35,  1.09it/s]Running loglikelihood requests:  58%|█████▊    | 231/400 [03:06<02:35,  1.09it/s]Running loglikelihood requests:  58%|█████▊    | 232/400 [03:07<02:34,  1.09it/s]Running loglikelihood requests:  58%|█████▊    | 233/400 [03:08<02:33,  1.09it/s]Running loglikelihood requests:  59%|█████▉    | 235/400 [03:08<01:56,  1.41it/s]Running loglikelihood requests:  59%|█████▉    | 236/400 [03:09<02:04,  1.31it/s]Running loglikelihood requests:  59%|█████▉    | 237/400 [03:10<02:11,  1.24it/s]Running loglikelihood requests:  60%|█████▉    | 238/400 [03:11<02:15,  1.20it/s]Running loglikelihood requests:  60%|█████▉    | 239/400 [03:12<02:18,  1.16it/s]Running loglikelihood requests:  60%|██████    | 240/400 [03:13<02:20,  1.14it/s]Running loglikelihood requests:  60%|██████    | 242/400 [03:14<01:49,  1.45it/s]Running loglikelihood requests:  61%|██████    | 243/400 [03:15<01:57,  1.34it/s]Running loglikelihood requests:  61%|██████    | 244/400 [03:16<02:03,  1.26it/s]Running loglikelihood requests:  61%|██████▏   | 245/400 [03:17<02:08,  1.21it/s]Running loglikelihood requests:  62%|██████▏   | 246/400 [03:18<02:11,  1.17it/s]Running loglikelihood requests:  62%|██████▏   | 247/400 [03:19<02:13,  1.15it/s]Running loglikelihood requests:  62%|██████▏   | 249/400 [03:20<01:43,  1.46it/s]Running loglikelihood requests:  62%|██████▎   | 250/400 [03:20<01:51,  1.35it/s]Running loglikelihood requests:  63%|██████▎   | 251/400 [03:21<01:57,  1.27it/s]Running loglikelihood requests:  63%|██████▎   | 252/400 [03:22<02:01,  1.22it/s]Running loglikelihood requests:  63%|██████▎   | 253/400 [03:23<02:04,  1.18it/s]Running loglikelihood requests:  64%|██████▎   | 254/400 [03:24<02:06,  1.15it/s]Running loglikelihood requests:  64%|██████▍   | 255/400 [03:25<02:07,  1.13it/s]Running loglikelihood requests:  64%|██████▍   | 256/400 [03:26<02:08,  1.12it/s]Running loglikelihood requests:  64%|██████▍   | 257/400 [03:27<02:08,  1.11it/s]Running loglikelihood requests:  64%|██████▍   | 258/400 [03:28<02:08,  1.11it/s]Running loglikelihood requests:  65%|██████▌   | 260/400 [03:29<01:37,  1.43it/s]Running loglikelihood requests:  65%|██████▌   | 261/400 [03:30<01:44,  1.33it/s]Running loglikelihood requests:  66%|██████▌   | 264/400 [03:31<01:11,  1.91it/s]Running loglikelihood requests:  66%|██████▋   | 265/400 [03:31<01:21,  1.67it/s]Running loglikelihood requests:  66%|██████▋   | 266/400 [03:32<01:29,  1.49it/s]Running loglikelihood requests:  67%|██████▋   | 267/400 [03:33<01:36,  1.37it/s]Running loglikelihood requests:  67%|██████▋   | 268/400 [03:34<01:42,  1.29it/s]Running loglikelihood requests:  67%|██████▋   | 269/400 [03:35<01:46,  1.23it/s]Running loglikelihood requests:  68%|██████▊   | 270/400 [03:36<01:49,  1.19it/s]Running loglikelihood requests:  68%|██████▊   | 271/400 [03:37<01:50,  1.16it/s]Running loglikelihood requests:  68%|██████▊   | 272/400 [03:38<01:51,  1.14it/s]Running loglikelihood requests:  68%|██████▊   | 273/400 [03:39<01:52,  1.13it/s]Running loglikelihood requests:  68%|██████▊   | 274/400 [03:40<01:52,  1.12it/s]Running loglikelihood requests:  69%|██████▉   | 277/400 [03:41<01:09,  1.77it/s]Running loglikelihood requests:  70%|██████▉   | 279/400 [03:41<01:03,  1.90it/s]Running loglikelihood requests:  70%|███████   | 281/400 [03:42<00:59,  1.99it/s]Running loglikelihood requests:  71%|███████   | 284/400 [03:43<00:48,  2.39it/s]Running loglikelihood requests:  71%|███████▏  | 285/400 [03:44<00:57,  2.00it/s]Running loglikelihood requests:  72%|███████▏  | 286/400 [03:45<01:05,  1.73it/s]Running loglikelihood requests:  72%|███████▏  | 287/400 [03:46<01:13,  1.54it/s]Running loglikelihood requests:  72%|███████▏  | 288/400 [03:47<01:19,  1.40it/s]Running loglikelihood requests:  72%|███████▏  | 289/400 [03:48<01:24,  1.31it/s]Running loglikelihood requests:  72%|███████▎  | 290/400 [03:49<01:27,  1.25it/s]Running loglikelihood requests:  73%|███████▎  | 291/400 [03:50<01:30,  1.21it/s]Running loglikelihood requests:  73%|███████▎  | 293/400 [03:51<01:11,  1.50it/s]Running loglikelihood requests:  74%|███████▎  | 294/400 [03:51<01:16,  1.38it/s]Running loglikelihood requests:  74%|███████▍  | 295/400 [03:52<01:20,  1.30it/s]Running loglikelihood requests:  74%|███████▍  | 296/400 [03:53<01:23,  1.24it/s]Running loglikelihood requests:  74%|███████▍  | 297/400 [03:54<01:25,  1.20it/s]Running loglikelihood requests:  75%|███████▍  | 299/400 [03:55<01:07,  1.50it/s]Running loglikelihood requests:  75%|███████▌  | 300/400 [03:56<01:12,  1.38it/s]Running loglikelihood requests:  75%|███████▌  | 301/400 [03:57<01:16,  1.30it/s]Running loglikelihood requests:  76%|███████▌  | 302/400 [03:58<01:18,  1.24it/s]Running loglikelihood requests:  76%|███████▌  | 304/400 [03:59<01:02,  1.53it/s]Running loglikelihood requests:  76%|███████▋  | 305/400 [04:00<01:07,  1.41it/s]Running loglikelihood requests:  77%|███████▋  | 307/400 [04:01<00:56,  1.65it/s]Running loglikelihood requests:  77%|███████▋  | 309/400 [04:01<00:49,  1.82it/s]Running loglikelihood requests:  78%|███████▊  | 310/400 [04:02<00:55,  1.61it/s]Running loglikelihood requests:  78%|███████▊  | 311/400 [04:03<01:00,  1.46it/s]Running loglikelihood requests:  78%|███████▊  | 312/400 [04:04<01:04,  1.36it/s]Running loglikelihood requests:  78%|███████▊  | 313/400 [04:05<01:07,  1.28it/s]Running loglikelihood requests:  79%|███████▉  | 315/400 [04:06<00:54,  1.56it/s]Running loglikelihood requests:  79%|███████▉  | 317/400 [04:07<00:47,  1.76it/s]Running loglikelihood requests:  80%|███████▉  | 318/400 [04:08<00:52,  1.57it/s]Running loglikelihood requests:  80%|███████▉  | 319/400 [04:09<00:56,  1.43it/s]Running loglikelihood requests:  80%|████████  | 321/400 [04:10<00:47,  1.67it/s]Running loglikelihood requests:  81%|████████  | 323/400 [04:10<00:41,  1.84it/s]Running loglikelihood requests:  82%|████████▏ | 326/400 [04:11<00:32,  2.29it/s]Running loglikelihood requests:  82%|████████▏ | 327/400 [04:12<00:37,  1.94it/s]Running loglikelihood requests:  82%|████████▏ | 328/400 [04:13<00:42,  1.69it/s]Running loglikelihood requests:  82%|████████▏ | 329/400 [04:14<00:46,  1.52it/s]Running loglikelihood requests:  82%|████████▎ | 330/400 [04:15<00:50,  1.40it/s]Running loglikelihood requests:  83%|████████▎ | 332/400 [04:16<00:41,  1.65it/s]Running loglikelihood requests:  83%|████████▎ | 333/400 [04:17<00:44,  1.49it/s]Running loglikelihood requests:  84%|████████▎ | 334/400 [04:18<00:47,  1.38it/s]Running loglikelihood requests:  84%|████████▍ | 335/400 [04:18<00:49,  1.30it/s]Running loglikelihood requests:  84%|████████▍ | 336/400 [04:19<00:51,  1.25it/s]Running loglikelihood requests:  84%|████████▍ | 337/400 [04:20<00:52,  1.21it/s]Running loglikelihood requests:  84%|████████▍ | 338/400 [04:21<00:52,  1.18it/s]Running loglikelihood requests:  85%|████████▍ | 339/400 [04:22<00:52,  1.16it/s]Running loglikelihood requests:  85%|████████▌ | 340/400 [04:23<00:52,  1.15it/s]Running loglikelihood requests:  85%|████████▌ | 341/400 [04:24<00:51,  1.14it/s]Running loglikelihood requests:  86%|████████▌ | 342/400 [04:25<00:50,  1.15it/s]Running loglikelihood requests:  86%|████████▌ | 343/400 [04:26<00:48,  1.17it/s]Running loglikelihood requests:  86%|████████▋ | 346/400 [04:26<00:28,  1.90it/s]Running loglikelihood requests:  87%|████████▋ | 348/400 [04:27<00:25,  2.07it/s]Running loglikelihood requests:  87%|████████▋ | 349/400 [04:28<00:27,  1.82it/s]Running loglikelihood requests:  88%|████████▊ | 350/400 [04:29<00:30,  1.65it/s]Running loglikelihood requests:  88%|████████▊ | 351/400 [04:30<00:32,  1.53it/s]Running loglikelihood requests:  88%|████████▊ | 352/400 [04:30<00:33,  1.44it/s]Running loglikelihood requests:  88%|████████▊ | 353/400 [04:31<00:34,  1.38it/s]Running loglikelihood requests:  88%|████████▊ | 354/400 [04:32<00:34,  1.33it/s]Running loglikelihood requests:  89%|████████▉ | 355/400 [04:33<00:34,  1.30it/s]Running loglikelihood requests:  89%|████████▉ | 356/400 [04:34<00:34,  1.28it/s]Running loglikelihood requests:  90%|████████▉ | 358/400 [04:34<00:25,  1.64it/s]Running loglikelihood requests:  90%|████████▉ | 359/400 [04:35<00:26,  1.52it/s]Running loglikelihood requests:  90%|█████████ | 360/400 [04:36<00:27,  1.43it/s]Running loglikelihood requests:  90%|█████████ | 361/400 [04:37<00:28,  1.37it/s]Running loglikelihood requests:  90%|█████████ | 362/400 [04:38<00:28,  1.33it/s]Running loglikelihood requests:  91%|█████████ | 364/400 [04:38<00:21,  1.68it/s]Running loglikelihood requests:  91%|█████████▏| 365/400 [04:39<00:22,  1.55it/s]Running loglikelihood requests:  92%|█████████▏| 367/400 [04:40<00:18,  1.82it/s]Running loglikelihood requests:  92%|█████████▏| 368/400 [04:41<00:19,  1.65it/s]Running loglikelihood requests:  92%|█████████▏| 369/400 [04:42<00:20,  1.53it/s]Running loglikelihood requests:  92%|█████████▎| 370/400 [04:43<00:20,  1.44it/s]Running loglikelihood requests:  93%|█████████▎| 371/400 [04:43<00:20,  1.38it/s]Running loglikelihood requests:  93%|█████████▎| 373/400 [04:44<00:15,  1.71it/s]Running loglikelihood requests:  94%|█████████▍| 375/400 [04:45<00:12,  1.94it/s]Running loglikelihood requests:  94%|█████████▍| 376/400 [04:46<00:13,  1.73it/s]Running loglikelihood requests:  95%|█████████▍| 379/400 [04:47<00:09,  2.33it/s]Running loglikelihood requests:  95%|█████████▌| 381/400 [04:47<00:07,  2.38it/s]Running loglikelihood requests:  96%|█████████▌| 382/400 [04:48<00:08,  2.04it/s]Running loglikelihood requests:  96%|█████████▌| 384/400 [04:49<00:07,  2.18it/s]Running loglikelihood requests:  96%|█████████▋| 386/400 [04:50<00:06,  2.27it/s]Running loglikelihood requests:  97%|█████████▋| 387/400 [04:51<00:06,  1.97it/s]Running loglikelihood requests:  97%|█████████▋| 389/400 [04:51<00:05,  2.13it/s]Running loglikelihood requests:  98%|█████████▊| 391/400 [04:52<00:04,  2.24it/s]Running loglikelihood requests:  98%|█████████▊| 393/400 [04:53<00:03,  2.32it/s]Running loglikelihood requests:  99%|█████████▉| 395/400 [04:54<00:02,  2.38it/s]Running loglikelihood requests:  99%|█████████▉| 397/400 [04:55<00:01,  2.42it/s]Running loglikelihood requests: 100%|█████████▉| 398/400 [04:55<00:00,  2.07it/s]Running loglikelihood requests: 100%|██████████| 400/400 [04:56<00:00,  2.20it/s]Running loglikelihood requests: 100%|██████████| 400/400 [04:56<00:00,  1.35it/s]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:3'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:3'}
full model:
{'mastermind_24_easy': {'alias': 'mastermind_24_easy', 'acc,none': 0.33, 'acc_stderr,none': 0.04725815626252609}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9705388716727775
0.9880227828623074
0.982119607496995
0.9941469535767887
0.984917378949356
0.9883609317461776
0.9752442385272352
0.9858149677985593
0.9966001900863091
0.9954965780195978
0.9982374916142641
0.987839947084121
0.9740425263242771
0.9802324544963317
0.9965978314947238
0.9890806289155061
0.971808392501808
0.9767123038440666
0.9820534501654181
0.9672299205809463
0.9753579231968917
0.9973837437819523
0.9951994747100236
0.9465102818948206
0.9757933184251313
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[7, 0, 1, 3, 6, 5, 4, 2]
tensor([7, 0, 1, 3, 6, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 0, 1, 3, 7, 5, 4, 2]
tensor([6, 0, 1, 3, 7, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 0, 1, 3, 6, 5, 4, 2]
tensor([7, 0, 1, 3, 6, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 0, 1, 3, 7, 5, 4, 2]
tensor([6, 0, 1, 3, 7, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 0, 1, 3, 6, 5, 4, 2]
tensor([7, 0, 1, 3, 6, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 0, 2, 3, 6, 5, 4, 1]
tensor([7, 0, 2, 3, 6, 5, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 3
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Normal merging for layer 4
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 5
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 6 to 31
done!
all done!
Model size: 12.0718 GB
211
cuda:3
cb
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:42<00:42, 42.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 25.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 27.89s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: cb] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: cb] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
WARNING:lm_eval.api.task:[Task: cb] metric f1 is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/cb?recursive=False&expand=False HTTP/1.1" 307 141
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/cb?recursive=False&expand=False HTTP/1.1" 200 347
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:filelock:Attempting to acquire lock 140229621275856 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_cb_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140229621275856 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_cb_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/cb/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140229621275856 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_cb_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140229621275856 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_cb_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140229612651296 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/cb/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140229612651296 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/cb/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/cb/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140229612651296 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/cb/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140229612651296 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/cb/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of cb from None to 0
INFO:lm_eval.api.task:Building contexts for cb on rank 0...
  0%|          | 0/56 [00:00<?, ?it/s]100%|██████████| 56/56 [00:00<00:00, 1541.70it/s]
DEBUG:lm_eval.evaluator:Task: cb; number of requests on this rank: 168
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/168 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/168 [00:02<06:19,  2.28s/it]Running loglikelihood requests:   1%|          | 2/168 [00:03<05:16,  1.91s/it]Running loglikelihood requests:   2%|▏         | 4/168 [00:05<03:21,  1.23s/it]Running loglikelihood requests:   3%|▎         | 5/168 [00:07<03:41,  1.36s/it]Running loglikelihood requests:   4%|▍         | 7/168 [00:08<02:51,  1.06s/it]Running loglikelihood requests:   5%|▍         | 8/168 [00:10<03:05,  1.16s/it]Running loglikelihood requests:   6%|▌         | 10/168 [00:11<02:27,  1.07it/s]Running loglikelihood requests:   7%|▋         | 11/168 [00:12<02:36,  1.00it/s]Running loglikelihood requests:   8%|▊         | 13/168 [00:13<02:09,  1.19it/s]Running loglikelihood requests:   8%|▊         | 14/168 [00:14<02:20,  1.10it/s]Running loglikelihood requests:  10%|▉         | 16/168 [00:16<01:59,  1.27it/s]Running loglikelihood requests:  10%|█         | 17/168 [00:17<02:11,  1.15it/s]Running loglikelihood requests:  11%|█▏        | 19/168 [00:18<01:51,  1.34it/s]Running loglikelihood requests:  12%|█▏        | 20/168 [00:19<02:01,  1.22it/s]Running loglikelihood requests:  13%|█▎        | 22/168 [00:20<01:43,  1.41it/s]Running loglikelihood requests:  14%|█▎        | 23/168 [00:21<01:52,  1.28it/s]Running loglikelihood requests:  15%|█▍        | 25/168 [00:22<01:36,  1.48it/s]Running loglikelihood requests:  15%|█▌        | 26/168 [00:23<01:46,  1.34it/s]Running loglikelihood requests:  17%|█▋        | 28/168 [00:24<01:31,  1.53it/s]Running loglikelihood requests:  17%|█▋        | 29/168 [00:25<01:40,  1.38it/s]Running loglikelihood requests:  18%|█▊        | 31/168 [00:26<01:27,  1.56it/s]Running loglikelihood requests:  19%|█▉        | 32/168 [00:27<01:36,  1.40it/s]Running loglikelihood requests:  20%|█▉        | 33/168 [00:28<01:44,  1.29it/s]Running loglikelihood requests:  21%|██        | 35/168 [00:29<01:28,  1.51it/s]Running loglikelihood requests:  22%|██▏       | 37/168 [00:30<01:18,  1.67it/s]Running loglikelihood requests:  23%|██▎       | 38/168 [00:31<01:28,  1.48it/s]Running loglikelihood requests:  24%|██▍       | 40/168 [00:32<01:17,  1.66it/s]Running loglikelihood requests:  24%|██▍       | 41/168 [00:33<01:25,  1.48it/s]Running loglikelihood requests:  26%|██▌       | 43/168 [00:34<01:14,  1.67it/s]Running loglikelihood requests:  26%|██▌       | 44/168 [00:35<01:23,  1.49it/s]Running loglikelihood requests:  27%|██▋       | 46/168 [00:36<01:11,  1.70it/s]Running loglikelihood requests:  28%|██▊       | 47/168 [00:37<01:19,  1.53it/s]Running loglikelihood requests:  29%|██▉       | 49/168 [00:37<01:08,  1.75it/s]Running loglikelihood requests:  30%|██▉       | 50/168 [00:38<01:14,  1.58it/s]Running loglikelihood requests:  30%|███       | 51/168 [00:39<01:20,  1.45it/s]Running loglikelihood requests:  31%|███       | 52/168 [00:40<01:25,  1.36it/s]Running loglikelihood requests:  32%|███▏      | 54/168 [00:41<01:09,  1.65it/s]Running loglikelihood requests:  33%|███▎      | 56/168 [00:42<01:00,  1.85it/s]Running loglikelihood requests:  35%|███▍      | 58/168 [00:43<00:55,  1.99it/s]Running loglikelihood requests:  35%|███▌      | 59/168 [00:43<01:02,  1.74it/s]Running loglikelihood requests:  36%|███▌      | 60/168 [00:44<01:08,  1.57it/s]Running loglikelihood requests:  37%|███▋      | 62/168 [00:45<00:59,  1.80it/s]Running loglikelihood requests:  38%|███▊      | 64/168 [00:46<00:53,  1.96it/s]Running loglikelihood requests:  39%|███▊      | 65/168 [00:47<00:59,  1.72it/s]Running loglikelihood requests:  40%|███▉      | 67/168 [00:48<00:52,  1.91it/s]Running loglikelihood requests:  40%|████      | 68/168 [00:49<00:58,  1.70it/s]Running loglikelihood requests:  42%|████▏     | 70/168 [00:49<00:51,  1.90it/s]Running loglikelihood requests:  42%|████▏     | 71/168 [00:50<00:57,  1.69it/s]Running loglikelihood requests:  43%|████▎     | 73/168 [00:51<00:49,  1.91it/s]Running loglikelihood requests:  44%|████▍     | 74/168 [00:52<00:55,  1.70it/s]Running loglikelihood requests:  45%|████▍     | 75/168 [00:53<00:59,  1.56it/s]Running loglikelihood requests:  45%|████▌     | 76/168 [00:54<01:03,  1.45it/s]Running loglikelihood requests:  46%|████▋     | 78/168 [00:54<00:51,  1.75it/s]Running loglikelihood requests:  48%|████▊     | 80/168 [00:55<00:45,  1.95it/s]Running loglikelihood requests:  49%|████▉     | 82/168 [00:56<00:41,  2.09it/s]Running loglikelihood requests:  49%|████▉     | 83/168 [00:57<00:46,  1.83it/s]Running loglikelihood requests:  51%|█████     | 85/168 [00:58<00:41,  2.01it/s]Running loglikelihood requests:  51%|█████     | 86/168 [00:59<00:46,  1.78it/s]Running loglikelihood requests:  52%|█████▏    | 88/168 [00:59<00:40,  2.00it/s]Running loglikelihood requests:  53%|█████▎    | 89/168 [01:00<00:44,  1.78it/s]Running loglikelihood requests:  54%|█████▎    | 90/168 [01:01<00:48,  1.62it/s]Running loglikelihood requests:  55%|█████▍    | 92/168 [01:02<00:40,  1.89it/s]Running loglikelihood requests:  56%|█████▌    | 94/168 [01:02<00:35,  2.09it/s]Running loglikelihood requests:  57%|█████▋    | 95/168 [01:03<00:39,  1.85it/s]Running loglikelihood requests:  58%|█████▊    | 97/168 [01:04<00:34,  2.07it/s]Running loglikelihood requests:  58%|█████▊    | 98/168 [01:05<00:37,  1.85it/s]Running loglikelihood requests:  60%|█████▉    | 100/168 [01:06<00:32,  2.08it/s]Running loglikelihood requests:  60%|██████    | 101/168 [01:06<00:36,  1.85it/s]Running loglikelihood requests:  61%|██████▏   | 103/168 [01:07<00:30,  2.10it/s]Running loglikelihood requests:  62%|██████▏   | 104/168 [01:08<00:34,  1.88it/s]Running loglikelihood requests:  62%|██████▎   | 105/168 [01:09<00:36,  1.72it/s]Running loglikelihood requests:  63%|██████▎   | 106/168 [01:09<00:38,  1.61it/s]Running loglikelihood requests:  64%|██████▍   | 108/168 [01:10<00:30,  1.94it/s]Running loglikelihood requests:  65%|██████▌   | 110/168 [01:11<00:26,  2.17it/s]Running loglikelihood requests:  66%|██████▌   | 111/168 [01:11<00:29,  1.92it/s]Running loglikelihood requests:  67%|██████▋   | 113/168 [01:12<00:25,  2.16it/s]Running loglikelihood requests:  68%|██████▊   | 114/168 [01:13<00:28,  1.92it/s]Running loglikelihood requests:  69%|██████▉   | 116/168 [01:14<00:23,  2.17it/s]Running loglikelihood requests:  70%|███████   | 118/168 [01:14<00:21,  2.34it/s]Running loglikelihood requests:  71%|███████   | 119/168 [01:15<00:23,  2.05it/s]Running loglikelihood requests:  71%|███████▏  | 120/168 [01:16<00:25,  1.85it/s]Running loglikelihood requests:  73%|███████▎  | 122/168 [01:17<00:21,  2.12it/s]Running loglikelihood requests:  74%|███████▍  | 124/168 [01:17<00:19,  2.32it/s]Running loglikelihood requests:  74%|███████▍  | 125/168 [01:18<00:21,  2.04it/s]Running loglikelihood requests:  76%|███████▌  | 127/168 [01:19<00:18,  2.27it/s]Running loglikelihood requests:  76%|███████▌  | 128/168 [01:19<00:19,  2.01it/s]Running loglikelihood requests:  77%|███████▋  | 129/168 [01:20<00:21,  1.83it/s]Running loglikelihood requests:  78%|███████▊  | 131/168 [01:21<00:17,  2.13it/s]Running loglikelihood requests:  79%|███████▉  | 133/168 [01:22<00:14,  2.37it/s]Running loglikelihood requests:  80%|███████▉  | 134/168 [01:22<00:16,  2.12it/s]Running loglikelihood requests:  81%|████████  | 136/168 [01:23<00:13,  2.39it/s]Running loglikelihood requests:  82%|████████▏ | 137/168 [01:24<00:14,  2.13it/s]Running loglikelihood requests:  82%|████████▏ | 138/168 [01:24<00:15,  1.95it/s]Running loglikelihood requests:  83%|████████▎ | 139/168 [01:25<00:15,  1.83it/s]Running loglikelihood requests:  83%|████████▎ | 140/168 [01:25<00:16,  1.74it/s]Running loglikelihood requests:  85%|████████▍ | 142/168 [01:26<00:12,  2.13it/s]Running loglikelihood requests:  86%|████████▌ | 144/168 [01:27<00:09,  2.41it/s]Running loglikelihood requests:  87%|████████▋ | 146/168 [01:27<00:08,  2.61it/s]Running loglikelihood requests:  88%|████████▊ | 148/168 [01:28<00:07,  2.76it/s]Running loglikelihood requests:  89%|████████▊ | 149/168 [01:29<00:07,  2.41it/s]Running loglikelihood requests:  89%|████████▉ | 150/168 [01:29<00:08,  2.16it/s]Running loglikelihood requests:  90%|█████████ | 152/168 [01:30<00:06,  2.46it/s]Running loglikelihood requests:  92%|█████████▏| 154/168 [01:31<00:05,  2.67it/s]Running loglikelihood requests:  92%|█████████▏| 155/168 [01:31<00:05,  2.36it/s]Running loglikelihood requests:  93%|█████████▎| 157/168 [01:32<00:04,  2.61it/s]Running loglikelihood requests:  94%|█████████▍| 158/168 [01:32<00:04,  2.31it/s]Running loglikelihood requests:  95%|█████████▌| 160/168 [01:33<00:03,  2.64it/s]Running loglikelihood requests:  96%|█████████▌| 161/168 [01:34<00:02,  2.39it/s]Running loglikelihood requests:  97%|█████████▋| 163/168 [01:34<00:01,  2.72it/s]Running loglikelihood requests:  98%|█████████▊| 164/168 [01:35<00:01,  2.46it/s]Running loglikelihood requests:  99%|█████████▉| 166/168 [01:35<00:00,  2.81it/s]Running loglikelihood requests:  99%|█████████▉| 167/168 [01:36<00:00,  2.53it/s]Running loglikelihood requests: 100%|██████████| 168/168 [01:36<00:00,  1.74it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:4'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:4'}
full model:
{'cb': {'alias': 'cb', 'acc,none': 0.44642857142857145, 'acc_stderr,none': 0.06703189227942397, 'f1,none': np.float64(0.2946127946127946), 'f1_stderr,none': 'N/A'}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8754479021250823
0.8035282713865031
0.9500865550679058
0.9469110267801961
0.9212663689239236
0.9365178765256775
0.671589453090406
0.6744336218334667
0.8076345549626031
0.7976470386087894
0.807072393829993
0.6600400889592184
0.7484904863798857
0.9363063771008698
0.6357711114598822
0.9166089836828051
0.6715835426958833
0.7409884813902188
0.412428535140908
0.8747283305135303
0.8491219158212996
0.9125101690232402
0.8366676259845086
0.705936129020536
0.7920385356495101
0.9243830461584077
0.9274604399644588
0.7835428130351293
0.8119111717866335
Total groups 67 exceeded the threshold, stopping comparison.
The group tensor is
[6, 3, 5, 2, 7, 1, 4, 0]
tensor([6, 3, 5, 2, 7, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 7, 3, 6, 1, 2, 0]
tensor([5, 4, 7, 3, 6, 1, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 1, 6, 3, 7, 2, 4, 0]
tensor([5, 1, 6, 3, 7, 2, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 2, 0, 1, 5, 0, 1, 3]
tensor([4, 2, 0, 1, 5, 0, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[4, 3, 0, 2, 1, 1, 5, 0]
tensor([4, 3, 0, 2, 1, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 2, 2, 1, 3, 0]
tensor([0, 3, 1, 2, 2, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1.0, 1]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 2 to 6
done!
Normal merging for layer 7
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 8 to 12
done!
Normal merging for layer 13
tensor([2, 5])
tensor(2)
tensor([3, 6])
tensor(3)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Normal merging for layer 14
tensor([2, 7])
tensor(2)
tensor([4, 5])
tensor(4)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 15 to 18
done!
Normal merging for layer 19
tensor([0, 7])
tensor(0)
tensor([2, 5])
tensor(2)
tensor([3, 4])
tensor(3)
tensor([1, 6])
tensor(1)
done!
Cross-layer merge completed for layers 20 to 23
done!
Normal merging for layer 24
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 25 to 31
done!
all done!
Model size: 11.9458 GB
95
cuda:4
mnli
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:42<00:42, 42.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 24.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 27.50s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: mnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: mnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140237198316832 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140237198316832 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140237198316832 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140237198316832 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140237198316832 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140237198316832 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140237198316832 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140237198316832 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of mnli from None to 0
INFO:lm_eval.api.task:Building contexts for mnli on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 119940.06it/s]
DEBUG:lm_eval.evaluator:Task: mnli; number of requests on this rank: 300
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/300 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/300 [00:01<09:09,  1.84s/it]Running loglikelihood requests:   1%|          | 2/300 [00:03<07:24,  1.49s/it]Running loglikelihood requests:   1%|▏         | 4/300 [00:04<04:12,  1.17it/s]Running loglikelihood requests:   2%|▏         | 5/300 [00:04<04:19,  1.14it/s]Running loglikelihood requests:   2%|▏         | 7/300 [00:05<03:17,  1.49it/s]Running loglikelihood requests:   3%|▎         | 8/300 [00:06<03:30,  1.38it/s]Running loglikelihood requests:   3%|▎         | 10/300 [00:07<02:51,  1.69it/s]Running loglikelihood requests:   4%|▎         | 11/300 [00:08<03:05,  1.56it/s]Running loglikelihood requests:   4%|▍         | 13/300 [00:09<02:35,  1.84it/s]Running loglikelihood requests:   5%|▍         | 14/300 [00:09<02:50,  1.68it/s]Running loglikelihood requests:   5%|▌         | 16/300 [00:10<02:24,  1.97it/s]Running loglikelihood requests:   6%|▌         | 17/300 [00:11<02:37,  1.79it/s]Running loglikelihood requests:   6%|▌         | 18/300 [00:12<02:48,  1.67it/s]Running loglikelihood requests:   7%|▋         | 20/300 [00:12<02:21,  1.98it/s]Running loglikelihood requests:   7%|▋         | 21/300 [00:13<02:35,  1.80it/s]Running loglikelihood requests:   8%|▊         | 23/300 [00:14<02:13,  2.08it/s]Running loglikelihood requests:   8%|▊         | 25/300 [00:15<02:00,  2.29it/s]Running loglikelihood requests:   9%|▊         | 26/300 [00:15<02:15,  2.03it/s]Running loglikelihood requests:   9%|▉         | 28/300 [00:16<02:00,  2.26it/s]Running loglikelihood requests:  10%|▉         | 29/300 [00:17<02:14,  2.01it/s]Running loglikelihood requests:  10%|█         | 30/300 [00:17<02:27,  1.83it/s]Running loglikelihood requests:  10%|█         | 31/300 [00:18<02:37,  1.71it/s]Running loglikelihood requests:  11%|█         | 32/300 [00:19<02:45,  1.62it/s]Running loglikelihood requests:  11%|█▏        | 34/300 [00:19<02:14,  1.98it/s]Running loglikelihood requests:  12%|█▏        | 36/300 [00:20<01:57,  2.24it/s]Running loglikelihood requests:  13%|█▎        | 38/300 [00:21<01:48,  2.42it/s]Running loglikelihood requests:  13%|█▎        | 40/300 [00:22<01:41,  2.57it/s]Running loglikelihood requests:  14%|█▎        | 41/300 [00:22<01:55,  2.24it/s]Running loglikelihood requests:  14%|█▍        | 42/300 [00:23<02:08,  2.01it/s]Running loglikelihood requests:  14%|█▍        | 43/300 [00:24<02:19,  1.85it/s]Running loglikelihood requests:  15%|█▌        | 45/300 [00:24<01:57,  2.17it/s]Running loglikelihood requests:  16%|█▌        | 47/300 [00:25<01:45,  2.40it/s]Running loglikelihood requests:  16%|█▌        | 48/300 [00:26<01:58,  2.12it/s]Running loglikelihood requests:  17%|█▋        | 50/300 [00:26<01:45,  2.37it/s]Running loglikelihood requests:  17%|█▋        | 52/300 [00:27<01:37,  2.55it/s]Running loglikelihood requests:  18%|█▊        | 53/300 [00:28<01:50,  2.24it/s]Running loglikelihood requests:  18%|█▊        | 54/300 [00:28<02:01,  2.02it/s]Running loglikelihood requests:  18%|█▊        | 55/300 [00:29<02:11,  1.86it/s]Running loglikelihood requests:  19%|█▉        | 57/300 [00:30<01:50,  2.20it/s]Running loglikelihood requests:  20%|█▉        | 59/300 [00:30<01:38,  2.45it/s]Running loglikelihood requests:  20%|██        | 61/300 [00:31<01:31,  2.62it/s]Running loglikelihood requests:  21%|██        | 62/300 [00:32<01:43,  2.29it/s]Running loglikelihood requests:  21%|██▏       | 64/300 [00:32<01:33,  2.52it/s]Running loglikelihood requests:  22%|██▏       | 65/300 [00:33<01:45,  2.22it/s]Running loglikelihood requests:  22%|██▏       | 66/300 [00:34<01:55,  2.02it/s]Running loglikelihood requests:  22%|██▏       | 67/300 [00:34<02:04,  1.87it/s]Running loglikelihood requests:  23%|██▎       | 69/300 [00:35<01:43,  2.23it/s]Running loglikelihood requests:  24%|██▎       | 71/300 [00:36<01:32,  2.49it/s]Running loglikelihood requests:  24%|██▍       | 72/300 [00:36<01:43,  2.21it/s]Running loglikelihood requests:  24%|██▍       | 73/300 [00:37<01:53,  2.01it/s]Running loglikelihood requests:  25%|██▌       | 75/300 [00:38<01:36,  2.33it/s]Running loglikelihood requests:  26%|██▌       | 77/300 [00:38<01:26,  2.56it/s]Running loglikelihood requests:  26%|██▋       | 79/300 [00:39<01:20,  2.73it/s]Running loglikelihood requests:  27%|██▋       | 80/300 [00:39<01:32,  2.38it/s]Running loglikelihood requests:  27%|██▋       | 81/300 [00:40<01:42,  2.14it/s]Running loglikelihood requests:  27%|██▋       | 82/300 [00:41<01:50,  1.97it/s]Running loglikelihood requests:  28%|██▊       | 83/300 [00:41<01:57,  1.85it/s]Running loglikelihood requests:  28%|██▊       | 85/300 [00:42<01:35,  2.24it/s]Running loglikelihood requests:  29%|██▊       | 86/300 [00:43<01:44,  2.04it/s]Running loglikelihood requests:  29%|██▉       | 88/300 [00:43<01:29,  2.37it/s]Running loglikelihood requests:  30%|██▉       | 89/300 [00:44<01:40,  2.09it/s]Running loglikelihood requests:  30%|███       | 91/300 [00:45<01:28,  2.37it/s]Running loglikelihood requests:  31%|███       | 93/300 [00:45<01:19,  2.59it/s]Running loglikelihood requests:  32%|███▏      | 95/300 [00:46<01:16,  2.70it/s]Running loglikelihood requests:  32%|███▏      | 96/300 [00:47<01:27,  2.34it/s]Running loglikelihood requests:  33%|███▎      | 98/300 [00:47<01:20,  2.52it/s]Running loglikelihood requests:  33%|███▎      | 99/300 [00:48<01:29,  2.25it/s]Running loglikelihood requests:  34%|███▎      | 101/300 [00:49<01:19,  2.52it/s]Running loglikelihood requests:  34%|███▍      | 102/300 [00:49<01:28,  2.24it/s]Running loglikelihood requests:  34%|███▍      | 103/300 [00:50<01:36,  2.04it/s]Running loglikelihood requests:  35%|███▍      | 104/300 [00:50<01:42,  1.90it/s]Running loglikelihood requests:  35%|███▌      | 105/300 [00:51<01:47,  1.81it/s]Running loglikelihood requests:  36%|███▌      | 107/300 [00:52<01:26,  2.22it/s]Running loglikelihood requests:  36%|███▌      | 108/300 [00:52<01:34,  2.04it/s]Running loglikelihood requests:  36%|███▋      | 109/300 [00:53<01:40,  1.91it/s]Running loglikelihood requests:  37%|███▋      | 111/300 [00:54<01:22,  2.29it/s]Running loglikelihood requests:  38%|███▊      | 113/300 [00:54<01:12,  2.56it/s]Running loglikelihood requests:  38%|███▊      | 115/300 [00:55<01:07,  2.76it/s]Running loglikelihood requests:  39%|███▉      | 117/300 [00:55<01:03,  2.90it/s]Running loglikelihood requests:  40%|███▉      | 119/300 [00:56<01:00,  3.00it/s]Running loglikelihood requests:  40%|████      | 121/300 [00:57<00:58,  3.07it/s]Running loglikelihood requests:  41%|████      | 122/300 [00:57<01:07,  2.64it/s]Running loglikelihood requests:  41%|████▏     | 124/300 [00:58<01:02,  2.80it/s]Running loglikelihood requests:  42%|████▏     | 125/300 [00:59<01:11,  2.46it/s]Running loglikelihood requests:  42%|████▏     | 126/300 [00:59<01:18,  2.22it/s]Running loglikelihood requests:  42%|████▏     | 127/300 [01:00<01:24,  2.05it/s]Running loglikelihood requests:  43%|████▎     | 129/300 [01:00<01:10,  2.43it/s]Running loglikelihood requests:  44%|████▎     | 131/300 [01:01<01:02,  2.70it/s]Running loglikelihood requests:  44%|████▍     | 133/300 [01:02<00:57,  2.91it/s]Running loglikelihood requests:  45%|████▍     | 134/300 [01:02<01:05,  2.55it/s]Running loglikelihood requests:  45%|████▌     | 135/300 [01:03<01:11,  2.30it/s]Running loglikelihood requests:  45%|████▌     | 136/300 [01:03<01:17,  2.13it/s]Running loglikelihood requests:  46%|████▌     | 137/300 [01:04<01:21,  2.01it/s]Running loglikelihood requests:  46%|████▌     | 138/300 [01:04<01:24,  1.92it/s]Running loglikelihood requests:  47%|████▋     | 140/300 [01:05<01:07,  2.38it/s]Running loglikelihood requests:  47%|████▋     | 142/300 [01:06<00:58,  2.70it/s]Running loglikelihood requests:  48%|████▊     | 143/300 [01:06<01:05,  2.41it/s]Running loglikelihood requests:  48%|████▊     | 145/300 [01:07<00:56,  2.73it/s]Running loglikelihood requests:  49%|████▊     | 146/300 [01:07<01:03,  2.43it/s]Running loglikelihood requests:  49%|████▉     | 147/300 [01:08<01:09,  2.22it/s]Running loglikelihood requests:  49%|████▉     | 148/300 [01:09<01:13,  2.07it/s]Running loglikelihood requests:  50%|█████     | 150/300 [01:09<01:00,  2.48it/s]Running loglikelihood requests:  51%|█████     | 152/300 [01:10<00:53,  2.78it/s]Running loglikelihood requests:  51%|█████▏    | 154/300 [01:10<00:48,  2.99it/s]Running loglikelihood requests:  52%|█████▏    | 155/300 [01:11<00:56,  2.59it/s]Running loglikelihood requests:  52%|█████▏    | 157/300 [01:11<00:50,  2.81it/s]Running loglikelihood requests:  53%|█████▎    | 159/300 [01:12<00:48,  2.92it/s]Running loglikelihood requests:  54%|█████▎    | 161/300 [01:13<00:46,  3.02it/s]Running loglikelihood requests:  54%|█████▍    | 162/300 [01:13<00:51,  2.65it/s]Running loglikelihood requests:  55%|█████▍    | 164/300 [01:14<00:46,  2.90it/s]Running loglikelihood requests:  55%|█████▌    | 165/300 [01:14<00:52,  2.57it/s]Running loglikelihood requests:  56%|█████▌    | 167/300 [01:15<00:47,  2.79it/s]Running loglikelihood requests:  56%|█████▋    | 169/300 [01:16<00:44,  2.97it/s]Running loglikelihood requests:  57%|█████▋    | 170/300 [01:16<00:50,  2.58it/s]Running loglikelihood requests:  57%|█████▋    | 171/300 [01:17<00:55,  2.32it/s]Running loglikelihood requests:  57%|█████▋    | 172/300 [01:17<00:59,  2.13it/s]Running loglikelihood requests:  58%|█████▊    | 173/300 [01:18<01:03,  2.01it/s]Running loglikelihood requests:  58%|█████▊    | 175/300 [01:19<00:50,  2.46it/s]Running loglikelihood requests:  59%|█████▉    | 177/300 [01:19<00:44,  2.79it/s]Running loglikelihood requests:  60%|█████▉    | 179/300 [01:20<00:40,  3.02it/s]Running loglikelihood requests:  60%|██████    | 180/300 [01:20<00:45,  2.66it/s]Running loglikelihood requests:  60%|██████    | 181/300 [01:21<00:49,  2.40it/s]Running loglikelihood requests:  61%|██████    | 183/300 [01:21<00:43,  2.69it/s]Running loglikelihood requests:  62%|██████▏   | 185/300 [01:22<00:39,  2.92it/s]Running loglikelihood requests:  62%|██████▏   | 186/300 [01:22<00:44,  2.59it/s]Running loglikelihood requests:  63%|██████▎   | 188/300 [01:23<00:38,  2.89it/s]Running loglikelihood requests:  63%|██████▎   | 190/300 [01:24<00:35,  3.10it/s]Running loglikelihood requests:  64%|██████▎   | 191/300 [01:24<00:40,  2.72it/s]Running loglikelihood requests:  64%|██████▍   | 192/300 [01:25<00:45,  2.39it/s]Running loglikelihood requests:  65%|██████▍   | 194/300 [01:25<00:38,  2.76it/s]Running loglikelihood requests:  65%|██████▌   | 195/300 [01:26<00:42,  2.49it/s]Running loglikelihood requests:  65%|██████▌   | 196/300 [01:26<00:45,  2.30it/s]Running loglikelihood requests:  66%|██████▌   | 198/300 [01:27<00:37,  2.72it/s]Running loglikelihood requests:  66%|██████▋   | 199/300 [01:27<00:41,  2.46it/s]Running loglikelihood requests:  67%|██████▋   | 200/300 [01:28<00:43,  2.28it/s]Running loglikelihood requests:  67%|██████▋   | 201/300 [01:29<00:46,  2.15it/s]Running loglikelihood requests:  68%|██████▊   | 203/300 [01:29<00:37,  2.62it/s]Running loglikelihood requests:  68%|██████▊   | 205/300 [01:30<00:32,  2.95it/s]Running loglikelihood requests:  69%|██████▊   | 206/300 [01:30<00:35,  2.63it/s]Running loglikelihood requests:  69%|██████▉   | 208/300 [01:31<00:31,  2.96it/s]Running loglikelihood requests:  70%|███████   | 210/300 [01:31<00:28,  3.14it/s]Running loglikelihood requests:  71%|███████   | 212/300 [01:32<00:26,  3.27it/s]Running loglikelihood requests:  71%|███████   | 213/300 [01:32<00:30,  2.83it/s]Running loglikelihood requests:  72%|███████▏  | 215/300 [01:33<00:27,  3.04it/s]Running loglikelihood requests:  72%|███████▏  | 216/300 [01:33<00:31,  2.70it/s]Running loglikelihood requests:  72%|███████▏  | 217/300 [01:34<00:33,  2.46it/s]Running loglikelihood requests:  73%|███████▎  | 219/300 [01:35<00:28,  2.85it/s]Running loglikelihood requests:  74%|███████▎  | 221/300 [01:35<00:25,  3.12it/s]Running loglikelihood requests:  74%|███████▍  | 222/300 [01:36<00:28,  2.75it/s]Running loglikelihood requests:  75%|███████▍  | 224/300 [01:36<00:25,  3.04it/s]Running loglikelihood requests:  75%|███████▌  | 225/300 [01:37<00:27,  2.70it/s]Running loglikelihood requests:  76%|███████▌  | 227/300 [01:37<00:24,  3.03it/s]Running loglikelihood requests:  76%|███████▌  | 228/300 [01:38<00:26,  2.69it/s]Running loglikelihood requests:  76%|███████▋  | 229/300 [01:38<00:28,  2.45it/s]Running loglikelihood requests:  77%|███████▋  | 231/300 [01:39<00:24,  2.84it/s]Running loglikelihood requests:  77%|███████▋  | 232/300 [01:39<00:26,  2.56it/s]Running loglikelihood requests:  78%|███████▊  | 233/300 [01:40<00:28,  2.36it/s]Running loglikelihood requests:  78%|███████▊  | 234/300 [01:40<00:29,  2.23it/s]Running loglikelihood requests:  78%|███████▊  | 235/300 [01:41<00:30,  2.14it/s]Running loglikelihood requests:  79%|███████▊  | 236/300 [01:41<00:30,  2.07it/s]Running loglikelihood requests:  79%|███████▉  | 237/300 [01:42<00:31,  2.03it/s]Running loglikelihood requests:  79%|███████▉  | 238/300 [01:42<00:31,  2.00it/s]Running loglikelihood requests:  80%|████████  | 240/300 [01:43<00:23,  2.55it/s]Running loglikelihood requests:  81%|████████  | 242/300 [01:43<00:19,  2.93it/s]Running loglikelihood requests:  81%|████████▏ | 244/300 [01:44<00:17,  3.21it/s]Running loglikelihood requests:  82%|████████▏ | 246/300 [01:44<00:15,  3.41it/s]Running loglikelihood requests:  83%|████████▎ | 248/300 [01:45<00:14,  3.55it/s]Running loglikelihood requests:  83%|████████▎ | 250/300 [01:46<00:13,  3.65it/s]Running loglikelihood requests:  84%|████████▍ | 252/300 [01:46<00:12,  3.72it/s]Running loglikelihood requests:  85%|████████▍ | 254/300 [01:47<00:12,  3.77it/s]Running loglikelihood requests:  85%|████████▌ | 256/300 [01:47<00:11,  3.81it/s]Running loglikelihood requests:  86%|████████▌ | 257/300 [01:48<00:13,  3.27it/s]Running loglikelihood requests:  86%|████████▌ | 258/300 [01:48<00:14,  2.88it/s]Running loglikelihood requests:  87%|████████▋ | 260/300 [01:49<00:12,  3.20it/s]Running loglikelihood requests:  87%|████████▋ | 261/300 [01:49<00:13,  2.85it/s]Running loglikelihood requests:  88%|████████▊ | 263/300 [01:50<00:11,  3.19it/s]Running loglikelihood requests:  88%|████████▊ | 264/300 [01:50<00:12,  2.83it/s]Running loglikelihood requests:  88%|████████▊ | 265/300 [01:51<00:13,  2.59it/s]Running loglikelihood requests:  89%|████████▊ | 266/300 [01:51<00:14,  2.42it/s]Running loglikelihood requests:  89%|████████▉ | 268/300 [01:52<00:11,  2.90it/s]Running loglikelihood requests:  90%|█████████ | 270/300 [01:52<00:09,  3.24it/s]Running loglikelihood requests:  91%|█████████ | 272/300 [01:53<00:08,  3.48it/s]Running loglikelihood requests:  91%|█████████▏| 274/300 [01:53<00:07,  3.67it/s]Running loglikelihood requests:  92%|█████████▏| 275/300 [01:54<00:07,  3.20it/s]Running loglikelihood requests:  92%|█████████▏| 277/300 [01:54<00:06,  3.48it/s]Running loglikelihood requests:  93%|█████████▎| 278/300 [01:54<00:07,  3.06it/s]Running loglikelihood requests:  93%|█████████▎| 279/300 [01:55<00:07,  2.77it/s]Running loglikelihood requests:  94%|█████████▎| 281/300 [01:55<00:05,  3.19it/s]Running loglikelihood requests:  94%|█████████▍| 282/300 [01:56<00:06,  2.86it/s]Running loglikelihood requests:  94%|█████████▍| 283/300 [01:56<00:06,  2.64it/s]Running loglikelihood requests:  95%|█████████▍| 284/300 [01:57<00:06,  2.47it/s]Running loglikelihood requests:  95%|█████████▌| 286/300 [01:57<00:04,  2.99it/s]Running loglikelihood requests:  96%|█████████▌| 288/300 [01:58<00:03,  3.36it/s]Running loglikelihood requests:  97%|█████████▋| 290/300 [01:58<00:02,  3.63it/s]Running loglikelihood requests:  97%|█████████▋| 292/300 [01:59<00:02,  3.83it/s]Running loglikelihood requests:  98%|█████████▊| 293/300 [01:59<00:02,  3.34it/s]Running loglikelihood requests:  98%|█████████▊| 294/300 [02:00<00:01,  3.00it/s]Running loglikelihood requests:  99%|█████████▊| 296/300 [02:00<00:01,  3.41it/s]Running loglikelihood requests:  99%|█████████▉| 298/300 [02:01<00:00,  3.74it/s]Running loglikelihood requests: 100%|█████████▉| 299/300 [02:01<00:00,  3.32it/s]Running loglikelihood requests: 100%|██████████| 300/300 [02:01<00:00,  2.47it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:5'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:5'}
full model:
{'mnli': {'alias': 'mnli', 'acc,none': 0.4, 'acc_stderr,none': 0.0492365963917331}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8171409999433525
0.8739893758684915
0.8333413990263023
0.8782828649633461
0.7602933913725962
0.7047335470608914
0.9813668198386444
0.7850949920891616
0.748983595161589
0.6557873189175887
0.7043210867322975
0.9420132312730142
0.9548348739467002
0.8459964595909506
0.6932219150662173
0.8804855383939617
0.868566987197908
0.8437399449827557
0.9153450822718411
0.8433082326287293
0.910397090611448
0.7626694638419581
0.752151649412453
0.8346986395359313
0.9530736745854924
0.8635918585764104
0.8647192950921815
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[4, 5, 6, 1, 7, 2, 3, 0]
tensor([4, 5, 6, 1, 7, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 4, 6, 2, 5, 1, 3, 0]
tensor([7, 4, 6, 2, 5, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 7, 3, 5, 1, 4, 0]
tensor([6, 2, 7, 3, 5, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 2, 5, 4, 6, 1, 3, 0]
tensor([7, 2, 5, 4, 6, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 5, 3, 1, 2, 4, 0]
tensor([0, 1, 5, 3, 1, 2, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 1, 2, 2, 3, 0]
tensor([0, 3, 1, 1, 2, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 2, 3, 1, 2, 0, 3, 1]
tensor([0, 2, 3, 1, 2, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 3, 1, 2, 2, 1, 3, 0]
tensor([0, 3, 1, 2, 2, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1.0, 1]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 4 to 8
done!
Normal merging for layer 9
tensor([0, 7])
tensor(0)
tensor([1, 4])
tensor(1)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 10 to 20
done!
Normal merging for layer 21
tensor([0, 7])
tensor(0)
tensor([2, 3])
tensor(2)
tensor([4, 5])
tensor(4)
tensor([1, 6])
tensor(1)
done!
Normal merging for layer 22
tensor([0, 5])
tensor(0)
tensor([3, 7])
tensor(3)
tensor([1, 4])
tensor(1)
tensor([2, 6])
tensor(2)
done!
Normal merging for layer 23
tensor([0, 7])
tensor(0)
tensor([2, 5])
tensor(2)
tensor([3, 4])
tensor(3)
tensor([1, 6])
tensor(1)
done!
Cross-layer merge completed for layers 24 to 25
done!
Normal merging for layer 26
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 27 to 31
done!
all done!
Model size: 12.5127 GB
78
cuda:5
copa
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:46<00:46, 46.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 27.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 30.17s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: copa] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: copa] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140237192887088 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_copa_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140237192887088 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_copa_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140237192887088 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_copa_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140237192887088 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_copa_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140237192883056 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140237192883056 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140237192883056 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140237192883056 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
DEBUG:lm_eval.api.task:Both target_delimiter " " and target choice: " the toilet filled with water." have whitespace
DEBUG:lm_eval.api.task:Both target_delimiter " " and target choice: " water flowed from the spout." have whitespace
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of copa from None to 0
INFO:lm_eval.api.task:Building contexts for copa on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 134346.70it/s]
DEBUG:lm_eval.evaluator:Task: copa; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<03:30,  1.06s/it]Running loglikelihood requests:   1%|          | 2/200 [00:01<02:21,  1.40it/s]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<01:58,  1.66it/s]Running loglikelihood requests:   2%|▏         | 4/200 [00:02<01:47,  1.82it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:02<01:40,  1.93it/s]Running loglikelihood requests:   3%|▎         | 6/200 [00:03<01:36,  2.00it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:03<01:33,  2.06it/s]Running loglikelihood requests:   4%|▍         | 8/200 [00:04<01:31,  2.10it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:04<01:30,  2.12it/s]Running loglikelihood requests:   5%|▌         | 10/200 [00:05<01:28,  2.14it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:05<01:27,  2.16it/s]Running loglikelihood requests:   6%|▌         | 12/200 [00:06<01:26,  2.18it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:06<01:25,  2.18it/s]Running loglikelihood requests:   7%|▋         | 14/200 [00:07<01:24,  2.19it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:07<01:23,  2.20it/s]Running loglikelihood requests:   8%|▊         | 16/200 [00:07<01:23,  2.21it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:08<01:22,  2.22it/s]Running loglikelihood requests:   9%|▉         | 18/200 [00:08<01:21,  2.22it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:09<01:21,  2.23it/s]Running loglikelihood requests:  10%|█         | 20/200 [00:09<01:20,  2.23it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:10<01:20,  2.23it/s]Running loglikelihood requests:  11%|█         | 22/200 [00:10<01:19,  2.23it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:11<01:19,  2.24it/s]Running loglikelihood requests:  12%|█▏        | 24/200 [00:11<01:18,  2.24it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:11<01:17,  2.25it/s]Running loglikelihood requests:  13%|█▎        | 26/200 [00:12<01:17,  2.24it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:12<01:17,  2.24it/s]Running loglikelihood requests:  14%|█▍        | 28/200 [00:13<01:16,  2.24it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:13<01:16,  2.24it/s]Running loglikelihood requests:  15%|█▌        | 30/200 [00:14<01:15,  2.25it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:14<01:15,  2.25it/s]Running loglikelihood requests:  16%|█▌        | 32/200 [00:15<01:14,  2.25it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:15<01:14,  2.25it/s]Running loglikelihood requests:  17%|█▋        | 34/200 [00:15<01:13,  2.25it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:16<01:13,  2.25it/s]Running loglikelihood requests:  18%|█▊        | 36/200 [00:16<01:12,  2.25it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:17<01:12,  2.25it/s]Running loglikelihood requests:  19%|█▉        | 38/200 [00:17<01:11,  2.26it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:18<01:11,  2.26it/s]Running loglikelihood requests:  20%|██        | 40/200 [00:18<01:10,  2.25it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:19<01:10,  2.25it/s]Running loglikelihood requests:  21%|██        | 42/200 [00:19<01:10,  2.25it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:19<01:09,  2.26it/s]Running loglikelihood requests:  22%|██▏       | 44/200 [00:20<01:09,  2.25it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:20<01:09,  2.24it/s]Running loglikelihood requests:  23%|██▎       | 46/200 [00:21<01:08,  2.25it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:21<01:08,  2.24it/s]Running loglikelihood requests:  24%|██▍       | 48/200 [00:22<01:08,  2.23it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:22<01:07,  2.24it/s]Running loglikelihood requests:  25%|██▌       | 50/200 [00:23<01:06,  2.25it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:23<01:06,  2.25it/s]Running loglikelihood requests:  26%|██▌       | 52/200 [00:23<01:05,  2.25it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:24<01:05,  2.26it/s]Running loglikelihood requests:  27%|██▋       | 54/200 [00:24<01:04,  2.27it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:25<01:03,  2.27it/s]Running loglikelihood requests:  28%|██▊       | 56/200 [00:25<01:03,  2.28it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:26<01:02,  2.28it/s]Running loglikelihood requests:  29%|██▉       | 58/200 [00:26<01:02,  2.28it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:27<01:01,  2.29it/s]Running loglikelihood requests:  30%|███       | 60/200 [00:27<01:01,  2.29it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:27<01:00,  2.29it/s]Running loglikelihood requests:  31%|███       | 62/200 [00:28<01:00,  2.29it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:28<00:59,  2.29it/s]Running loglikelihood requests:  32%|███▏      | 64/200 [00:29<00:59,  2.29it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:29<00:58,  2.30it/s]Running loglikelihood requests:  33%|███▎      | 66/200 [00:30<00:58,  2.30it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:30<00:57,  2.30it/s]Running loglikelihood requests:  34%|███▍      | 68/200 [00:30<00:57,  2.30it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:31<00:56,  2.30it/s]Running loglikelihood requests:  35%|███▌      | 70/200 [00:31<00:56,  2.30it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:32<00:56,  2.29it/s]Running loglikelihood requests:  36%|███▌      | 72/200 [00:32<00:55,  2.30it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:33<00:55,  2.31it/s]Running loglikelihood requests:  37%|███▋      | 74/200 [00:33<00:54,  2.31it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:33<00:54,  2.31it/s]Running loglikelihood requests:  38%|███▊      | 76/200 [00:34<00:53,  2.32it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:34<00:53,  2.32it/s]Running loglikelihood requests:  39%|███▉      | 78/200 [00:35<00:52,  2.33it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:35<00:51,  2.33it/s]Running loglikelihood requests:  40%|████      | 80/200 [00:36<00:51,  2.33it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:36<00:51,  2.33it/s]Running loglikelihood requests:  41%|████      | 82/200 [00:36<00:50,  2.32it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:37<00:50,  2.32it/s]Running loglikelihood requests:  42%|████▏     | 84/200 [00:37<00:49,  2.33it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:38<00:49,  2.33it/s]Running loglikelihood requests:  43%|████▎     | 86/200 [00:38<00:48,  2.33it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:39<00:48,  2.33it/s]Running loglikelihood requests:  44%|████▍     | 88/200 [00:39<00:48,  2.33it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:39<00:47,  2.33it/s]Running loglikelihood requests:  45%|████▌     | 90/200 [00:40<00:47,  2.33it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:40<00:46,  2.33it/s]Running loglikelihood requests:  46%|████▌     | 92/200 [00:41<00:46,  2.32it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:41<00:46,  2.32it/s]Running loglikelihood requests:  47%|████▋     | 94/200 [00:42<00:45,  2.32it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:42<00:45,  2.32it/s]Running loglikelihood requests:  48%|████▊     | 96/200 [00:43<00:44,  2.32it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:43<00:44,  2.32it/s]Running loglikelihood requests:  49%|████▉     | 98/200 [00:43<00:43,  2.33it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:44<00:43,  2.34it/s]Running loglikelihood requests:  50%|█████     | 100/200 [00:44<00:42,  2.34it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:45<00:42,  2.34it/s]Running loglikelihood requests:  51%|█████     | 102/200 [00:45<00:41,  2.34it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:45<00:41,  2.35it/s]Running loglikelihood requests:  52%|█████▏    | 104/200 [00:46<00:40,  2.36it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:46<00:40,  2.36it/s]Running loglikelihood requests:  53%|█████▎    | 106/200 [00:47<00:39,  2.35it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:47<00:39,  2.36it/s]Running loglikelihood requests:  54%|█████▍    | 108/200 [00:48<00:39,  2.36it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:48<00:38,  2.36it/s]Running loglikelihood requests:  55%|█████▌    | 110/200 [00:48<00:38,  2.35it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:49<00:37,  2.35it/s]Running loglikelihood requests:  56%|█████▌    | 112/200 [00:49<00:37,  2.35it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:50<00:36,  2.35it/s]Running loglikelihood requests:  57%|█████▋    | 114/200 [00:50<00:36,  2.37it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:51<00:35,  2.37it/s]Running loglikelihood requests:  58%|█████▊    | 116/200 [00:51<00:35,  2.38it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:51<00:34,  2.39it/s]Running loglikelihood requests:  59%|█████▉    | 118/200 [00:52<00:34,  2.39it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:52<00:33,  2.39it/s]Running loglikelihood requests:  60%|██████    | 120/200 [00:53<00:33,  2.38it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:53<00:33,  2.38it/s]Running loglikelihood requests:  61%|██████    | 122/200 [00:53<00:32,  2.38it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:54<00:32,  2.38it/s]Running loglikelihood requests:  62%|██████▏   | 124/200 [00:54<00:31,  2.40it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:55<00:31,  2.39it/s]Running loglikelihood requests:  63%|██████▎   | 126/200 [00:55<00:30,  2.39it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:56<00:30,  2.39it/s]Running loglikelihood requests:  64%|██████▍   | 128/200 [00:56<00:30,  2.39it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:56<00:29,  2.38it/s]Running loglikelihood requests:  65%|██████▌   | 130/200 [00:57<00:29,  2.39it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:57<00:28,  2.39it/s]Running loglikelihood requests:  66%|██████▌   | 132/200 [00:58<00:28,  2.39it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:58<00:28,  2.38it/s]Running loglikelihood requests:  67%|██████▋   | 134/200 [00:59<00:27,  2.38it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:59<00:27,  2.38it/s]Running loglikelihood requests:  68%|██████▊   | 136/200 [00:59<00:26,  2.38it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [01:00<00:26,  2.39it/s]Running loglikelihood requests:  69%|██████▉   | 138/200 [01:00<00:25,  2.39it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [01:01<00:25,  2.39it/s]Running loglikelihood requests:  70%|███████   | 140/200 [01:01<00:25,  2.39it/s]Running loglikelihood requests:  70%|███████   | 141/200 [01:01<00:24,  2.40it/s]Running loglikelihood requests:  71%|███████   | 142/200 [01:02<00:24,  2.40it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [01:02<00:23,  2.40it/s]Running loglikelihood requests:  72%|███████▏  | 144/200 [01:03<00:23,  2.41it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:03<00:22,  2.41it/s]Running loglikelihood requests:  73%|███████▎  | 146/200 [01:04<00:22,  2.41it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [01:04<00:21,  2.43it/s]Running loglikelihood requests:  74%|███████▍  | 148/200 [01:04<00:21,  2.42it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:05<00:21,  2.42it/s]Running loglikelihood requests:  75%|███████▌  | 150/200 [01:05<00:20,  2.42it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:06<00:20,  2.41it/s]Running loglikelihood requests:  76%|███████▌  | 152/200 [01:06<00:19,  2.41it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:06<00:19,  2.41it/s]Running loglikelihood requests:  77%|███████▋  | 154/200 [01:07<00:19,  2.42it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:07<00:18,  2.41it/s]Running loglikelihood requests:  78%|███████▊  | 156/200 [01:08<00:18,  2.43it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:08<00:17,  2.43it/s]Running loglikelihood requests:  79%|███████▉  | 158/200 [01:08<00:17,  2.42it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:09<00:16,  2.43it/s]Running loglikelihood requests:  80%|████████  | 160/200 [01:09<00:16,  2.44it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:10<00:15,  2.44it/s]Running loglikelihood requests:  81%|████████  | 162/200 [01:10<00:15,  2.44it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:11<00:15,  2.45it/s]Running loglikelihood requests:  82%|████████▏ | 164/200 [01:11<00:14,  2.44it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:11<00:14,  2.44it/s]Running loglikelihood requests:  83%|████████▎ | 166/200 [01:12<00:13,  2.44it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:12<00:13,  2.44it/s]Running loglikelihood requests:  84%|████████▍ | 168/200 [01:13<00:13,  2.44it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:13<00:12,  2.44it/s]Running loglikelihood requests:  85%|████████▌ | 170/200 [01:13<00:12,  2.42it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:14<00:11,  2.42it/s]Running loglikelihood requests:  86%|████████▌ | 172/200 [01:14<00:11,  2.43it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:15<00:11,  2.44it/s]Running loglikelihood requests:  87%|████████▋ | 174/200 [01:15<00:10,  2.44it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:15<00:10,  2.46it/s]Running loglikelihood requests:  88%|████████▊ | 176/200 [01:16<00:09,  2.46it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:16<00:09,  2.47it/s]Running loglikelihood requests:  89%|████████▉ | 178/200 [01:17<00:08,  2.46it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:17<00:08,  2.45it/s]Running loglikelihood requests:  90%|█████████ | 180/200 [01:17<00:08,  2.46it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:18<00:07,  2.46it/s]Running loglikelihood requests:  91%|█████████ | 182/200 [01:18<00:07,  2.45it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:19<00:06,  2.46it/s]Running loglikelihood requests:  92%|█████████▏| 184/200 [01:19<00:06,  2.47it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:19<00:06,  2.46it/s]Running loglikelihood requests:  93%|█████████▎| 186/200 [01:20<00:05,  2.45it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:20<00:05,  2.47it/s]Running loglikelihood requests:  94%|█████████▍| 188/200 [01:21<00:04,  2.48it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:21<00:04,  2.49it/s]Running loglikelihood requests:  95%|█████████▌| 190/200 [01:21<00:03,  2.51it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:22<00:03,  2.51it/s]Running loglikelihood requests:  96%|█████████▌| 192/200 [01:22<00:03,  2.51it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:23<00:02,  2.51it/s]Running loglikelihood requests:  97%|█████████▋| 194/200 [01:23<00:02,  2.52it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:23<00:01,  2.55it/s]Running loglikelihood requests:  98%|█████████▊| 196/200 [01:24<00:01,  2.55it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:24<00:01,  2.57it/s]Running loglikelihood requests:  99%|█████████▉| 198/200 [01:25<00:00,  2.58it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:25<00:00,  2.59it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:25<00:00,  2.61it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:25<00:00,  2.33it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:6'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:6'}
full model:
{'copa': {'alias': 'copa', 'acc,none': 0.82, 'acc_stderr,none': 0.03861229196653691}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7827148355678417
0.8244609672079803
0.649635438690672
0.6067147398962018
0.806517251330611
0.9304450428937726
0.9492767057467371
0.8146777207922447
0.5943920512469448
0.6904992046065734
0.898578027138337
0.9772061768478973
0.9053772479641496
0.8020143483317842
0.5125553000556808
0.7018400400551881
0.8957865573767195
0.6076519427757794
0.975563476608663
0.9639212062070597
0.9296418237714587
0.9150145595079396
0.9197448761365332
0.7066536936990038
0.6639302750778917
0.912306872752127
0.7698141900267782
0.6230420491138425
0.8289959673107662
Total groups 68 exceeded the threshold, stopping comparison.
The group tensor is
[7, 3, 4, 1, 2, 6, 5, 0]
tensor([7, 3, 4, 1, 2, 6, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 4, 1, 0, 5, 7, 2]
tensor([6, 3, 4, 1, 0, 5, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 4, 3, 2, 0, 5, 6, 1]
tensor([7, 4, 3, 2, 0, 5, 6, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 6, 4, 2, 1, 3, 5, 0]
tensor([7, 6, 4, 2, 1, 3, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 3, 0, 1, 2, 3, 2]
tensor([0, 1, 3, 0, 1, 2, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 3, 1, 0, 2, 3, 2]
tensor([0, 1, 3, 1, 0, 2, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 0, 1.0, 1.0, 1.0, 1]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 2 to 3
done!
Normal merging for layer 4
tensor([4])
tensor(4)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 5 to 6
done!
Normal merging for layer 7
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 8 to 15
done!
Normal merging for layer 16
tensor([0, 3])
tensor(0)
tensor([1, 4])
tensor(1)
tensor([5, 7])
tensor(5)
tensor([2, 6])
tensor(2)
done!
Cross-layer merge completed for layers 17 to 21
done!
Normal merging for layer 22
tensor([0, 4])
tensor(0)
tensor([1, 3])
tensor(1)
tensor([5, 7])
tensor(5)
tensor([2, 6])
tensor(2)
done!
Cross-layer merge completed for layers 23 to 27
done!
Normal merging for layer 28
tensor([0, 3])
tensor(0)
tensor([1, 2, 4, 5, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 29 to 30
done!
Normal merging for layer 31
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
all done!
Model size: 12.1348 GB
13
cuda:6
mastermind_46_easy
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:43<00:43, 43.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:57<00:00, 26.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:57<00:00, 28.79s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_46_mcq_random HTTP/1.1" 200 778
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/flair/mastermind_46_mcq_random/flair/mastermind_46_mcq_random.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_46_mcq_random HTTP/1.1" 200 786
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/flair/mastermind_46_mcq_random/resolve/544d077942975b1664c0bc4fd54df026050329a4/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/flair/mastermind_46_mcq_random/resolve/544d077942975b1664c0bc4fd54df026050329a4/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:filelock:Attempting to acquire lock 140238673647312 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_46_mcq_random_default_0.0.0_544d077942975b1664c0bc4fd54df026050329a4.lock
DEBUG:filelock:Lock 140238673647312 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_46_mcq_random_default_0.0.0_544d077942975b1664c0bc4fd54df026050329a4.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_46_mcq_random/default/0.0.0/544d077942975b1664c0bc4fd54df026050329a4/dataset_info.json
DEBUG:filelock:Attempting to release lock 140238673647312 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_46_mcq_random_default_0.0.0_544d077942975b1664c0bc4fd54df026050329a4.lock
DEBUG:filelock:Lock 140238673647312 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_46_mcq_random_default_0.0.0_544d077942975b1664c0bc4fd54df026050329a4.lock
DEBUG:filelock:Attempting to acquire lock 140237734704240 on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_46_mcq_random/default/0.0.0/544d077942975b1664c0bc4fd54df026050329a4_builder.lock
DEBUG:filelock:Lock 140237734704240 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_46_mcq_random/default/0.0.0/544d077942975b1664c0bc4fd54df026050329a4_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_46_mcq_random/default/0.0.0/544d077942975b1664c0bc4fd54df026050329a4/dataset_info.json
DEBUG:filelock:Attempting to release lock 140237734704240 on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_46_mcq_random/default/0.0.0/544d077942975b1664c0bc4fd54df026050329a4_builder.lock
DEBUG:filelock:Lock 140237734704240 released on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_46_mcq_random/default/0.0.0/544d077942975b1664c0bc4fd54df026050329a4_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of mastermind_46_easy from None to 0
INFO:lm_eval.api.task:Building contexts for mastermind_46_easy on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1486.88it/s]
DEBUG:lm_eval.evaluator:Task: mastermind_46_easy; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:02<13:21,  2.01s/it]Running loglikelihood requests:   0%|          | 2/400 [00:03<11:19,  1.71s/it]Running loglikelihood requests:   1%|          | 3/400 [00:04<10:37,  1.60s/it]Running loglikelihood requests:   1%|          | 4/400 [00:06<10:16,  1.56s/it]Running loglikelihood requests:   1%|▏         | 5/400 [00:07<10:04,  1.53s/it]Running loglikelihood requests:   2%|▏         | 6/400 [00:09<09:55,  1.51s/it]Running loglikelihood requests:   2%|▏         | 7/400 [00:10<09:49,  1.50s/it]Running loglikelihood requests:   2%|▏         | 8/400 [00:12<09:48,  1.50s/it]Running loglikelihood requests:   2%|▏         | 9/400 [00:13<09:44,  1.50s/it]Running loglikelihood requests:   2%|▎         | 10/400 [00:15<09:36,  1.48s/it]Running loglikelihood requests:   3%|▎         | 11/400 [00:16<09:30,  1.47s/it]Running loglikelihood requests:   3%|▎         | 12/400 [00:18<09:26,  1.46s/it]Running loglikelihood requests:   3%|▎         | 13/400 [00:19<09:23,  1.46s/it]Running loglikelihood requests:   4%|▎         | 14/400 [00:21<09:19,  1.45s/it]Running loglikelihood requests:   4%|▍         | 15/400 [00:22<09:17,  1.45s/it]Running loglikelihood requests:   4%|▍         | 16/400 [00:23<09:14,  1.44s/it]Running loglikelihood requests:   4%|▍         | 17/400 [00:25<09:11,  1.44s/it]Running loglikelihood requests:   4%|▍         | 18/400 [00:26<09:10,  1.44s/it]Running loglikelihood requests:   5%|▍         | 19/400 [00:28<09:08,  1.44s/it]Running loglikelihood requests:   5%|▌         | 20/400 [00:29<09:06,  1.44s/it]Running loglikelihood requests:   5%|▌         | 21/400 [00:31<09:04,  1.44s/it]Running loglikelihood requests:   6%|▌         | 22/400 [00:32<09:03,  1.44s/it]Running loglikelihood requests:   6%|▌         | 23/400 [00:34<09:01,  1.44s/it]Running loglikelihood requests:   6%|▌         | 24/400 [00:35<08:59,  1.44s/it]Running loglikelihood requests:   6%|▋         | 25/400 [00:36<08:58,  1.43s/it]Running loglikelihood requests:   6%|▋         | 26/400 [00:38<08:56,  1.43s/it]Running loglikelihood requests:   7%|▋         | 27/400 [00:39<08:54,  1.43s/it]Running loglikelihood requests:   7%|▋         | 28/400 [00:41<08:52,  1.43s/it]Running loglikelihood requests:   7%|▋         | 29/400 [00:42<08:50,  1.43s/it]Running loglikelihood requests:   8%|▊         | 30/400 [00:44<08:49,  1.43s/it]Running loglikelihood requests:   8%|▊         | 31/400 [00:45<08:47,  1.43s/it]Running loglikelihood requests:   8%|▊         | 32/400 [00:46<08:46,  1.43s/it]Running loglikelihood requests:   8%|▊         | 33/400 [00:48<08:45,  1.43s/it]Running loglikelihood requests:   8%|▊         | 34/400 [00:49<08:44,  1.43s/it]Running loglikelihood requests:   9%|▉         | 35/400 [00:51<08:43,  1.43s/it]Running loglikelihood requests:   9%|▉         | 36/400 [00:52<08:41,  1.43s/it]Running loglikelihood requests:   9%|▉         | 37/400 [00:54<08:40,  1.43s/it]Running loglikelihood requests:  10%|▉         | 38/400 [00:55<08:38,  1.43s/it]Running loglikelihood requests:  10%|▉         | 39/400 [00:56<08:38,  1.44s/it]Running loglikelihood requests:  10%|█         | 40/400 [00:58<08:38,  1.44s/it]Running loglikelihood requests:  10%|█         | 41/400 [00:59<08:38,  1.44s/it]Running loglikelihood requests:  10%|█         | 42/400 [01:01<08:35,  1.44s/it]Running loglikelihood requests:  11%|█         | 43/400 [01:02<08:31,  1.43s/it]Running loglikelihood requests:  11%|█         | 44/400 [01:04<08:29,  1.43s/it]Running loglikelihood requests:  11%|█▏        | 45/400 [01:05<08:27,  1.43s/it]Running loglikelihood requests:  12%|█▏        | 46/400 [01:06<08:25,  1.43s/it]Running loglikelihood requests:  12%|█▏        | 47/400 [01:08<08:23,  1.43s/it]Running loglikelihood requests:  12%|█▏        | 48/400 [01:09<08:21,  1.43s/it]Running loglikelihood requests:  12%|█▏        | 49/400 [01:11<08:19,  1.42s/it]Running loglikelihood requests:  12%|█▎        | 50/400 [01:12<08:18,  1.42s/it]Running loglikelihood requests:  13%|█▎        | 51/400 [01:14<08:16,  1.42s/it]Running loglikelihood requests:  13%|█▎        | 52/400 [01:15<08:14,  1.42s/it]Running loglikelihood requests:  13%|█▎        | 53/400 [01:16<08:12,  1.42s/it]Running loglikelihood requests:  14%|█▎        | 54/400 [01:18<08:10,  1.42s/it]Running loglikelihood requests:  14%|█▍        | 55/400 [01:19<08:09,  1.42s/it]Running loglikelihood requests:  14%|█▍        | 56/400 [01:21<08:07,  1.42s/it]Running loglikelihood requests:  14%|█▍        | 57/400 [01:22<08:06,  1.42s/it]Running loglikelihood requests:  14%|█▍        | 58/400 [01:24<08:04,  1.42s/it]Running loglikelihood requests:  15%|█▍        | 59/400 [01:25<08:03,  1.42s/it]Running loglikelihood requests:  15%|█▌        | 60/400 [01:26<08:01,  1.42s/it]Running loglikelihood requests:  15%|█▌        | 61/400 [01:28<07:59,  1.41s/it]Running loglikelihood requests:  16%|█▌        | 62/400 [01:29<08:02,  1.43s/it]Running loglikelihood requests:  16%|█▌        | 63/400 [01:31<08:05,  1.44s/it]Running loglikelihood requests:  16%|█▌        | 64/400 [01:32<08:07,  1.45s/it]Running loglikelihood requests:  16%|█▋        | 65/400 [01:34<08:08,  1.46s/it]Running loglikelihood requests:  16%|█▋        | 66/400 [01:35<08:07,  1.46s/it]Running loglikelihood requests:  17%|█▋        | 67/400 [01:37<08:02,  1.45s/it]Running loglikelihood requests:  17%|█▋        | 68/400 [01:38<07:58,  1.44s/it]Running loglikelihood requests:  17%|█▋        | 69/400 [01:39<07:54,  1.43s/it]Running loglikelihood requests:  18%|█▊        | 71/400 [01:41<06:02,  1.10s/it]Running loglikelihood requests:  18%|█▊        | 72/400 [01:42<06:27,  1.18s/it]Running loglikelihood requests:  18%|█▊        | 73/400 [01:44<06:46,  1.24s/it]Running loglikelihood requests:  18%|█▊        | 74/400 [01:45<07:00,  1.29s/it]Running loglikelihood requests:  19%|█▉        | 75/400 [01:46<07:10,  1.33s/it]Running loglikelihood requests:  19%|█▉        | 76/400 [01:48<07:17,  1.35s/it]Running loglikelihood requests:  19%|█▉        | 77/400 [01:49<07:22,  1.37s/it]Running loglikelihood requests:  20%|█▉        | 78/400 [01:51<07:26,  1.39s/it]Running loglikelihood requests:  20%|█▉        | 79/400 [01:52<07:28,  1.40s/it]Running loglikelihood requests:  20%|██        | 80/400 [01:54<07:28,  1.40s/it]Running loglikelihood requests:  20%|██        | 81/400 [01:55<07:28,  1.41s/it]Running loglikelihood requests:  20%|██        | 82/400 [01:56<07:28,  1.41s/it]Running loglikelihood requests:  21%|██        | 83/400 [01:58<07:28,  1.42s/it]Running loglikelihood requests:  21%|██        | 84/400 [01:59<07:28,  1.42s/it]Running loglikelihood requests:  21%|██▏       | 85/400 [02:01<07:28,  1.42s/it]Running loglikelihood requests:  22%|██▏       | 86/400 [02:02<07:27,  1.42s/it]Running loglikelihood requests:  22%|██▏       | 87/400 [02:04<07:24,  1.42s/it]Running loglikelihood requests:  22%|██▏       | 88/400 [02:05<07:22,  1.42s/it]Running loglikelihood requests:  22%|██▏       | 89/400 [02:06<07:20,  1.42s/it]Running loglikelihood requests:  22%|██▎       | 90/400 [02:08<07:18,  1.41s/it]Running loglikelihood requests:  23%|██▎       | 91/400 [02:09<07:15,  1.41s/it]Running loglikelihood requests:  23%|██▎       | 92/400 [02:11<07:13,  1.41s/it]Running loglikelihood requests:  23%|██▎       | 93/400 [02:12<07:11,  1.41s/it]Running loglikelihood requests:  24%|██▎       | 94/400 [02:13<07:09,  1.40s/it]Running loglikelihood requests:  24%|██▍       | 95/400 [02:15<07:07,  1.40s/it]Running loglikelihood requests:  24%|██▍       | 96/400 [02:16<07:06,  1.40s/it]Running loglikelihood requests:  24%|██▍       | 97/400 [02:18<07:04,  1.40s/it]Running loglikelihood requests:  24%|██▍       | 98/400 [02:19<07:03,  1.40s/it]Running loglikelihood requests:  25%|██▍       | 99/400 [02:20<07:01,  1.40s/it]Running loglikelihood requests:  25%|██▌       | 100/400 [02:22<07:00,  1.40s/it]Running loglikelihood requests:  25%|██▌       | 101/400 [02:23<06:59,  1.40s/it]Running loglikelihood requests:  26%|██▌       | 102/400 [02:25<06:57,  1.40s/it]Running loglikelihood requests:  26%|██▌       | 103/400 [02:26<06:55,  1.40s/it]Running loglikelihood requests:  26%|██▌       | 104/400 [02:27<06:54,  1.40s/it]Running loglikelihood requests:  26%|██▋       | 105/400 [02:29<06:52,  1.40s/it]Running loglikelihood requests:  26%|██▋       | 106/400 [02:30<06:50,  1.40s/it]Running loglikelihood requests:  27%|██▋       | 107/400 [02:32<06:49,  1.40s/it]Running loglikelihood requests:  27%|██▋       | 108/400 [02:33<06:47,  1.40s/it]Running loglikelihood requests:  27%|██▋       | 109/400 [02:34<06:46,  1.40s/it]Running loglikelihood requests:  28%|██▊       | 110/400 [02:36<06:44,  1.40s/it]Running loglikelihood requests:  28%|██▊       | 111/400 [02:37<06:43,  1.40s/it]Running loglikelihood requests:  28%|██▊       | 112/400 [02:39<06:41,  1.40s/it]Running loglikelihood requests:  28%|██▊       | 113/400 [02:40<06:40,  1.39s/it]Running loglikelihood requests:  28%|██▊       | 114/400 [02:41<06:38,  1.39s/it]Running loglikelihood requests:  29%|██▉       | 115/400 [02:43<06:37,  1.39s/it]Running loglikelihood requests:  29%|██▉       | 116/400 [02:44<06:35,  1.39s/it]Running loglikelihood requests:  29%|██▉       | 117/400 [02:45<06:33,  1.39s/it]Running loglikelihood requests:  30%|██▉       | 118/400 [02:47<06:32,  1.39s/it]Running loglikelihood requests:  30%|██▉       | 119/400 [02:48<06:31,  1.39s/it]Running loglikelihood requests:  30%|███       | 120/400 [02:50<06:29,  1.39s/it]Running loglikelihood requests:  30%|███       | 121/400 [02:51<06:28,  1.39s/it]Running loglikelihood requests:  30%|███       | 122/400 [02:52<06:27,  1.39s/it]Running loglikelihood requests:  31%|███       | 123/400 [02:54<06:26,  1.40s/it]Running loglikelihood requests:  31%|███       | 124/400 [02:55<06:24,  1.39s/it]Running loglikelihood requests:  31%|███▏      | 125/400 [02:57<06:23,  1.40s/it]Running loglikelihood requests:  32%|███▏      | 126/400 [02:58<06:22,  1.40s/it]Running loglikelihood requests:  32%|███▏      | 127/400 [02:59<06:22,  1.40s/it]Running loglikelihood requests:  32%|███▏      | 128/400 [03:01<06:22,  1.40s/it]Running loglikelihood requests:  32%|███▏      | 129/400 [03:02<06:23,  1.42s/it]Running loglikelihood requests:  32%|███▎      | 130/400 [03:04<06:21,  1.41s/it]Running loglikelihood requests:  33%|███▎      | 131/400 [03:05<06:17,  1.40s/it]Running loglikelihood requests:  33%|███▎      | 132/400 [03:06<06:14,  1.40s/it]Running loglikelihood requests:  33%|███▎      | 133/400 [03:08<06:14,  1.40s/it]Running loglikelihood requests:  34%|███▎      | 134/400 [03:09<06:16,  1.41s/it]Running loglikelihood requests:  34%|███▍      | 135/400 [03:11<06:16,  1.42s/it]Running loglikelihood requests:  34%|███▍      | 136/400 [03:12<06:16,  1.43s/it]Running loglikelihood requests:  34%|███▍      | 137/400 [03:14<06:15,  1.43s/it]Running loglikelihood requests:  34%|███▍      | 138/400 [03:15<06:14,  1.43s/it]Running loglikelihood requests:  35%|███▍      | 139/400 [03:16<06:13,  1.43s/it]Running loglikelihood requests:  35%|███▌      | 140/400 [03:18<06:12,  1.43s/it]Running loglikelihood requests:  35%|███▌      | 141/400 [03:19<06:11,  1.43s/it]Running loglikelihood requests:  36%|███▌      | 142/400 [03:21<06:09,  1.43s/it]Running loglikelihood requests:  36%|███▌      | 143/400 [03:22<06:08,  1.43s/it]Running loglikelihood requests:  36%|███▌      | 144/400 [03:24<06:07,  1.43s/it]Running loglikelihood requests:  36%|███▋      | 145/400 [03:25<06:00,  1.41s/it]Running loglikelihood requests:  36%|███▋      | 146/400 [03:26<05:55,  1.40s/it]Running loglikelihood requests:  37%|███▋      | 147/400 [03:28<05:51,  1.39s/it]Running loglikelihood requests:  37%|███▋      | 148/400 [03:29<05:47,  1.38s/it]Running loglikelihood requests:  37%|███▋      | 149/400 [03:30<05:44,  1.37s/it]Running loglikelihood requests:  38%|███▊      | 150/400 [03:32<05:42,  1.37s/it]Running loglikelihood requests:  38%|███▊      | 151/400 [03:33<05:40,  1.37s/it]Running loglikelihood requests:  38%|███▊      | 152/400 [03:35<05:38,  1.37s/it]Running loglikelihood requests:  38%|███▊      | 153/400 [03:36<05:36,  1.36s/it]Running loglikelihood requests:  38%|███▊      | 154/400 [03:37<05:35,  1.36s/it]Running loglikelihood requests:  39%|███▉      | 155/400 [03:39<05:33,  1.36s/it]Running loglikelihood requests:  39%|███▉      | 156/400 [03:40<05:32,  1.36s/it]Running loglikelihood requests:  39%|███▉      | 157/400 [03:41<05:30,  1.36s/it]Running loglikelihood requests:  40%|███▉      | 158/400 [03:43<05:29,  1.36s/it]Running loglikelihood requests:  40%|███▉      | 159/400 [03:44<05:27,  1.36s/it]Running loglikelihood requests:  40%|████      | 160/400 [03:45<05:25,  1.36s/it]Running loglikelihood requests:  40%|████      | 161/400 [03:47<05:24,  1.36s/it]Running loglikelihood requests:  40%|████      | 162/400 [03:48<05:22,  1.36s/it]Running loglikelihood requests:  41%|████      | 163/400 [03:49<05:21,  1.35s/it]Running loglikelihood requests:  41%|████      | 164/400 [03:51<05:19,  1.35s/it]Running loglikelihood requests:  41%|████▏     | 165/400 [03:52<05:18,  1.36s/it]Running loglikelihood requests:  42%|████▏     | 166/400 [03:54<05:17,  1.36s/it]Running loglikelihood requests:  42%|████▏     | 167/400 [03:55<05:17,  1.36s/it]Running loglikelihood requests:  42%|████▏     | 168/400 [03:56<05:15,  1.36s/it]Running loglikelihood requests:  42%|████▏     | 169/400 [03:58<05:14,  1.36s/it]Running loglikelihood requests:  42%|████▎     | 170/400 [03:59<05:12,  1.36s/it]Running loglikelihood requests:  43%|████▎     | 171/400 [04:00<05:11,  1.36s/it]Running loglikelihood requests:  43%|████▎     | 172/400 [04:02<05:12,  1.37s/it]Running loglikelihood requests:  43%|████▎     | 173/400 [04:03<05:10,  1.37s/it]Running loglikelihood requests:  44%|████▎     | 174/400 [04:04<05:07,  1.36s/it]Running loglikelihood requests:  44%|████▍     | 175/400 [04:06<05:04,  1.35s/it]Running loglikelihood requests:  44%|████▍     | 176/400 [04:07<05:02,  1.35s/it]Running loglikelihood requests:  44%|████▍     | 177/400 [04:08<05:00,  1.35s/it]Running loglikelihood requests:  44%|████▍     | 178/400 [04:10<04:58,  1.34s/it]Running loglikelihood requests:  45%|████▍     | 179/400 [04:11<04:56,  1.34s/it]Running loglikelihood requests:  45%|████▌     | 180/400 [04:13<04:55,  1.34s/it]Running loglikelihood requests:  45%|████▌     | 181/400 [04:14<04:53,  1.34s/it]Running loglikelihood requests:  46%|████▌     | 182/400 [04:15<04:51,  1.34s/it]Running loglikelihood requests:  46%|████▌     | 183/400 [04:17<04:50,  1.34s/it]Running loglikelihood requests:  46%|████▌     | 184/400 [04:18<04:48,  1.34s/it]Running loglikelihood requests:  46%|████▋     | 185/400 [04:19<04:46,  1.33s/it]Running loglikelihood requests:  46%|████▋     | 186/400 [04:21<04:45,  1.33s/it]Running loglikelihood requests:  47%|████▋     | 187/400 [04:22<04:43,  1.33s/it]Running loglikelihood requests:  47%|████▋     | 188/400 [04:23<04:42,  1.33s/it]Running loglikelihood requests:  47%|████▋     | 189/400 [04:24<04:40,  1.33s/it]Running loglikelihood requests:  48%|████▊     | 190/400 [04:26<04:39,  1.33s/it]Running loglikelihood requests:  48%|████▊     | 191/400 [04:27<04:37,  1.33s/it]Running loglikelihood requests:  48%|████▊     | 192/400 [04:28<04:36,  1.33s/it]Running loglikelihood requests:  48%|████▊     | 193/400 [04:30<04:34,  1.33s/it]Running loglikelihood requests:  48%|████▊     | 194/400 [04:31<04:33,  1.33s/it]Running loglikelihood requests:  49%|████▉     | 195/400 [04:32<04:31,  1.33s/it]Running loglikelihood requests:  49%|████▉     | 196/400 [04:34<04:30,  1.33s/it]Running loglikelihood requests:  49%|████▉     | 197/400 [04:35<04:28,  1.32s/it]Running loglikelihood requests:  50%|████▉     | 198/400 [04:36<04:27,  1.32s/it]Running loglikelihood requests:  50%|████▉     | 199/400 [04:38<04:26,  1.32s/it]Running loglikelihood requests:  50%|█████     | 200/400 [04:39<04:24,  1.32s/it]Running loglikelihood requests:  50%|█████     | 201/400 [04:40<04:23,  1.32s/it]Running loglikelihood requests:  50%|█████     | 202/400 [04:42<04:21,  1.32s/it]Running loglikelihood requests:  51%|█████     | 203/400 [04:43<04:20,  1.32s/it]Running loglikelihood requests:  51%|█████     | 204/400 [04:44<04:18,  1.32s/it]Running loglikelihood requests:  51%|█████▏    | 205/400 [04:46<04:17,  1.32s/it]Running loglikelihood requests:  52%|█████▏    | 206/400 [04:47<04:15,  1.32s/it]Running loglikelihood requests:  52%|█████▏    | 207/400 [04:48<04:14,  1.32s/it]Running loglikelihood requests:  52%|█████▏    | 208/400 [04:50<04:13,  1.32s/it]Running loglikelihood requests:  52%|█████▏    | 209/400 [04:51<04:11,  1.32s/it]Running loglikelihood requests:  52%|█████▎    | 210/400 [04:52<04:10,  1.32s/it]Running loglikelihood requests:  53%|█████▎    | 211/400 [04:54<04:09,  1.32s/it]Running loglikelihood requests:  53%|█████▎    | 212/400 [04:55<04:08,  1.32s/it]Running loglikelihood requests:  53%|█████▎    | 213/400 [04:56<04:07,  1.32s/it]Running loglikelihood requests:  54%|█████▎    | 214/400 [04:58<04:05,  1.32s/it]Running loglikelihood requests:  54%|█████▍    | 215/400 [04:59<04:03,  1.32s/it]Running loglikelihood requests:  54%|█████▍    | 216/400 [05:00<04:02,  1.32s/it]Running loglikelihood requests:  54%|█████▍    | 217/400 [05:01<04:01,  1.32s/it]Running loglikelihood requests:  55%|█████▍    | 218/400 [05:03<04:00,  1.32s/it]Running loglikelihood requests:  55%|█████▍    | 219/400 [05:04<03:59,  1.32s/it]Running loglikelihood requests:  55%|█████▌    | 220/400 [05:05<03:57,  1.32s/it]Running loglikelihood requests:  55%|█████▌    | 221/400 [05:07<03:55,  1.32s/it]Running loglikelihood requests:  56%|█████▌    | 222/400 [05:08<03:53,  1.31s/it]Running loglikelihood requests:  56%|█████▌    | 223/400 [05:09<03:52,  1.31s/it]Running loglikelihood requests:  56%|█████▌    | 224/400 [05:11<03:50,  1.31s/it]Running loglikelihood requests:  56%|█████▋    | 225/400 [05:12<03:48,  1.31s/it]Running loglikelihood requests:  56%|█████▋    | 226/400 [05:13<03:46,  1.30s/it]Running loglikelihood requests:  57%|█████▋    | 227/400 [05:15<03:45,  1.30s/it]Running loglikelihood requests:  57%|█████▋    | 228/400 [05:16<03:43,  1.30s/it]Running loglikelihood requests:  57%|█████▋    | 229/400 [05:17<03:43,  1.31s/it]Running loglikelihood requests:  57%|█████▊    | 230/400 [05:18<03:41,  1.30s/it]Running loglikelihood requests:  58%|█████▊    | 231/400 [05:20<03:40,  1.30s/it]Running loglikelihood requests:  58%|█████▊    | 232/400 [05:21<03:38,  1.30s/it]Running loglikelihood requests:  58%|█████▊    | 233/400 [05:22<03:37,  1.30s/it]Running loglikelihood requests:  58%|█████▊    | 234/400 [05:24<03:35,  1.30s/it]Running loglikelihood requests:  59%|█████▉    | 235/400 [05:25<03:34,  1.30s/it]Running loglikelihood requests:  59%|█████▉    | 236/400 [05:26<03:32,  1.30s/it]Running loglikelihood requests:  59%|█████▉    | 237/400 [05:28<03:31,  1.30s/it]Running loglikelihood requests:  60%|█████▉    | 238/400 [05:29<03:29,  1.30s/it]Running loglikelihood requests:  60%|█████▉    | 239/400 [05:30<03:28,  1.29s/it]Running loglikelihood requests:  60%|██████    | 240/400 [05:31<03:26,  1.29s/it]Running loglikelihood requests:  60%|██████    | 241/400 [05:33<03:24,  1.29s/it]Running loglikelihood requests:  60%|██████    | 242/400 [05:34<03:23,  1.29s/it]Running loglikelihood requests:  61%|██████    | 243/400 [05:35<03:21,  1.29s/it]Running loglikelihood requests:  61%|██████    | 244/400 [05:37<03:20,  1.28s/it]Running loglikelihood requests:  61%|██████▏   | 245/400 [05:38<03:18,  1.28s/it]Running loglikelihood requests:  62%|██████▏   | 246/400 [05:39<03:17,  1.28s/it]Running loglikelihood requests:  62%|██████▏   | 247/400 [05:40<03:15,  1.28s/it]Running loglikelihood requests:  62%|██████▏   | 248/400 [05:42<03:14,  1.28s/it]Running loglikelihood requests:  62%|██████▏   | 249/400 [05:43<03:12,  1.28s/it]Running loglikelihood requests:  62%|██████▎   | 250/400 [05:44<03:11,  1.28s/it]Running loglikelihood requests:  63%|██████▎   | 251/400 [05:46<03:10,  1.28s/it]Running loglikelihood requests:  63%|██████▎   | 252/400 [05:47<03:08,  1.28s/it]Running loglikelihood requests:  63%|██████▎   | 253/400 [05:48<03:07,  1.27s/it]Running loglikelihood requests:  64%|██████▎   | 254/400 [05:49<03:06,  1.27s/it]Running loglikelihood requests:  64%|██████▍   | 255/400 [05:51<03:04,  1.27s/it]Running loglikelihood requests:  64%|██████▍   | 256/400 [05:52<03:03,  1.27s/it]Running loglikelihood requests:  64%|██████▍   | 257/400 [05:53<03:01,  1.27s/it]Running loglikelihood requests:  64%|██████▍   | 258/400 [05:54<03:00,  1.27s/it]Running loglikelihood requests:  65%|██████▍   | 259/400 [05:56<02:59,  1.27s/it]Running loglikelihood requests:  65%|██████▌   | 260/400 [05:57<02:57,  1.27s/it]Running loglikelihood requests:  65%|██████▌   | 261/400 [05:58<02:56,  1.27s/it]Running loglikelihood requests:  66%|██████▌   | 262/400 [05:59<02:54,  1.27s/it]Running loglikelihood requests:  66%|██████▌   | 263/400 [06:01<02:53,  1.27s/it]Running loglikelihood requests:  66%|██████▌   | 264/400 [06:02<02:52,  1.27s/it]Running loglikelihood requests:  66%|██████▋   | 265/400 [06:03<02:51,  1.27s/it]Running loglikelihood requests:  66%|██████▋   | 266/400 [06:05<02:50,  1.27s/it]Running loglikelihood requests:  67%|██████▋   | 267/400 [06:06<02:49,  1.28s/it]Running loglikelihood requests:  67%|██████▋   | 268/400 [06:07<02:47,  1.27s/it]Running loglikelihood requests:  67%|██████▋   | 269/400 [06:08<02:46,  1.27s/it]Running loglikelihood requests:  68%|██████▊   | 270/400 [06:10<02:44,  1.27s/it]Running loglikelihood requests:  68%|██████▊   | 271/400 [06:11<02:43,  1.27s/it]Running loglikelihood requests:  68%|██████▊   | 272/400 [06:12<02:41,  1.26s/it]Running loglikelihood requests:  68%|██████▊   | 273/400 [06:13<02:40,  1.26s/it]Running loglikelihood requests:  68%|██████▊   | 274/400 [06:15<02:38,  1.26s/it]Running loglikelihood requests:  69%|██████▉   | 275/400 [06:16<02:37,  1.26s/it]Running loglikelihood requests:  69%|██████▉   | 276/400 [06:17<02:35,  1.26s/it]Running loglikelihood requests:  69%|██████▉   | 277/400 [06:18<02:34,  1.26s/it]Running loglikelihood requests:  70%|██████▉   | 278/400 [06:20<02:33,  1.25s/it]Running loglikelihood requests:  70%|██████▉   | 279/400 [06:21<02:31,  1.25s/it]Running loglikelihood requests:  70%|███████   | 280/400 [06:22<02:30,  1.25s/it]Running loglikelihood requests:  70%|███████   | 281/400 [06:23<02:29,  1.25s/it]Running loglikelihood requests:  70%|███████   | 282/400 [06:25<02:27,  1.25s/it]Running loglikelihood requests:  71%|███████   | 283/400 [06:26<02:26,  1.25s/it]Running loglikelihood requests:  71%|███████   | 284/400 [06:27<02:24,  1.25s/it]Running loglikelihood requests:  71%|███████▏  | 285/400 [06:28<02:23,  1.25s/it]Running loglikelihood requests:  72%|███████▏  | 286/400 [06:30<02:22,  1.25s/it]Running loglikelihood requests:  72%|███████▏  | 287/400 [06:31<02:20,  1.25s/it]Running loglikelihood requests:  72%|███████▏  | 288/400 [06:32<02:19,  1.25s/it]Running loglikelihood requests:  72%|███████▏  | 289/400 [06:33<02:18,  1.25s/it]Running loglikelihood requests:  72%|███████▎  | 290/400 [06:35<02:17,  1.25s/it]Running loglikelihood requests:  73%|███████▎  | 291/400 [06:36<02:15,  1.25s/it]Running loglikelihood requests:  73%|███████▎  | 292/400 [06:37<02:14,  1.25s/it]Running loglikelihood requests:  73%|███████▎  | 293/400 [06:38<02:13,  1.24s/it]Running loglikelihood requests:  74%|███████▎  | 294/400 [06:40<02:11,  1.24s/it]Running loglikelihood requests:  74%|███████▍  | 295/400 [06:41<02:10,  1.24s/it]Running loglikelihood requests:  74%|███████▍  | 296/400 [06:42<02:09,  1.24s/it]Running loglikelihood requests:  74%|███████▍  | 297/400 [06:43<02:07,  1.24s/it]Running loglikelihood requests:  74%|███████▍  | 298/400 [06:45<02:06,  1.24s/it]Running loglikelihood requests:  75%|███████▍  | 299/400 [06:46<02:04,  1.24s/it]Running loglikelihood requests:  75%|███████▌  | 300/400 [06:47<02:03,  1.24s/it]Running loglikelihood requests:  75%|███████▌  | 301/400 [06:48<02:02,  1.23s/it]Running loglikelihood requests:  76%|███████▌  | 302/400 [06:50<02:00,  1.23s/it]Running loglikelihood requests:  76%|███████▌  | 303/400 [06:51<01:59,  1.23s/it]Running loglikelihood requests:  76%|███████▌  | 304/400 [06:52<01:58,  1.23s/it]Running loglikelihood requests:  76%|███████▋  | 305/400 [06:53<01:56,  1.23s/it]Running loglikelihood requests:  76%|███████▋  | 306/400 [06:54<01:55,  1.23s/it]Running loglikelihood requests:  77%|███████▋  | 307/400 [06:56<01:54,  1.23s/it]Running loglikelihood requests:  77%|███████▋  | 308/400 [06:57<01:53,  1.23s/it]Running loglikelihood requests:  77%|███████▋  | 309/400 [06:58<01:52,  1.23s/it]Running loglikelihood requests:  78%|███████▊  | 310/400 [06:59<01:50,  1.23s/it]Running loglikelihood requests:  78%|███████▊  | 311/400 [07:01<01:49,  1.23s/it]Running loglikelihood requests:  78%|███████▊  | 312/400 [07:02<01:48,  1.23s/it]Running loglikelihood requests:  78%|███████▊  | 313/400 [07:03<01:47,  1.23s/it]Running loglikelihood requests:  78%|███████▊  | 314/400 [07:04<01:45,  1.23s/it]Running loglikelihood requests:  79%|███████▉  | 315/400 [07:06<01:45,  1.24s/it]Running loglikelihood requests:  79%|███████▉  | 316/400 [07:07<01:44,  1.24s/it]Running loglikelihood requests:  79%|███████▉  | 317/400 [07:08<01:42,  1.24s/it]Running loglikelihood requests:  80%|███████▉  | 318/400 [07:09<01:41,  1.23s/it]Running loglikelihood requests:  80%|███████▉  | 319/400 [07:10<01:39,  1.23s/it]Running loglikelihood requests:  80%|████████  | 320/400 [07:12<01:38,  1.23s/it]Running loglikelihood requests:  80%|████████  | 321/400 [07:13<01:36,  1.22s/it]Running loglikelihood requests:  80%|████████  | 322/400 [07:14<01:35,  1.22s/it]Running loglikelihood requests:  81%|████████  | 323/400 [07:15<01:34,  1.22s/it]Running loglikelihood requests:  81%|████████  | 324/400 [07:17<01:32,  1.22s/it]Running loglikelihood requests:  81%|████████▏ | 325/400 [07:18<01:31,  1.22s/it]Running loglikelihood requests:  82%|████████▏ | 326/400 [07:19<01:30,  1.22s/it]Running loglikelihood requests:  82%|████████▏ | 327/400 [07:20<01:28,  1.22s/it]Running loglikelihood requests:  82%|████████▏ | 328/400 [07:21<01:27,  1.22s/it]Running loglikelihood requests:  82%|████████▏ | 329/400 [07:23<01:26,  1.21s/it]Running loglikelihood requests:  82%|████████▎ | 330/400 [07:24<01:24,  1.21s/it]Running loglikelihood requests:  83%|████████▎ | 331/400 [07:25<01:23,  1.21s/it]Running loglikelihood requests:  83%|████████▎ | 332/400 [07:26<01:22,  1.21s/it]Running loglikelihood requests:  83%|████████▎ | 333/400 [07:27<01:21,  1.21s/it]Running loglikelihood requests:  84%|████████▎ | 334/400 [07:29<01:19,  1.21s/it]Running loglikelihood requests:  84%|████████▍ | 335/400 [07:30<01:18,  1.21s/it]Running loglikelihood requests:  84%|████████▍ | 336/400 [07:31<01:17,  1.21s/it]Running loglikelihood requests:  84%|████████▍ | 337/400 [07:32<01:16,  1.21s/it]Running loglikelihood requests:  84%|████████▍ | 338/400 [07:34<01:14,  1.21s/it]Running loglikelihood requests:  85%|████████▍ | 339/400 [07:35<01:13,  1.21s/it]Running loglikelihood requests:  85%|████████▌ | 340/400 [07:36<01:12,  1.21s/it]Running loglikelihood requests:  85%|████████▌ | 341/400 [07:37<01:11,  1.20s/it]Running loglikelihood requests:  86%|████████▌ | 342/400 [07:38<01:09,  1.20s/it]Running loglikelihood requests:  86%|████████▌ | 343/400 [07:40<01:08,  1.20s/it]Running loglikelihood requests:  86%|████████▌ | 344/400 [07:41<01:07,  1.20s/it]Running loglikelihood requests:  86%|████████▋ | 345/400 [07:42<01:06,  1.20s/it]Running loglikelihood requests:  86%|████████▋ | 346/400 [07:43<01:05,  1.20s/it]Running loglikelihood requests:  87%|████████▋ | 347/400 [07:44<01:03,  1.20s/it]Running loglikelihood requests:  87%|████████▋ | 348/400 [07:46<01:02,  1.20s/it]Running loglikelihood requests:  87%|████████▋ | 349/400 [07:47<01:01,  1.20s/it]Running loglikelihood requests:  88%|████████▊ | 350/400 [07:48<01:00,  1.20s/it]Running loglikelihood requests:  88%|████████▊ | 351/400 [07:49<00:58,  1.20s/it]Running loglikelihood requests:  88%|████████▊ | 352/400 [07:50<00:57,  1.20s/it]Running loglikelihood requests:  88%|████████▊ | 353/400 [07:52<00:56,  1.20s/it]Running loglikelihood requests:  88%|████████▊ | 354/400 [07:53<00:55,  1.20s/it]Running loglikelihood requests:  89%|████████▉ | 355/400 [07:54<00:54,  1.20s/it]Running loglikelihood requests:  89%|████████▉ | 356/400 [07:55<00:52,  1.20s/it]Running loglikelihood requests:  89%|████████▉ | 357/400 [07:56<00:51,  1.20s/it]Running loglikelihood requests:  90%|████████▉ | 358/400 [07:58<00:50,  1.20s/it]Running loglikelihood requests:  90%|████████▉ | 359/400 [07:59<00:49,  1.20s/it]Running loglikelihood requests:  90%|█████████ | 360/400 [08:00<00:48,  1.20s/it]Running loglikelihood requests:  90%|█████████ | 361/400 [08:01<00:46,  1.20s/it]Running loglikelihood requests:  90%|█████████ | 362/400 [08:02<00:45,  1.20s/it]Running loglikelihood requests:  91%|█████████ | 363/400 [08:04<00:44,  1.20s/it]Running loglikelihood requests:  91%|█████████ | 364/400 [08:05<00:43,  1.20s/it]Running loglikelihood requests:  91%|█████████▏| 365/400 [08:06<00:42,  1.20s/it]Running loglikelihood requests:  92%|█████████▏| 366/400 [08:07<00:41,  1.21s/it]Running loglikelihood requests:  92%|█████████▏| 367/400 [08:08<00:39,  1.21s/it]Running loglikelihood requests:  92%|█████████▏| 368/400 [08:10<00:38,  1.21s/it]Running loglikelihood requests:  92%|█████████▏| 369/400 [08:11<00:37,  1.20s/it]Running loglikelihood requests:  92%|█████████▎| 370/400 [08:12<00:35,  1.20s/it]Running loglikelihood requests:  93%|█████████▎| 371/400 [08:13<00:34,  1.20s/it]Running loglikelihood requests:  93%|█████████▎| 372/400 [08:14<00:33,  1.19s/it]Running loglikelihood requests:  93%|█████████▎| 373/400 [08:16<00:32,  1.19s/it]Running loglikelihood requests:  94%|█████████▎| 374/400 [08:17<00:30,  1.19s/it]Running loglikelihood requests:  94%|█████████▍| 375/400 [08:18<00:29,  1.19s/it]Running loglikelihood requests:  94%|█████████▍| 376/400 [08:19<00:28,  1.18s/it]Running loglikelihood requests:  94%|█████████▍| 377/400 [08:20<00:27,  1.18s/it]Running loglikelihood requests:  94%|█████████▍| 378/400 [08:21<00:25,  1.18s/it]Running loglikelihood requests:  95%|█████████▍| 379/400 [08:23<00:24,  1.18s/it]Running loglikelihood requests:  95%|█████████▌| 380/400 [08:24<00:23,  1.17s/it]Running loglikelihood requests:  95%|█████████▌| 381/400 [08:25<00:22,  1.17s/it]Running loglikelihood requests:  96%|█████████▌| 382/400 [08:26<00:21,  1.17s/it]Running loglikelihood requests:  96%|█████████▌| 383/400 [08:27<00:19,  1.17s/it]Running loglikelihood requests:  96%|█████████▌| 384/400 [08:28<00:18,  1.16s/it]Running loglikelihood requests:  96%|█████████▋| 385/400 [08:30<00:17,  1.15s/it]Running loglikelihood requests:  96%|█████████▋| 386/400 [08:31<00:16,  1.15s/it]Running loglikelihood requests:  97%|█████████▋| 387/400 [08:32<00:14,  1.14s/it]Running loglikelihood requests:  97%|█████████▋| 388/400 [08:33<00:13,  1.14s/it]Running loglikelihood requests:  97%|█████████▋| 389/400 [08:34<00:12,  1.11s/it]Running loglikelihood requests:  98%|█████████▊| 390/400 [08:35<00:10,  1.09s/it]Running loglikelihood requests:  98%|█████████▊| 391/400 [08:36<00:09,  1.08s/it]Running loglikelihood requests:  98%|█████████▊| 392/400 [08:37<00:08,  1.07s/it]Running loglikelihood requests:  98%|█████████▊| 393/400 [08:38<00:07,  1.06s/it]Running loglikelihood requests:  98%|█████████▊| 394/400 [08:39<00:06,  1.05s/it]Running loglikelihood requests:  99%|█████████▉| 395/400 [08:40<00:05,  1.04s/it]Running loglikelihood requests:  99%|█████████▉| 396/400 [08:41<00:04,  1.04s/it]Running loglikelihood requests:  99%|█████████▉| 397/400 [08:42<00:03,  1.02s/it]Running loglikelihood requests: 100%|█████████▉| 398/400 [08:43<00:02,  1.00s/it]Running loglikelihood requests: 100%|█████████▉| 399/400 [08:44<00:00,  1.00it/s]Running loglikelihood requests: 100%|██████████| 400/400 [08:45<00:00,  1.01it/s]Running loglikelihood requests: 100%|██████████| 400/400 [08:45<00:00,  1.31s/it]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:7'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:7'}
full model:
{'mastermind_46_easy': {'alias': 'mastermind_46_easy', 'acc,none': 0.75, 'acc_stderr,none': 0.04351941398892446}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9900730655060652
0.9975621416797388
0.9971758007187725
0.9980230222224056
0.9889479987910904
0.9764864248764976
0.9903944102306983
0.9944743493554844
0.9959595842537624
0.9872350810596702
0.9630442697748985
0.9836939129473328
0.9680042833072414
0.9739989664157891
0.9945965755000291
0.9874567967365809
0.9910190996776087
0.9888895436517939
0.9820307305549418
0.9881803990776891
0.9859944271811626
0.9931460182904469
0.984259193892523
0.99673879588459
0.9800909214289261
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[5, 2, 0, 3, 7, 6, 1, 4]
tensor([5, 2, 0, 3, 7, 6, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 0, 3, 7, 6, 1, 4]
tensor([5, 2, 0, 3, 7, 6, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 0, 3, 7, 6, 1, 4]
tensor([5, 2, 0, 3, 7, 6, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 0, 3, 7, 6, 1, 4]
tensor([5, 2, 0, 3, 7, 6, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 0, 3, 7, 6, 1, 4]
tensor([5, 2, 0, 3, 7, 6, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 0, 3, 7, 6, 1, 4]
tensor([5, 2, 0, 3, 7, 6, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
done!
Normal merging for layer 3
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
done!
Normal merging for layer 4
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
done!
Normal merging for layer 5
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 6 to 31
done!
all done!
Model size: 12.0718 GB
107
cuda:7
wic
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:02<00:00, 28.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:02<00:00, 31.23s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wic] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wic] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/wic?recursive=False&expand=False HTTP/1.1" 307 142
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/wic?recursive=False&expand=False HTTP/1.1" 200 356
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140237734458576 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wic_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140237734458576 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wic_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140237734458576 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wic_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140237734458576 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wic_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140229879220128 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140229879220128 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140229879220128 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140229879220128 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wic from None to 0
INFO:lm_eval.api.task:Building contexts for wic on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1582.36it/s]
DEBUG:lm_eval.evaluator:Task: wic; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<04:26,  1.34s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<01:59,  1.64it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:02<01:31,  2.12it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:03<01:20,  2.41it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:04<01:13,  2.60it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:04<01:09,  2.72it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:05<01:06,  2.82it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:06<01:03,  2.90it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:06<01:01,  2.95it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:07<01:00,  2.99it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:08<00:59,  3.01it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:08<00:58,  3.04it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:09<00:57,  3.06it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:09<00:56,  3.07it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:10<00:55,  3.08it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:11<00:54,  3.10it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:11<00:53,  3.11it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:12<00:52,  3.13it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:13<00:51,  3.14it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:13<00:51,  3.15it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:14<00:50,  3.17it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:15<00:49,  3.17it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:15<00:48,  3.18it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:16<00:48,  3.18it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:16<00:47,  3.19it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:17<00:46,  3.19it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:18<00:45,  3.20it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:18<00:45,  3.21it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:19<00:44,  3.21it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:19<00:43,  3.21it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:20<00:43,  3.22it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:21<00:42,  3.22it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:21<00:41,  3.22it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:22<00:41,  3.22it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:23<00:40,  3.23it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:23<00:39,  3.23it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:24<00:39,  3.24it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:24<00:38,  3.25it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:25<00:37,  3.25it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:26<00:37,  3.25it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:26<00:36,  3.26it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:27<00:35,  3.27it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:27<00:35,  3.27it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:28<00:34,  3.28it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:29<00:33,  3.28it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:29<00:33,  3.28it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:30<00:32,  3.28it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:31<00:31,  3.29it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:31<00:31,  3.29it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:32<00:30,  3.28it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:32<00:30,  3.27it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:33<00:29,  3.27it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:34<00:28,  3.28it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:34<00:28,  3.28it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:35<00:27,  3.28it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:35<00:27,  3.29it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:36<00:26,  3.30it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:37<00:25,  3.30it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:37<00:25,  3.30it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:38<00:24,  3.30it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:38<00:23,  3.30it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:39<00:23,  3.30it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:40<00:22,  3.30it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:40<00:22,  3.29it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:41<00:21,  3.30it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:41<00:20,  3.30it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:42<00:20,  3.31it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:43<00:19,  3.31it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:43<00:19,  3.31it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:44<00:18,  3.32it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:44<00:17,  3.31it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:45<00:17,  3.31it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [00:46<00:16,  3.32it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [00:46<00:15,  3.32it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [00:47<00:15,  3.32it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [00:47<00:14,  3.32it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [00:48<00:14,  3.32it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [00:49<00:13,  3.34it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [00:49<00:12,  3.36it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [00:50<00:12,  3.37it/s]Running loglikelihood requests:  80%|████████  | 161/200 [00:50<00:11,  3.39it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [00:51<00:10,  3.40it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [00:52<00:10,  3.40it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [00:52<00:09,  3.41it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [00:53<00:09,  3.41it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [00:53<00:08,  3.43it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [00:54<00:07,  3.44it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [00:55<00:07,  3.45it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [00:55<00:06,  3.47it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [00:56<00:06,  3.48it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [00:56<00:05,  3.49it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [00:57<00:04,  3.49it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [00:57<00:04,  3.49it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [00:58<00:03,  3.50it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [00:58<00:03,  3.51it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [00:59<00:02,  3.52it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:00<00:01,  3.53it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:00<00:01,  3.54it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:01<00:00,  3.55it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:01<00:00,  3.57it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:01<00:00,  3.24it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:0'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}
full model:
{'wic': {'alias': 'wic', 'acc,none': 0.47, 'acc_stderr,none': 0.05016135580465919}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7015569150227223
0.5302361010977743
0.6131123609930033
0.8131827739550247
0.5456264918897312
0.5653128506125247
0.9024119585362896
0.8122852497904204
0.9072724946141106
0.866764102055741
0.8260299199157425
0.7472915500213457
0.8874866998217976
0.7441602305581367
0.22948143665096393
0.6763976434023368
0.5909756859477309
0.6775915070630182
0.8311737665735953
0.5882947608660276
0.7888779075700829
0.9530393862783458
0.7563942945196994
0.7021129984434293
0.9133573687405422
0.8864659884483975
0.43949477197814607
0.49530015739760547
0.9835705252160515
0.7015569150227223
0.5302361010977743
0.6131123609930033
0.8131827739550247
0.5456264918897312
0.5653128506125247
0.9024119585362896
0.8122852497904204
0.9072724946141106
0.866764102055741
0.8260299199157425
0.7472915500213457
0.8874866998217976
0.7441602305581367
Total groups 76 exceeded the threshold, stopping comparison.
The group tensor is
[6, 5, 2, 3, 7, 0, 4, 1]
tensor([6, 5, 2, 3, 7, 0, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 6, 1, 5, 4, 2, 3, 0]
tensor([7, 6, 1, 5, 4, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 6, 2, 4, 5, 0, 3, 1]
tensor([7, 6, 2, 4, 5, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 0, 0, 5, 1, 1, 3, 2]
tensor([4, 0, 0, 5, 1, 1, 3, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 0, 2, 5, 3, 4, 1]
tensor([0, 1, 0, 2, 5, 3, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 0, 1, 4, 1, 2, 3, 0]
tensor([5, 0, 1, 4, 1, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 1, 1, 3, 4, 2, 5, 0]
tensor([0, 1, 1, 3, 4, 2, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 1, 0, 2, 3, 1, 2, 3]
tensor([0, 1, 0, 2, 3, 1, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 2 to 3
done!
Normal merging for layer 4
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 5 to 7
done!
Normal merging for layer 8
tensor([1, 2])
tensor(1)
tensor([4, 5])
tensor(4)
tensor([7])
tensor(7)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([3])
tensor(3)
done!
Cross-layer merge completed for layers 9 to 10
done!
Normal merging for layer 11
tensor([0, 2])
tensor(0)
tensor([1, 7])
tensor(1)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 12 to 13
done!
Normal merging for layer 14
tensor([1, 7])
tensor(1)
tensor([2, 4])
tensor(2)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
done!
Normal merging for layer 15
tensor([0, 7])
tensor(0)
tensor([1, 2])
tensor(1)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
done!
Normal merging for layer 16
tensor([0, 2])
tensor(0)
tensor([1, 5])
tensor(1)
tensor([3, 6])
tensor(3)
tensor([4, 7])
tensor(4)
done!
Cross-layer merge completed for layers 17 to 31
done!
all done!
Model size: 12.5757 GB
106
cuda:0
boolq
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:41<00:41, 41.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 24.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.90s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:filelock:Attempting to acquire lock 140238671049360 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_boolq_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140238671049360 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_boolq_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140238671049360 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_boolq_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140238671049360 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_boolq_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140215190837376 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140215190837376 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215190837376 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140215190837376 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of boolq from None to 0
INFO:lm_eval.api.task:Building contexts for boolq on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2551.26it/s]
DEBUG:lm_eval.evaluator:Task: boolq; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:04<14:43,  4.44s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:06<05:56,  1.81s/it]Running loglikelihood requests:   2%|▎         | 5/200 [00:07<04:18,  1.33s/it]Running loglikelihood requests:   4%|▎         | 7/200 [00:09<03:36,  1.12s/it]Running loglikelihood requests:   4%|▍         | 9/200 [00:11<03:12,  1.01s/it]Running loglikelihood requests:   6%|▌         | 11/200 [00:12<02:54,  1.08it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:14<02:42,  1.15it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:15<02:33,  1.20it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:17<02:26,  1.25it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:18<02:19,  1.30it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:20<02:14,  1.33it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:21<02:10,  1.36it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:22<02:06,  1.39it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:24<02:02,  1.41it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:25<02:00,  1.42it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:27<01:57,  1.44it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:28<01:55,  1.45it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:29<01:52,  1.46it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:31<01:50,  1.48it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:32<01:48,  1.49it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:33<01:46,  1.50it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:34<01:44,  1.51it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:36<01:41,  1.52it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:37<01:39,  1.54it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:38<01:36,  1.56it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:39<01:34,  1.57it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:41<01:32,  1.59it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:42<01:30,  1.59it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:43<01:28,  1.61it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:44<01:26,  1.63it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:46<01:24,  1.64it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:47<01:22,  1.65it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:48<01:21,  1.66it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:49<01:19,  1.68it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:50<01:17,  1.69it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:51<01:15,  1.72it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:53<01:13,  1.74it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:54<01:10,  1.76it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:55<01:08,  1.78it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:56<01:07,  1.80it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:57<01:05,  1.81it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:58<01:04,  1.82it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:59<01:02,  1.83it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [01:00<01:01,  1.84it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [01:01<01:00,  1.85it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [01:02<00:58,  1.85it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [01:03<00:57,  1.86it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [01:04<00:56,  1.87it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [01:05<00:55,  1.87it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [01:07<00:53,  1.88it/s]Running loglikelihood requests:  50%|█████     | 101/200 [01:08<00:52,  1.89it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [01:09<00:51,  1.90it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [01:10<00:49,  1.91it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [01:11<00:48,  1.92it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [01:12<00:47,  1.93it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [01:13<00:45,  1.94it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [01:14<00:44,  1.95it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [01:15<00:43,  1.96it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [01:16<00:42,  1.97it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [01:17<00:40,  1.98it/s]Running loglikelihood requests:  60%|██████    | 121/200 [01:18<00:39,  2.00it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [01:19<00:37,  2.03it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [01:20<00:36,  2.06it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [01:21<00:35,  2.07it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [01:21<00:34,  2.09it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [01:22<00:32,  2.12it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [01:23<00:31,  2.15it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [01:24<00:29,  2.17it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [01:25<00:28,  2.18it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [01:26<00:27,  2.19it/s]Running loglikelihood requests:  70%|███████   | 141/200 [01:27<00:26,  2.21it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [01:28<00:25,  2.25it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:29<00:24,  2.28it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [01:29<00:22,  2.31it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:30<00:21,  2.33it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:31<00:20,  2.35it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:32<00:19,  2.36it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:33<00:18,  2.39it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:34<00:17,  2.41it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:34<00:16,  2.44it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:35<00:15,  2.46it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:36<00:14,  2.48it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:37<00:13,  2.50it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:37<00:13,  2.53it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:38<00:12,  2.55it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:39<00:11,  2.59it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:40<00:10,  2.63it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:40<00:09,  2.67it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:41<00:08,  2.70it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:42<00:07,  2.72it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:43<00:06,  2.75it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:43<00:06,  2.77it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:44<00:05,  2.81it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:45<00:04,  2.86it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:45<00:03,  2.90it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:46<00:03,  2.93it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:47<00:02,  2.95it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:47<00:01,  3.01it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:48<00:00,  3.13it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:48<00:00,  3.30it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:48<00:00,  1.84it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:1'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:1'}
full model:
{'boolq': {'alias': 'boolq', 'acc,none': 0.67, 'acc_stderr,none': 0.04725815626252609}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9782993007407264
0.4824830207482711
0.6583529368570694
0.8822475410601947
0.3059812833421515
0.7645942683746021
0.5411564675926485
0.6399758236302138
0.752913626333875
0.9170504110771489
0.8755679311709376
0.9130694495301263
0.6399088029710222
0.5907791688883935
0.8704540476128766
0.484807489124531
0.7579322019225017
0.8465026175931075
0.8104840653949666
0.671147278193032
0.7709951349967222
0.532915988335396
0.6066099270395096
0.5511989097245372
0.4671998655475952
0.6078287002452507
0.3992240879306912
0.5299030614769079
0.5709371890677749
0.9782993007407264
0.4824830207482711
0.6583529368570694
0.8822475410601947
0.3059812833421515
0.7645942683746021
0.5411564675926485
0.6399758236302138
0.752913626333875
0.9170504110771489
0.8755679311709376
0.9130694495301263
0.6399088029710222
0.5907791688883935
0.8704540476128766
0.484807489124531
0.7579322019225017
0.8465026175931075
0.8104840653949666
0.671147278193032
0.7709951349967222
0.532915988335396
0.6066099270395096
0.5511989097245372
0.4671998655475952
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[6, 2, 7, 0, 5, 3, 4, 1]
tensor([6, 2, 7, 0, 5, 3, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 2, 3, 0, 6, 1, 4, 5]
tensor([7, 2, 3, 0, 6, 1, 4, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 1, 7, 0, 6, 4, 2, 3]
tensor([5, 1, 7, 0, 6, 4, 2, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 3, 5, 1, 7, 2, 6, 0]
tensor([4, 3, 5, 1, 7, 2, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 6, 7, 3, 4, 5, 0, 2]
tensor([1, 6, 7, 3, 4, 5, 0, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 4, 1, 7, 2, 5, 0]
tensor([6, 3, 4, 1, 7, 2, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 2
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
done!
Normal merging for layer 4
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
done!
Normal merging for layer 5
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 6 to 31
done!
all done!
Model size: 12.0718 GB
137
cuda:1
wic
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:41<00:41, 41.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 24.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.75s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wic] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wic] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140229628720768 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wic_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140229628720768 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wic_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140229628720768 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wic_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140229628720768 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wic_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140229617069920 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140229617069920 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140229617069920 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140229617069920 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wic from None to 0
INFO:lm_eval.api.task:Building contexts for wic on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1584.13it/s]
DEBUG:lm_eval.evaluator:Task: wic; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<03:53,  1.17s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:01<01:49,  1.79it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:02<01:27,  2.24it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:03<01:17,  2.49it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:03<01:11,  2.66it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:04<01:08,  2.78it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:05<01:05,  2.87it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:05<01:03,  2.93it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:06<01:01,  2.97it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:07<01:00,  3.01it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:07<00:59,  3.03it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:08<00:58,  3.04it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:09<00:57,  3.05it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:09<00:56,  3.06it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:10<00:55,  3.07it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:11<00:54,  3.09it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:11<00:53,  3.11it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:12<00:52,  3.14it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:12<00:51,  3.15it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:13<00:50,  3.17it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:14<00:50,  3.18it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:14<00:49,  3.19it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:15<00:48,  3.20it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:16<00:47,  3.20it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:16<00:47,  3.21it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:17<00:46,  3.21it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:17<00:45,  3.22it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:18<00:44,  3.23it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:19<00:44,  3.23it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:19<00:43,  3.23it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:20<00:42,  3.23it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:20<00:42,  3.24it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:21<00:41,  3.24it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:22<00:41,  3.24it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:22<00:40,  3.25it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:23<00:39,  3.25it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:24<00:38,  3.26it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:24<00:38,  3.27it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:25<00:37,  3.28it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:25<00:36,  3.28it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:26<00:36,  3.29it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:27<00:35,  3.30it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:27<00:34,  3.31it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:28<00:34,  3.31it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:28<00:33,  3.31it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:29<00:33,  3.24it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:30<00:33,  3.19it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:30<00:33,  3.12it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:31<00:33,  3.11it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:32<00:32,  3.10it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:32<00:32,  3.09it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:33<00:31,  3.09it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:34<00:30,  3.09it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:34<00:30,  3.09it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:35<00:29,  3.09it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:36<00:28,  3.10it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:36<00:28,  3.10it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:37<00:27,  3.10it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:37<00:26,  3.10it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:38<00:26,  3.07it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:39<00:25,  3.14it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:39<00:24,  3.19it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:40<00:23,  3.23it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:41<00:22,  3.26it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:41<00:21,  3.28it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:42<00:20,  3.30it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:42<00:20,  3.32it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:43<00:19,  3.33it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:44<00:18,  3.34it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:44<00:18,  3.35it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:45<00:17,  3.36it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:45<00:16,  3.37it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [00:46<00:16,  3.38it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [00:46<00:15,  3.39it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [00:47<00:15,  3.39it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [00:48<00:14,  3.40it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [00:48<00:13,  3.40it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [00:49<00:13,  3.40it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [00:49<00:12,  3.41it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [00:50<00:11,  3.42it/s]Running loglikelihood requests:  80%|████████  | 161/200 [00:51<00:11,  3.42it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [00:51<00:10,  3.43it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [00:52<00:10,  3.43it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [00:52<00:09,  3.44it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [00:53<00:08,  3.45it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [00:53<00:08,  3.46it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [00:54<00:07,  3.47it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [00:55<00:07,  3.47it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [00:55<00:06,  3.49it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [00:56<00:06,  3.50it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [00:56<00:05,  3.51it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [00:57<00:04,  3.51it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [00:57<00:04,  3.51it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [00:58<00:03,  3.52it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [00:59<00:03,  3.53it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [00:59<00:02,  3.54it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:00<00:01,  3.55it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:00<00:01,  3.56it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:01<00:00,  3.57it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:01<00:00,  3.58it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:01<00:00,  3.23it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:2'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:2'}
full model:
{'wic': {'alias': 'wic', 'acc,none': 0.47, 'acc_stderr,none': 0.05016135580465919}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7015569150227223
0.5302361010977743
0.6131123609930033
0.8131827739550247
0.5456264918897312
0.5653128506125247
0.9024119585362896
0.8122852497904204
0.9072724946141106
0.866764102055741
0.8260299199157425
0.7472915500213457
0.8874866998217976
0.7441602305581367
0.22948143665096393
0.6763976434023368
0.5909756859477309
0.6775915070630182
0.8311737665735953
0.5882947608660276
0.7888779075700829
0.9530393862783458
0.7563942945196994
0.7021129984434293
0.9133573687405422
0.8864659884483975
0.43949477197814607
0.49530015739760547
0.9835705252160515
0.7015569150227223
0.5302361010977743
0.6131123609930033
0.8131827739550247
0.5456264918897312
0.5653128506125247
0.9024119585362896
0.8122852497904204
0.9072724946141106
0.866764102055741
0.8260299199157425
0.7472915500213457
0.8874866998217976
0.7441602305581367
Total groups 76 exceeded the threshold, stopping comparison.
The group tensor is
[6, 5, 2, 3, 7, 0, 4, 1]
tensor([6, 5, 2, 3, 7, 0, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 6, 1, 5, 4, 2, 3, 0]
tensor([7, 6, 1, 5, 4, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 6, 2, 4, 5, 0, 3, 1]
tensor([7, 6, 2, 4, 5, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 0, 0, 5, 1, 1, 3, 2]
tensor([4, 0, 0, 5, 1, 1, 3, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 0, 2, 5, 3, 4, 1]
tensor([0, 1, 0, 2, 5, 3, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 0, 1, 4, 1, 2, 3, 0]
tensor([5, 0, 1, 4, 1, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 1, 1, 3, 4, 2, 5, 0]
tensor([0, 1, 1, 3, 4, 2, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 1, 0, 2, 3, 1, 2, 3]
tensor([0, 1, 0, 2, 3, 1, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 2 to 3
done!
Normal merging for layer 4
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 5 to 7
done!
Normal merging for layer 8
tensor([1, 2])
tensor(1)
tensor([4, 5])
tensor(4)
tensor([7])
tensor(7)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([3])
tensor(3)
done!
Cross-layer merge completed for layers 9 to 10
done!
Normal merging for layer 11
tensor([0, 2])
tensor(0)
tensor([1, 7])
tensor(1)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 12 to 13
done!
Normal merging for layer 14
tensor([1, 7])
tensor(1)
tensor([2, 4])
tensor(2)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
done!
Normal merging for layer 15
tensor([0, 7])
tensor(0)
tensor([1, 2])
tensor(1)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
done!
Normal merging for layer 16
tensor([0, 2])
tensor(0)
tensor([1, 5])
tensor(1)
tensor([3, 6])
tensor(3)
tensor([4, 7])
tensor(4)
done!
Cross-layer merge completed for layers 17 to 31
done!
all done!
Model size: 12.5757 GB
169
cuda:2
mastermind_24_easy
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:40<00:40, 40.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 24.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.63s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_24_mcq_random HTTP/1.1" 200 779
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/flair/mastermind_24_mcq_random/flair/mastermind_24_mcq_random.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_24_mcq_random HTTP/1.1" 200 779
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/flair/mastermind_24_mcq_random/resolve/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/flair/mastermind_24_mcq_random/resolve/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:filelock:Attempting to acquire lock 140212903039616 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_24_mcq_random_default_0.0.0_cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5.lock
DEBUG:filelock:Lock 140212903039616 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_24_mcq_random_default_0.0.0_cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5/dataset_info.json
DEBUG:filelock:Attempting to release lock 140212903039616 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_24_mcq_random_default_0.0.0_cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5.lock
DEBUG:filelock:Lock 140212903039616 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_24_mcq_random_default_0.0.0_cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5.lock
DEBUG:filelock:Attempting to acquire lock 140229628711408 on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5_builder.lock
DEBUG:filelock:Lock 140229628711408 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5/dataset_info.json
DEBUG:filelock:Attempting to release lock 140229628711408 on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5_builder.lock
DEBUG:filelock:Lock 140229628711408 released on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of mastermind_24_easy from None to 0
INFO:lm_eval.api.task:Building contexts for mastermind_24_easy on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1511.04it/s]
DEBUG:lm_eval.evaluator:Task: mastermind_24_easy; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:01<10:06,  1.52s/it]Running loglikelihood requests:   0%|          | 2/400 [00:02<08:18,  1.25s/it]Running loglikelihood requests:   1%|          | 3/400 [00:03<07:42,  1.17s/it]Running loglikelihood requests:   1%|          | 4/400 [00:04<07:25,  1.13s/it]Running loglikelihood requests:   1%|▏         | 5/400 [00:05<07:14,  1.10s/it]Running loglikelihood requests:   2%|▏         | 6/400 [00:06<07:06,  1.08s/it]Running loglikelihood requests:   2%|▏         | 7/400 [00:07<07:01,  1.07s/it]Running loglikelihood requests:   2%|▏         | 8/400 [00:08<06:57,  1.06s/it]Running loglikelihood requests:   2%|▏         | 9/400 [00:09<06:54,  1.06s/it]Running loglikelihood requests:   2%|▎         | 10/400 [00:11<06:52,  1.06s/it]Running loglikelihood requests:   3%|▎         | 12/400 [00:12<05:12,  1.24it/s]Running loglikelihood requests:   3%|▎         | 13/400 [00:13<05:35,  1.16it/s]Running loglikelihood requests:   4%|▎         | 14/400 [00:14<05:52,  1.10it/s]Running loglikelihood requests:   4%|▍         | 16/400 [00:15<04:45,  1.34it/s]Running loglikelihood requests:   4%|▍         | 17/400 [00:16<05:11,  1.23it/s]Running loglikelihood requests:   4%|▍         | 18/400 [00:17<05:32,  1.15it/s]Running loglikelihood requests:   5%|▍         | 19/400 [00:18<05:49,  1.09it/s]Running loglikelihood requests:   5%|▌         | 20/400 [00:19<06:01,  1.05it/s]Running loglikelihood requests:   5%|▌         | 21/400 [00:20<06:11,  1.02it/s]Running loglikelihood requests:   6%|▌         | 22/400 [00:21<06:15,  1.01it/s]Running loglikelihood requests:   6%|▌         | 23/400 [00:22<06:18,  1.00s/it]Running loglikelihood requests:   6%|▋         | 25/400 [00:23<04:52,  1.28it/s]Running loglikelihood requests:   6%|▋         | 26/400 [00:24<05:14,  1.19it/s]Running loglikelihood requests:   7%|▋         | 27/400 [00:25<05:31,  1.12it/s]Running loglikelihood requests:   7%|▋         | 29/400 [00:26<04:30,  1.37it/s]Running loglikelihood requests:   8%|▊         | 30/400 [00:27<04:55,  1.25it/s]Running loglikelihood requests:   8%|▊         | 31/400 [00:28<05:15,  1.17it/s]Running loglikelihood requests:   8%|▊         | 32/400 [00:29<05:30,  1.11it/s]Running loglikelihood requests:   8%|▊         | 34/400 [00:30<04:28,  1.37it/s]Running loglikelihood requests:   9%|▉         | 35/400 [00:31<04:52,  1.25it/s]Running loglikelihood requests:  10%|▉         | 38/400 [00:32<03:26,  1.75it/s]Running loglikelihood requests:  10%|▉         | 39/400 [00:33<03:56,  1.52it/s]Running loglikelihood requests:  10%|█         | 40/400 [00:34<04:24,  1.36it/s]Running loglikelihood requests:  10%|█         | 41/400 [00:35<04:47,  1.25it/s]Running loglikelihood requests:  10%|█         | 42/400 [00:36<05:06,  1.17it/s]Running loglikelihood requests:  11%|█         | 43/400 [00:37<05:20,  1.11it/s]Running loglikelihood requests:  11%|█         | 44/400 [00:38<05:31,  1.07it/s]Running loglikelihood requests:  11%|█▏        | 45/400 [00:39<05:38,  1.05it/s]Running loglikelihood requests:  12%|█▏        | 47/400 [00:40<04:26,  1.33it/s]Running loglikelihood requests:  12%|█▏        | 48/400 [00:41<04:47,  1.22it/s]Running loglikelihood requests:  12%|█▏        | 49/400 [00:42<05:04,  1.15it/s]Running loglikelihood requests:  12%|█▎        | 50/400 [00:43<05:18,  1.10it/s]Running loglikelihood requests:  13%|█▎        | 52/400 [00:44<04:16,  1.36it/s]Running loglikelihood requests:  13%|█▎        | 53/400 [00:45<04:38,  1.25it/s]Running loglikelihood requests:  14%|█▎        | 54/400 [00:46<04:55,  1.17it/s]Running loglikelihood requests:  14%|█▍        | 56/400 [00:47<04:03,  1.41it/s]Running loglikelihood requests:  14%|█▍        | 57/400 [00:48<04:27,  1.28it/s]Running loglikelihood requests:  14%|█▍        | 58/400 [00:49<04:46,  1.20it/s]Running loglikelihood requests:  15%|█▍        | 59/400 [00:50<05:00,  1.13it/s]Running loglikelihood requests:  15%|█▌        | 60/400 [00:51<05:12,  1.09it/s]Running loglikelihood requests:  15%|█▌        | 61/400 [00:53<05:19,  1.06it/s]Running loglikelihood requests:  16%|█▌        | 62/400 [00:54<05:25,  1.04it/s]Running loglikelihood requests:  16%|█▌        | 63/400 [00:55<05:28,  1.02it/s]Running loglikelihood requests:  16%|█▌        | 64/400 [00:56<05:30,  1.02it/s]Running loglikelihood requests:  16%|█▋        | 65/400 [00:57<05:31,  1.01it/s]Running loglikelihood requests:  16%|█▋        | 66/400 [00:58<05:31,  1.01it/s]Running loglikelihood requests:  17%|█▋        | 67/400 [00:59<05:31,  1.00it/s]Running loglikelihood requests:  17%|█▋        | 68/400 [01:00<05:31,  1.00it/s]Running loglikelihood requests:  17%|█▋        | 69/400 [01:01<05:35,  1.01s/it]Running loglikelihood requests:  18%|█▊        | 70/400 [01:02<05:43,  1.04s/it]Running loglikelihood requests:  18%|█▊        | 71/400 [01:03<05:49,  1.06s/it]Running loglikelihood requests:  18%|█▊        | 72/400 [01:04<05:52,  1.07s/it]Running loglikelihood requests:  18%|█▊        | 73/400 [01:05<05:53,  1.08s/it]Running loglikelihood requests:  18%|█▊        | 74/400 [01:06<05:53,  1.09s/it]Running loglikelihood requests:  19%|█▉        | 76/400 [01:07<04:32,  1.19it/s]Running loglikelihood requests:  19%|█▉        | 77/400 [01:08<04:53,  1.10it/s]Running loglikelihood requests:  20%|█▉        | 78/400 [01:09<05:09,  1.04it/s]Running loglikelihood requests:  20%|█▉        | 79/400 [01:11<05:21,  1.00s/it]Running loglikelihood requests:  20%|██        | 80/400 [01:12<05:29,  1.03s/it]Running loglikelihood requests:  20%|██        | 81/400 [01:13<05:33,  1.05s/it]Running loglikelihood requests:  20%|██        | 82/400 [01:14<05:36,  1.06s/it]Running loglikelihood requests:  21%|██        | 83/400 [01:15<05:38,  1.07s/it]Running loglikelihood requests:  21%|██        | 84/400 [01:16<05:39,  1.07s/it]Running loglikelihood requests:  21%|██▏       | 85/400 [01:17<05:39,  1.08s/it]Running loglikelihood requests:  22%|██▏       | 87/400 [01:18<04:20,  1.20it/s]Running loglikelihood requests:  22%|██▏       | 88/400 [01:19<04:39,  1.12it/s]Running loglikelihood requests:  22%|██▏       | 89/400 [01:20<04:52,  1.06it/s]Running loglikelihood requests:  22%|██▎       | 90/400 [01:21<05:03,  1.02it/s]Running loglikelihood requests:  23%|██▎       | 91/400 [01:22<05:11,  1.01s/it]Running loglikelihood requests:  23%|██▎       | 92/400 [01:24<05:16,  1.03s/it]Running loglikelihood requests:  24%|██▎       | 94/400 [01:25<04:07,  1.24it/s]Running loglikelihood requests:  24%|██▍       | 95/400 [01:26<04:26,  1.14it/s]Running loglikelihood requests:  24%|██▍       | 96/400 [01:27<04:41,  1.08it/s]Running loglikelihood requests:  24%|██▍       | 98/400 [01:28<03:50,  1.31it/s]Running loglikelihood requests:  25%|██▌       | 100/400 [01:29<03:20,  1.49it/s]Running loglikelihood requests:  26%|██▌       | 102/400 [01:30<03:01,  1.64it/s]Running loglikelihood requests:  26%|██▌       | 103/400 [01:31<03:24,  1.45it/s]Running loglikelihood requests:  26%|██▌       | 104/400 [01:32<03:44,  1.32it/s]Running loglikelihood requests:  27%|██▋       | 107/400 [01:33<02:41,  1.82it/s]Running loglikelihood requests:  27%|██▋       | 108/400 [01:34<03:05,  1.57it/s]Running loglikelihood requests:  27%|██▋       | 109/400 [01:35<03:27,  1.40it/s]Running loglikelihood requests:  28%|██▊       | 110/400 [01:36<03:45,  1.28it/s]Running loglikelihood requests:  28%|██▊       | 111/400 [01:37<04:00,  1.20it/s]Running loglikelihood requests:  28%|██▊       | 112/400 [01:38<04:12,  1.14it/s]Running loglikelihood requests:  28%|██▊       | 113/400 [01:39<04:21,  1.10it/s]Running loglikelihood requests:  28%|██▊       | 114/400 [01:40<04:26,  1.07it/s]Running loglikelihood requests:  29%|██▉       | 116/400 [01:41<03:29,  1.35it/s]Running loglikelihood requests:  29%|██▉       | 117/400 [01:42<03:46,  1.25it/s]Running loglikelihood requests:  30%|██▉       | 118/400 [01:43<03:59,  1.18it/s]Running loglikelihood requests:  30%|███       | 121/400 [01:44<02:40,  1.73it/s]Running loglikelihood requests:  30%|███       | 122/400 [01:45<03:03,  1.52it/s]Running loglikelihood requests:  31%|███       | 123/400 [01:46<03:22,  1.36it/s]Running loglikelihood requests:  31%|███       | 124/400 [01:47<03:39,  1.26it/s]Running loglikelihood requests:  31%|███▏      | 125/400 [01:48<03:50,  1.19it/s]Running loglikelihood requests:  32%|███▏      | 126/400 [01:49<03:59,  1.15it/s]Running loglikelihood requests:  32%|███▏      | 127/400 [01:50<04:05,  1.11it/s]Running loglikelihood requests:  32%|███▏      | 128/400 [01:51<04:09,  1.09it/s]Running loglikelihood requests:  32%|███▎      | 130/400 [01:52<03:14,  1.39it/s]Running loglikelihood requests:  33%|███▎      | 132/400 [01:53<02:48,  1.59it/s]Running loglikelihood requests:  33%|███▎      | 133/400 [01:54<03:06,  1.43it/s]Running loglikelihood requests:  34%|███▎      | 134/400 [01:55<03:22,  1.31it/s]Running loglikelihood requests:  34%|███▍      | 135/400 [01:55<03:34,  1.23it/s]Running loglikelihood requests:  34%|███▍      | 136/400 [01:56<03:44,  1.18it/s]Running loglikelihood requests:  34%|███▍      | 137/400 [01:57<03:51,  1.14it/s]Running loglikelihood requests:  34%|███▍      | 138/400 [01:58<03:56,  1.11it/s]Running loglikelihood requests:  35%|███▍      | 139/400 [01:59<03:59,  1.09it/s]Running loglikelihood requests:  35%|███▌      | 140/400 [02:00<04:01,  1.08it/s]Running loglikelihood requests:  35%|███▌      | 141/400 [02:01<04:02,  1.07it/s]Running loglikelihood requests:  36%|███▌      | 142/400 [02:02<04:02,  1.06it/s]Running loglikelihood requests:  36%|███▌      | 143/400 [02:03<04:03,  1.06it/s]Running loglikelihood requests:  36%|███▋      | 145/400 [02:04<03:07,  1.36it/s]Running loglikelihood requests:  36%|███▋      | 146/400 [02:05<03:20,  1.27it/s]Running loglikelihood requests:  37%|███▋      | 147/400 [02:06<03:31,  1.20it/s]Running loglikelihood requests:  37%|███▋      | 148/400 [02:07<03:38,  1.15it/s]Running loglikelihood requests:  37%|███▋      | 149/400 [02:08<03:43,  1.12it/s]Running loglikelihood requests:  38%|███▊      | 150/400 [02:09<03:47,  1.10it/s]Running loglikelihood requests:  38%|███▊      | 152/400 [02:10<02:57,  1.39it/s]Running loglikelihood requests:  38%|███▊      | 153/400 [02:11<03:11,  1.29it/s]Running loglikelihood requests:  38%|███▊      | 154/400 [02:12<03:22,  1.21it/s]Running loglikelihood requests:  39%|███▉      | 155/400 [02:13<03:30,  1.16it/s]Running loglikelihood requests:  39%|███▉      | 157/400 [02:14<02:48,  1.44it/s]Running loglikelihood requests:  40%|███▉      | 158/400 [02:15<03:03,  1.32it/s]Running loglikelihood requests:  40%|████      | 160/400 [02:16<02:34,  1.56it/s]Running loglikelihood requests:  40%|████      | 161/400 [02:17<02:50,  1.40it/s]Running loglikelihood requests:  40%|████      | 162/400 [02:18<03:03,  1.30it/s]Running loglikelihood requests:  41%|████      | 163/400 [02:18<03:13,  1.22it/s]Running loglikelihood requests:  41%|████▏     | 165/400 [02:19<02:38,  1.49it/s]Running loglikelihood requests:  42%|████▏     | 166/400 [02:20<02:52,  1.35it/s]Running loglikelihood requests:  42%|████▏     | 167/400 [02:21<03:04,  1.26it/s]Running loglikelihood requests:  42%|████▏     | 168/400 [02:22<03:13,  1.20it/s]Running loglikelihood requests:  42%|████▎     | 170/400 [02:23<02:37,  1.46it/s]Running loglikelihood requests:  43%|████▎     | 171/400 [02:24<02:50,  1.34it/s]Running loglikelihood requests:  43%|████▎     | 172/400 [02:25<03:01,  1.25it/s]Running loglikelihood requests:  43%|████▎     | 173/400 [02:26<03:10,  1.19it/s]Running loglikelihood requests:  44%|████▎     | 174/400 [02:27<03:17,  1.15it/s]Running loglikelihood requests:  44%|████▍     | 175/400 [02:28<03:22,  1.11it/s]Running loglikelihood requests:  44%|████▍     | 176/400 [02:29<03:25,  1.09it/s]Running loglikelihood requests:  44%|████▍     | 178/400 [02:30<02:40,  1.38it/s]Running loglikelihood requests:  45%|████▍     | 179/400 [02:31<02:52,  1.28it/s]Running loglikelihood requests:  45%|████▌     | 180/400 [02:32<03:01,  1.21it/s]Running loglikelihood requests:  45%|████▌     | 181/400 [02:33<03:08,  1.16it/s]Running loglikelihood requests:  46%|████▌     | 183/400 [02:34<02:30,  1.44it/s]Running loglikelihood requests:  46%|████▌     | 184/400 [02:35<02:43,  1.32it/s]Running loglikelihood requests:  47%|████▋     | 187/400 [02:36<01:53,  1.87it/s]Running loglikelihood requests:  47%|████▋     | 188/400 [02:37<02:10,  1.63it/s]Running loglikelihood requests:  47%|████▋     | 189/400 [02:38<02:24,  1.46it/s]Running loglikelihood requests:  48%|████▊     | 190/400 [02:39<02:36,  1.34it/s]Running loglikelihood requests:  48%|████▊     | 191/400 [02:39<02:46,  1.26it/s]Running loglikelihood requests:  48%|████▊     | 192/400 [02:40<02:53,  1.20it/s]Running loglikelihood requests:  48%|████▊     | 193/400 [02:41<02:58,  1.16it/s]Running loglikelihood requests:  48%|████▊     | 194/400 [02:42<03:02,  1.13it/s]Running loglikelihood requests:  49%|████▉     | 196/400 [02:43<02:23,  1.43it/s]Running loglikelihood requests:  49%|████▉     | 197/400 [02:44<02:34,  1.32it/s]Running loglikelihood requests:  50%|████▉     | 198/400 [02:45<02:43,  1.24it/s]Running loglikelihood requests:  50%|████▉     | 199/400 [02:46<02:49,  1.19it/s]Running loglikelihood requests:  50%|█████     | 200/400 [02:47<02:54,  1.15it/s]Running loglikelihood requests:  50%|█████     | 202/400 [02:48<02:17,  1.44it/s]Running loglikelihood requests:  51%|█████     | 203/400 [02:49<02:28,  1.33it/s]Running loglikelihood requests:  51%|█████     | 204/400 [02:50<02:37,  1.25it/s]Running loglikelihood requests:  51%|█████▏    | 205/400 [02:51<02:43,  1.19it/s]Running loglikelihood requests:  52%|█████▏    | 206/400 [02:52<02:48,  1.15it/s]Running loglikelihood requests:  52%|█████▏    | 207/400 [02:53<02:51,  1.13it/s]Running loglikelihood requests:  52%|█████▎    | 210/400 [02:54<01:48,  1.74it/s]Running loglikelihood requests:  53%|█████▎    | 212/400 [02:55<01:41,  1.86it/s]Running loglikelihood requests:  53%|█████▎    | 213/400 [02:55<01:55,  1.62it/s]Running loglikelihood requests:  54%|█████▍    | 215/400 [02:56<01:44,  1.77it/s]Running loglikelihood requests:  54%|█████▍    | 216/400 [02:57<01:58,  1.56it/s]Running loglikelihood requests:  54%|█████▍    | 217/400 [02:58<02:09,  1.41it/s]Running loglikelihood requests:  55%|█████▍    | 219/400 [02:59<01:51,  1.62it/s]Running loglikelihood requests:  55%|█████▌    | 220/400 [03:00<02:03,  1.45it/s]Running loglikelihood requests:  55%|█████▌    | 221/400 [03:01<02:13,  1.34it/s]Running loglikelihood requests:  56%|█████▌    | 222/400 [03:02<02:21,  1.25it/s]Running loglikelihood requests:  56%|█████▌    | 223/400 [03:03<02:27,  1.20it/s]Running loglikelihood requests:  56%|█████▌    | 224/400 [03:04<02:32,  1.16it/s]Running loglikelihood requests:  56%|█████▋    | 225/400 [03:05<02:34,  1.13it/s]Running loglikelihood requests:  56%|█████▋    | 226/400 [03:06<02:36,  1.11it/s]Running loglikelihood requests:  57%|█████▋    | 227/400 [03:07<02:37,  1.10it/s]Running loglikelihood requests:  57%|█████▋    | 228/400 [03:08<02:38,  1.09it/s]Running loglikelihood requests:  57%|█████▋    | 229/400 [03:09<02:38,  1.08it/s]Running loglikelihood requests:  57%|█████▊    | 230/400 [03:10<02:38,  1.08it/s]Running loglikelihood requests:  58%|█████▊    | 231/400 [03:11<02:37,  1.07it/s]Running loglikelihood requests:  58%|█████▊    | 232/400 [03:11<02:37,  1.07it/s]Running loglikelihood requests:  58%|█████▊    | 233/400 [03:12<02:36,  1.07it/s]Running loglikelihood requests:  59%|█████▉    | 235/400 [03:13<01:58,  1.39it/s]Running loglikelihood requests:  59%|█████▉    | 236/400 [03:14<02:07,  1.29it/s]Running loglikelihood requests:  59%|█████▉    | 237/400 [03:15<02:13,  1.22it/s]Running loglikelihood requests:  60%|█████▉    | 238/400 [03:16<02:17,  1.18it/s]Running loglikelihood requests:  60%|█████▉    | 239/400 [03:17<02:20,  1.14it/s]Running loglikelihood requests:  60%|██████    | 240/400 [03:18<02:23,  1.12it/s]Running loglikelihood requests:  60%|██████    | 242/400 [03:19<01:51,  1.42it/s]Running loglikelihood requests:  61%|██████    | 243/400 [03:20<01:59,  1.31it/s]Running loglikelihood requests:  61%|██████    | 244/400 [03:21<02:06,  1.23it/s]Running loglikelihood requests:  61%|██████▏   | 245/400 [03:22<02:11,  1.18it/s]Running loglikelihood requests:  62%|██████▏   | 246/400 [03:23<02:14,  1.14it/s]Running loglikelihood requests:  62%|██████▏   | 247/400 [03:24<02:16,  1.12it/s]Running loglikelihood requests:  62%|██████▏   | 249/400 [03:25<01:46,  1.42it/s]Running loglikelihood requests:  62%|██████▎   | 250/400 [03:26<01:53,  1.32it/s]Running loglikelihood requests:  63%|██████▎   | 251/400 [03:27<02:00,  1.24it/s]Running loglikelihood requests:  63%|██████▎   | 252/400 [03:27<02:04,  1.19it/s]Running loglikelihood requests:  63%|██████▎   | 253/400 [03:28<02:07,  1.15it/s]Running loglikelihood requests:  64%|██████▎   | 254/400 [03:29<02:09,  1.12it/s]Running loglikelihood requests:  64%|██████▍   | 255/400 [03:30<02:10,  1.11it/s]Running loglikelihood requests:  64%|██████▍   | 256/400 [03:31<02:11,  1.09it/s]Running loglikelihood requests:  64%|██████▍   | 257/400 [03:32<02:11,  1.09it/s]Running loglikelihood requests:  64%|██████▍   | 258/400 [03:33<02:11,  1.08it/s]Running loglikelihood requests:  65%|██████▌   | 260/400 [03:34<01:40,  1.40it/s]Running loglikelihood requests:  65%|██████▌   | 261/400 [03:35<01:47,  1.30it/s]Running loglikelihood requests:  66%|██████▌   | 264/400 [03:36<01:12,  1.87it/s]Running loglikelihood requests:  66%|██████▋   | 265/400 [03:37<01:23,  1.63it/s]Running loglikelihood requests:  66%|██████▋   | 266/400 [03:38<01:32,  1.45it/s]Running loglikelihood requests:  67%|██████▋   | 267/400 [03:39<01:39,  1.34it/s]Running loglikelihood requests:  67%|██████▋   | 268/400 [03:40<01:44,  1.26it/s]Running loglikelihood requests:  67%|██████▋   | 269/400 [03:41<01:50,  1.19it/s]Running loglikelihood requests:  68%|██████▊   | 270/400 [03:42<01:55,  1.13it/s]Running loglikelihood requests:  68%|██████▊   | 271/400 [03:43<01:58,  1.09it/s]Running loglikelihood requests:  68%|██████▊   | 272/400 [03:44<02:01,  1.06it/s]Running loglikelihood requests:  68%|██████▊   | 273/400 [03:45<02:03,  1.03it/s]Running loglikelihood requests:  68%|██████▊   | 274/400 [03:46<02:04,  1.01it/s]Running loglikelihood requests:  69%|██████▉   | 277/400 [03:47<01:17,  1.60it/s]Running loglikelihood requests:  70%|██████▉   | 279/400 [03:48<01:10,  1.71it/s]Running loglikelihood requests:  70%|███████   | 281/400 [03:49<01:05,  1.81it/s]Running loglikelihood requests:  71%|███████   | 284/400 [03:50<00:52,  2.22it/s]Running loglikelihood requests:  71%|███████▏  | 285/400 [03:51<01:00,  1.90it/s]Running loglikelihood requests:  72%|███████▏  | 286/400 [03:51<01:08,  1.66it/s]Running loglikelihood requests:  72%|███████▏  | 287/400 [03:52<01:15,  1.50it/s]Running loglikelihood requests:  72%|███████▏  | 288/400 [03:53<01:21,  1.38it/s]Running loglikelihood requests:  72%|███████▏  | 289/400 [03:54<01:25,  1.30it/s]Running loglikelihood requests:  72%|███████▎  | 290/400 [03:55<01:28,  1.24it/s]Running loglikelihood requests:  73%|███████▎  | 291/400 [03:56<01:31,  1.20it/s]Running loglikelihood requests:  73%|███████▎  | 293/400 [03:57<01:11,  1.49it/s]Running loglikelihood requests:  74%|███████▎  | 294/400 [03:58<01:16,  1.38it/s]Running loglikelihood requests:  74%|███████▍  | 295/400 [03:59<01:21,  1.29it/s]Running loglikelihood requests:  74%|███████▍  | 296/400 [04:00<01:24,  1.24it/s]Running loglikelihood requests:  74%|███████▍  | 297/400 [04:01<01:26,  1.20it/s]Running loglikelihood requests:  75%|███████▍  | 299/400 [04:01<01:07,  1.50it/s]Running loglikelihood requests:  75%|███████▌  | 300/400 [04:02<01:12,  1.38it/s]Running loglikelihood requests:  75%|███████▌  | 301/400 [04:03<01:18,  1.26it/s]Running loglikelihood requests:  76%|███████▌  | 302/400 [04:04<01:22,  1.19it/s]Running loglikelihood requests:  76%|███████▌  | 304/400 [04:05<01:04,  1.48it/s]Running loglikelihood requests:  76%|███████▋  | 305/400 [04:06<01:09,  1.37it/s]Running loglikelihood requests:  77%|███████▋  | 307/400 [04:07<00:57,  1.61it/s]Running loglikelihood requests:  77%|███████▋  | 309/400 [04:08<00:51,  1.78it/s]Running loglikelihood requests:  78%|███████▊  | 310/400 [04:09<00:57,  1.58it/s]Running loglikelihood requests:  78%|███████▊  | 311/400 [04:10<01:02,  1.43it/s]Running loglikelihood requests:  78%|███████▊  | 312/400 [04:11<01:06,  1.33it/s]Running loglikelihood requests:  78%|███████▊  | 313/400 [04:12<01:09,  1.26it/s]Running loglikelihood requests:  79%|███████▉  | 315/400 [04:13<00:55,  1.53it/s]Running loglikelihood requests:  79%|███████▉  | 317/400 [04:14<00:48,  1.73it/s]Running loglikelihood requests:  80%|███████▉  | 318/400 [04:14<00:53,  1.54it/s]Running loglikelihood requests:  80%|███████▉  | 319/400 [04:15<00:57,  1.40it/s]Running loglikelihood requests:  80%|████████  | 321/400 [04:16<00:48,  1.64it/s]Running loglikelihood requests:  81%|████████  | 323/400 [04:17<00:42,  1.80it/s]Running loglikelihood requests:  82%|████████▏ | 326/400 [04:18<00:32,  2.24it/s]Running loglikelihood requests:  82%|████████▏ | 327/400 [04:19<00:38,  1.90it/s]Running loglikelihood requests:  82%|████████▏ | 328/400 [04:20<00:43,  1.66it/s]Running loglikelihood requests:  82%|████████▏ | 329/400 [04:21<00:47,  1.49it/s]Running loglikelihood requests:  82%|████████▎ | 330/400 [04:22<00:50,  1.37it/s]Running loglikelihood requests:  83%|████████▎ | 332/400 [04:23<00:41,  1.62it/s]Running loglikelihood requests:  83%|████████▎ | 333/400 [04:24<00:45,  1.46it/s]Running loglikelihood requests:  84%|████████▎ | 334/400 [04:24<00:48,  1.35it/s]Running loglikelihood requests:  84%|████████▍ | 335/400 [04:25<00:50,  1.28it/s]Running loglikelihood requests:  84%|████████▍ | 336/400 [04:26<00:52,  1.22it/s]Running loglikelihood requests:  84%|████████▍ | 337/400 [04:27<00:53,  1.19it/s]Running loglikelihood requests:  84%|████████▍ | 338/400 [04:28<00:53,  1.16it/s]Running loglikelihood requests:  85%|████████▍ | 339/400 [04:29<00:53,  1.14it/s]Running loglikelihood requests:  85%|████████▌ | 340/400 [04:30<00:53,  1.13it/s]Running loglikelihood requests:  85%|████████▌ | 341/400 [04:31<00:51,  1.15it/s]Running loglikelihood requests:  86%|████████▌ | 342/400 [04:32<00:49,  1.17it/s]Running loglikelihood requests:  86%|████████▌ | 343/400 [04:32<00:48,  1.18it/s]Running loglikelihood requests:  86%|████████▋ | 346/400 [04:33<00:28,  1.89it/s]Running loglikelihood requests:  87%|████████▋ | 348/400 [04:34<00:25,  2.04it/s]Running loglikelihood requests:  87%|████████▋ | 349/400 [04:35<00:28,  1.80it/s]Running loglikelihood requests:  88%|████████▊ | 350/400 [04:36<00:30,  1.62it/s]Running loglikelihood requests:  88%|████████▊ | 351/400 [04:37<00:32,  1.50it/s]Running loglikelihood requests:  88%|████████▊ | 352/400 [04:37<00:34,  1.41it/s]Running loglikelihood requests:  88%|████████▊ | 353/400 [04:38<00:34,  1.34it/s]Running loglikelihood requests:  88%|████████▊ | 354/400 [04:39<00:35,  1.30it/s]Running loglikelihood requests:  89%|████████▉ | 355/400 [04:40<00:35,  1.27it/s]Running loglikelihood requests:  89%|████████▉ | 356/400 [04:41<00:35,  1.25it/s]Running loglikelihood requests:  90%|████████▉ | 358/400 [04:42<00:26,  1.60it/s]Running loglikelihood requests:  90%|████████▉ | 359/400 [04:42<00:27,  1.48it/s]Running loglikelihood requests:  90%|█████████ | 360/400 [04:43<00:28,  1.40it/s]Running loglikelihood requests:  90%|█████████ | 361/400 [04:44<00:29,  1.34it/s]Running loglikelihood requests:  90%|█████████ | 362/400 [04:45<00:29,  1.30it/s]Running loglikelihood requests:  91%|█████████ | 364/400 [04:46<00:22,  1.63it/s]Running loglikelihood requests:  91%|█████████▏| 365/400 [04:47<00:23,  1.51it/s]Running loglikelihood requests:  92%|█████████▏| 367/400 [04:47<00:18,  1.78it/s]Running loglikelihood requests:  92%|█████████▏| 368/400 [04:48<00:19,  1.61it/s]Running loglikelihood requests:  92%|█████████▏| 369/400 [04:49<00:20,  1.49it/s]Running loglikelihood requests:  92%|█████████▎| 370/400 [04:50<00:21,  1.41it/s]Running loglikelihood requests:  93%|█████████▎| 371/400 [04:51<00:21,  1.35it/s]Running loglikelihood requests:  93%|█████████▎| 373/400 [04:52<00:16,  1.67it/s]Running loglikelihood requests:  94%|█████████▍| 375/400 [04:52<00:13,  1.89it/s]Running loglikelihood requests:  94%|█████████▍| 376/400 [04:53<00:14,  1.69it/s]Running loglikelihood requests:  95%|█████████▍| 379/400 [04:54<00:09,  2.27it/s]Running loglikelihood requests:  95%|█████████▌| 381/400 [04:55<00:08,  2.32it/s]Running loglikelihood requests:  96%|█████████▌| 382/400 [04:56<00:09,  1.99it/s]Running loglikelihood requests:  96%|█████████▌| 384/400 [04:56<00:07,  2.12it/s]Running loglikelihood requests:  96%|█████████▋| 386/400 [04:57<00:06,  2.21it/s]Running loglikelihood requests:  97%|█████████▋| 387/400 [04:58<00:06,  1.91it/s]Running loglikelihood requests:  97%|█████████▋| 389/400 [04:59<00:05,  2.06it/s]Running loglikelihood requests:  98%|█████████▊| 391/400 [05:00<00:04,  2.17it/s]Running loglikelihood requests:  98%|█████████▊| 393/400 [05:01<00:03,  2.25it/s]Running loglikelihood requests:  99%|█████████▉| 395/400 [05:01<00:02,  2.31it/s]Running loglikelihood requests:  99%|█████████▉| 397/400 [05:02<00:01,  2.28it/s]Running loglikelihood requests: 100%|█████████▉| 398/400 [05:03<00:01,  1.97it/s]Running loglikelihood requests: 100%|██████████| 400/400 [05:04<00:00,  2.11it/s]Running loglikelihood requests: 100%|██████████| 400/400 [05:04<00:00,  1.31it/s]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:3'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:3'}
full model:
{'mastermind_24_easy': {'alias': 'mastermind_24_easy', 'acc,none': 0.33, 'acc_stderr,none': 0.04725815626252609}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9705388716727775
0.9880227828623074
0.982119607496995
0.9941469535767887
0.984917378949356
0.9883609317461776
0.9752442385272352
0.9858149677985593
0.9966001900863091
0.9954965780195978
0.9982374916142641
0.987839947084121
0.9740425263242771
0.9802324544963317
0.9965978314947238
0.9890806289155061
0.971808392501808
0.9767123038440666
0.9820534501654181
0.9672299205809463
0.9753579231968917
0.9973837437819523
0.9951994747100236
0.9465102818948206
0.9757933184251313
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[7, 0, 1, 3, 6, 5, 4, 2]
tensor([7, 0, 1, 3, 6, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 0, 1, 3, 7, 5, 4, 2]
tensor([6, 0, 1, 3, 7, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 0, 1, 3, 6, 5, 4, 2]
tensor([7, 0, 1, 3, 6, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 0, 1, 3, 7, 5, 4, 2]
tensor([6, 0, 1, 3, 7, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 0, 1, 3, 6, 5, 4, 2]
tensor([7, 0, 1, 3, 6, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 0, 2, 3, 6, 5, 4, 1]
tensor([7, 0, 2, 3, 6, 5, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 3
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Normal merging for layer 4
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 5
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 6 to 31
done!
all done!
Model size: 12.0718 GB
66
cuda:3
mastermind_35_easy
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:41<00:41, 41.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 24.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 27.48s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_35_mcq_random HTTP/1.1" 200 772
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/flair/mastermind_35_mcq_random/flair/mastermind_35_mcq_random.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_35_mcq_random HTTP/1.1" 200 780
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/flair/mastermind_35_mcq_random/resolve/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_35_mcq_random/revision/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 780
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_35_mcq_random/tree/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1?recursive=False&expand=False HTTP/1.1" 200 290
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_35_mcq_random/tree/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1/data?recursive=False&expand=False HTTP/1.1" 200 359
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_35_mcq_random/revision/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 780
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/flair/mastermind_35_mcq_random/resolve/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:filelock:Attempting to acquire lock 140215736639728 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_35_mcq_random_default_0.0.0_15dd5105771e9c8d2d3ea71c8d44fffda374a7a1.lock
DEBUG:filelock:Lock 140215736639728 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_35_mcq_random_default_0.0.0_15dd5105771e9c8d2d3ea71c8d44fffda374a7a1.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_35_mcq_random/default/0.0.0/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215736639728 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_35_mcq_random_default_0.0.0_15dd5105771e9c8d2d3ea71c8d44fffda374a7a1.lock
DEBUG:filelock:Lock 140215736639728 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_35_mcq_random_default_0.0.0_15dd5105771e9c8d2d3ea71c8d44fffda374a7a1.lock
DEBUG:filelock:Attempting to acquire lock 140215736647600 on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_35_mcq_random/default/0.0.0/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1_builder.lock
DEBUG:filelock:Lock 140215736647600 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_35_mcq_random/default/0.0.0/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_35_mcq_random/default/0.0.0/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215736647600 on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_35_mcq_random/default/0.0.0/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1_builder.lock
DEBUG:filelock:Lock 140215736647600 released on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_35_mcq_random/default/0.0.0/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of mastermind_35_easy from None to 0
INFO:lm_eval.api.task:Building contexts for mastermind_35_easy on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1524.20it/s]
DEBUG:lm_eval.evaluator:Task: mastermind_35_easy; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:01<11:48,  1.78s/it]Running loglikelihood requests:   0%|          | 2/400 [00:03<09:39,  1.46s/it]Running loglikelihood requests:   1%|          | 3/400 [00:04<08:54,  1.35s/it]Running loglikelihood requests:   1%|          | 4/400 [00:05<08:32,  1.29s/it]Running loglikelihood requests:   1%|▏         | 5/400 [00:06<08:18,  1.26s/it]Running loglikelihood requests:   2%|▏         | 6/400 [00:07<08:09,  1.24s/it]Running loglikelihood requests:   2%|▏         | 7/400 [00:09<08:02,  1.23s/it]Running loglikelihood requests:   2%|▏         | 8/400 [00:10<07:58,  1.22s/it]Running loglikelihood requests:   2%|▏         | 9/400 [00:11<07:56,  1.22s/it]Running loglikelihood requests:   2%|▎         | 10/400 [00:12<07:54,  1.22s/it]Running loglikelihood requests:   3%|▎         | 11/400 [00:13<07:51,  1.21s/it]Running loglikelihood requests:   3%|▎         | 12/400 [00:15<07:48,  1.21s/it]Running loglikelihood requests:   3%|▎         | 13/400 [00:16<07:46,  1.21s/it]Running loglikelihood requests:   4%|▎         | 14/400 [00:17<07:44,  1.20s/it]Running loglikelihood requests:   4%|▍         | 15/400 [00:18<07:43,  1.20s/it]Running loglikelihood requests:   4%|▍         | 16/400 [00:19<07:42,  1.20s/it]Running loglikelihood requests:   4%|▍         | 17/400 [00:21<07:40,  1.20s/it]Running loglikelihood requests:   4%|▍         | 18/400 [00:22<07:38,  1.20s/it]Running loglikelihood requests:   5%|▍         | 19/400 [00:23<07:35,  1.20s/it]Running loglikelihood requests:   5%|▌         | 20/400 [00:24<07:34,  1.20s/it]Running loglikelihood requests:   5%|▌         | 21/400 [00:25<07:32,  1.19s/it]Running loglikelihood requests:   6%|▌         | 23/400 [00:27<05:46,  1.09it/s]Running loglikelihood requests:   6%|▌         | 24/400 [00:28<06:14,  1.00it/s]Running loglikelihood requests:   6%|▋         | 25/400 [00:29<06:32,  1.05s/it]Running loglikelihood requests:   6%|▋         | 26/400 [00:30<06:43,  1.08s/it]Running loglikelihood requests:   7%|▋         | 27/400 [00:31<07:01,  1.13s/it]Running loglikelihood requests:   7%|▋         | 28/400 [00:33<07:17,  1.18s/it]Running loglikelihood requests:   7%|▋         | 29/400 [00:34<07:29,  1.21s/it]Running loglikelihood requests:   8%|▊         | 30/400 [00:35<07:32,  1.22s/it]Running loglikelihood requests:   8%|▊         | 31/400 [00:36<07:26,  1.21s/it]Running loglikelihood requests:   8%|▊         | 32/400 [00:38<07:22,  1.20s/it]Running loglikelihood requests:   8%|▊         | 33/400 [00:39<07:20,  1.20s/it]Running loglikelihood requests:   8%|▊         | 34/400 [00:40<07:19,  1.20s/it]Running loglikelihood requests:   9%|▉         | 35/400 [00:41<07:18,  1.20s/it]Running loglikelihood requests:   9%|▉         | 36/400 [00:42<07:16,  1.20s/it]Running loglikelihood requests:   9%|▉         | 37/400 [00:44<07:13,  1.19s/it]Running loglikelihood requests:  10%|▉         | 38/400 [00:45<07:10,  1.19s/it]Running loglikelihood requests:  10%|▉         | 39/400 [00:46<07:07,  1.18s/it]Running loglikelihood requests:  10%|█         | 40/400 [00:47<07:06,  1.19s/it]Running loglikelihood requests:  10%|█         | 41/400 [00:48<07:04,  1.18s/it]Running loglikelihood requests:  10%|█         | 42/400 [00:49<07:02,  1.18s/it]Running loglikelihood requests:  11%|█         | 43/400 [00:51<07:00,  1.18s/it]Running loglikelihood requests:  11%|█         | 44/400 [00:52<06:58,  1.18s/it]Running loglikelihood requests:  11%|█▏        | 45/400 [00:53<06:57,  1.18s/it]Running loglikelihood requests:  12%|█▏        | 46/400 [00:54<07:06,  1.20s/it]Running loglikelihood requests:  12%|█▏        | 47/400 [00:56<07:12,  1.23s/it]Running loglikelihood requests:  12%|█▏        | 48/400 [00:57<07:15,  1.24s/it]Running loglikelihood requests:  12%|█▏        | 49/400 [00:58<07:18,  1.25s/it]Running loglikelihood requests:  12%|█▎        | 50/400 [00:59<07:09,  1.23s/it]Running loglikelihood requests:  13%|█▎        | 51/400 [01:00<07:01,  1.21s/it]Running loglikelihood requests:  13%|█▎        | 52/400 [01:02<06:56,  1.20s/it]Running loglikelihood requests:  13%|█▎        | 53/400 [01:03<06:51,  1.19s/it]Running loglikelihood requests:  14%|█▎        | 54/400 [01:04<06:48,  1.18s/it]Running loglikelihood requests:  14%|█▍        | 55/400 [01:05<06:45,  1.17s/it]Running loglikelihood requests:  14%|█▍        | 56/400 [01:06<06:42,  1.17s/it]Running loglikelihood requests:  14%|█▍        | 57/400 [01:07<06:40,  1.17s/it]Running loglikelihood requests:  14%|█▍        | 58/400 [01:09<06:39,  1.17s/it]Running loglikelihood requests:  15%|█▍        | 59/400 [01:10<06:38,  1.17s/it]Running loglikelihood requests:  15%|█▌        | 60/400 [01:11<06:37,  1.17s/it]Running loglikelihood requests:  15%|█▌        | 61/400 [01:12<06:36,  1.17s/it]Running loglikelihood requests:  16%|█▌        | 62/400 [01:13<06:35,  1.17s/it]Running loglikelihood requests:  16%|█▌        | 63/400 [01:14<06:34,  1.17s/it]Running loglikelihood requests:  16%|█▌        | 64/400 [01:16<06:32,  1.17s/it]Running loglikelihood requests:  16%|█▋        | 65/400 [01:17<06:31,  1.17s/it]Running loglikelihood requests:  16%|█▋        | 66/400 [01:18<06:30,  1.17s/it]Running loglikelihood requests:  17%|█▋        | 67/400 [01:19<06:29,  1.17s/it]Running loglikelihood requests:  17%|█▋        | 68/400 [01:20<06:27,  1.17s/it]Running loglikelihood requests:  17%|█▋        | 69/400 [01:21<06:26,  1.17s/it]Running loglikelihood requests:  18%|█▊        | 70/400 [01:23<06:24,  1.17s/it]Running loglikelihood requests:  18%|█▊        | 71/400 [01:24<06:23,  1.17s/it]Running loglikelihood requests:  18%|█▊        | 72/400 [01:25<06:22,  1.17s/it]Running loglikelihood requests:  18%|█▊        | 73/400 [01:26<06:20,  1.16s/it]Running loglikelihood requests:  18%|█▊        | 74/400 [01:27<06:19,  1.16s/it]Running loglikelihood requests:  19%|█▉        | 75/400 [01:28<06:18,  1.16s/it]Running loglikelihood requests:  19%|█▉        | 76/400 [01:30<06:16,  1.16s/it]Running loglikelihood requests:  19%|█▉        | 77/400 [01:31<06:15,  1.16s/it]Running loglikelihood requests:  20%|█▉        | 78/400 [01:32<06:14,  1.16s/it]Running loglikelihood requests:  20%|█▉        | 79/400 [01:33<06:12,  1.16s/it]Running loglikelihood requests:  20%|██        | 80/400 [01:34<06:12,  1.16s/it]Running loglikelihood requests:  20%|██        | 81/400 [01:35<06:10,  1.16s/it]Running loglikelihood requests:  20%|██        | 82/400 [01:37<06:09,  1.16s/it]Running loglikelihood requests:  21%|██        | 83/400 [01:38<06:08,  1.16s/it]Running loglikelihood requests:  21%|██        | 84/400 [01:39<06:08,  1.17s/it]Running loglikelihood requests:  21%|██▏       | 85/400 [01:40<06:07,  1.17s/it]Running loglikelihood requests:  22%|██▏       | 86/400 [01:41<06:06,  1.17s/it]Running loglikelihood requests:  22%|██▏       | 87/400 [01:42<06:05,  1.17s/it]Running loglikelihood requests:  22%|██▏       | 88/400 [01:44<06:03,  1.17s/it]Running loglikelihood requests:  22%|██▏       | 89/400 [01:45<06:02,  1.17s/it]Running loglikelihood requests:  22%|██▎       | 90/400 [01:46<06:01,  1.17s/it]Running loglikelihood requests:  23%|██▎       | 91/400 [01:47<06:00,  1.17s/it]Running loglikelihood requests:  23%|██▎       | 92/400 [01:48<05:59,  1.17s/it]Running loglikelihood requests:  23%|██▎       | 93/400 [01:49<05:57,  1.17s/it]Running loglikelihood requests:  24%|██▎       | 94/400 [01:51<05:56,  1.17s/it]Running loglikelihood requests:  24%|██▍       | 95/400 [01:52<05:55,  1.16s/it]Running loglikelihood requests:  24%|██▍       | 96/400 [01:53<05:54,  1.16s/it]Running loglikelihood requests:  24%|██▍       | 97/400 [01:54<05:53,  1.17s/it]Running loglikelihood requests:  24%|██▍       | 98/400 [01:55<05:52,  1.17s/it]Running loglikelihood requests:  25%|██▍       | 99/400 [01:56<05:51,  1.17s/it]Running loglikelihood requests:  25%|██▌       | 100/400 [01:58<05:50,  1.17s/it]Running loglikelihood requests:  25%|██▌       | 101/400 [01:59<05:49,  1.17s/it]Running loglikelihood requests:  26%|██▌       | 102/400 [02:00<05:47,  1.17s/it]Running loglikelihood requests:  26%|██▌       | 103/400 [02:01<05:46,  1.17s/it]Running loglikelihood requests:  26%|██▌       | 104/400 [02:02<05:44,  1.16s/it]Running loglikelihood requests:  26%|██▋       | 105/400 [02:03<05:43,  1.16s/it]Running loglikelihood requests:  26%|██▋       | 106/400 [02:05<05:41,  1.16s/it]Running loglikelihood requests:  27%|██▋       | 107/400 [02:06<05:40,  1.16s/it]Running loglikelihood requests:  27%|██▋       | 108/400 [02:07<05:40,  1.17s/it]Running loglikelihood requests:  27%|██▋       | 109/400 [02:08<05:41,  1.17s/it]Running loglikelihood requests:  28%|██▊       | 110/400 [02:09<05:41,  1.18s/it]Running loglikelihood requests:  28%|██▊       | 112/400 [02:10<04:19,  1.11it/s]Running loglikelihood requests:  28%|██▊       | 113/400 [02:12<04:37,  1.04it/s]Running loglikelihood requests:  28%|██▊       | 114/400 [02:13<04:50,  1.02s/it]Running loglikelihood requests:  29%|██▉       | 115/400 [02:14<05:00,  1.06s/it]Running loglikelihood requests:  29%|██▉       | 116/400 [02:15<05:07,  1.08s/it]Running loglikelihood requests:  29%|██▉       | 117/400 [02:16<05:12,  1.10s/it]Running loglikelihood requests:  30%|██▉       | 118/400 [02:17<05:15,  1.12s/it]Running loglikelihood requests:  30%|██▉       | 119/400 [02:18<05:17,  1.13s/it]Running loglikelihood requests:  30%|███       | 120/400 [02:20<05:19,  1.14s/it]Running loglikelihood requests:  30%|███       | 121/400 [02:21<05:19,  1.15s/it]Running loglikelihood requests:  30%|███       | 122/400 [02:22<05:19,  1.15s/it]Running loglikelihood requests:  31%|███       | 123/400 [02:23<05:19,  1.15s/it]Running loglikelihood requests:  31%|███       | 124/400 [02:24<05:18,  1.15s/it]Running loglikelihood requests:  31%|███▏      | 125/400 [02:25<05:17,  1.15s/it]Running loglikelihood requests:  32%|███▏      | 126/400 [02:27<05:16,  1.16s/it]Running loglikelihood requests:  32%|███▏      | 127/400 [02:28<05:14,  1.15s/it]Running loglikelihood requests:  32%|███▏      | 128/400 [02:29<05:13,  1.15s/it]Running loglikelihood requests:  32%|███▏      | 129/400 [02:30<05:12,  1.15s/it]Running loglikelihood requests:  32%|███▎      | 130/400 [02:31<05:11,  1.15s/it]Running loglikelihood requests:  33%|███▎      | 131/400 [02:32<05:10,  1.15s/it]Running loglikelihood requests:  33%|███▎      | 132/400 [02:34<05:09,  1.15s/it]Running loglikelihood requests:  33%|███▎      | 133/400 [02:35<05:08,  1.15s/it]Running loglikelihood requests:  34%|███▎      | 134/400 [02:36<05:07,  1.16s/it]Running loglikelihood requests:  34%|███▍      | 135/400 [02:37<05:05,  1.15s/it]Running loglikelihood requests:  34%|███▍      | 136/400 [02:38<05:04,  1.15s/it]Running loglikelihood requests:  34%|███▍      | 137/400 [02:39<05:02,  1.15s/it]Running loglikelihood requests:  34%|███▍      | 138/400 [02:40<05:01,  1.15s/it]Running loglikelihood requests:  35%|███▍      | 139/400 [02:42<05:00,  1.15s/it]Running loglikelihood requests:  35%|███▌      | 140/400 [02:43<04:59,  1.15s/it]Running loglikelihood requests:  35%|███▌      | 141/400 [02:44<04:58,  1.15s/it]Running loglikelihood requests:  36%|███▌      | 142/400 [02:45<04:57,  1.15s/it]Running loglikelihood requests:  36%|███▌      | 143/400 [02:46<04:55,  1.15s/it]Running loglikelihood requests:  36%|███▌      | 144/400 [02:47<04:54,  1.15s/it]Running loglikelihood requests:  36%|███▋      | 145/400 [02:48<04:52,  1.15s/it]Running loglikelihood requests:  36%|███▋      | 146/400 [02:50<04:52,  1.15s/it]Running loglikelihood requests:  37%|███▋      | 147/400 [02:51<04:50,  1.15s/it]Running loglikelihood requests:  37%|███▋      | 148/400 [02:52<04:48,  1.15s/it]Running loglikelihood requests:  37%|███▋      | 149/400 [02:53<04:46,  1.14s/it]Running loglikelihood requests:  38%|███▊      | 150/400 [02:54<04:45,  1.14s/it]Running loglikelihood requests:  38%|███▊      | 151/400 [02:55<04:43,  1.14s/it]Running loglikelihood requests:  38%|███▊      | 152/400 [02:56<04:42,  1.14s/it]Running loglikelihood requests:  38%|███▊      | 153/400 [02:58<04:41,  1.14s/it]Running loglikelihood requests:  38%|███▊      | 154/400 [02:59<04:40,  1.14s/it]Running loglikelihood requests:  39%|███▉      | 155/400 [03:00<04:39,  1.14s/it]Running loglikelihood requests:  39%|███▉      | 156/400 [03:01<04:38,  1.14s/it]Running loglikelihood requests:  39%|███▉      | 157/400 [03:02<04:36,  1.14s/it]Running loglikelihood requests:  40%|███▉      | 158/400 [03:03<04:35,  1.14s/it]Running loglikelihood requests:  40%|███▉      | 159/400 [03:04<04:34,  1.14s/it]Running loglikelihood requests:  40%|████      | 160/400 [03:06<04:33,  1.14s/it]Running loglikelihood requests:  40%|████      | 161/400 [03:07<04:31,  1.14s/it]Running loglikelihood requests:  40%|████      | 162/400 [03:08<04:30,  1.14s/it]Running loglikelihood requests:  41%|████      | 163/400 [03:09<04:29,  1.14s/it]Running loglikelihood requests:  41%|████      | 164/400 [03:10<04:28,  1.14s/it]Running loglikelihood requests:  41%|████▏     | 165/400 [03:11<04:27,  1.14s/it]Running loglikelihood requests:  42%|████▏     | 166/400 [03:12<04:26,  1.14s/it]Running loglikelihood requests:  42%|████▏     | 167/400 [03:14<04:24,  1.13s/it]Running loglikelihood requests:  42%|████▏     | 168/400 [03:15<04:23,  1.13s/it]Running loglikelihood requests:  42%|████▏     | 169/400 [03:16<04:21,  1.13s/it]Running loglikelihood requests:  42%|████▎     | 170/400 [03:17<04:20,  1.13s/it]Running loglikelihood requests:  43%|████▎     | 171/400 [03:18<04:19,  1.13s/it]Running loglikelihood requests:  43%|████▎     | 172/400 [03:19<04:18,  1.13s/it]Running loglikelihood requests:  43%|████▎     | 173/400 [03:20<04:16,  1.13s/it]Running loglikelihood requests:  44%|████▎     | 174/400 [03:21<04:14,  1.13s/it]Running loglikelihood requests:  44%|████▍     | 175/400 [03:23<04:13,  1.12s/it]Running loglikelihood requests:  44%|████▍     | 177/400 [03:24<03:12,  1.16it/s]Running loglikelihood requests:  44%|████▍     | 178/400 [03:25<03:25,  1.08it/s]Running loglikelihood requests:  45%|████▍     | 179/400 [03:26<03:36,  1.02it/s]Running loglikelihood requests:  45%|████▌     | 180/400 [03:27<03:48,  1.04s/it]Running loglikelihood requests:  45%|████▌     | 181/400 [03:28<03:53,  1.07s/it]Running loglikelihood requests:  46%|████▌     | 182/400 [03:29<03:57,  1.09s/it]Running loglikelihood requests:  46%|████▌     | 183/400 [03:31<03:58,  1.10s/it]Running loglikelihood requests:  46%|████▌     | 184/400 [03:32<03:58,  1.10s/it]Running loglikelihood requests:  46%|████▋     | 185/400 [03:33<03:57,  1.11s/it]Running loglikelihood requests:  46%|████▋     | 186/400 [03:34<03:57,  1.11s/it]Running loglikelihood requests:  47%|████▋     | 187/400 [03:35<03:56,  1.11s/it]Running loglikelihood requests:  47%|████▋     | 188/400 [03:36<03:55,  1.11s/it]Running loglikelihood requests:  47%|████▋     | 189/400 [03:37<03:54,  1.11s/it]Running loglikelihood requests:  48%|████▊     | 190/400 [03:38<03:53,  1.11s/it]Running loglikelihood requests:  48%|████▊     | 191/400 [03:39<03:52,  1.11s/it]Running loglikelihood requests:  48%|████▊     | 192/400 [03:41<03:51,  1.11s/it]Running loglikelihood requests:  48%|████▊     | 193/400 [03:42<03:50,  1.11s/it]Running loglikelihood requests:  48%|████▊     | 194/400 [03:43<03:50,  1.12s/it]Running loglikelihood requests:  49%|████▉     | 195/400 [03:44<03:49,  1.12s/it]Running loglikelihood requests:  49%|████▉     | 196/400 [03:45<03:48,  1.12s/it]Running loglikelihood requests:  49%|████▉     | 197/400 [03:46<03:46,  1.12s/it]Running loglikelihood requests:  50%|████▉     | 198/400 [03:47<03:45,  1.12s/it]Running loglikelihood requests:  50%|████▉     | 199/400 [03:48<03:48,  1.14s/it]Running loglikelihood requests:  50%|█████     | 200/400 [03:50<03:46,  1.13s/it]Running loglikelihood requests:  50%|█████     | 201/400 [03:51<03:44,  1.13s/it]Running loglikelihood requests:  50%|█████     | 202/400 [03:52<03:42,  1.12s/it]Running loglikelihood requests:  51%|█████     | 203/400 [03:53<03:40,  1.12s/it]Running loglikelihood requests:  51%|█████▏    | 205/400 [03:54<02:48,  1.16it/s]Running loglikelihood requests:  52%|█████▏    | 206/400 [03:55<02:59,  1.08it/s]Running loglikelihood requests:  52%|█████▏    | 207/400 [03:56<03:07,  1.03it/s]Running loglikelihood requests:  52%|█████▏    | 208/400 [03:57<03:14,  1.01s/it]Running loglikelihood requests:  52%|█████▏    | 209/400 [03:58<03:18,  1.04s/it]Running loglikelihood requests:  52%|█████▎    | 210/400 [04:00<03:21,  1.06s/it]Running loglikelihood requests:  53%|█████▎    | 211/400 [04:01<03:23,  1.08s/it]Running loglikelihood requests:  53%|█████▎    | 212/400 [04:02<03:24,  1.09s/it]Running loglikelihood requests:  53%|█████▎    | 213/400 [04:03<03:24,  1.10s/it]Running loglikelihood requests:  54%|█████▎    | 214/400 [04:04<03:24,  1.10s/it]Running loglikelihood requests:  54%|█████▍    | 215/400 [04:05<03:24,  1.10s/it]Running loglikelihood requests:  54%|█████▍    | 216/400 [04:06<03:23,  1.10s/it]Running loglikelihood requests:  54%|█████▍    | 217/400 [04:07<03:22,  1.11s/it]Running loglikelihood requests:  55%|█████▍    | 218/400 [04:08<03:21,  1.11s/it]Running loglikelihood requests:  55%|█████▍    | 219/400 [04:10<03:20,  1.11s/it]Running loglikelihood requests:  55%|█████▌    | 220/400 [04:11<03:19,  1.11s/it]Running loglikelihood requests:  55%|█████▌    | 221/400 [04:12<03:18,  1.11s/it]Running loglikelihood requests:  56%|█████▌    | 222/400 [04:13<03:17,  1.11s/it]Running loglikelihood requests:  56%|█████▌    | 223/400 [04:14<03:16,  1.11s/it]Running loglikelihood requests:  56%|█████▌    | 224/400 [04:15<03:15,  1.11s/it]Running loglikelihood requests:  56%|█████▋    | 225/400 [04:16<03:14,  1.11s/it]Running loglikelihood requests:  56%|█████▋    | 226/400 [04:17<03:13,  1.11s/it]Running loglikelihood requests:  57%|█████▋    | 227/400 [04:18<03:12,  1.11s/it]Running loglikelihood requests:  57%|█████▋    | 228/400 [04:20<03:11,  1.11s/it]Running loglikelihood requests:  57%|█████▋    | 229/400 [04:21<03:10,  1.12s/it]Running loglikelihood requests:  57%|█████▊    | 230/400 [04:22<03:09,  1.12s/it]Running loglikelihood requests:  58%|█████▊    | 231/400 [04:23<03:08,  1.11s/it]Running loglikelihood requests:  58%|█████▊    | 232/400 [04:24<03:07,  1.11s/it]Running loglikelihood requests:  58%|█████▊    | 233/400 [04:25<03:05,  1.11s/it]Running loglikelihood requests:  58%|█████▊    | 234/400 [04:26<03:04,  1.11s/it]Running loglikelihood requests:  59%|█████▉    | 235/400 [04:27<03:03,  1.11s/it]Running loglikelihood requests:  59%|█████▉    | 236/400 [04:29<03:02,  1.11s/it]Running loglikelihood requests:  59%|█████▉    | 237/400 [04:30<03:01,  1.11s/it]Running loglikelihood requests:  60%|█████▉    | 238/400 [04:31<03:00,  1.11s/it]Running loglikelihood requests:  60%|█████▉    | 239/400 [04:32<02:59,  1.11s/it]Running loglikelihood requests:  60%|██████    | 241/400 [04:33<02:16,  1.17it/s]Running loglikelihood requests:  60%|██████    | 242/400 [04:34<02:25,  1.09it/s]Running loglikelihood requests:  61%|██████    | 243/400 [04:35<02:32,  1.03it/s]Running loglikelihood requests:  61%|██████    | 244/400 [04:36<02:37,  1.01s/it]Running loglikelihood requests:  61%|██████▏   | 245/400 [04:37<02:40,  1.04s/it]Running loglikelihood requests:  62%|██████▏   | 246/400 [04:39<02:42,  1.06s/it]Running loglikelihood requests:  62%|██████▏   | 247/400 [04:40<02:44,  1.07s/it]Running loglikelihood requests:  62%|██████▏   | 248/400 [04:41<02:44,  1.08s/it]Running loglikelihood requests:  62%|██████▏   | 249/400 [04:42<02:44,  1.09s/it]Running loglikelihood requests:  62%|██████▎   | 250/400 [04:43<02:44,  1.10s/it]Running loglikelihood requests:  63%|██████▎   | 251/400 [04:44<02:43,  1.10s/it]Running loglikelihood requests:  63%|██████▎   | 252/400 [04:45<02:43,  1.10s/it]Running loglikelihood requests:  63%|██████▎   | 253/400 [04:46<02:42,  1.11s/it]Running loglikelihood requests:  64%|██████▍   | 255/400 [04:47<02:04,  1.17it/s]Running loglikelihood requests:  64%|██████▍   | 256/400 [04:49<02:12,  1.08it/s]Running loglikelihood requests:  64%|██████▍   | 257/400 [04:50<02:19,  1.03it/s]Running loglikelihood requests:  64%|██████▍   | 258/400 [04:51<02:23,  1.01s/it]Running loglikelihood requests:  65%|██████▍   | 259/400 [04:52<02:26,  1.04s/it]Running loglikelihood requests:  65%|██████▌   | 260/400 [04:53<02:27,  1.05s/it]Running loglikelihood requests:  65%|██████▌   | 261/400 [04:54<02:28,  1.07s/it]Running loglikelihood requests:  66%|██████▌   | 262/400 [04:55<02:28,  1.08s/it]Running loglikelihood requests:  66%|██████▌   | 263/400 [04:56<02:28,  1.08s/it]Running loglikelihood requests:  66%|██████▌   | 264/400 [04:57<02:28,  1.09s/it]Running loglikelihood requests:  66%|██████▋   | 265/400 [04:58<02:27,  1.09s/it]Running loglikelihood requests:  66%|██████▋   | 266/400 [05:00<02:26,  1.10s/it]Running loglikelihood requests:  67%|██████▋   | 267/400 [05:01<02:25,  1.10s/it]Running loglikelihood requests:  67%|██████▋   | 268/400 [05:02<02:24,  1.10s/it]Running loglikelihood requests:  68%|██████▊   | 270/400 [05:03<01:49,  1.19it/s]Running loglikelihood requests:  68%|██████▊   | 271/400 [05:04<01:56,  1.10it/s]Running loglikelihood requests:  68%|██████▊   | 272/400 [05:05<02:02,  1.05it/s]Running loglikelihood requests:  68%|██████▊   | 273/400 [05:06<02:05,  1.01it/s]Running loglikelihood requests:  68%|██████▊   | 274/400 [05:07<02:08,  1.02s/it]Running loglikelihood requests:  69%|██████▉   | 275/400 [05:08<02:09,  1.04s/it]Running loglikelihood requests:  69%|██████▉   | 276/400 [05:09<02:10,  1.05s/it]Running loglikelihood requests:  69%|██████▉   | 277/400 [05:11<02:10,  1.06s/it]Running loglikelihood requests:  70%|██████▉   | 278/400 [05:12<02:10,  1.07s/it]Running loglikelihood requests:  70%|██████▉   | 279/400 [05:13<02:10,  1.07s/it]Running loglikelihood requests:  70%|███████   | 280/400 [05:14<02:09,  1.08s/it]Running loglikelihood requests:  70%|███████   | 281/400 [05:15<02:08,  1.08s/it]Running loglikelihood requests:  70%|███████   | 282/400 [05:16<02:07,  1.08s/it]Running loglikelihood requests:  71%|███████   | 283/400 [05:17<02:06,  1.08s/it]Running loglikelihood requests:  71%|███████   | 284/400 [05:18<02:05,  1.08s/it]Running loglikelihood requests:  71%|███████▏  | 285/400 [05:19<02:04,  1.08s/it]Running loglikelihood requests:  72%|███████▏  | 286/400 [05:20<02:03,  1.08s/it]Running loglikelihood requests:  72%|███████▏  | 287/400 [05:21<02:02,  1.08s/it]Running loglikelihood requests:  72%|███████▏  | 288/400 [05:22<02:01,  1.08s/it]Running loglikelihood requests:  72%|███████▏  | 289/400 [05:24<02:00,  1.08s/it]Running loglikelihood requests:  72%|███████▎  | 290/400 [05:25<01:58,  1.08s/it]Running loglikelihood requests:  73%|███████▎  | 291/400 [05:26<01:57,  1.08s/it]Running loglikelihood requests:  73%|███████▎  | 292/400 [05:27<01:56,  1.08s/it]Running loglikelihood requests:  73%|███████▎  | 293/400 [05:28<01:55,  1.08s/it]Running loglikelihood requests:  74%|███████▎  | 294/400 [05:29<01:54,  1.08s/it]Running loglikelihood requests:  74%|███████▍  | 295/400 [05:30<01:53,  1.08s/it]Running loglikelihood requests:  74%|███████▍  | 296/400 [05:31<01:52,  1.08s/it]Running loglikelihood requests:  74%|███████▍  | 297/400 [05:32<01:51,  1.08s/it]Running loglikelihood requests:  74%|███████▍  | 298/400 [05:33<01:49,  1.08s/it]Running loglikelihood requests:  75%|███████▍  | 299/400 [05:34<01:48,  1.08s/it]Running loglikelihood requests:  75%|███████▌  | 300/400 [05:35<01:47,  1.08s/it]Running loglikelihood requests:  75%|███████▌  | 301/400 [05:36<01:46,  1.07s/it]Running loglikelihood requests:  76%|███████▌  | 302/400 [05:38<01:45,  1.07s/it]Running loglikelihood requests:  76%|███████▌  | 304/400 [05:39<01:19,  1.21it/s]Running loglikelihood requests:  76%|███████▋  | 305/400 [05:40<01:24,  1.13it/s]Running loglikelihood requests:  76%|███████▋  | 306/400 [05:41<01:28,  1.06it/s]Running loglikelihood requests:  77%|███████▋  | 307/400 [05:42<01:30,  1.02it/s]Running loglikelihood requests:  77%|███████▋  | 308/400 [05:43<01:32,  1.00s/it]Running loglikelihood requests:  77%|███████▋  | 309/400 [05:44<01:33,  1.02s/it]Running loglikelihood requests:  78%|███████▊  | 310/400 [05:45<01:33,  1.04s/it]Running loglikelihood requests:  78%|███████▊  | 311/400 [05:46<01:33,  1.05s/it]Running loglikelihood requests:  78%|███████▊  | 312/400 [05:47<01:32,  1.05s/it]Running loglikelihood requests:  78%|███████▊  | 313/400 [05:48<01:32,  1.06s/it]Running loglikelihood requests:  78%|███████▊  | 314/400 [05:49<01:31,  1.06s/it]Running loglikelihood requests:  79%|███████▉  | 315/400 [05:50<01:30,  1.07s/it]Running loglikelihood requests:  79%|███████▉  | 316/400 [05:51<01:29,  1.07s/it]Running loglikelihood requests:  79%|███████▉  | 317/400 [05:53<01:28,  1.06s/it]Running loglikelihood requests:  80%|███████▉  | 318/400 [05:54<01:27,  1.06s/it]Running loglikelihood requests:  80%|███████▉  | 319/400 [05:55<01:26,  1.06s/it]Running loglikelihood requests:  80%|████████  | 320/400 [05:56<01:25,  1.06s/it]Running loglikelihood requests:  80%|████████  | 321/400 [05:57<01:24,  1.06s/it]Running loglikelihood requests:  80%|████████  | 322/400 [05:58<01:23,  1.06s/it]Running loglikelihood requests:  81%|████████  | 323/400 [05:59<01:22,  1.07s/it]Running loglikelihood requests:  81%|████████  | 324/400 [06:00<01:20,  1.07s/it]Running loglikelihood requests:  81%|████████▏ | 325/400 [06:01<01:19,  1.07s/it]Running loglikelihood requests:  82%|████████▏ | 326/400 [06:02<01:18,  1.07s/it]Running loglikelihood requests:  82%|████████▏ | 327/400 [06:03<01:17,  1.07s/it]Running loglikelihood requests:  82%|████████▏ | 328/400 [06:04<01:16,  1.06s/it]Running loglikelihood requests:  82%|████████▎ | 330/400 [06:05<00:57,  1.22it/s]Running loglikelihood requests:  83%|████████▎ | 331/400 [06:06<01:00,  1.14it/s]Running loglikelihood requests:  83%|████████▎ | 332/400 [06:07<01:03,  1.07it/s]Running loglikelihood requests:  83%|████████▎ | 333/400 [06:08<01:04,  1.03it/s]Running loglikelihood requests:  84%|████████▎ | 334/400 [06:10<01:05,  1.00it/s]Running loglikelihood requests:  84%|████████▍ | 335/400 [06:11<01:06,  1.02s/it]Running loglikelihood requests:  84%|████████▍ | 336/400 [06:12<01:05,  1.03s/it]Running loglikelihood requests:  84%|████████▍ | 337/400 [06:13<01:05,  1.03s/it]Running loglikelihood requests:  84%|████████▍ | 338/400 [06:14<01:04,  1.04s/it]Running loglikelihood requests:  85%|████████▍ | 339/400 [06:15<01:03,  1.04s/it]Running loglikelihood requests:  85%|████████▌ | 340/400 [06:16<01:02,  1.04s/it]Running loglikelihood requests:  85%|████████▌ | 341/400 [06:17<01:01,  1.04s/it]Running loglikelihood requests:  86%|████████▌ | 342/400 [06:18<01:00,  1.05s/it]Running loglikelihood requests:  86%|████████▌ | 343/400 [06:19<00:59,  1.05s/it]Running loglikelihood requests:  86%|████████▌ | 344/400 [06:20<00:58,  1.05s/it]Running loglikelihood requests:  86%|████████▋ | 345/400 [06:21<00:57,  1.05s/it]Running loglikelihood requests:  86%|████████▋ | 346/400 [06:22<00:56,  1.05s/it]Running loglikelihood requests:  87%|████████▋ | 347/400 [06:23<00:55,  1.05s/it]Running loglikelihood requests:  87%|████████▋ | 348/400 [06:24<00:54,  1.04s/it]Running loglikelihood requests:  87%|████████▋ | 349/400 [06:25<00:53,  1.04s/it]Running loglikelihood requests:  88%|████████▊ | 350/400 [06:26<00:51,  1.04s/it]Running loglikelihood requests:  88%|████████▊ | 351/400 [06:27<00:51,  1.06s/it]Running loglikelihood requests:  88%|████████▊ | 352/400 [06:28<00:50,  1.05s/it]Running loglikelihood requests:  88%|████████▊ | 353/400 [06:29<00:49,  1.04s/it]Running loglikelihood requests:  88%|████████▊ | 354/400 [06:31<00:47,  1.04s/it]Running loglikelihood requests:  89%|████████▉ | 355/400 [06:32<00:46,  1.04s/it]Running loglikelihood requests:  89%|████████▉ | 356/400 [06:33<00:45,  1.03s/it]Running loglikelihood requests:  89%|████████▉ | 357/400 [06:34<00:44,  1.03s/it]Running loglikelihood requests:  90%|████████▉ | 358/400 [06:35<00:43,  1.03s/it]Running loglikelihood requests:  90%|████████▉ | 359/400 [06:36<00:42,  1.03s/it]Running loglikelihood requests:  90%|█████████ | 360/400 [06:37<00:41,  1.03s/it]Running loglikelihood requests:  90%|█████████ | 361/400 [06:38<00:39,  1.02s/it]Running loglikelihood requests:  90%|█████████ | 362/400 [06:39<00:38,  1.02s/it]Running loglikelihood requests:  91%|█████████ | 363/400 [06:40<00:37,  1.02s/it]Running loglikelihood requests:  91%|█████████ | 364/400 [06:41<00:36,  1.02s/it]Running loglikelihood requests:  91%|█████████▏| 365/400 [06:42<00:35,  1.01s/it]Running loglikelihood requests:  92%|█████████▏| 366/400 [06:43<00:34,  1.01s/it]Running loglikelihood requests:  92%|█████████▏| 367/400 [06:44<00:33,  1.01s/it]Running loglikelihood requests:  92%|█████████▏| 368/400 [06:45<00:32,  1.01s/it]Running loglikelihood requests:  92%|█████████▏| 369/400 [06:46<00:32,  1.04s/it]Running loglikelihood requests:  92%|█████████▎| 370/400 [06:47<00:30,  1.03s/it]Running loglikelihood requests:  93%|█████████▎| 371/400 [06:48<00:29,  1.02s/it]Running loglikelihood requests:  93%|█████████▎| 372/400 [06:49<00:28,  1.01s/it]Running loglikelihood requests:  93%|█████████▎| 373/400 [06:50<00:27,  1.01s/it]Running loglikelihood requests:  94%|█████████▎| 374/400 [06:51<00:26,  1.00s/it]Running loglikelihood requests:  94%|█████████▍| 375/400 [06:52<00:24,  1.00it/s]Running loglikelihood requests:  94%|█████████▍| 376/400 [06:53<00:23,  1.01it/s]Running loglikelihood requests:  94%|█████████▍| 377/400 [06:54<00:22,  1.01it/s]Running loglikelihood requests:  94%|█████████▍| 378/400 [06:55<00:21,  1.01it/s]Running loglikelihood requests:  95%|█████████▍| 379/400 [06:56<00:20,  1.02it/s]Running loglikelihood requests:  95%|█████████▌| 380/400 [06:57<00:19,  1.02it/s]Running loglikelihood requests:  95%|█████████▌| 381/400 [06:58<00:18,  1.02it/s]Running loglikelihood requests:  96%|█████████▌| 382/400 [06:59<00:17,  1.02it/s]Running loglikelihood requests:  96%|█████████▌| 383/400 [07:00<00:16,  1.02it/s]Running loglikelihood requests:  96%|█████████▌| 384/400 [07:01<00:15,  1.02it/s]Running loglikelihood requests:  96%|█████████▋| 385/400 [07:02<00:14,  1.02it/s]Running loglikelihood requests:  96%|█████████▋| 386/400 [07:03<00:13,  1.02it/s]Running loglikelihood requests:  97%|█████████▋| 387/400 [07:04<00:12,  1.02it/s]Running loglikelihood requests:  97%|█████████▋| 388/400 [07:05<00:11,  1.02it/s]Running loglikelihood requests:  97%|█████████▋| 389/400 [07:06<00:10,  1.02it/s]Running loglikelihood requests:  98%|█████████▊| 390/400 [07:07<00:09,  1.02it/s]Running loglikelihood requests:  98%|█████████▊| 391/400 [07:08<00:08,  1.02it/s]Running loglikelihood requests:  98%|█████████▊| 392/400 [07:08<00:07,  1.02it/s]Running loglikelihood requests:  98%|█████████▊| 393/400 [07:09<00:06,  1.02it/s]Running loglikelihood requests:  98%|█████████▊| 394/400 [07:10<00:05,  1.02it/s]Running loglikelihood requests:  99%|█████████▉| 395/400 [07:11<00:04,  1.03it/s]Running loglikelihood requests:  99%|█████████▉| 396/400 [07:12<00:03,  1.03it/s]Running loglikelihood requests:  99%|█████████▉| 397/400 [07:13<00:02,  1.04it/s]Running loglikelihood requests: 100%|█████████▉| 398/400 [07:14<00:01,  1.04it/s]Running loglikelihood requests: 100%|█████████▉| 399/400 [07:15<00:00,  1.05it/s]Running loglikelihood requests: 100%|██████████| 400/400 [07:16<00:00,  1.05it/s]Running loglikelihood requests: 100%|██████████| 400/400 [07:16<00:00,  1.09s/it]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:4'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:4'}
full model:
{'mastermind_35_easy': {'alias': 'mastermind_35_easy', 'acc,none': 0.51, 'acc_stderr,none': 0.05024183937956913}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9685093904417202
0.9837389482734848
0.9887891422859466
0.9728424941793791
0.9273096247071001
0.9956884174036557
0.9936991917224709
0.990637728847171
0.984416562505352
0.9357041463019401
0.957486187236088
0.9794440444506461
0.9854000882321717
0.989321120949945
0.9948124947717892
0.9950699411775289
0.9589145403056732
0.9488498047330014
0.979093344902028
0.9861615563222426
0.9944647439314634
0.9974738465425171
0.9932459641322177
0.9612113452387381
0.9566187588663586
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[5, 1, 0, 3, 7, 6, 2, 4]
tensor([5, 1, 0, 3, 7, 6, 2, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 0, 3, 7, 5, 2, 4]
tensor([6, 1, 0, 3, 7, 5, 2, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 1, 0, 3, 7, 6, 2, 4]
tensor([5, 1, 0, 3, 7, 6, 2, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 0, 3, 7, 5, 2, 4]
tensor([6, 1, 0, 3, 7, 5, 2, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 1, 0, 3, 7, 6, 2, 4]
tensor([5, 1, 0, 3, 7, 6, 2, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 0, 3, 7, 5, 2, 4]
tensor([6, 1, 0, 3, 7, 5, 2, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([2])
tensor(2)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([2])
tensor(2)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
done!
Normal merging for layer 3
tensor([2])
tensor(2)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Normal merging for layer 4
tensor([2])
tensor(2)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
done!
Normal merging for layer 5
tensor([2])
tensor(2)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 6 to 31
done!
all done!
Model size: 12.0718 GB
18
cuda:4
sciq
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:40<00:40, 40.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 24.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.70s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/sciq HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/sciq HTTP/1.1" 200 1238
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/sciq/sciq.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/sciq HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/sciq HTTP/1.1" 200 1238
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/sciq/resolve/2c94ad3e1aafab77146f384e23536f97a4849815/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/sciq/resolve/2c94ad3e1aafab77146f384e23536f97a4849815/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/sciq/revision/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 111
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/sciq/revision/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 1238
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/sciq/tree/2c94ad3e1aafab77146f384e23536f97a4849815?recursive=False&expand=False HTTP/1.1" 307 136
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/sciq/tree/2c94ad3e1aafab77146f384e23536f97a4849815?recursive=False&expand=False HTTP/1.1" 200 291
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/sciq/tree/2c94ad3e1aafab77146f384e23536f97a4849815/data?recursive=False&expand=False HTTP/1.1" 307 141
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/sciq/tree/2c94ad3e1aafab77146f384e23536f97a4849815/data?recursive=False&expand=False HTTP/1.1" 200 358
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/sciq/revision/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 111
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/sciq/revision/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 1238
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/sciq/resolve/2c94ad3e1aafab77146f384e23536f97a4849815/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/sciq/resolve/2c94ad3e1aafab77146f384e23536f97a4849815/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140215184258592 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_sciq_default_0.0.0_2c94ad3e1aafab77146f384e23536f97a4849815.lock
DEBUG:filelock:Lock 140215184258592 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_sciq_default_0.0.0_2c94ad3e1aafab77146f384e23536f97a4849815.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215184258592 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_sciq_default_0.0.0_2c94ad3e1aafab77146f384e23536f97a4849815.lock
DEBUG:filelock:Lock 140215184258592 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_sciq_default_0.0.0_2c94ad3e1aafab77146f384e23536f97a4849815.lock
DEBUG:filelock:Attempting to acquire lock 140229617062816 on /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815_builder.lock
DEBUG:filelock:Lock 140229617062816 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815/dataset_info.json
DEBUG:filelock:Attempting to release lock 140229617062816 on /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815_builder.lock
DEBUG:filelock:Lock 140229617062816 released on /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of sciq from None to 0
INFO:lm_eval.api.task:Building contexts for sciq on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1032.59it/s]
DEBUG:lm_eval.evaluator:Task: sciq; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:03<25:46,  3.88s/it]Running loglikelihood requests:   0%|          | 2/400 [00:07<23:53,  3.60s/it]Running loglikelihood requests:   1%|          | 3/400 [00:10<23:23,  3.53s/it]Running loglikelihood requests:   1%|          | 4/400 [00:14<22:58,  3.48s/it]Running loglikelihood requests:   1%|▏         | 5/400 [00:17<22:15,  3.38s/it]Running loglikelihood requests:   2%|▏         | 6/400 [00:20<21:47,  3.32s/it]Running loglikelihood requests:   2%|▏         | 7/400 [00:23<21:29,  3.28s/it]Running loglikelihood requests:   2%|▏         | 8/400 [00:26<21:14,  3.25s/it]Running loglikelihood requests:   2%|▏         | 9/400 [00:29<20:26,  3.14s/it]Running loglikelihood requests:   2%|▎         | 10/400 [00:32<19:50,  3.05s/it]Running loglikelihood requests:   3%|▎         | 11/400 [00:35<19:23,  2.99s/it]Running loglikelihood requests:   3%|▎         | 12/400 [00:38<19:02,  2.95s/it]Running loglikelihood requests:   3%|▎         | 13/400 [00:41<18:24,  2.85s/it]Running loglikelihood requests:   4%|▎         | 14/400 [00:43<17:57,  2.79s/it]Running loglikelihood requests:   4%|▍         | 15/400 [00:46<17:36,  2.74s/it]Running loglikelihood requests:   4%|▍         | 16/400 [00:48<17:21,  2.71s/it]Running loglikelihood requests:   4%|▍         | 17/400 [00:51<17:00,  2.66s/it]Running loglikelihood requests:   4%|▍         | 18/400 [00:54<16:43,  2.63s/it]Running loglikelihood requests:   5%|▍         | 19/400 [00:56<16:31,  2.60s/it]Running loglikelihood requests:   5%|▌         | 20/400 [00:59<16:22,  2.58s/it]Running loglikelihood requests:   5%|▌         | 21/400 [01:01<16:13,  2.57s/it]Running loglikelihood requests:   6%|▌         | 22/400 [01:04<16:05,  2.55s/it]Running loglikelihood requests:   6%|▌         | 23/400 [01:06<15:57,  2.54s/it]Running loglikelihood requests:   6%|▌         | 24/400 [01:09<15:51,  2.53s/it]Running loglikelihood requests:   6%|▋         | 25/400 [01:11<15:30,  2.48s/it]Running loglikelihood requests:   6%|▋         | 26/400 [01:13<15:15,  2.45s/it]Running loglikelihood requests:   7%|▋         | 27/400 [01:16<15:06,  2.43s/it]Running loglikelihood requests:   7%|▋         | 28/400 [01:18<14:53,  2.40s/it]Running loglikelihood requests:   7%|▋         | 29/400 [01:20<14:20,  2.32s/it]Running loglikelihood requests:   8%|▊         | 30/400 [01:22<13:56,  2.26s/it]Running loglikelihood requests:   8%|▊         | 31/400 [01:25<13:38,  2.22s/it]Running loglikelihood requests:   8%|▊         | 32/400 [01:27<13:24,  2.19s/it]Running loglikelihood requests:   8%|▊         | 33/400 [01:29<13:00,  2.13s/it]Running loglikelihood requests:   8%|▊         | 34/400 [01:31<12:42,  2.08s/it]Running loglikelihood requests:   9%|▉         | 35/400 [01:33<12:28,  2.05s/it]Running loglikelihood requests:   9%|▉         | 36/400 [01:35<12:18,  2.03s/it]Running loglikelihood requests:   9%|▉         | 37/400 [01:37<12:10,  2.01s/it]Running loglikelihood requests:  10%|▉         | 38/400 [01:39<12:05,  2.00s/it]Running loglikelihood requests:  10%|█         | 40/400 [01:40<09:12,  1.53s/it]Running loglikelihood requests:  10%|█         | 41/400 [01:42<09:36,  1.61s/it]Running loglikelihood requests:  10%|█         | 42/400 [01:44<09:54,  1.66s/it]Running loglikelihood requests:  11%|█         | 43/400 [01:46<10:07,  1.70s/it]Running loglikelihood requests:  11%|█         | 44/400 [01:48<10:16,  1.73s/it]Running loglikelihood requests:  11%|█▏        | 45/400 [01:49<10:15,  1.73s/it]Running loglikelihood requests:  12%|█▏        | 46/400 [01:51<10:14,  1.73s/it]Running loglikelihood requests:  12%|█▏        | 47/400 [01:53<10:12,  1.73s/it]Running loglikelihood requests:  12%|█▏        | 48/400 [01:55<10:10,  1.74s/it]Running loglikelihood requests:  12%|█▏        | 49/400 [01:56<10:08,  1.73s/it]Running loglikelihood requests:  12%|█▎        | 50/400 [01:58<10:06,  1.73s/it]Running loglikelihood requests:  13%|█▎        | 51/400 [02:00<10:03,  1.73s/it]Running loglikelihood requests:  13%|█▎        | 52/400 [02:02<10:01,  1.73s/it]Running loglikelihood requests:  13%|█▎        | 53/400 [02:03<09:41,  1.68s/it]Running loglikelihood requests:  14%|█▎        | 54/400 [02:05<09:27,  1.64s/it]Running loglikelihood requests:  14%|█▍        | 55/400 [02:06<09:17,  1.62s/it]Running loglikelihood requests:  14%|█▍        | 57/400 [02:08<06:57,  1.22s/it]Running loglikelihood requests:  14%|█▍        | 58/400 [02:09<07:19,  1.28s/it]Running loglikelihood requests:  15%|█▍        | 59/400 [02:11<07:36,  1.34s/it]Running loglikelihood requests:  16%|█▌        | 62/400 [02:12<05:01,  1.12it/s]Running loglikelihood requests:  16%|█▌        | 63/400 [02:14<05:41,  1.01s/it]Running loglikelihood requests:  16%|█▌        | 64/400 [02:15<06:16,  1.12s/it]Running loglikelihood requests:  16%|█▋        | 65/400 [02:17<06:40,  1.20s/it]Running loglikelihood requests:  16%|█▋        | 66/400 [02:18<07:00,  1.26s/it]Running loglikelihood requests:  17%|█▋        | 67/400 [02:20<07:14,  1.30s/it]Running loglikelihood requests:  17%|█▋        | 68/400 [02:21<07:25,  1.34s/it]Running loglikelihood requests:  17%|█▋        | 69/400 [02:22<07:30,  1.36s/it]Running loglikelihood requests:  18%|█▊        | 70/400 [02:24<07:33,  1.38s/it]Running loglikelihood requests:  18%|█▊        | 71/400 [02:25<07:35,  1.39s/it]Running loglikelihood requests:  18%|█▊        | 72/400 [02:27<07:40,  1.40s/it]Running loglikelihood requests:  18%|█▊        | 73/400 [02:28<07:45,  1.43s/it]Running loglikelihood requests:  18%|█▊        | 74/400 [02:30<07:48,  1.44s/it]Running loglikelihood requests:  19%|█▉        | 77/400 [02:31<04:51,  1.11it/s]Running loglikelihood requests:  20%|█▉        | 78/400 [02:32<05:26,  1.01s/it]Running loglikelihood requests:  20%|█▉        | 79/400 [02:34<05:56,  1.11s/it]Running loglikelihood requests:  20%|██        | 81/400 [02:35<04:57,  1.07it/s]Running loglikelihood requests:  20%|██        | 82/400 [02:36<05:20,  1.01s/it]Running loglikelihood requests:  21%|██        | 83/400 [02:38<05:38,  1.07s/it]Running loglikelihood requests:  21%|██        | 84/400 [02:39<05:52,  1.12s/it]Running loglikelihood requests:  21%|██▏       | 85/400 [02:40<06:02,  1.15s/it]Running loglikelihood requests:  22%|██▏       | 86/400 [02:41<06:07,  1.17s/it]Running loglikelihood requests:  22%|██▏       | 89/400 [02:43<03:55,  1.32it/s]Running loglikelihood requests:  22%|██▎       | 90/400 [02:44<04:23,  1.18it/s]Running loglikelihood requests:  23%|██▎       | 91/400 [02:45<04:48,  1.07it/s]Running loglikelihood requests:  23%|██▎       | 92/400 [02:46<05:07,  1.00it/s]Running loglikelihood requests:  23%|██▎       | 93/400 [02:48<05:23,  1.05s/it]Running loglikelihood requests:  24%|██▍       | 97/400 [02:49<03:02,  1.66it/s]Running loglikelihood requests:  24%|██▍       | 98/400 [02:50<03:33,  1.41it/s]Running loglikelihood requests:  25%|██▍       | 99/400 [02:51<04:01,  1.25it/s]Running loglikelihood requests:  25%|██▌       | 100/400 [02:52<04:26,  1.13it/s]Running loglikelihood requests:  25%|██▌       | 101/400 [02:53<04:46,  1.04it/s]Running loglikelihood requests:  26%|██▌       | 102/400 [02:55<05:02,  1.01s/it]Running loglikelihood requests:  26%|██▌       | 103/400 [02:56<05:14,  1.06s/it]Running loglikelihood requests:  26%|██▌       | 104/400 [02:57<05:22,  1.09s/it]Running loglikelihood requests:  26%|██▋       | 105/400 [02:58<05:27,  1.11s/it]Running loglikelihood requests:  26%|██▋       | 106/400 [02:59<05:30,  1.12s/it]Running loglikelihood requests:  27%|██▋       | 107/400 [03:00<05:32,  1.14s/it]Running loglikelihood requests:  27%|██▋       | 108/400 [03:02<05:33,  1.14s/it]Running loglikelihood requests:  27%|██▋       | 109/400 [03:03<05:32,  1.14s/it]Running loglikelihood requests:  28%|██▊       | 110/400 [03:04<05:31,  1.14s/it]Running loglikelihood requests:  28%|██▊       | 111/400 [03:05<05:29,  1.14s/it]Running loglikelihood requests:  28%|██▊       | 112/400 [03:06<05:28,  1.14s/it]Running loglikelihood requests:  28%|██▊       | 113/400 [03:07<05:25,  1.14s/it]Running loglikelihood requests:  28%|██▊       | 114/400 [03:08<05:23,  1.13s/it]Running loglikelihood requests:  29%|██▉       | 115/400 [03:10<05:21,  1.13s/it]Running loglikelihood requests:  29%|██▉       | 116/400 [03:11<05:19,  1.13s/it]Running loglikelihood requests:  29%|██▉       | 117/400 [03:12<05:16,  1.12s/it]Running loglikelihood requests:  30%|██▉       | 118/400 [03:13<05:12,  1.11s/it]Running loglikelihood requests:  30%|██▉       | 119/400 [03:14<05:10,  1.10s/it]Running loglikelihood requests:  30%|███       | 120/400 [03:15<05:07,  1.10s/it]Running loglikelihood requests:  30%|███       | 121/400 [03:16<05:05,  1.10s/it]Running loglikelihood requests:  30%|███       | 122/400 [03:17<05:04,  1.09s/it]Running loglikelihood requests:  31%|███       | 123/400 [03:18<05:02,  1.09s/it]Running loglikelihood requests:  31%|███       | 124/400 [03:19<05:01,  1.09s/it]Running loglikelihood requests:  31%|███▏      | 125/400 [03:20<04:58,  1.08s/it]Running loglikelihood requests:  32%|███▏      | 126/400 [03:22<04:55,  1.08s/it]Running loglikelihood requests:  32%|███▏      | 127/400 [03:23<04:53,  1.07s/it]Running loglikelihood requests:  32%|███▏      | 128/400 [03:24<04:50,  1.07s/it]Running loglikelihood requests:  32%|███▏      | 129/400 [03:25<04:48,  1.06s/it]Running loglikelihood requests:  32%|███▎      | 130/400 [03:26<04:46,  1.06s/it]Running loglikelihood requests:  33%|███▎      | 131/400 [03:27<04:44,  1.06s/it]Running loglikelihood requests:  34%|███▎      | 134/400 [03:28<02:55,  1.52it/s]Running loglikelihood requests:  34%|███▍      | 135/400 [03:29<03:16,  1.35it/s]Running loglikelihood requests:  34%|███▍      | 136/400 [03:30<03:34,  1.23it/s]Running loglikelihood requests:  34%|███▍      | 137/400 [03:31<03:49,  1.15it/s]Running loglikelihood requests:  34%|███▍      | 138/400 [03:32<04:00,  1.09it/s]Running loglikelihood requests:  35%|███▍      | 139/400 [03:33<04:07,  1.05it/s]Running loglikelihood requests:  36%|███▌      | 142/400 [03:34<02:41,  1.60it/s]Running loglikelihood requests:  36%|███▌      | 143/400 [03:35<03:02,  1.41it/s]Running loglikelihood requests:  36%|███▌      | 144/400 [03:36<03:21,  1.27it/s]Running loglikelihood requests:  36%|███▋      | 145/400 [03:37<03:36,  1.18it/s]Running loglikelihood requests:  36%|███▋      | 146/400 [03:38<03:47,  1.12it/s]Running loglikelihood requests:  37%|███▋      | 147/400 [03:39<03:56,  1.07it/s]Running loglikelihood requests:  37%|███▋      | 148/400 [03:40<04:02,  1.04it/s]Running loglikelihood requests:  37%|███▋      | 149/400 [03:41<04:05,  1.02it/s]Running loglikelihood requests:  38%|███▊      | 150/400 [03:42<04:07,  1.01it/s]Running loglikelihood requests:  38%|███▊      | 151/400 [03:43<04:08,  1.00it/s]Running loglikelihood requests:  38%|███▊      | 152/400 [03:44<04:08,  1.00s/it]Running loglikelihood requests:  38%|███▊      | 153/400 [03:45<04:08,  1.01s/it]Running loglikelihood requests:  38%|███▊      | 154/400 [03:46<04:08,  1.01s/it]Running loglikelihood requests:  39%|███▉      | 155/400 [03:47<04:07,  1.01s/it]Running loglikelihood requests:  39%|███▉      | 156/400 [03:48<04:07,  1.01s/it]Running loglikelihood requests:  39%|███▉      | 157/400 [03:50<04:05,  1.01s/it]Running loglikelihood requests:  40%|███▉      | 158/400 [03:51<04:04,  1.01s/it]Running loglikelihood requests:  40%|███▉      | 159/400 [03:52<04:03,  1.01s/it]Running loglikelihood requests:  40%|████      | 160/400 [03:53<04:02,  1.01s/it]Running loglikelihood requests:  40%|████      | 161/400 [03:54<04:01,  1.01s/it]Running loglikelihood requests:  40%|████      | 162/400 [03:55<03:59,  1.01s/it]Running loglikelihood requests:  41%|████      | 163/400 [03:56<03:58,  1.00s/it]Running loglikelihood requests:  41%|████      | 164/400 [03:57<03:56,  1.00s/it]Running loglikelihood requests:  41%|████▏     | 165/400 [03:58<03:53,  1.00it/s]Running loglikelihood requests:  42%|████▏     | 166/400 [03:59<03:52,  1.01it/s]Running loglikelihood requests:  42%|████▏     | 167/400 [03:59<03:51,  1.01it/s]Running loglikelihood requests:  42%|████▏     | 168/400 [04:00<03:50,  1.01it/s]Running loglikelihood requests:  42%|████▏     | 169/400 [04:01<03:48,  1.01it/s]Running loglikelihood requests:  42%|████▎     | 170/400 [04:02<03:47,  1.01it/s]Running loglikelihood requests:  43%|████▎     | 172/400 [04:03<02:52,  1.32it/s]Running loglikelihood requests:  43%|████▎     | 173/400 [04:04<03:03,  1.24it/s]Running loglikelihood requests:  44%|████▎     | 174/400 [04:05<03:09,  1.19it/s]Running loglikelihood requests:  44%|████▍     | 177/400 [04:06<02:04,  1.78it/s]Running loglikelihood requests:  44%|████▍     | 178/400 [04:07<02:20,  1.57it/s]Running loglikelihood requests:  45%|████▍     | 179/400 [04:08<02:34,  1.43it/s]Running loglikelihood requests:  45%|████▌     | 180/400 [04:09<02:46,  1.32it/s]Running loglikelihood requests:  45%|████▌     | 181/400 [04:10<02:54,  1.26it/s]Running loglikelihood requests:  46%|████▌     | 182/400 [04:11<03:00,  1.21it/s]Running loglikelihood requests:  46%|████▌     | 183/400 [04:12<03:03,  1.18it/s]Running loglikelihood requests:  46%|████▌     | 184/400 [04:13<03:06,  1.16it/s]Running loglikelihood requests:  46%|████▋     | 185/400 [04:14<03:08,  1.14it/s]Running loglikelihood requests:  46%|████▋     | 186/400 [04:14<03:08,  1.13it/s]Running loglikelihood requests:  47%|████▋     | 187/400 [04:15<03:09,  1.13it/s]Running loglikelihood requests:  47%|████▋     | 188/400 [04:16<03:08,  1.12it/s]Running loglikelihood requests:  47%|████▋     | 189/400 [04:17<03:08,  1.12it/s]Running loglikelihood requests:  48%|████▊     | 190/400 [04:18<03:07,  1.12it/s]Running loglikelihood requests:  48%|████▊     | 191/400 [04:19<03:05,  1.12it/s]Running loglikelihood requests:  48%|████▊     | 192/400 [04:20<03:04,  1.13it/s]Running loglikelihood requests:  48%|████▊     | 193/400 [04:21<03:02,  1.14it/s]Running loglikelihood requests:  48%|████▊     | 194/400 [04:22<03:00,  1.14it/s]Running loglikelihood requests:  49%|████▉     | 195/400 [04:22<02:58,  1.15it/s]Running loglikelihood requests:  49%|████▉     | 196/400 [04:23<02:57,  1.15it/s]Running loglikelihood requests:  49%|████▉     | 197/400 [04:24<02:55,  1.16it/s]Running loglikelihood requests:  50%|████▉     | 198/400 [04:25<02:53,  1.16it/s]Running loglikelihood requests:  50%|████▉     | 199/400 [04:26<02:52,  1.17it/s]Running loglikelihood requests:  50%|█████     | 200/400 [04:27<02:51,  1.17it/s]Running loglikelihood requests:  50%|█████     | 201/400 [04:28<02:49,  1.17it/s]Running loglikelihood requests:  50%|█████     | 202/400 [04:28<02:48,  1.18it/s]Running loglikelihood requests:  51%|█████     | 203/400 [04:29<02:47,  1.18it/s]Running loglikelihood requests:  51%|█████     | 204/400 [04:30<02:46,  1.18it/s]Running loglikelihood requests:  51%|█████▏    | 205/400 [04:31<02:45,  1.18it/s]Running loglikelihood requests:  52%|█████▏    | 206/400 [04:32<02:44,  1.18it/s]Running loglikelihood requests:  52%|█████▏    | 207/400 [04:33<02:43,  1.18it/s]Running loglikelihood requests:  52%|█████▏    | 208/400 [04:33<02:42,  1.18it/s]Running loglikelihood requests:  52%|█████▏    | 209/400 [04:34<02:41,  1.18it/s]Running loglikelihood requests:  53%|█████▎    | 212/400 [04:35<01:39,  1.89it/s]Running loglikelihood requests:  53%|█████▎    | 213/400 [04:36<01:50,  1.69it/s]Running loglikelihood requests:  54%|█████▎    | 214/400 [04:37<01:59,  1.55it/s]Running loglikelihood requests:  54%|█████▍    | 215/400 [04:38<02:07,  1.46it/s]Running loglikelihood requests:  54%|█████▍    | 216/400 [04:38<02:12,  1.39it/s]Running loglikelihood requests:  54%|█████▍    | 217/400 [04:39<02:16,  1.34it/s]Running loglikelihood requests:  55%|█████▍    | 218/400 [04:40<02:19,  1.30it/s]Running loglikelihood requests:  55%|█████▍    | 219/400 [04:41<02:21,  1.28it/s]Running loglikelihood requests:  55%|█████▌    | 220/400 [04:42<02:22,  1.26it/s]Running loglikelihood requests:  55%|█████▌    | 221/400 [04:43<02:22,  1.25it/s]Running loglikelihood requests:  56%|█████▌    | 222/400 [04:43<02:22,  1.25it/s]Running loglikelihood requests:  56%|█████▌    | 223/400 [04:44<02:22,  1.24it/s]Running loglikelihood requests:  56%|█████▌    | 224/400 [04:45<02:21,  1.24it/s]Running loglikelihood requests:  56%|█████▋    | 225/400 [04:46<02:20,  1.24it/s]Running loglikelihood requests:  56%|█████▋    | 226/400 [04:47<02:20,  1.24it/s]Running loglikelihood requests:  57%|█████▋    | 227/400 [04:47<02:19,  1.24it/s]Running loglikelihood requests:  57%|█████▋    | 228/400 [04:48<02:18,  1.24it/s]Running loglikelihood requests:  57%|█████▋    | 229/400 [04:49<02:17,  1.24it/s]Running loglikelihood requests:  57%|█████▊    | 230/400 [04:50<02:16,  1.24it/s]Running loglikelihood requests:  58%|█████▊    | 231/400 [04:51<02:15,  1.24it/s]Running loglikelihood requests:  58%|█████▊    | 232/400 [04:51<02:15,  1.24it/s]Running loglikelihood requests:  58%|█████▊    | 233/400 [04:52<02:14,  1.24it/s]Running loglikelihood requests:  58%|█████▊    | 234/400 [04:53<02:12,  1.25it/s]Running loglikelihood requests:  59%|█████▉    | 235/400 [04:54<02:11,  1.25it/s]Running loglikelihood requests:  59%|█████▉    | 236/400 [04:55<02:11,  1.25it/s]Running loglikelihood requests:  59%|█████▉    | 237/400 [04:55<02:10,  1.25it/s]Running loglikelihood requests:  60%|██████    | 240/400 [04:56<01:19,  2.00it/s]Running loglikelihood requests:  60%|██████    | 241/400 [04:57<01:28,  1.79it/s]Running loglikelihood requests:  60%|██████    | 242/400 [04:58<01:36,  1.64it/s]Running loglikelihood requests:  61%|██████    | 243/400 [04:59<01:42,  1.54it/s]Running loglikelihood requests:  61%|██████    | 244/400 [04:59<01:46,  1.46it/s]Running loglikelihood requests:  61%|██████▏   | 245/400 [05:00<01:49,  1.41it/s]Running loglikelihood requests:  62%|██████▏   | 246/400 [05:01<01:51,  1.38it/s]Running loglikelihood requests:  62%|██████▏   | 247/400 [05:02<01:52,  1.36it/s]Running loglikelihood requests:  62%|██████▏   | 248/400 [05:02<01:53,  1.34it/s]Running loglikelihood requests:  62%|██████▏   | 249/400 [05:03<01:53,  1.33it/s]Running loglikelihood requests:  62%|██████▎   | 250/400 [05:04<01:53,  1.32it/s]Running loglikelihood requests:  63%|██████▎   | 251/400 [05:05<01:52,  1.32it/s]Running loglikelihood requests:  63%|██████▎   | 252/400 [05:05<01:52,  1.32it/s]Running loglikelihood requests:  63%|██████▎   | 253/400 [05:06<01:51,  1.32it/s]Running loglikelihood requests:  64%|██████▎   | 254/400 [05:07<01:50,  1.32it/s]Running loglikelihood requests:  64%|██████▍   | 255/400 [05:08<01:49,  1.32it/s]Running loglikelihood requests:  64%|██████▍   | 256/400 [05:08<01:49,  1.32it/s]Running loglikelihood requests:  64%|██████▍   | 257/400 [05:09<01:48,  1.32it/s]Running loglikelihood requests:  64%|██████▍   | 258/400 [05:10<01:47,  1.33it/s]Running loglikelihood requests:  65%|██████▍   | 259/400 [05:11<01:46,  1.33it/s]Running loglikelihood requests:  65%|██████▌   | 260/400 [05:11<01:45,  1.33it/s]Running loglikelihood requests:  65%|██████▌   | 261/400 [05:12<01:42,  1.36it/s]Running loglikelihood requests:  66%|██████▌   | 262/400 [05:13<01:40,  1.37it/s]Running loglikelihood requests:  66%|██████▌   | 263/400 [05:14<01:38,  1.39it/s]Running loglikelihood requests:  66%|██████▌   | 264/400 [05:14<01:37,  1.40it/s]Running loglikelihood requests:  66%|██████▋   | 265/400 [05:15<01:34,  1.43it/s]Running loglikelihood requests:  66%|██████▋   | 266/400 [05:16<01:32,  1.45it/s]Running loglikelihood requests:  67%|██████▋   | 267/400 [05:16<01:31,  1.45it/s]Running loglikelihood requests:  67%|██████▋   | 268/400 [05:17<01:30,  1.45it/s]Running loglikelihood requests:  67%|██████▋   | 269/400 [05:18<01:28,  1.48it/s]Running loglikelihood requests:  68%|██████▊   | 270/400 [05:18<01:26,  1.50it/s]Running loglikelihood requests:  68%|██████▊   | 271/400 [05:19<01:26,  1.49it/s]Running loglikelihood requests:  68%|██████▊   | 272/400 [05:20<01:25,  1.49it/s]Running loglikelihood requests:  68%|██████▊   | 273/400 [05:20<01:23,  1.52it/s]Running loglikelihood requests:  68%|██████▊   | 274/400 [05:21<01:21,  1.54it/s]Running loglikelihood requests:  69%|██████▉   | 275/400 [05:21<01:20,  1.56it/s]Running loglikelihood requests:  69%|██████▉   | 276/400 [05:22<01:19,  1.56it/s]Running loglikelihood requests:  69%|██████▉   | 277/400 [05:23<01:17,  1.59it/s]Running loglikelihood requests:  70%|██████▉   | 278/400 [05:23<01:15,  1.62it/s]Running loglikelihood requests:  70%|██████▉   | 279/400 [05:24<01:14,  1.63it/s]Running loglikelihood requests:  70%|███████   | 280/400 [05:25<01:13,  1.64it/s]Running loglikelihood requests:  70%|███████   | 281/400 [05:25<01:11,  1.65it/s]Running loglikelihood requests:  70%|███████   | 282/400 [05:26<01:11,  1.66it/s]Running loglikelihood requests:  71%|███████   | 283/400 [05:26<01:10,  1.66it/s]Running loglikelihood requests:  71%|███████   | 284/400 [05:27<01:09,  1.66it/s]Running loglikelihood requests:  71%|███████▏  | 285/400 [05:28<01:09,  1.67it/s]Running loglikelihood requests:  72%|███████▏  | 286/400 [05:28<01:08,  1.67it/s]Running loglikelihood requests:  72%|███████▏  | 287/400 [05:29<01:07,  1.68it/s]Running loglikelihood requests:  72%|███████▏  | 288/400 [05:29<01:06,  1.68it/s]Running loglikelihood requests:  72%|███████▏  | 289/400 [05:30<01:06,  1.68it/s]Running loglikelihood requests:  72%|███████▎  | 290/400 [05:30<01:05,  1.68it/s]Running loglikelihood requests:  73%|███████▎  | 291/400 [05:31<01:04,  1.69it/s]Running loglikelihood requests:  73%|███████▎  | 292/400 [05:32<01:03,  1.70it/s]Running loglikelihood requests:  73%|███████▎  | 293/400 [05:32<01:02,  1.71it/s]Running loglikelihood requests:  74%|███████▎  | 294/400 [05:33<01:01,  1.71it/s]Running loglikelihood requests:  74%|███████▍  | 295/400 [05:33<01:00,  1.72it/s]Running loglikelihood requests:  74%|███████▍  | 296/400 [05:34<01:00,  1.73it/s]Running loglikelihood requests:  74%|███████▍  | 297/400 [05:35<00:59,  1.73it/s]Running loglikelihood requests:  74%|███████▍  | 298/400 [05:35<00:58,  1.73it/s]Running loglikelihood requests:  75%|███████▍  | 299/400 [05:36<00:58,  1.74it/s]Running loglikelihood requests:  75%|███████▌  | 300/400 [05:36<00:57,  1.74it/s]Running loglikelihood requests:  75%|███████▌  | 301/400 [05:37<00:56,  1.74it/s]Running loglikelihood requests:  76%|███████▌  | 302/400 [05:37<00:56,  1.74it/s]Running loglikelihood requests:  76%|███████▌  | 303/400 [05:38<00:55,  1.75it/s]Running loglikelihood requests:  76%|███████▌  | 304/400 [05:39<00:54,  1.75it/s]Running loglikelihood requests:  76%|███████▋  | 305/400 [05:39<00:54,  1.75it/s]Running loglikelihood requests:  76%|███████▋  | 306/400 [05:40<00:53,  1.75it/s]Running loglikelihood requests:  77%|███████▋  | 307/400 [05:40<00:52,  1.75it/s]Running loglikelihood requests:  78%|███████▊  | 310/400 [05:41<00:31,  2.82it/s]Running loglikelihood requests:  78%|███████▊  | 311/400 [05:41<00:35,  2.51it/s]Running loglikelihood requests:  78%|███████▊  | 312/400 [05:42<00:38,  2.29it/s]Running loglikelihood requests:  78%|███████▊  | 313/400 [05:42<00:40,  2.14it/s]Running loglikelihood requests:  78%|███████▊  | 314/400 [05:43<00:42,  2.03it/s]Running loglikelihood requests:  79%|███████▉  | 315/400 [05:44<00:43,  1.95it/s]Running loglikelihood requests:  79%|███████▉  | 316/400 [05:44<00:44,  1.90it/s]Running loglikelihood requests:  79%|███████▉  | 317/400 [05:45<00:44,  1.88it/s]Running loglikelihood requests:  80%|███████▉  | 318/400 [05:45<00:44,  1.85it/s]Running loglikelihood requests:  80%|███████▉  | 319/400 [05:46<00:43,  1.85it/s]Running loglikelihood requests:  80%|████████  | 321/400 [05:46<00:32,  2.40it/s]Running loglikelihood requests:  80%|████████  | 322/400 [05:47<00:34,  2.25it/s]Running loglikelihood requests:  81%|████████  | 323/400 [05:47<00:36,  2.13it/s]Running loglikelihood requests:  81%|████████  | 324/400 [05:48<00:37,  2.05it/s]Running loglikelihood requests:  81%|████████▏ | 325/400 [05:49<00:37,  2.00it/s]Running loglikelihood requests:  82%|████████▏ | 326/400 [05:49<00:37,  1.96it/s]Running loglikelihood requests:  82%|████████▏ | 327/400 [05:50<00:37,  1.95it/s]Running loglikelihood requests:  82%|████████▏ | 328/400 [05:50<00:37,  1.93it/s]Running loglikelihood requests:  82%|████████▏ | 329/400 [05:51<00:36,  1.92it/s]Running loglikelihood requests:  82%|████████▎ | 330/400 [05:51<00:36,  1.92it/s]Running loglikelihood requests:  83%|████████▎ | 331/400 [05:52<00:36,  1.91it/s]Running loglikelihood requests:  83%|████████▎ | 332/400 [05:52<00:35,  1.91it/s]Running loglikelihood requests:  83%|████████▎ | 333/400 [05:53<00:34,  1.92it/s]Running loglikelihood requests:  84%|████████▎ | 334/400 [05:53<00:34,  1.94it/s]Running loglikelihood requests:  84%|████████▍ | 335/400 [05:54<00:33,  1.94it/s]Running loglikelihood requests:  84%|████████▍ | 336/400 [05:54<00:32,  1.95it/s]Running loglikelihood requests:  84%|████████▍ | 337/400 [05:55<00:32,  1.96it/s]Running loglikelihood requests:  84%|████████▍ | 338/400 [05:55<00:31,  1.97it/s]Running loglikelihood requests:  85%|████████▍ | 339/400 [05:56<00:30,  1.97it/s]Running loglikelihood requests:  85%|████████▌ | 340/400 [05:56<00:30,  1.98it/s]Running loglikelihood requests:  85%|████████▌ | 341/400 [05:57<00:29,  1.98it/s]Running loglikelihood requests:  86%|████████▌ | 342/400 [05:57<00:29,  1.98it/s]Running loglikelihood requests:  86%|████████▌ | 343/400 [05:58<00:28,  1.98it/s]Running loglikelihood requests:  86%|████████▌ | 344/400 [05:58<00:28,  1.99it/s]Running loglikelihood requests:  86%|████████▋ | 345/400 [05:59<00:27,  1.99it/s]Running loglikelihood requests:  86%|████████▋ | 346/400 [05:59<00:27,  2.00it/s]Running loglikelihood requests:  87%|████████▋ | 347/400 [06:00<00:26,  2.00it/s]Running loglikelihood requests:  87%|████████▋ | 348/400 [06:00<00:25,  2.00it/s]Running loglikelihood requests:  87%|████████▋ | 349/400 [06:01<00:25,  2.00it/s]Running loglikelihood requests:  88%|████████▊ | 350/400 [06:01<00:24,  2.00it/s]Running loglikelihood requests:  88%|████████▊ | 351/400 [06:02<00:24,  2.01it/s]Running loglikelihood requests:  88%|████████▊ | 352/400 [06:02<00:23,  2.00it/s]Running loglikelihood requests:  88%|████████▊ | 353/400 [06:03<00:23,  2.01it/s]Running loglikelihood requests:  88%|████████▊ | 354/400 [06:03<00:22,  2.02it/s]Running loglikelihood requests:  89%|████████▉ | 355/400 [06:04<00:22,  2.02it/s]Running loglikelihood requests:  89%|████████▉ | 356/400 [06:04<00:21,  2.02it/s]Running loglikelihood requests:  89%|████████▉ | 357/400 [06:05<00:21,  2.03it/s]Running loglikelihood requests:  90%|████████▉ | 358/400 [06:05<00:20,  2.03it/s]Running loglikelihood requests:  90%|████████▉ | 359/400 [06:06<00:20,  2.04it/s]Running loglikelihood requests:  90%|█████████ | 360/400 [06:06<00:19,  2.04it/s]Running loglikelihood requests:  90%|█████████ | 361/400 [06:07<00:19,  2.04it/s]Running loglikelihood requests:  90%|█████████ | 362/400 [06:07<00:18,  2.04it/s]Running loglikelihood requests:  91%|█████████ | 363/400 [06:08<00:18,  2.05it/s]Running loglikelihood requests:  91%|█████████ | 364/400 [06:08<00:17,  2.06it/s]Running loglikelihood requests:  91%|█████████▏| 365/400 [06:09<00:17,  2.06it/s]Running loglikelihood requests:  92%|█████████▏| 366/400 [06:09<00:16,  2.06it/s]Running loglikelihood requests:  92%|█████████▏| 368/400 [06:10<00:11,  2.68it/s]Running loglikelihood requests:  92%|█████████▏| 369/400 [06:10<00:12,  2.50it/s]Running loglikelihood requests:  92%|█████████▎| 370/400 [06:11<00:12,  2.37it/s]Running loglikelihood requests:  93%|█████████▎| 371/400 [06:11<00:12,  2.28it/s]Running loglikelihood requests:  93%|█████████▎| 373/400 [06:12<00:09,  2.83it/s]Running loglikelihood requests:  94%|█████████▎| 374/400 [06:12<00:09,  2.61it/s]Running loglikelihood requests:  94%|█████████▍| 375/400 [06:12<00:10,  2.45it/s]Running loglikelihood requests:  94%|█████████▍| 376/400 [06:13<00:10,  2.35it/s]Running loglikelihood requests:  94%|█████████▍| 377/400 [06:13<00:10,  2.28it/s]Running loglikelihood requests:  94%|█████████▍| 378/400 [06:14<00:09,  2.24it/s]Running loglikelihood requests:  95%|█████████▍| 379/400 [06:14<00:09,  2.21it/s]Running loglikelihood requests:  95%|█████████▌| 380/400 [06:15<00:09,  2.19it/s]Running loglikelihood requests:  95%|█████████▌| 381/400 [06:15<00:08,  2.18it/s]Running loglikelihood requests:  96%|█████████▌| 382/400 [06:16<00:08,  2.17it/s]Running loglikelihood requests:  96%|█████████▌| 383/400 [06:16<00:07,  2.17it/s]Running loglikelihood requests:  96%|█████████▌| 384/400 [06:17<00:07,  2.18it/s]Running loglikelihood requests:  96%|█████████▋| 385/400 [06:17<00:06,  2.18it/s]Running loglikelihood requests:  96%|█████████▋| 386/400 [06:18<00:06,  2.19it/s]Running loglikelihood requests:  97%|█████████▋| 387/400 [06:18<00:05,  2.19it/s]Running loglikelihood requests:  97%|█████████▋| 388/400 [06:18<00:05,  2.20it/s]Running loglikelihood requests:  97%|█████████▋| 389/400 [06:19<00:05,  2.20it/s]Running loglikelihood requests:  98%|█████████▊| 390/400 [06:19<00:04,  2.20it/s]Running loglikelihood requests:  98%|█████████▊| 391/400 [06:20<00:04,  2.20it/s]Running loglikelihood requests:  98%|█████████▊| 392/400 [06:20<00:03,  2.20it/s]Running loglikelihood requests:  98%|█████████▊| 393/400 [06:21<00:03,  2.21it/s]Running loglikelihood requests:  98%|█████████▊| 394/400 [06:21<00:02,  2.21it/s]Running loglikelihood requests:  99%|█████████▉| 395/400 [06:22<00:02,  2.21it/s]Running loglikelihood requests:  99%|█████████▉| 396/400 [06:22<00:01,  2.21it/s]Running loglikelihood requests:  99%|█████████▉| 397/400 [06:23<00:01,  2.22it/s]Running loglikelihood requests: 100%|█████████▉| 398/400 [06:23<00:00,  2.23it/s]Running loglikelihood requests: 100%|█████████▉| 399/400 [06:23<00:00,  2.23it/s]Running loglikelihood requests: 100%|██████████| 400/400 [06:24<00:00,  2.24it/s]Running loglikelihood requests: 100%|██████████| 400/400 [06:24<00:00,  1.04it/s]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:5'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:5'}
full model:
{'sciq': {'alias': 'sciq', 'acc,none': 0.94, 'acc_stderr,none': 0.023868325657594204, 'acc_norm,none': 0.91, 'acc_norm_stderr,none': 0.028762349126466136}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.969062788859705
0.9024924890572922
0.7706109127217512
0.8221264026535647
0.9190490061886575
0.9866654579796295
0.6586322754204971
0.7962110384246164
0.8195614021629236
0.7124178311176441
0.787697814339696
0.7034455022322618
0.8136386046534271
0.8174990104652458
0.6784276389594894
0.8698440245672888
0.8886492811850213
0.6541737276411673
0.6560861559753316
0.8139845219953913
0.6714741870309046
0.6164364868717988
0.8331581872497299
0.9065420049234512
0.9246185715568276
0.7477515960551026
0.574165362968651
0.8586446364199891
0.8889771415746612
Total groups 70 exceeded the threshold, stopping comparison.
The group tensor is
[7, 3, 4, 2, 6, 1, 5, 0]
tensor([7, 3, 4, 2, 6, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 5, 3, 4, 0, 7, 1]
tensor([6, 2, 5, 3, 4, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 3, 6, 2, 7, 1, 4, 0]
tensor([5, 3, 6, 2, 7, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 4, 2, 1, 3, 5, 1]
tensor([0, 0, 4, 2, 1, 3, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 3, 4, 5, 0, 1, 1]
tensor([0, 2, 3, 4, 5, 0, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 0, 2, 2, 3, 1]
tensor([0, 3, 1, 0, 2, 2, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 3, 1, 1, 2, 2, 3, 0]
tensor([0, 3, 1, 1, 2, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 1, 1.0, 1.0, 1.0, 1.0, 1]
tensor([0, 0, 1, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 2 to 4
done!
Normal merging for layer 5
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 6 to 9
done!
Normal merging for layer 10
tensor([0, 1])
tensor(0)
tensor([4, 7])
tensor(4)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 11 to 12
done!
Normal merging for layer 13
tensor([0, 5])
tensor(0)
tensor([6, 7])
tensor(6)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 14 to 19
done!
Normal merging for layer 20
tensor([0, 3])
tensor(0)
tensor([2, 7])
tensor(2)
tensor([4, 5])
tensor(4)
tensor([1, 6])
tensor(1)
done!
Normal merging for layer 21
tensor([0, 7])
tensor(0)
tensor([2, 3])
tensor(2)
tensor([4, 5])
tensor(4)
tensor([1, 6])
tensor(1)
done!
Cross-layer merge completed for layers 22 to 23
done!
Normal merging for layer 24
tensor([0, 1])
tensor(0)
tensor([2, 3, 4, 5, 6, 7])
tensor(2)
done!
Cross-layer merge completed for layers 25 to 31
done!
all done!
Model size: 12.2608 GB
189
cuda:5
wsc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:41<00:41, 41.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 25.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 27.53s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wsc] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wsc] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/wsc.fixed?recursive=False&expand=False HTTP/1.1" 307 148
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/wsc.fixed?recursive=False&expand=False HTTP/1.1" 200 356
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:filelock:Attempting to acquire lock 140229878714960 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wsc.fixed_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140229878714960 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wsc.fixed_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140229878714960 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wsc.fixed_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140229878714960 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wsc.fixed_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140212902829552 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140212902829552 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140212902829552 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140212902829552 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wsc from None to 0
INFO:lm_eval.api.task:Building contexts for wsc on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 86249.31it/s]
DEBUG:lm_eval.evaluator:Task: wsc; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<04:24,  1.33s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:07,  1.54it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:02<01:07,  2.87it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:03<01:10,  2.72it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:04<01:11,  2.64it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:05<01:12,  2.59it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:06<01:12,  2.55it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:07<01:12,  2.53it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:07<01:11,  2.53it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:08<01:10,  2.53it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:09<01:09,  2.54it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:10<01:08,  2.54it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:10<01:07,  2.54it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:11<01:07,  2.54it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:12<01:06,  2.54it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:13<01:05,  2.55it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:14<01:04,  2.55it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:14<01:03,  2.56it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:15<01:02,  2.56it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:16<01:02,  2.56it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:17<01:01,  2.56it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:17<01:00,  2.57it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:18<00:59,  2.58it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:19<00:58,  2.59it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:20<00:57,  2.60it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:21<00:56,  2.61it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:21<00:55,  2.61it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:22<00:54,  2.63it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:23<00:53,  2.65it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:24<00:52,  2.65it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:24<00:51,  2.67it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:25<00:50,  2.68it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:26<00:49,  2.69it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:26<00:48,  2.70it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:27<00:47,  2.70it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:28<00:46,  2.72it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:29<00:45,  2.73it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:29<00:44,  2.74it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:30<00:44,  2.75it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:31<00:43,  2.76it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:32<00:41,  2.79it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:32<00:40,  2.82it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:33<00:39,  2.83it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:34<00:38,  2.85it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:34<00:38,  2.86it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:35<00:37,  2.87it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:36<00:36,  2.88it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:36<00:35,  2.91it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:37<00:34,  2.94it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:38<00:33,  2.97it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:38<00:32,  2.98it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:39<00:31,  3.00it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:40<00:30,  3.00it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:40<00:30,  3.01it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:41<00:29,  3.02it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:42<00:28,  3.06it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:42<00:20,  4.02it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:43<00:16,  4.72it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:44<00:18,  4.27it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:44<00:18,  3.96it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:45<00:19,  3.74it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:45<00:19,  3.59it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:46<00:19,  3.50it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:47<00:19,  3.44it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:47<00:19,  3.40it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:48<00:18,  3.38it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:48<00:18,  3.37it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:49<00:17,  3.35it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:50<00:16,  3.35it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [00:50<00:16,  3.36it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [00:51<00:15,  3.37it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [00:51<00:15,  3.38it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [00:52<00:14,  3.39it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [00:53<00:13,  3.40it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [00:53<00:13,  3.41it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [00:54<00:12,  3.42it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [00:54<00:11,  3.43it/s]Running loglikelihood requests:  80%|████████  | 161/200 [00:55<00:11,  3.44it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [00:55<00:10,  3.45it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [00:56<00:10,  3.45it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [00:57<00:09,  3.45it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [00:57<00:08,  3.45it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [00:58<00:08,  3.46it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [00:58<00:07,  3.47it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [00:59<00:07,  3.47it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [00:59<00:06,  3.49it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:00<00:05,  3.51it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:01<00:05,  3.53it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:01<00:04,  3.54it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:02<00:04,  3.55it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:02<00:03,  3.56it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:03<00:03,  3.58it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:03<00:02,  3.59it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:04<00:01,  3.61it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:04<00:01,  3.63it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:05<00:00,  3.65it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:06<00:00,  3.67it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:06<00:00,  3.03it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:6'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:6'}
full model:
{'wsc': {'alias': 'wsc', 'acc,none': 0.39, 'acc_stderr,none': 0.04902071300001973}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8279363410631274
0.19299873358047231
0.24697427628971416
0.9498246079459217
0.9208790959024347
0.5433529265767616
0.7541489406957872
0.8642404861438043
0.9451768410410958
0.6364904470004392
0.8605038470072015
0.8806563523485148
0.7320941418431636
0.7167995199876819
0.9558598958577776
0.7974395317006763
0.8001507378395173
0.8689133172877098
0.6142386786363212
0.6039861720637983
0.8068446902119512
0.6154342295548068
0.8415314242927469
0.42042589459680124
0.7443787948515065
0.7081725962998161
0.7223774755032815
0.9338202429729527
0.7700364698774241
0.8279363410631274
0.19299873358047231
0.24697427628971416
0.9498246079459217
0.9208790959024347
0.5433529265767616
0.7541489406957872
0.8642404861438043
0.9451768410410958
0.6364904470004392
0.8605038470072015
0.8806563523485148
0.7320941418431636
0.7167995199876819
0.9558598958577776
0.7974395317006763
0.8001507378395173
0.8689133172877098
0.6142386786363212
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[6, 7, 2, 4, 5, 1, 3, 0]
tensor([6, 7, 2, 4, 5, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 7, 3, 6, 2, 5, 4, 0]
tensor([1, 7, 3, 6, 2, 5, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 5, 6, 3, 7, 4, 2, 0]
tensor([1, 5, 6, 3, 7, 4, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 5, 3, 6, 7, 4, 2, 0]
tensor([1, 5, 3, 6, 7, 4, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 7, 5, 6, 1, 2, 3, 0]
tensor([4, 7, 5, 6, 1, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 7, 1, 5, 6, 3, 2, 0]
tensor([4, 7, 1, 5, 6, 3, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1, 1.0, 0, 1.0]
tensor([0, 1, 1, 1, 1, 1, 0, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 3 to 4
done!
Normal merging for layer 5
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
done!
Normal merging for layer 6
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
done!
Normal merging for layer 7
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
done!
Cross-layer merge completed for layers 8 to 28
done!
Normal merging for layer 29
tensor([0, 6])
tensor(0)
tensor([1, 2, 3, 4, 5, 7])
tensor(1)
done!
Cross-layer merge completed for layers 30 to 31
done!
all done!
Model size: 12.3238 GB
210
cuda:6
logiqa
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:41<00:41, 41.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 24.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 27.09s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/logiqa HTTP/1.1" 200 743
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/EleutherAI/logiqa/EleutherAI/logiqa.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): datasets-server.hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://datasets-server.hf-mirror.com:443 "GET /parquet?dataset=EleutherAI/logiqa HTTP/1.1" 302 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET / HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/EleutherAI/logiqa/EleutherAI/logiqa.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/logiqa/resolve/main/logiqa.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/logiqa/resolve/main/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/logiqa/resolve/main/README.md HTTP/1.1" 200 0
DEBUG:filelock:Attempting to acquire lock 140215183331632 on /public/home/zouyifei001/.cache/huggingface/modules/datasets_modules/datasets/EleutherAI--logiqa.lock
DEBUG:filelock:Lock 140215183331632 acquired on /public/home/zouyifei001/.cache/huggingface/modules/datasets_modules/datasets/EleutherAI--logiqa.lock
DEBUG:filelock:Attempting to release lock 140215183331632 on /public/home/zouyifei001/.cache/huggingface/modules/datasets_modules/datasets/EleutherAI--logiqa.lock
DEBUG:filelock:Lock 140215183331632 released on /public/home/zouyifei001/.cache/huggingface/modules/datasets_modules/datasets/EleutherAI--logiqa.lock
DEBUG:filelock:Attempting to acquire lock 140215720127920 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___logiqa_logiqa_0.0.1_5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81.lock
DEBUG:filelock:Lock 140215720127920 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___logiqa_logiqa_0.0.1_5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215720127920 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___logiqa_logiqa_0.0.1_5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81.lock
DEBUG:filelock:Lock 140215720127920 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___logiqa_logiqa_0.0.1_5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81.lock
DEBUG:filelock:Attempting to acquire lock 140236790979792 on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81_builder.lock
DEBUG:filelock:Lock 140236790979792 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81/dataset_info.json
DEBUG:filelock:Attempting to release lock 140236790979792 on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81_builder.lock
DEBUG:filelock:Lock 140236790979792 released on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of logiqa from None to 0
INFO:lm_eval.api.task:Building contexts for logiqa on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 3047.88it/s]
DEBUG:lm_eval.evaluator:Task: logiqa; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:02<19:12,  2.89s/it]Running loglikelihood requests:   0%|          | 2/400 [00:05<17:15,  2.60s/it]Running loglikelihood requests:   1%|          | 3/400 [00:07<16:24,  2.48s/it]Running loglikelihood requests:   1%|          | 4/400 [00:09<15:52,  2.41s/it]Running loglikelihood requests:   1%|▏         | 5/400 [00:12<15:27,  2.35s/it]Running loglikelihood requests:   2%|▏         | 6/400 [00:14<15:11,  2.31s/it]Running loglikelihood requests:   2%|▏         | 7/400 [00:16<15:00,  2.29s/it]Running loglikelihood requests:   2%|▏         | 8/400 [00:18<14:50,  2.27s/it]Running loglikelihood requests:   2%|▏         | 9/400 [00:21<14:40,  2.25s/it]Running loglikelihood requests:   2%|▎         | 10/400 [00:23<14:31,  2.23s/it]Running loglikelihood requests:   3%|▎         | 11/400 [00:25<14:19,  2.21s/it]Running loglikelihood requests:   3%|▎         | 12/400 [00:27<14:06,  2.18s/it]Running loglikelihood requests:   3%|▎         | 13/400 [00:29<13:55,  2.16s/it]Running loglikelihood requests:   4%|▎         | 14/400 [00:31<13:46,  2.14s/it]Running loglikelihood requests:   4%|▍         | 15/400 [00:33<13:40,  2.13s/it]Running loglikelihood requests:   4%|▍         | 16/400 [00:35<13:34,  2.12s/it]Running loglikelihood requests:   4%|▍         | 17/400 [00:38<13:30,  2.12s/it]Running loglikelihood requests:   4%|▍         | 18/400 [00:40<13:25,  2.11s/it]Running loglikelihood requests:   5%|▍         | 19/400 [00:42<13:19,  2.10s/it]Running loglikelihood requests:   5%|▌         | 20/400 [00:44<13:15,  2.09s/it]Running loglikelihood requests:   5%|▌         | 21/400 [00:46<13:09,  2.08s/it]Running loglikelihood requests:   6%|▌         | 22/400 [00:48<13:04,  2.08s/it]Running loglikelihood requests:   6%|▌         | 23/400 [00:50<12:59,  2.07s/it]Running loglikelihood requests:   6%|▌         | 24/400 [00:52<12:55,  2.06s/it]Running loglikelihood requests:   6%|▋         | 25/400 [00:54<12:49,  2.05s/it]Running loglikelihood requests:   6%|▋         | 26/400 [00:56<12:43,  2.04s/it]Running loglikelihood requests:   7%|▋         | 27/400 [00:58<12:37,  2.03s/it]Running loglikelihood requests:   7%|▋         | 28/400 [01:00<12:33,  2.03s/it]Running loglikelihood requests:   7%|▋         | 29/400 [01:02<12:28,  2.02s/it]Running loglikelihood requests:   8%|▊         | 30/400 [01:04<12:25,  2.01s/it]Running loglikelihood requests:   8%|▊         | 31/400 [01:06<12:27,  2.02s/it]Running loglikelihood requests:   8%|▊         | 32/400 [01:08<12:19,  2.01s/it]Running loglikelihood requests:   8%|▊         | 33/400 [01:10<12:12,  2.00s/it]Running loglikelihood requests:   8%|▊         | 34/400 [01:12<12:07,  1.99s/it]Running loglikelihood requests:   9%|▉         | 35/400 [01:14<12:02,  1.98s/it]Running loglikelihood requests:   9%|▉         | 36/400 [01:16<12:06,  2.00s/it]Running loglikelihood requests:   9%|▉         | 37/400 [01:18<11:59,  1.98s/it]Running loglikelihood requests:  10%|▉         | 38/400 [01:20<11:46,  1.95s/it]Running loglikelihood requests:  10%|▉         | 39/400 [01:22<11:35,  1.93s/it]Running loglikelihood requests:  10%|█         | 40/400 [01:24<11:27,  1.91s/it]Running loglikelihood requests:  10%|█         | 41/400 [01:25<11:20,  1.90s/it]Running loglikelihood requests:  10%|█         | 42/400 [01:27<11:14,  1.88s/it]Running loglikelihood requests:  11%|█         | 43/400 [01:29<11:09,  1.87s/it]Running loglikelihood requests:  11%|█         | 44/400 [01:31<11:04,  1.87s/it]Running loglikelihood requests:  11%|█▏        | 45/400 [01:33<11:00,  1.86s/it]Running loglikelihood requests:  12%|█▏        | 46/400 [01:35<10:56,  1.85s/it]Running loglikelihood requests:  12%|█▏        | 47/400 [01:37<10:52,  1.85s/it]Running loglikelihood requests:  12%|█▏        | 48/400 [01:38<10:48,  1.84s/it]Running loglikelihood requests:  12%|█▏        | 49/400 [01:40<10:47,  1.84s/it]Running loglikelihood requests:  12%|█▎        | 50/400 [01:42<10:46,  1.85s/it]Running loglikelihood requests:  13%|█▎        | 51/400 [01:44<10:46,  1.85s/it]Running loglikelihood requests:  13%|█▎        | 52/400 [01:46<10:41,  1.84s/it]Running loglikelihood requests:  13%|█▎        | 53/400 [01:48<10:37,  1.84s/it]Running loglikelihood requests:  14%|█▎        | 54/400 [01:49<10:33,  1.83s/it]Running loglikelihood requests:  14%|█▍        | 55/400 [01:51<10:28,  1.82s/it]Running loglikelihood requests:  14%|█▍        | 56/400 [01:53<10:24,  1.81s/it]Running loglikelihood requests:  14%|█▍        | 57/400 [01:55<10:17,  1.80s/it]Running loglikelihood requests:  14%|█▍        | 58/400 [01:57<10:13,  1.79s/it]Running loglikelihood requests:  15%|█▍        | 59/400 [01:58<10:09,  1.79s/it]Running loglikelihood requests:  15%|█▌        | 60/400 [02:00<10:04,  1.78s/it]Running loglikelihood requests:  15%|█▌        | 61/400 [02:02<09:58,  1.77s/it]Running loglikelihood requests:  16%|█▌        | 62/400 [02:04<09:52,  1.75s/it]Running loglikelihood requests:  16%|█▌        | 63/400 [02:05<09:47,  1.74s/it]Running loglikelihood requests:  16%|█▌        | 64/400 [02:07<09:44,  1.74s/it]Running loglikelihood requests:  16%|█▋        | 65/400 [02:09<09:38,  1.73s/it]Running loglikelihood requests:  16%|█▋        | 66/400 [02:10<09:34,  1.72s/it]Running loglikelihood requests:  17%|█▋        | 67/400 [02:12<09:29,  1.71s/it]Running loglikelihood requests:  17%|█▋        | 68/400 [02:14<09:24,  1.70s/it]Running loglikelihood requests:  17%|█▋        | 69/400 [02:15<09:20,  1.69s/it]Running loglikelihood requests:  18%|█▊        | 70/400 [02:17<09:16,  1.69s/it]Running loglikelihood requests:  18%|█▊        | 71/400 [02:19<09:14,  1.68s/it]Running loglikelihood requests:  18%|█▊        | 72/400 [02:20<09:10,  1.68s/it]Running loglikelihood requests:  18%|█▊        | 73/400 [02:22<09:05,  1.67s/it]Running loglikelihood requests:  18%|█▊        | 74/400 [02:24<09:00,  1.66s/it]Running loglikelihood requests:  19%|█▉        | 75/400 [02:25<08:56,  1.65s/it]Running loglikelihood requests:  19%|█▉        | 76/400 [02:27<08:52,  1.64s/it]Running loglikelihood requests:  19%|█▉        | 77/400 [02:29<08:47,  1.63s/it]Running loglikelihood requests:  20%|█▉        | 78/400 [02:30<08:42,  1.62s/it]Running loglikelihood requests:  20%|█▉        | 79/400 [02:32<08:38,  1.62s/it]Running loglikelihood requests:  20%|██        | 80/400 [02:33<08:36,  1.61s/it]Running loglikelihood requests:  20%|██        | 81/400 [02:35<08:32,  1.61s/it]Running loglikelihood requests:  20%|██        | 82/400 [02:37<08:28,  1.60s/it]Running loglikelihood requests:  21%|██        | 83/400 [02:38<08:24,  1.59s/it]Running loglikelihood requests:  21%|██        | 84/400 [02:40<08:22,  1.59s/it]Running loglikelihood requests:  21%|██▏       | 85/400 [02:41<08:18,  1.58s/it]Running loglikelihood requests:  22%|██▏       | 86/400 [02:43<08:15,  1.58s/it]Running loglikelihood requests:  22%|██▏       | 87/400 [02:44<08:12,  1.57s/it]Running loglikelihood requests:  22%|██▏       | 88/400 [02:46<08:09,  1.57s/it]Running loglikelihood requests:  22%|██▏       | 89/400 [02:48<08:08,  1.57s/it]Running loglikelihood requests:  22%|██▎       | 90/400 [02:49<08:06,  1.57s/it]Running loglikelihood requests:  23%|██▎       | 91/400 [02:51<08:04,  1.57s/it]Running loglikelihood requests:  23%|██▎       | 92/400 [02:52<08:02,  1.57s/it]Running loglikelihood requests:  23%|██▎       | 93/400 [02:54<08:00,  1.56s/it]Running loglikelihood requests:  24%|██▎       | 94/400 [02:55<07:57,  1.56s/it]Running loglikelihood requests:  24%|██▍       | 95/400 [02:57<07:55,  1.56s/it]Running loglikelihood requests:  24%|██▍       | 96/400 [02:58<07:52,  1.55s/it]Running loglikelihood requests:  24%|██▍       | 97/400 [03:00<07:50,  1.55s/it]Running loglikelihood requests:  24%|██▍       | 98/400 [03:02<07:47,  1.55s/it]Running loglikelihood requests:  25%|██▍       | 99/400 [03:03<07:46,  1.55s/it]Running loglikelihood requests:  25%|██▌       | 100/400 [03:05<07:44,  1.55s/it]Running loglikelihood requests:  25%|██▌       | 101/400 [03:06<07:42,  1.55s/it]Running loglikelihood requests:  26%|██▌       | 102/400 [03:08<07:42,  1.55s/it]Running loglikelihood requests:  26%|██▌       | 103/400 [03:09<07:41,  1.56s/it]Running loglikelihood requests:  26%|██▌       | 104/400 [03:11<07:41,  1.56s/it]Running loglikelihood requests:  26%|██▋       | 105/400 [03:12<07:38,  1.55s/it]Running loglikelihood requests:  26%|██▋       | 106/400 [03:14<07:35,  1.55s/it]Running loglikelihood requests:  27%|██▋       | 107/400 [03:16<07:32,  1.54s/it]Running loglikelihood requests:  27%|██▋       | 108/400 [03:17<07:28,  1.53s/it]Running loglikelihood requests:  27%|██▋       | 109/400 [03:19<07:24,  1.53s/it]Running loglikelihood requests:  28%|██▊       | 110/400 [03:20<07:22,  1.52s/it]Running loglikelihood requests:  28%|██▊       | 111/400 [03:22<07:19,  1.52s/it]Running loglikelihood requests:  28%|██▊       | 112/400 [03:23<07:16,  1.52s/it]Running loglikelihood requests:  28%|██▊       | 113/400 [03:25<07:14,  1.51s/it]Running loglikelihood requests:  28%|██▊       | 114/400 [03:26<07:11,  1.51s/it]Running loglikelihood requests:  29%|██▉       | 115/400 [03:28<07:08,  1.51s/it]Running loglikelihood requests:  29%|██▉       | 116/400 [03:29<07:06,  1.50s/it]Running loglikelihood requests:  29%|██▉       | 117/400 [03:31<07:04,  1.50s/it]Running loglikelihood requests:  30%|██▉       | 118/400 [03:32<07:02,  1.50s/it]Running loglikelihood requests:  30%|██▉       | 119/400 [03:34<06:59,  1.49s/it]Running loglikelihood requests:  30%|███       | 120/400 [03:35<06:57,  1.49s/it]Running loglikelihood requests:  30%|███       | 121/400 [03:37<06:55,  1.49s/it]Running loglikelihood requests:  30%|███       | 122/400 [03:38<06:53,  1.49s/it]Running loglikelihood requests:  31%|███       | 123/400 [03:39<06:51,  1.48s/it]Running loglikelihood requests:  31%|███       | 124/400 [03:41<06:48,  1.48s/it]Running loglikelihood requests:  31%|███▏      | 125/400 [03:42<06:46,  1.48s/it]Running loglikelihood requests:  32%|███▏      | 126/400 [03:44<06:45,  1.48s/it]Running loglikelihood requests:  32%|███▏      | 127/400 [03:45<06:42,  1.48s/it]Running loglikelihood requests:  32%|███▏      | 128/400 [03:47<06:40,  1.47s/it]Running loglikelihood requests:  32%|███▏      | 129/400 [03:48<06:38,  1.47s/it]Running loglikelihood requests:  32%|███▎      | 130/400 [03:50<06:36,  1.47s/it]Running loglikelihood requests:  33%|███▎      | 131/400 [03:51<06:34,  1.47s/it]Running loglikelihood requests:  33%|███▎      | 132/400 [03:53<06:32,  1.46s/it]Running loglikelihood requests:  33%|███▎      | 133/400 [03:54<06:30,  1.46s/it]Running loglikelihood requests:  34%|███▎      | 134/400 [03:56<06:28,  1.46s/it]Running loglikelihood requests:  34%|███▍      | 135/400 [03:57<06:27,  1.46s/it]Running loglikelihood requests:  34%|███▍      | 136/400 [03:59<06:26,  1.46s/it]Running loglikelihood requests:  34%|███▍      | 137/400 [04:00<06:25,  1.47s/it]Running loglikelihood requests:  34%|███▍      | 138/400 [04:01<06:24,  1.47s/it]Running loglikelihood requests:  35%|███▍      | 139/400 [04:03<06:22,  1.47s/it]Running loglikelihood requests:  35%|███▌      | 140/400 [04:04<06:21,  1.47s/it]Running loglikelihood requests:  36%|███▌      | 142/400 [04:06<04:50,  1.13s/it]Running loglikelihood requests:  36%|███▌      | 143/400 [04:07<05:11,  1.21s/it]Running loglikelihood requests:  36%|███▌      | 144/400 [04:09<05:26,  1.28s/it]Running loglikelihood requests:  36%|███▋      | 145/400 [04:10<05:37,  1.32s/it]Running loglikelihood requests:  36%|███▋      | 146/400 [04:12<05:45,  1.36s/it]Running loglikelihood requests:  37%|███▋      | 147/400 [04:13<05:50,  1.39s/it]Running loglikelihood requests:  37%|███▋      | 148/400 [04:15<05:54,  1.40s/it]Running loglikelihood requests:  37%|███▋      | 149/400 [04:16<05:56,  1.42s/it]Running loglikelihood requests:  38%|███▊      | 150/400 [04:18<05:56,  1.43s/it]Running loglikelihood requests:  38%|███▊      | 151/400 [04:19<05:56,  1.43s/it]Running loglikelihood requests:  38%|███▊      | 152/400 [04:20<05:56,  1.44s/it]Running loglikelihood requests:  38%|███▊      | 153/400 [04:22<05:55,  1.44s/it]Running loglikelihood requests:  38%|███▊      | 154/400 [04:23<05:54,  1.44s/it]Running loglikelihood requests:  39%|███▉      | 155/400 [04:25<05:53,  1.44s/it]Running loglikelihood requests:  39%|███▉      | 156/400 [04:26<05:51,  1.44s/it]Running loglikelihood requests:  39%|███▉      | 157/400 [04:28<05:50,  1.44s/it]Running loglikelihood requests:  40%|███▉      | 158/400 [04:29<05:48,  1.44s/it]Running loglikelihood requests:  40%|███▉      | 159/400 [04:30<05:47,  1.44s/it]Running loglikelihood requests:  40%|████      | 160/400 [04:32<05:45,  1.44s/it]Running loglikelihood requests:  40%|████      | 161/400 [04:33<05:43,  1.44s/it]Running loglikelihood requests:  40%|████      | 162/400 [04:35<05:42,  1.44s/it]Running loglikelihood requests:  41%|████      | 163/400 [04:36<05:42,  1.44s/it]Running loglikelihood requests:  41%|████      | 164/400 [04:38<05:40,  1.44s/it]Running loglikelihood requests:  41%|████▏     | 165/400 [04:39<05:39,  1.45s/it]Running loglikelihood requests:  42%|████▏     | 166/400 [04:41<05:37,  1.44s/it]Running loglikelihood requests:  42%|████▏     | 167/400 [04:42<05:34,  1.44s/it]Running loglikelihood requests:  42%|████▏     | 168/400 [04:43<05:32,  1.43s/it]Running loglikelihood requests:  42%|████▏     | 169/400 [04:45<05:29,  1.43s/it]Running loglikelihood requests:  42%|████▎     | 170/400 [04:46<05:28,  1.43s/it]Running loglikelihood requests:  43%|████▎     | 171/400 [04:48<05:26,  1.42s/it]Running loglikelihood requests:  43%|████▎     | 172/400 [04:49<05:24,  1.42s/it]Running loglikelihood requests:  43%|████▎     | 173/400 [04:51<05:22,  1.42s/it]Running loglikelihood requests:  44%|████▎     | 174/400 [04:52<05:20,  1.42s/it]Running loglikelihood requests:  44%|████▍     | 175/400 [04:53<05:17,  1.41s/it]Running loglikelihood requests:  44%|████▍     | 176/400 [04:55<05:15,  1.41s/it]Running loglikelihood requests:  44%|████▍     | 177/400 [04:56<05:13,  1.41s/it]Running loglikelihood requests:  44%|████▍     | 178/400 [04:58<05:11,  1.40s/it]Running loglikelihood requests:  45%|████▍     | 179/400 [04:59<05:10,  1.40s/it]Running loglikelihood requests:  45%|████▌     | 180/400 [05:00<05:07,  1.40s/it]Running loglikelihood requests:  45%|████▌     | 181/400 [05:02<05:05,  1.40s/it]Running loglikelihood requests:  46%|████▌     | 182/400 [05:03<05:04,  1.40s/it]Running loglikelihood requests:  46%|████▌     | 183/400 [05:04<05:02,  1.39s/it]Running loglikelihood requests:  46%|████▌     | 184/400 [05:06<05:00,  1.39s/it]Running loglikelihood requests:  46%|████▋     | 185/400 [05:07<04:58,  1.39s/it]Running loglikelihood requests:  46%|████▋     | 186/400 [05:09<04:56,  1.38s/it]Running loglikelihood requests:  47%|████▋     | 187/400 [05:10<04:54,  1.38s/it]Running loglikelihood requests:  47%|████▋     | 188/400 [05:11<04:52,  1.38s/it]Running loglikelihood requests:  47%|████▋     | 189/400 [05:13<04:51,  1.38s/it]Running loglikelihood requests:  48%|████▊     | 190/400 [05:14<04:49,  1.38s/it]Running loglikelihood requests:  48%|████▊     | 191/400 [05:16<04:47,  1.38s/it]Running loglikelihood requests:  48%|████▊     | 192/400 [05:17<04:46,  1.38s/it]Running loglikelihood requests:  48%|████▊     | 193/400 [05:18<04:44,  1.37s/it]Running loglikelihood requests:  48%|████▊     | 194/400 [05:20<04:41,  1.37s/it]Running loglikelihood requests:  49%|████▉     | 195/400 [05:21<04:40,  1.37s/it]Running loglikelihood requests:  49%|████▉     | 196/400 [05:22<04:37,  1.36s/it]Running loglikelihood requests:  49%|████▉     | 197/400 [05:24<04:35,  1.36s/it]Running loglikelihood requests:  50%|████▉     | 198/400 [05:25<04:34,  1.36s/it]Running loglikelihood requests:  50%|████▉     | 199/400 [05:26<04:32,  1.36s/it]Running loglikelihood requests:  50%|█████     | 200/400 [05:28<04:31,  1.36s/it]Running loglikelihood requests:  50%|█████     | 201/400 [05:29<04:30,  1.36s/it]Running loglikelihood requests:  50%|█████     | 202/400 [05:30<04:28,  1.36s/it]Running loglikelihood requests:  51%|█████     | 203/400 [05:32<04:26,  1.35s/it]Running loglikelihood requests:  51%|█████     | 204/400 [05:33<04:24,  1.35s/it]Running loglikelihood requests:  51%|█████▏    | 205/400 [05:35<04:23,  1.35s/it]Running loglikelihood requests:  52%|█████▏    | 206/400 [05:36<04:22,  1.35s/it]Running loglikelihood requests:  52%|█████▏    | 207/400 [05:37<04:20,  1.35s/it]Running loglikelihood requests:  52%|█████▏    | 208/400 [05:39<04:19,  1.35s/it]Running loglikelihood requests:  52%|█████▏    | 209/400 [05:40<04:17,  1.35s/it]Running loglikelihood requests:  52%|█████▎    | 210/400 [05:41<04:15,  1.35s/it]Running loglikelihood requests:  53%|█████▎    | 211/400 [05:43<04:14,  1.34s/it]Running loglikelihood requests:  53%|█████▎    | 212/400 [05:44<04:12,  1.34s/it]Running loglikelihood requests:  53%|█████▎    | 213/400 [05:45<04:11,  1.34s/it]Running loglikelihood requests:  54%|█████▎    | 214/400 [05:47<04:10,  1.34s/it]Running loglikelihood requests:  54%|█████▍    | 215/400 [05:48<04:08,  1.34s/it]Running loglikelihood requests:  54%|█████▍    | 216/400 [05:49<04:07,  1.34s/it]Running loglikelihood requests:  54%|█████▍    | 217/400 [05:51<04:06,  1.34s/it]Running loglikelihood requests:  55%|█████▍    | 218/400 [05:52<04:04,  1.34s/it]Running loglikelihood requests:  55%|█████▍    | 219/400 [05:53<04:02,  1.34s/it]Running loglikelihood requests:  55%|█████▌    | 220/400 [05:55<04:01,  1.34s/it]Running loglikelihood requests:  55%|█████▌    | 221/400 [05:56<03:59,  1.34s/it]Running loglikelihood requests:  56%|█████▌    | 222/400 [05:57<03:57,  1.34s/it]Running loglikelihood requests:  56%|█████▌    | 223/400 [05:59<03:56,  1.33s/it]Running loglikelihood requests:  56%|█████▌    | 224/400 [06:00<03:55,  1.34s/it]Running loglikelihood requests:  56%|█████▋    | 225/400 [06:01<03:53,  1.33s/it]Running loglikelihood requests:  56%|█████▋    | 226/400 [06:03<03:51,  1.33s/it]Running loglikelihood requests:  57%|█████▋    | 227/400 [06:04<03:50,  1.33s/it]Running loglikelihood requests:  57%|█████▋    | 228/400 [06:05<03:49,  1.34s/it]Running loglikelihood requests:  57%|█████▋    | 229/400 [06:07<03:48,  1.34s/it]Running loglikelihood requests:  57%|█████▊    | 230/400 [06:08<03:47,  1.34s/it]Running loglikelihood requests:  58%|█████▊    | 231/400 [06:09<03:45,  1.33s/it]Running loglikelihood requests:  58%|█████▊    | 232/400 [06:11<03:46,  1.35s/it]Running loglikelihood requests:  58%|█████▊    | 233/400 [06:12<03:48,  1.37s/it]Running loglikelihood requests:  58%|█████▊    | 234/400 [06:13<03:45,  1.36s/it]Running loglikelihood requests:  59%|█████▉    | 235/400 [06:15<03:44,  1.36s/it]Running loglikelihood requests:  59%|█████▉    | 236/400 [06:16<03:41,  1.35s/it]Running loglikelihood requests:  59%|█████▉    | 237/400 [06:17<03:38,  1.34s/it]Running loglikelihood requests:  60%|█████▉    | 238/400 [06:19<03:36,  1.33s/it]Running loglikelihood requests:  60%|█████▉    | 239/400 [06:20<03:33,  1.33s/it]Running loglikelihood requests:  60%|██████    | 240/400 [06:21<03:32,  1.33s/it]Running loglikelihood requests:  60%|██████    | 241/400 [06:23<03:30,  1.32s/it]Running loglikelihood requests:  60%|██████    | 242/400 [06:24<03:28,  1.32s/it]Running loglikelihood requests:  61%|██████    | 243/400 [06:25<03:27,  1.32s/it]Running loglikelihood requests:  61%|██████    | 244/400 [06:27<03:25,  1.32s/it]Running loglikelihood requests:  61%|██████▏   | 245/400 [06:28<03:24,  1.32s/it]Running loglikelihood requests:  62%|██████▏   | 246/400 [06:29<03:23,  1.32s/it]Running loglikelihood requests:  62%|██████▏   | 247/400 [06:31<03:21,  1.32s/it]Running loglikelihood requests:  62%|██████▏   | 248/400 [06:32<03:20,  1.32s/it]Running loglikelihood requests:  62%|██████▏   | 249/400 [06:33<03:18,  1.31s/it]Running loglikelihood requests:  62%|██████▎   | 250/400 [06:35<03:17,  1.31s/it]Running loglikelihood requests:  63%|██████▎   | 251/400 [06:36<03:15,  1.31s/it]Running loglikelihood requests:  63%|██████▎   | 252/400 [06:37<03:14,  1.31s/it]Running loglikelihood requests:  63%|██████▎   | 253/400 [06:39<03:12,  1.31s/it]Running loglikelihood requests:  64%|██████▎   | 254/400 [06:40<03:10,  1.31s/it]Running loglikelihood requests:  64%|██████▍   | 255/400 [06:41<03:09,  1.31s/it]Running loglikelihood requests:  64%|██████▍   | 256/400 [06:42<03:07,  1.30s/it]Running loglikelihood requests:  64%|██████▍   | 257/400 [06:44<03:06,  1.30s/it]Running loglikelihood requests:  64%|██████▍   | 258/400 [06:45<03:04,  1.30s/it]Running loglikelihood requests:  65%|██████▍   | 259/400 [06:46<03:03,  1.30s/it]Running loglikelihood requests:  65%|██████▌   | 260/400 [06:48<03:01,  1.30s/it]Running loglikelihood requests:  65%|██████▌   | 261/400 [06:49<03:00,  1.30s/it]Running loglikelihood requests:  66%|██████▌   | 262/400 [06:50<02:59,  1.30s/it]Running loglikelihood requests:  66%|██████▌   | 263/400 [06:51<02:56,  1.29s/it]Running loglikelihood requests:  66%|██████▌   | 264/400 [06:53<02:55,  1.29s/it]Running loglikelihood requests:  66%|██████▋   | 265/400 [06:54<02:53,  1.28s/it]Running loglikelihood requests:  66%|██████▋   | 266/400 [06:55<02:51,  1.28s/it]Running loglikelihood requests:  67%|██████▋   | 267/400 [06:57<02:50,  1.28s/it]Running loglikelihood requests:  67%|██████▋   | 268/400 [06:58<02:49,  1.28s/it]Running loglikelihood requests:  67%|██████▋   | 269/400 [06:59<02:48,  1.28s/it]Running loglikelihood requests:  68%|██████▊   | 270/400 [07:00<02:46,  1.28s/it]Running loglikelihood requests:  68%|██████▊   | 271/400 [07:02<02:45,  1.28s/it]Running loglikelihood requests:  68%|██████▊   | 272/400 [07:03<02:45,  1.29s/it]Running loglikelihood requests:  68%|██████▊   | 273/400 [07:04<02:45,  1.31s/it]Running loglikelihood requests:  68%|██████▊   | 274/400 [07:06<02:43,  1.30s/it]Running loglikelihood requests:  69%|██████▉   | 275/400 [07:07<02:41,  1.29s/it]Running loglikelihood requests:  69%|██████▉   | 276/400 [07:08<02:39,  1.29s/it]Running loglikelihood requests:  69%|██████▉   | 277/400 [07:09<02:37,  1.28s/it]Running loglikelihood requests:  70%|██████▉   | 278/400 [07:11<02:35,  1.27s/it]Running loglikelihood requests:  70%|███████   | 280/400 [07:12<01:57,  1.02it/s]Running loglikelihood requests:  70%|███████   | 281/400 [07:13<02:04,  1.05s/it]Running loglikelihood requests:  70%|███████   | 282/400 [07:15<02:10,  1.10s/it]Running loglikelihood requests:  71%|███████   | 283/400 [07:16<02:14,  1.15s/it]Running loglikelihood requests:  71%|███████   | 284/400 [07:17<02:16,  1.18s/it]Running loglikelihood requests:  71%|███████▏  | 285/400 [07:18<02:17,  1.20s/it]Running loglikelihood requests:  72%|███████▏  | 286/400 [07:20<02:18,  1.22s/it]Running loglikelihood requests:  72%|███████▏  | 287/400 [07:21<02:18,  1.23s/it]Running loglikelihood requests:  72%|███████▏  | 288/400 [07:22<02:18,  1.23s/it]Running loglikelihood requests:  72%|███████▏  | 289/400 [07:23<02:16,  1.23s/it]Running loglikelihood requests:  72%|███████▎  | 290/400 [07:25<02:15,  1.23s/it]Running loglikelihood requests:  73%|███████▎  | 291/400 [07:26<02:13,  1.23s/it]Running loglikelihood requests:  73%|███████▎  | 292/400 [07:27<02:12,  1.23s/it]Running loglikelihood requests:  73%|███████▎  | 293/400 [07:28<02:11,  1.23s/it]Running loglikelihood requests:  74%|███████▎  | 294/400 [07:29<02:09,  1.23s/it]Running loglikelihood requests:  74%|███████▍  | 295/400 [07:31<02:08,  1.22s/it]Running loglikelihood requests:  74%|███████▍  | 296/400 [07:32<02:07,  1.22s/it]Running loglikelihood requests:  74%|███████▍  | 297/400 [07:33<02:05,  1.22s/it]Running loglikelihood requests:  74%|███████▍  | 298/400 [07:34<02:04,  1.22s/it]Running loglikelihood requests:  75%|███████▍  | 299/400 [07:36<02:03,  1.22s/it]Running loglikelihood requests:  75%|███████▌  | 300/400 [07:37<02:02,  1.22s/it]Running loglikelihood requests:  75%|███████▌  | 301/400 [07:38<02:01,  1.22s/it]Running loglikelihood requests:  76%|███████▌  | 302/400 [07:39<01:59,  1.22s/it]Running loglikelihood requests:  76%|███████▌  | 303/400 [07:40<01:57,  1.21s/it]Running loglikelihood requests:  76%|███████▌  | 304/400 [07:42<01:56,  1.21s/it]Running loglikelihood requests:  76%|███████▋  | 305/400 [07:43<01:54,  1.21s/it]Running loglikelihood requests:  76%|███████▋  | 306/400 [07:44<01:53,  1.20s/it]Running loglikelihood requests:  77%|███████▋  | 307/400 [07:45<01:51,  1.20s/it]Running loglikelihood requests:  77%|███████▋  | 308/400 [07:46<01:50,  1.20s/it]Running loglikelihood requests:  77%|███████▋  | 309/400 [07:48<01:48,  1.19s/it]Running loglikelihood requests:  78%|███████▊  | 310/400 [07:49<01:47,  1.19s/it]Running loglikelihood requests:  78%|███████▊  | 311/400 [07:50<01:45,  1.19s/it]Running loglikelihood requests:  78%|███████▊  | 312/400 [07:51<01:44,  1.19s/it]Running loglikelihood requests:  78%|███████▊  | 313/400 [07:52<01:43,  1.19s/it]Running loglikelihood requests:  78%|███████▊  | 314/400 [07:53<01:42,  1.19s/it]Running loglikelihood requests:  79%|███████▉  | 315/400 [07:55<01:40,  1.19s/it]Running loglikelihood requests:  79%|███████▉  | 316/400 [07:56<01:39,  1.18s/it]Running loglikelihood requests:  79%|███████▉  | 317/400 [07:57<01:38,  1.18s/it]Running loglikelihood requests:  80%|███████▉  | 318/400 [07:58<01:36,  1.18s/it]Running loglikelihood requests:  80%|███████▉  | 319/400 [07:59<01:35,  1.18s/it]Running loglikelihood requests:  80%|████████  | 320/400 [08:01<01:34,  1.18s/it]Running loglikelihood requests:  80%|████████  | 321/400 [08:02<01:32,  1.18s/it]Running loglikelihood requests:  80%|████████  | 322/400 [08:03<01:31,  1.17s/it]Running loglikelihood requests:  81%|████████  | 323/400 [08:04<01:30,  1.17s/it]Running loglikelihood requests:  81%|████████  | 324/400 [08:05<01:28,  1.17s/it]Running loglikelihood requests:  81%|████████▏ | 325/400 [08:06<01:27,  1.17s/it]Running loglikelihood requests:  82%|████████▏ | 326/400 [08:08<01:26,  1.17s/it]Running loglikelihood requests:  82%|████████▏ | 327/400 [08:09<01:25,  1.16s/it]Running loglikelihood requests:  82%|████████▏ | 328/400 [08:10<01:23,  1.16s/it]Running loglikelihood requests:  82%|████████▏ | 329/400 [08:11<01:22,  1.16s/it]Running loglikelihood requests:  82%|████████▎ | 330/400 [08:12<01:21,  1.16s/it]Running loglikelihood requests:  83%|████████▎ | 331/400 [08:13<01:20,  1.16s/it]Running loglikelihood requests:  83%|████████▎ | 332/400 [08:14<01:18,  1.16s/it]Running loglikelihood requests:  83%|████████▎ | 333/400 [08:16<01:17,  1.16s/it]Running loglikelihood requests:  84%|████████▎ | 334/400 [08:17<01:16,  1.15s/it]Running loglikelihood requests:  84%|████████▍ | 335/400 [08:18<01:14,  1.15s/it]Running loglikelihood requests:  84%|████████▍ | 336/400 [08:19<01:13,  1.15s/it]Running loglikelihood requests:  84%|████████▍ | 337/400 [08:20<01:12,  1.15s/it]Running loglikelihood requests:  84%|████████▍ | 338/400 [08:21<01:10,  1.14s/it]Running loglikelihood requests:  85%|████████▍ | 339/400 [08:22<01:09,  1.14s/it]Running loglikelihood requests:  85%|████████▌ | 340/400 [08:24<01:08,  1.14s/it]Running loglikelihood requests:  85%|████████▌ | 341/400 [08:25<01:07,  1.14s/it]Running loglikelihood requests:  86%|████████▌ | 342/400 [08:26<01:05,  1.14s/it]Running loglikelihood requests:  86%|████████▌ | 343/400 [08:27<01:04,  1.13s/it]Running loglikelihood requests:  86%|████████▌ | 344/400 [08:28<01:03,  1.13s/it]Running loglikelihood requests:  86%|████████▋ | 345/400 [08:29<01:02,  1.13s/it]Running loglikelihood requests:  86%|████████▋ | 346/400 [08:30<01:01,  1.13s/it]Running loglikelihood requests:  87%|████████▋ | 347/400 [08:32<00:59,  1.13s/it]Running loglikelihood requests:  87%|████████▋ | 348/400 [08:33<00:58,  1.13s/it]Running loglikelihood requests:  87%|████████▋ | 349/400 [08:34<00:57,  1.13s/it]Running loglikelihood requests:  88%|████████▊ | 350/400 [08:35<00:56,  1.12s/it]Running loglikelihood requests:  88%|████████▊ | 351/400 [08:36<00:55,  1.12s/it]Running loglikelihood requests:  88%|████████▊ | 352/400 [08:37<00:53,  1.12s/it]Running loglikelihood requests:  88%|████████▊ | 353/400 [08:38<00:52,  1.12s/it]Running loglikelihood requests:  88%|████████▊ | 354/400 [08:39<00:51,  1.12s/it]Running loglikelihood requests:  89%|████████▉ | 355/400 [08:40<00:50,  1.11s/it]Running loglikelihood requests:  89%|████████▉ | 356/400 [08:42<00:48,  1.11s/it]Running loglikelihood requests:  89%|████████▉ | 357/400 [08:43<00:47,  1.11s/it]Running loglikelihood requests:  90%|████████▉ | 358/400 [08:44<00:46,  1.11s/it]Running loglikelihood requests:  90%|████████▉ | 359/400 [08:45<00:45,  1.11s/it]Running loglikelihood requests:  90%|█████████ | 360/400 [08:46<00:44,  1.10s/it]Running loglikelihood requests:  90%|█████████ | 361/400 [08:47<00:42,  1.10s/it]Running loglikelihood requests:  90%|█████████ | 362/400 [08:48<00:41,  1.10s/it]Running loglikelihood requests:  91%|█████████ | 363/400 [08:49<00:40,  1.10s/it]Running loglikelihood requests:  91%|█████████ | 364/400 [08:50<00:39,  1.10s/it]Running loglikelihood requests:  91%|█████████▏| 365/400 [08:51<00:38,  1.10s/it]Running loglikelihood requests:  92%|█████████▏| 366/400 [08:53<00:37,  1.10s/it]Running loglikelihood requests:  92%|█████████▏| 367/400 [08:54<00:36,  1.09s/it]Running loglikelihood requests:  92%|█████████▏| 368/400 [08:55<00:34,  1.09s/it]Running loglikelihood requests:  92%|█████████▏| 369/400 [08:56<00:33,  1.09s/it]Running loglikelihood requests:  92%|█████████▎| 370/400 [08:57<00:32,  1.09s/it]Running loglikelihood requests:  93%|█████████▎| 371/400 [08:58<00:31,  1.08s/it]Running loglikelihood requests:  93%|█████████▎| 372/400 [08:59<00:30,  1.08s/it]Running loglikelihood requests:  93%|█████████▎| 373/400 [09:00<00:29,  1.08s/it]Running loglikelihood requests:  94%|█████████▎| 374/400 [09:01<00:27,  1.07s/it]Running loglikelihood requests:  94%|█████████▍| 375/400 [09:02<00:26,  1.07s/it]Running loglikelihood requests:  94%|█████████▍| 376/400 [09:03<00:25,  1.07s/it]Running loglikelihood requests:  94%|█████████▍| 377/400 [09:04<00:24,  1.06s/it]Running loglikelihood requests:  94%|█████████▍| 378/400 [09:05<00:23,  1.06s/it]Running loglikelihood requests:  95%|█████████▍| 379/400 [09:06<00:22,  1.05s/it]Running loglikelihood requests:  95%|█████████▌| 380/400 [09:07<00:20,  1.04s/it]Running loglikelihood requests:  95%|█████████▌| 381/400 [09:08<00:19,  1.03s/it]Running loglikelihood requests:  96%|█████████▌| 382/400 [09:09<00:18,  1.02s/it]Running loglikelihood requests:  96%|█████████▌| 383/400 [09:10<00:17,  1.01s/it]Running loglikelihood requests:  96%|█████████▌| 384/400 [09:11<00:15,  1.00it/s]Running loglikelihood requests:  96%|█████████▋| 385/400 [09:12<00:14,  1.01it/s]Running loglikelihood requests:  96%|█████████▋| 386/400 [09:13<00:13,  1.01it/s]Running loglikelihood requests:  97%|█████████▋| 387/400 [09:14<00:12,  1.02it/s]Running loglikelihood requests:  97%|█████████▋| 389/400 [09:15<00:08,  1.34it/s]Running loglikelihood requests:  98%|█████████▊| 390/400 [09:16<00:07,  1.26it/s]Running loglikelihood requests:  98%|█████████▊| 391/400 [09:17<00:07,  1.21it/s]Running loglikelihood requests:  98%|█████████▊| 393/400 [09:18<00:04,  1.52it/s]Running loglikelihood requests:  98%|█████████▊| 394/400 [09:19<00:04,  1.41it/s]Running loglikelihood requests:  99%|█████████▉| 395/400 [09:20<00:03,  1.34it/s]Running loglikelihood requests:  99%|█████████▉| 396/400 [09:21<00:03,  1.28it/s]Running loglikelihood requests:  99%|█████████▉| 397/400 [09:21<00:02,  1.25it/s]Running loglikelihood requests: 100%|█████████▉| 398/400 [09:22<00:01,  1.23it/s]Running loglikelihood requests: 100%|█████████▉| 399/400 [09:23<00:00,  1.21it/s]Running loglikelihood requests: 100%|██████████| 400/400 [09:24<00:00,  1.20it/s]Running loglikelihood requests: 100%|██████████| 400/400 [09:24<00:00,  1.41s/it]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:7'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:7'}
full model:
{'logiqa': {'alias': 'logiqa', 'acc,none': 0.29, 'acc_stderr,none': 0.045604802157206865, 'acc_norm,none': 0.33, 'acc_norm_stderr,none': 0.04725815626252609}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9179964803140478
0.7817057225882229
0.8413553272072316
0.9274797474668193
0.8768807463293081
0.9494139907523571
0.8960692461846443
0.9131107283061946
0.6329173647892901
0.8375042173336539
0.8817471801904351
0.8172295355829869
0.7824572665005357
0.9227400642857845
0.9246594853497696
0.8075911590072223
0.6900210787422486
0.599615993999193
0.9308030044211123
0.9504015361511146
0.8866807231108503
0.540104242930401
0.6701728801805507
0.9744992661822648
0.8193037468812308
0.840784693447352
0.9052511591891966
Total groups 70 exceeded the threshold, stopping comparison.
The group tensor is
[3, 6, 7, 1, 5, 2, 4, 0]
tensor([3, 6, 7, 1, 5, 2, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 2, 6, 3, 7, 1, 5, 0]
tensor([4, 2, 6, 3, 7, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 6, 1, 7, 2, 4, 0]
tensor([5, 3, 6, 1, 7, 2, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 1, 7, 2, 3, 0]
tensor([5, 4, 6, 1, 7, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 3, 0, 2, 1, 0, 4, 1]
tensor([5, 3, 0, 2, 1, 0, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 0, 5, 0, 1, 2, 3, 1]
tensor([4, 0, 5, 0, 1, 2, 3, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 4 to 8
done!
Normal merging for layer 9
tensor([2, 5])
tensor(2)
tensor([4, 7])
tensor(4)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 10 to 13
done!
Normal merging for layer 14
tensor([1, 3])
tensor(1)
tensor([4, 7])
tensor(4)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 15 to 31
done!
all done!
Model size: 11.9458 GB
109
cuda:7
sciq
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 27.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 30.65s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/sciq HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/sciq HTTP/1.1" 200 1238
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/sciq/sciq.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/sciq HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/sciq HTTP/1.1" 200 1238
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/sciq/resolve/2c94ad3e1aafab77146f384e23536f97a4849815/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/sciq/resolve/2c94ad3e1aafab77146f384e23536f97a4849815/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/sciq/resolve/2c94ad3e1aafab77146f384e23536f97a4849815/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/sciq/resolve/2c94ad3e1aafab77146f384e23536f97a4849815/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140229615656144 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_sciq_default_0.0.0_2c94ad3e1aafab77146f384e23536f97a4849815.lock
DEBUG:filelock:Lock 140229615656144 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_sciq_default_0.0.0_2c94ad3e1aafab77146f384e23536f97a4849815.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815/dataset_info.json
DEBUG:filelock:Attempting to release lock 140229615656144 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_sciq_default_0.0.0_2c94ad3e1aafab77146f384e23536f97a4849815.lock
DEBUG:filelock:Lock 140229615656144 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_sciq_default_0.0.0_2c94ad3e1aafab77146f384e23536f97a4849815.lock
DEBUG:filelock:Attempting to acquire lock 140215722517536 on /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815_builder.lock
DEBUG:filelock:Lock 140215722517536 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215722517536 on /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815_builder.lock
DEBUG:filelock:Lock 140215722517536 released on /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of sciq from None to 0
INFO:lm_eval.api.task:Building contexts for sciq on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1033.25it/s]
DEBUG:lm_eval.evaluator:Task: sciq; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:04<27:47,  4.18s/it]Running loglikelihood requests:   0%|          | 2/400 [00:07<25:06,  3.78s/it]Running loglikelihood requests:   1%|          | 3/400 [00:11<24:09,  3.65s/it]Running loglikelihood requests:   1%|          | 4/400 [00:14<23:40,  3.59s/it]Running loglikelihood requests:   1%|▏         | 5/400 [00:17<22:50,  3.47s/it]Running loglikelihood requests:   2%|▏         | 6/400 [00:21<22:40,  3.45s/it]Running loglikelihood requests:   2%|▏         | 7/400 [00:24<22:28,  3.43s/it]Running loglikelihood requests:   2%|▏         | 8/400 [00:27<21:59,  3.37s/it]Running loglikelihood requests:   2%|▏         | 9/400 [00:30<20:56,  3.21s/it]Running loglikelihood requests:   2%|▎         | 10/400 [00:33<20:10,  3.10s/it]Running loglikelihood requests:   3%|▎         | 11/400 [00:36<19:37,  3.03s/it]Running loglikelihood requests:   3%|▎         | 12/400 [00:39<19:13,  2.97s/it]Running loglikelihood requests:   3%|▎         | 13/400 [00:42<18:31,  2.87s/it]Running loglikelihood requests:   4%|▎         | 14/400 [00:44<18:00,  2.80s/it]Running loglikelihood requests:   4%|▍         | 15/400 [00:47<17:38,  2.75s/it]Running loglikelihood requests:   4%|▍         | 16/400 [00:49<17:22,  2.71s/it]Running loglikelihood requests:   4%|▍         | 17/400 [00:52<17:03,  2.67s/it]Running loglikelihood requests:   4%|▍         | 18/400 [00:55<16:48,  2.64s/it]Running loglikelihood requests:   5%|▍         | 19/400 [00:57<16:34,  2.61s/it]Running loglikelihood requests:   5%|▌         | 20/400 [01:00<16:19,  2.58s/it]Running loglikelihood requests:   5%|▌         | 21/400 [01:02<16:07,  2.55s/it]Running loglikelihood requests:   6%|▌         | 22/400 [01:05<15:56,  2.53s/it]Running loglikelihood requests:   6%|▌         | 23/400 [01:07<15:47,  2.51s/it]Running loglikelihood requests:   6%|▌         | 24/400 [01:10<15:40,  2.50s/it]Running loglikelihood requests:   6%|▋         | 25/400 [01:12<15:17,  2.45s/it]Running loglikelihood requests:   6%|▋         | 26/400 [01:14<15:00,  2.41s/it]Running loglikelihood requests:   7%|▋         | 27/400 [01:16<14:47,  2.38s/it]Running loglikelihood requests:   7%|▋         | 28/400 [01:19<14:37,  2.36s/it]Running loglikelihood requests:   7%|▋         | 29/400 [01:21<14:07,  2.28s/it]Running loglikelihood requests:   8%|▊         | 30/400 [01:23<13:45,  2.23s/it]Running loglikelihood requests:   8%|▊         | 31/400 [01:25<13:28,  2.19s/it]Running loglikelihood requests:   8%|▊         | 32/400 [01:27<13:15,  2.16s/it]Running loglikelihood requests:   8%|▊         | 33/400 [01:29<12:52,  2.11s/it]Running loglikelihood requests:   8%|▊         | 34/400 [01:31<12:35,  2.06s/it]Running loglikelihood requests:   9%|▉         | 35/400 [01:33<12:21,  2.03s/it]Running loglikelihood requests:   9%|▉         | 36/400 [01:35<12:12,  2.01s/it]Running loglikelihood requests:   9%|▉         | 37/400 [01:37<12:04,  2.00s/it]Running loglikelihood requests:  10%|▉         | 38/400 [01:39<11:59,  1.99s/it]Running loglikelihood requests:  10%|█         | 40/400 [01:41<09:07,  1.52s/it]Running loglikelihood requests:  10%|█         | 41/400 [01:43<09:31,  1.59s/it]Running loglikelihood requests:  10%|█         | 42/400 [01:45<09:49,  1.65s/it]Running loglikelihood requests:  11%|█         | 43/400 [01:46<10:03,  1.69s/it]Running loglikelihood requests:  11%|█         | 44/400 [01:48<10:12,  1.72s/it]Running loglikelihood requests:  11%|█▏        | 45/400 [01:50<10:12,  1.73s/it]Running loglikelihood requests:  12%|█▏        | 46/400 [01:52<10:11,  1.73s/it]Running loglikelihood requests:  12%|█▏        | 47/400 [01:53<10:09,  1.73s/it]Running loglikelihood requests:  12%|█▏        | 48/400 [01:55<10:08,  1.73s/it]Running loglikelihood requests:  12%|█▏        | 49/400 [01:57<10:06,  1.73s/it]Running loglikelihood requests:  12%|█▎        | 50/400 [01:59<10:03,  1.73s/it]Running loglikelihood requests:  13%|█▎        | 51/400 [02:00<10:01,  1.72s/it]Running loglikelihood requests:  13%|█▎        | 52/400 [02:02<09:59,  1.72s/it]Running loglikelihood requests:  13%|█▎        | 53/400 [02:04<09:38,  1.67s/it]Running loglikelihood requests:  14%|█▎        | 54/400 [02:05<09:24,  1.63s/it]Running loglikelihood requests:  14%|█▍        | 55/400 [02:07<09:12,  1.60s/it]Running loglikelihood requests:  14%|█▍        | 57/400 [02:08<06:53,  1.21s/it]Running loglikelihood requests:  14%|█▍        | 58/400 [02:10<07:15,  1.27s/it]Running loglikelihood requests:  15%|█▍        | 59/400 [02:11<07:32,  1.33s/it]Running loglikelihood requests:  16%|█▌        | 62/400 [02:13<04:59,  1.13it/s]Running loglikelihood requests:  16%|█▌        | 63/400 [02:14<05:38,  1.00s/it]Running loglikelihood requests:  16%|█▌        | 64/400 [02:15<06:12,  1.11s/it]Running loglikelihood requests:  16%|█▋        | 65/400 [02:17<06:36,  1.18s/it]Running loglikelihood requests:  16%|█▋        | 66/400 [02:18<06:55,  1.24s/it]Running loglikelihood requests:  17%|█▋        | 67/400 [02:20<07:10,  1.29s/it]Running loglikelihood requests:  17%|█▋        | 68/400 [02:21<07:20,  1.33s/it]Running loglikelihood requests:  17%|█▋        | 69/400 [02:23<07:25,  1.35s/it]Running loglikelihood requests:  18%|█▊        | 70/400 [02:24<07:28,  1.36s/it]Running loglikelihood requests:  18%|█▊        | 71/400 [02:25<07:30,  1.37s/it]Running loglikelihood requests:  18%|█▊        | 72/400 [02:27<07:30,  1.37s/it]Running loglikelihood requests:  18%|█▊        | 73/400 [02:28<07:31,  1.38s/it]Running loglikelihood requests:  18%|█▊        | 74/400 [02:30<07:31,  1.38s/it]Running loglikelihood requests:  19%|█▉        | 77/400 [02:31<04:38,  1.16it/s]Running loglikelihood requests:  20%|█▉        | 78/400 [02:32<05:11,  1.03it/s]Running loglikelihood requests:  20%|█▉        | 79/400 [02:34<05:39,  1.06s/it]Running loglikelihood requests:  20%|██        | 81/400 [02:35<04:41,  1.13it/s]Running loglikelihood requests:  20%|██        | 82/400 [02:36<05:03,  1.05it/s]Running loglikelihood requests:  21%|██        | 83/400 [02:37<05:22,  1.02s/it]Running loglikelihood requests:  21%|██        | 84/400 [02:38<05:37,  1.07s/it]Running loglikelihood requests:  21%|██▏       | 85/400 [02:40<05:47,  1.10s/it]Running loglikelihood requests:  22%|██▏       | 86/400 [02:41<05:54,  1.13s/it]Running loglikelihood requests:  22%|██▏       | 89/400 [02:42<03:47,  1.37it/s]Running loglikelihood requests:  22%|██▎       | 90/400 [02:43<04:15,  1.21it/s]Running loglikelihood requests:  23%|██▎       | 91/400 [02:44<04:39,  1.10it/s]Running loglikelihood requests:  23%|██▎       | 92/400 [02:46<04:59,  1.03it/s]Running loglikelihood requests:  23%|██▎       | 93/400 [02:47<05:15,  1.03s/it]Running loglikelihood requests:  24%|██▍       | 97/400 [02:48<02:58,  1.70it/s]Running loglikelihood requests:  24%|██▍       | 98/400 [02:49<03:28,  1.45it/s]Running loglikelihood requests:  25%|██▍       | 99/400 [02:50<03:55,  1.28it/s]Running loglikelihood requests:  25%|██▌       | 100/400 [02:51<04:19,  1.15it/s]Running loglikelihood requests:  25%|██▌       | 101/400 [02:53<04:39,  1.07it/s]Running loglikelihood requests:  26%|██▌       | 102/400 [02:54<04:55,  1.01it/s]Running loglikelihood requests:  26%|██▌       | 103/400 [02:55<05:06,  1.03s/it]Running loglikelihood requests:  26%|██▌       | 104/400 [02:56<05:15,  1.07s/it]Running loglikelihood requests:  26%|██▋       | 105/400 [02:57<05:20,  1.09s/it]Running loglikelihood requests:  26%|██▋       | 106/400 [02:58<05:23,  1.10s/it]Running loglikelihood requests:  27%|██▋       | 107/400 [02:59<05:24,  1.11s/it]Running loglikelihood requests:  27%|██▋       | 108/400 [03:01<05:25,  1.11s/it]Running loglikelihood requests:  27%|██▋       | 109/400 [03:02<05:24,  1.11s/it]Running loglikelihood requests:  28%|██▊       | 110/400 [03:03<05:22,  1.11s/it]Running loglikelihood requests:  28%|██▊       | 111/400 [03:04<05:21,  1.11s/it]Running loglikelihood requests:  28%|██▊       | 112/400 [03:05<05:20,  1.11s/it]Running loglikelihood requests:  28%|██▊       | 113/400 [03:06<05:18,  1.11s/it]Running loglikelihood requests:  28%|██▊       | 114/400 [03:07<05:16,  1.11s/it]Running loglikelihood requests:  29%|██▉       | 115/400 [03:08<05:14,  1.10s/it]Running loglikelihood requests:  29%|██▉       | 116/400 [03:09<05:12,  1.10s/it]Running loglikelihood requests:  29%|██▉       | 117/400 [03:10<05:09,  1.09s/it]Running loglikelihood requests:  30%|██▉       | 118/400 [03:12<05:05,  1.08s/it]Running loglikelihood requests:  30%|██▉       | 119/400 [03:13<05:03,  1.08s/it]Running loglikelihood requests:  30%|███       | 120/400 [03:14<05:00,  1.07s/it]Running loglikelihood requests:  30%|███       | 121/400 [03:15<05:01,  1.08s/it]Running loglikelihood requests:  30%|███       | 122/400 [03:16<05:00,  1.08s/it]Running loglikelihood requests:  31%|███       | 123/400 [03:17<04:58,  1.08s/it]Running loglikelihood requests:  31%|███       | 124/400 [03:18<04:55,  1.07s/it]Running loglikelihood requests:  31%|███▏      | 125/400 [03:19<04:52,  1.06s/it]Running loglikelihood requests:  32%|███▏      | 126/400 [03:20<04:49,  1.06s/it]Running loglikelihood requests:  32%|███▏      | 127/400 [03:21<04:46,  1.05s/it]Running loglikelihood requests:  32%|███▏      | 128/400 [03:22<04:44,  1.05s/it]Running loglikelihood requests:  32%|███▏      | 129/400 [03:23<04:43,  1.05s/it]Running loglikelihood requests:  32%|███▎      | 130/400 [03:24<04:41,  1.04s/it]Running loglikelihood requests:  33%|███▎      | 131/400 [03:25<04:39,  1.04s/it]Running loglikelihood requests:  34%|███▎      | 134/400 [03:26<02:51,  1.55it/s]Running loglikelihood requests:  34%|███▍      | 135/400 [03:27<03:12,  1.38it/s]Running loglikelihood requests:  34%|███▍      | 136/400 [03:28<03:29,  1.26it/s]Running loglikelihood requests:  34%|███▍      | 137/400 [03:29<03:43,  1.18it/s]Running loglikelihood requests:  34%|███▍      | 138/400 [03:30<03:54,  1.12it/s]Running loglikelihood requests:  35%|███▍      | 139/400 [03:31<04:02,  1.08it/s]Running loglikelihood requests:  36%|███▌      | 142/400 [03:32<02:37,  1.64it/s]Running loglikelihood requests:  36%|███▌      | 143/400 [03:33<02:58,  1.44it/s]Running loglikelihood requests:  36%|███▌      | 144/400 [03:34<03:16,  1.31it/s]Running loglikelihood requests:  36%|███▋      | 145/400 [03:35<03:30,  1.21it/s]Running loglikelihood requests:  36%|███▋      | 146/400 [03:36<03:41,  1.15it/s]Running loglikelihood requests:  37%|███▋      | 147/400 [03:37<03:49,  1.10it/s]Running loglikelihood requests:  37%|███▋      | 148/400 [03:38<03:56,  1.07it/s]Running loglikelihood requests:  37%|███▋      | 149/400 [03:39<03:59,  1.05it/s]Running loglikelihood requests:  38%|███▊      | 150/400 [03:40<04:01,  1.03it/s]Running loglikelihood requests:  38%|███▊      | 151/400 [03:41<04:02,  1.03it/s]Running loglikelihood requests:  38%|███▊      | 152/400 [03:42<04:03,  1.02it/s]Running loglikelihood requests:  38%|███▊      | 153/400 [03:43<04:03,  1.02it/s]Running loglikelihood requests:  38%|███▊      | 154/400 [03:44<04:03,  1.01it/s]Running loglikelihood requests:  39%|███▉      | 155/400 [03:45<04:02,  1.01it/s]Running loglikelihood requests:  39%|███▉      | 156/400 [03:46<04:01,  1.01it/s]Running loglikelihood requests:  39%|███▉      | 157/400 [03:47<04:00,  1.01it/s]Running loglikelihood requests:  40%|███▉      | 158/400 [03:48<03:58,  1.01it/s]Running loglikelihood requests:  40%|███▉      | 159/400 [03:49<03:57,  1.01it/s]Running loglikelihood requests:  40%|████      | 160/400 [03:50<03:56,  1.01it/s]Running loglikelihood requests:  40%|████      | 161/400 [03:51<03:55,  1.02it/s]Running loglikelihood requests:  40%|████      | 162/400 [03:52<03:53,  1.02it/s]Running loglikelihood requests:  41%|████      | 163/400 [03:53<03:52,  1.02it/s]Running loglikelihood requests:  41%|████      | 164/400 [03:54<03:50,  1.02it/s]Running loglikelihood requests:  41%|████▏     | 165/400 [03:55<03:48,  1.03it/s]Running loglikelihood requests:  42%|████▏     | 166/400 [03:56<03:46,  1.03it/s]Running loglikelihood requests:  42%|████▏     | 167/400 [03:57<03:44,  1.04it/s]Running loglikelihood requests:  42%|████▏     | 168/400 [03:58<03:43,  1.04it/s]Running loglikelihood requests:  42%|████▏     | 169/400 [03:59<03:41,  1.04it/s]Running loglikelihood requests:  42%|████▎     | 170/400 [04:00<03:39,  1.05it/s]Running loglikelihood requests:  43%|████▎     | 172/400 [04:01<02:47,  1.36it/s]Running loglikelihood requests:  43%|████▎     | 173/400 [04:02<02:58,  1.27it/s]Running loglikelihood requests:  44%|████▎     | 174/400 [04:03<03:05,  1.22it/s]Running loglikelihood requests:  44%|████▍     | 177/400 [04:04<02:02,  1.81it/s]Running loglikelihood requests:  44%|████▍     | 178/400 [04:05<02:19,  1.59it/s]Running loglikelihood requests:  45%|████▍     | 179/400 [04:06<02:33,  1.44it/s]Running loglikelihood requests:  45%|████▌     | 180/400 [04:06<02:44,  1.33it/s]Running loglikelihood requests:  45%|████▌     | 181/400 [04:07<02:53,  1.26it/s]Running loglikelihood requests:  46%|████▌     | 182/400 [04:08<02:58,  1.22it/s]Running loglikelihood requests:  46%|████▌     | 183/400 [04:09<03:01,  1.19it/s]Running loglikelihood requests:  46%|████▌     | 184/400 [04:10<03:03,  1.17it/s]Running loglikelihood requests:  46%|████▋     | 185/400 [04:11<03:05,  1.16it/s]Running loglikelihood requests:  46%|████▋     | 186/400 [04:12<03:05,  1.15it/s]Running loglikelihood requests:  47%|████▋     | 187/400 [04:13<03:05,  1.15it/s]Running loglikelihood requests:  47%|████▋     | 188/400 [04:14<03:05,  1.15it/s]Running loglikelihood requests:  47%|████▋     | 189/400 [04:14<03:04,  1.14it/s]Running loglikelihood requests:  48%|████▊     | 190/400 [04:15<03:03,  1.15it/s]Running loglikelihood requests:  48%|████▊     | 191/400 [04:16<03:02,  1.15it/s]Running loglikelihood requests:  48%|████▊     | 192/400 [04:17<03:00,  1.15it/s]Running loglikelihood requests:  48%|████▊     | 193/400 [04:18<02:58,  1.16it/s]Running loglikelihood requests:  48%|████▊     | 194/400 [04:19<02:56,  1.17it/s]Running loglikelihood requests:  49%|████▉     | 195/400 [04:20<02:55,  1.17it/s]Running loglikelihood requests:  49%|████▉     | 196/400 [04:20<02:53,  1.17it/s]Running loglikelihood requests:  49%|████▉     | 197/400 [04:21<02:52,  1.18it/s]Running loglikelihood requests:  50%|████▉     | 198/400 [04:22<02:50,  1.19it/s]Running loglikelihood requests:  50%|████▉     | 199/400 [04:23<02:48,  1.19it/s]Running loglikelihood requests:  50%|█████     | 200/400 [04:24<02:47,  1.19it/s]Running loglikelihood requests:  50%|█████     | 201/400 [04:25<02:46,  1.20it/s]Running loglikelihood requests:  50%|█████     | 202/400 [04:25<02:45,  1.20it/s]Running loglikelihood requests:  51%|█████     | 203/400 [04:26<02:44,  1.20it/s]Running loglikelihood requests:  51%|█████     | 204/400 [04:27<02:42,  1.20it/s]Running loglikelihood requests:  51%|█████▏    | 205/400 [04:28<02:41,  1.20it/s]Running loglikelihood requests:  52%|█████▏    | 206/400 [04:29<02:41,  1.20it/s]Running loglikelihood requests:  52%|█████▏    | 207/400 [04:30<02:40,  1.21it/s]Running loglikelihood requests:  52%|█████▏    | 208/400 [04:30<02:39,  1.21it/s]Running loglikelihood requests:  52%|█████▏    | 209/400 [04:31<02:38,  1.21it/s]Running loglikelihood requests:  53%|█████▎    | 212/400 [04:32<01:37,  1.93it/s]Running loglikelihood requests:  53%|█████▎    | 213/400 [04:33<01:48,  1.73it/s]Running loglikelihood requests:  54%|█████▎    | 214/400 [04:34<01:57,  1.58it/s]Running loglikelihood requests:  54%|█████▍    | 215/400 [04:35<02:04,  1.48it/s]Running loglikelihood requests:  54%|█████▍    | 216/400 [04:35<02:10,  1.41it/s]Running loglikelihood requests:  54%|█████▍    | 217/400 [04:36<02:14,  1.37it/s]Running loglikelihood requests:  55%|█████▍    | 218/400 [04:37<02:16,  1.33it/s]Running loglikelihood requests:  55%|█████▍    | 219/400 [04:38<02:18,  1.31it/s]Running loglikelihood requests:  55%|█████▌    | 220/400 [04:39<02:19,  1.29it/s]Running loglikelihood requests:  55%|█████▌    | 221/400 [04:39<02:20,  1.28it/s]Running loglikelihood requests:  56%|█████▌    | 222/400 [04:40<02:19,  1.27it/s]Running loglikelihood requests:  56%|█████▌    | 223/400 [04:41<02:19,  1.27it/s]Running loglikelihood requests:  56%|█████▌    | 224/400 [04:42<02:18,  1.27it/s]Running loglikelihood requests:  56%|█████▋    | 225/400 [04:42<02:17,  1.27it/s]Running loglikelihood requests:  56%|█████▋    | 226/400 [04:43<02:17,  1.27it/s]Running loglikelihood requests:  57%|█████▋    | 227/400 [04:44<02:16,  1.27it/s]Running loglikelihood requests:  57%|█████▋    | 228/400 [04:45<02:15,  1.27it/s]Running loglikelihood requests:  57%|█████▋    | 229/400 [04:46<02:14,  1.27it/s]Running loglikelihood requests:  57%|█████▊    | 230/400 [04:46<02:13,  1.27it/s]Running loglikelihood requests:  58%|█████▊    | 231/400 [04:47<02:13,  1.27it/s]Running loglikelihood requests:  58%|█████▊    | 232/400 [04:48<02:12,  1.27it/s]Running loglikelihood requests:  58%|█████▊    | 233/400 [04:49<02:11,  1.27it/s]Running loglikelihood requests:  58%|█████▊    | 234/400 [04:50<02:10,  1.27it/s]Running loglikelihood requests:  59%|█████▉    | 235/400 [04:50<02:09,  1.28it/s]Running loglikelihood requests:  59%|█████▉    | 236/400 [04:51<02:08,  1.28it/s]Running loglikelihood requests:  59%|█████▉    | 237/400 [04:52<02:07,  1.28it/s]Running loglikelihood requests:  60%|██████    | 240/400 [04:53<01:17,  2.06it/s]Running loglikelihood requests:  60%|██████    | 241/400 [04:53<01:26,  1.84it/s]Running loglikelihood requests:  60%|██████    | 242/400 [04:54<01:33,  1.68it/s]Running loglikelihood requests:  61%|██████    | 243/400 [04:55<01:39,  1.58it/s]Running loglikelihood requests:  61%|██████    | 244/400 [04:56<01:43,  1.50it/s]Running loglikelihood requests:  61%|██████▏   | 245/400 [04:56<01:46,  1.45it/s]Running loglikelihood requests:  62%|██████▏   | 246/400 [04:57<01:48,  1.41it/s]Running loglikelihood requests:  62%|██████▏   | 247/400 [04:58<01:50,  1.39it/s]Running loglikelihood requests:  62%|██████▏   | 248/400 [04:59<01:51,  1.37it/s]Running loglikelihood requests:  62%|██████▏   | 249/400 [04:59<01:51,  1.36it/s]Running loglikelihood requests:  62%|██████▎   | 250/400 [05:00<01:51,  1.35it/s]Running loglikelihood requests:  63%|██████▎   | 251/400 [05:01<01:50,  1.35it/s]Running loglikelihood requests:  63%|██████▎   | 252/400 [05:02<01:49,  1.35it/s]Running loglikelihood requests:  63%|██████▎   | 253/400 [05:02<01:49,  1.35it/s]Running loglikelihood requests:  64%|██████▎   | 254/400 [05:03<01:48,  1.35it/s]Running loglikelihood requests:  64%|██████▍   | 255/400 [05:04<01:47,  1.35it/s]Running loglikelihood requests:  64%|██████▍   | 256/400 [05:05<01:46,  1.35it/s]Running loglikelihood requests:  64%|██████▍   | 257/400 [05:05<01:45,  1.35it/s]Running loglikelihood requests:  64%|██████▍   | 258/400 [05:06<01:45,  1.35it/s]Running loglikelihood requests:  65%|██████▍   | 259/400 [05:07<01:44,  1.35it/s]Running loglikelihood requests:  65%|██████▌   | 260/400 [05:08<01:43,  1.36it/s]Running loglikelihood requests:  65%|██████▌   | 261/400 [05:08<01:40,  1.38it/s]Running loglikelihood requests:  66%|██████▌   | 262/400 [05:09<01:38,  1.40it/s]Running loglikelihood requests:  66%|██████▌   | 263/400 [05:10<01:36,  1.42it/s]Running loglikelihood requests:  66%|██████▌   | 264/400 [05:10<01:35,  1.43it/s]Running loglikelihood requests:  66%|██████▋   | 265/400 [05:11<01:32,  1.46it/s]Running loglikelihood requests:  66%|██████▋   | 266/400 [05:12<01:30,  1.48it/s]Running loglikelihood requests:  67%|██████▋   | 267/400 [05:12<01:28,  1.50it/s]Running loglikelihood requests:  67%|██████▋   | 268/400 [05:13<01:27,  1.51it/s]Running loglikelihood requests:  67%|██████▋   | 269/400 [05:14<01:25,  1.53it/s]Running loglikelihood requests:  68%|██████▊   | 270/400 [05:14<01:24,  1.54it/s]Running loglikelihood requests:  68%|██████▊   | 271/400 [05:15<01:22,  1.56it/s]Running loglikelihood requests:  68%|██████▊   | 272/400 [05:15<01:21,  1.57it/s]Running loglikelihood requests:  68%|██████▊   | 273/400 [05:16<01:19,  1.59it/s]Running loglikelihood requests:  68%|██████▊   | 274/400 [05:17<01:18,  1.61it/s]Running loglikelihood requests:  69%|██████▉   | 275/400 [05:17<01:16,  1.63it/s]Running loglikelihood requests:  69%|██████▉   | 276/400 [05:18<01:15,  1.65it/s]Running loglikelihood requests:  69%|██████▉   | 277/400 [05:18<01:14,  1.66it/s]Running loglikelihood requests:  70%|██████▉   | 278/400 [05:19<01:13,  1.67it/s]Running loglikelihood requests:  70%|██████▉   | 279/400 [05:20<01:12,  1.68it/s]Running loglikelihood requests:  70%|███████   | 280/400 [05:20<01:11,  1.69it/s]Running loglikelihood requests:  70%|███████   | 281/400 [05:21<01:10,  1.70it/s]Running loglikelihood requests:  70%|███████   | 282/400 [05:21<01:09,  1.70it/s]Running loglikelihood requests:  71%|███████   | 283/400 [05:22<01:08,  1.71it/s]Running loglikelihood requests:  71%|███████   | 284/400 [05:23<01:07,  1.71it/s]Running loglikelihood requests:  71%|███████▏  | 285/400 [05:23<01:07,  1.72it/s]Running loglikelihood requests:  72%|███████▏  | 286/400 [05:24<01:06,  1.72it/s]Running loglikelihood requests:  72%|███████▏  | 287/400 [05:24<01:05,  1.72it/s]Running loglikelihood requests:  72%|███████▏  | 288/400 [05:25<01:04,  1.73it/s]Running loglikelihood requests:  72%|███████▏  | 289/400 [05:25<01:04,  1.73it/s]Running loglikelihood requests:  72%|███████▎  | 290/400 [05:26<01:03,  1.73it/s]Running loglikelihood requests:  73%|███████▎  | 291/400 [05:27<01:02,  1.73it/s]Running loglikelihood requests:  73%|███████▎  | 292/400 [05:27<01:02,  1.73it/s]Running loglikelihood requests:  73%|███████▎  | 293/400 [05:28<01:01,  1.74it/s]Running loglikelihood requests:  74%|███████▎  | 294/400 [05:28<01:00,  1.74it/s]Running loglikelihood requests:  74%|███████▍  | 295/400 [05:29<01:00,  1.75it/s]Running loglikelihood requests:  74%|███████▍  | 296/400 [05:29<00:59,  1.75it/s]Running loglikelihood requests:  74%|███████▍  | 297/400 [05:30<00:58,  1.76it/s]Running loglikelihood requests:  74%|███████▍  | 298/400 [05:31<00:57,  1.76it/s]Running loglikelihood requests:  75%|███████▍  | 299/400 [05:31<00:57,  1.76it/s]Running loglikelihood requests:  75%|███████▌  | 300/400 [05:32<00:56,  1.76it/s]Running loglikelihood requests:  75%|███████▌  | 301/400 [05:32<00:56,  1.76it/s]Running loglikelihood requests:  76%|███████▌  | 302/400 [05:33<00:55,  1.76it/s]Running loglikelihood requests:  76%|███████▌  | 303/400 [05:33<00:54,  1.77it/s]Running loglikelihood requests:  76%|███████▌  | 304/400 [05:34<00:54,  1.77it/s]Running loglikelihood requests:  76%|███████▋  | 305/400 [05:35<00:53,  1.77it/s]Running loglikelihood requests:  76%|███████▋  | 306/400 [05:35<00:52,  1.78it/s]Running loglikelihood requests:  77%|███████▋  | 307/400 [05:36<00:52,  1.78it/s]Running loglikelihood requests:  78%|███████▊  | 310/400 [05:36<00:31,  2.85it/s]Running loglikelihood requests:  78%|███████▊  | 311/400 [05:37<00:35,  2.54it/s]Running loglikelihood requests:  78%|███████▊  | 312/400 [05:37<00:38,  2.31it/s]Running loglikelihood requests:  78%|███████▊  | 313/400 [05:38<00:40,  2.14it/s]Running loglikelihood requests:  78%|███████▊  | 314/400 [05:38<00:42,  2.03it/s]Running loglikelihood requests:  79%|███████▉  | 315/400 [05:39<00:43,  1.95it/s]Running loglikelihood requests:  79%|███████▉  | 316/400 [05:40<00:44,  1.90it/s]Running loglikelihood requests:  79%|███████▉  | 317/400 [05:40<00:44,  1.87it/s]Running loglikelihood requests:  80%|███████▉  | 318/400 [05:41<00:44,  1.85it/s]Running loglikelihood requests:  80%|███████▉  | 319/400 [05:41<00:43,  1.84it/s]Running loglikelihood requests:  80%|████████  | 321/400 [05:42<00:33,  2.39it/s]Running loglikelihood requests:  80%|████████  | 322/400 [05:42<00:35,  2.23it/s]Running loglikelihood requests:  81%|████████  | 323/400 [05:43<00:36,  2.12it/s]Running loglikelihood requests:  81%|████████  | 324/400 [05:43<00:37,  2.04it/s]Running loglikelihood requests:  81%|████████▏ | 325/400 [05:44<00:37,  1.99it/s]Running loglikelihood requests:  82%|████████▏ | 326/400 [05:44<00:37,  1.96it/s]Running loglikelihood requests:  82%|████████▏ | 327/400 [05:45<00:37,  1.95it/s]Running loglikelihood requests:  82%|████████▏ | 328/400 [05:46<00:37,  1.94it/s]Running loglikelihood requests:  82%|████████▏ | 329/400 [05:46<00:36,  1.94it/s]Running loglikelihood requests:  82%|████████▎ | 330/400 [05:47<00:36,  1.93it/s]Running loglikelihood requests:  83%|████████▎ | 331/400 [05:47<00:35,  1.93it/s]Running loglikelihood requests:  83%|████████▎ | 332/400 [05:48<00:35,  1.93it/s]Running loglikelihood requests:  83%|████████▎ | 333/400 [05:48<00:34,  1.94it/s]Running loglikelihood requests:  84%|████████▎ | 334/400 [05:49<00:33,  1.95it/s]Running loglikelihood requests:  84%|████████▍ | 335/400 [05:49<00:33,  1.96it/s]Running loglikelihood requests:  84%|████████▍ | 336/400 [05:50<00:32,  1.97it/s]Running loglikelihood requests:  84%|████████▍ | 337/400 [05:50<00:31,  1.98it/s]Running loglikelihood requests:  84%|████████▍ | 338/400 [05:51<00:31,  1.99it/s]Running loglikelihood requests:  85%|████████▍ | 339/400 [05:51<00:30,  1.99it/s]Running loglikelihood requests:  85%|████████▌ | 340/400 [05:52<00:30,  2.00it/s]Running loglikelihood requests:  85%|████████▌ | 341/400 [05:52<00:29,  2.00it/s]Running loglikelihood requests:  86%|████████▌ | 342/400 [05:53<00:29,  2.00it/s]Running loglikelihood requests:  86%|████████▌ | 343/400 [05:53<00:28,  2.00it/s]Running loglikelihood requests:  86%|████████▌ | 344/400 [05:54<00:27,  2.00it/s]Running loglikelihood requests:  86%|████████▋ | 345/400 [05:54<00:27,  2.01it/s]Running loglikelihood requests:  86%|████████▋ | 346/400 [05:55<00:26,  2.02it/s]Running loglikelihood requests:  87%|████████▋ | 347/400 [05:55<00:26,  2.02it/s]Running loglikelihood requests:  87%|████████▋ | 348/400 [05:56<00:25,  2.02it/s]Running loglikelihood requests:  87%|████████▋ | 349/400 [05:56<00:25,  2.03it/s]Running loglikelihood requests:  88%|████████▊ | 350/400 [05:57<00:24,  2.03it/s]Running loglikelihood requests:  88%|████████▊ | 351/400 [05:57<00:24,  2.03it/s]Running loglikelihood requests:  88%|████████▊ | 352/400 [05:58<00:23,  2.03it/s]Running loglikelihood requests:  88%|████████▊ | 353/400 [05:58<00:23,  2.04it/s]Running loglikelihood requests:  88%|████████▊ | 354/400 [05:59<00:22,  2.04it/s]Running loglikelihood requests:  89%|████████▉ | 355/400 [05:59<00:22,  2.04it/s]Running loglikelihood requests:  89%|████████▉ | 356/400 [06:00<00:21,  2.05it/s]Running loglikelihood requests:  89%|████████▉ | 357/400 [06:00<00:20,  2.05it/s]Running loglikelihood requests:  90%|████████▉ | 358/400 [06:00<00:20,  2.06it/s]Running loglikelihood requests:  90%|████████▉ | 359/400 [06:01<00:19,  2.06it/s]Running loglikelihood requests:  90%|█████████ | 360/400 [06:01<00:19,  2.07it/s]Running loglikelihood requests:  90%|█████████ | 361/400 [06:02<00:18,  2.07it/s]Running loglikelihood requests:  90%|█████████ | 362/400 [06:02<00:18,  2.07it/s]Running loglikelihood requests:  91%|█████████ | 363/400 [06:03<00:17,  2.08it/s]Running loglikelihood requests:  91%|█████████ | 364/400 [06:03<00:17,  2.08it/s]Running loglikelihood requests:  91%|█████████▏| 365/400 [06:04<00:16,  2.08it/s]Running loglikelihood requests:  92%|█████████▏| 366/400 [06:04<00:16,  2.08it/s]Running loglikelihood requests:  92%|█████████▏| 368/400 [06:05<00:11,  2.71it/s]Running loglikelihood requests:  92%|█████████▏| 369/400 [06:05<00:12,  2.52it/s]Running loglikelihood requests:  92%|█████████▎| 370/400 [06:06<00:12,  2.39it/s]Running loglikelihood requests:  93%|█████████▎| 371/400 [06:06<00:12,  2.30it/s]Running loglikelihood requests:  93%|█████████▎| 373/400 [06:07<00:09,  2.86it/s]Running loglikelihood requests:  94%|█████████▎| 374/400 [06:07<00:09,  2.64it/s]Running loglikelihood requests:  94%|█████████▍| 375/400 [06:08<00:10,  2.48it/s]Running loglikelihood requests:  94%|█████████▍| 376/400 [06:08<00:10,  2.38it/s]Running loglikelihood requests:  94%|█████████▍| 377/400 [06:09<00:09,  2.31it/s]Running loglikelihood requests:  94%|█████████▍| 378/400 [06:09<00:09,  2.26it/s]Running loglikelihood requests:  95%|█████████▍| 379/400 [06:10<00:09,  2.24it/s]Running loglikelihood requests:  95%|█████████▌| 380/400 [06:10<00:09,  2.22it/s]Running loglikelihood requests:  95%|█████████▌| 381/400 [06:10<00:08,  2.20it/s]Running loglikelihood requests:  96%|█████████▌| 382/400 [06:11<00:08,  2.20it/s]Running loglikelihood requests:  96%|█████████▌| 383/400 [06:11<00:07,  2.20it/s]Running loglikelihood requests:  96%|█████████▌| 384/400 [06:12<00:07,  2.20it/s]Running loglikelihood requests:  96%|█████████▋| 385/400 [06:12<00:06,  2.21it/s]Running loglikelihood requests:  96%|█████████▋| 386/400 [06:13<00:06,  2.21it/s]Running loglikelihood requests:  97%|█████████▋| 387/400 [06:13<00:05,  2.22it/s]Running loglikelihood requests:  97%|█████████▋| 388/400 [06:14<00:05,  2.22it/s]Running loglikelihood requests:  97%|█████████▋| 389/400 [06:14<00:04,  2.22it/s]Running loglikelihood requests:  98%|█████████▊| 390/400 [06:14<00:04,  2.23it/s]Running loglikelihood requests:  98%|█████████▊| 391/400 [06:15<00:04,  2.23it/s]Running loglikelihood requests:  98%|█████████▊| 392/400 [06:15<00:03,  2.24it/s]Running loglikelihood requests:  98%|█████████▊| 393/400 [06:16<00:03,  2.20it/s]Running loglikelihood requests:  98%|█████████▊| 394/400 [06:16<00:02,  2.19it/s]Running loglikelihood requests:  99%|█████████▉| 395/400 [06:17<00:02,  2.20it/s]Running loglikelihood requests:  99%|█████████▉| 396/400 [06:17<00:01,  2.21it/s]Running loglikelihood requests:  99%|█████████▉| 397/400 [06:18<00:01,  2.22it/s]Running loglikelihood requests: 100%|█████████▉| 398/400 [06:18<00:00,  2.24it/s]Running loglikelihood requests: 100%|█████████▉| 399/400 [06:19<00:00,  2.24it/s]Running loglikelihood requests: 100%|██████████| 400/400 [06:19<00:00,  2.25it/s]Running loglikelihood requests: 100%|██████████| 400/400 [06:19<00:00,  1.05it/s]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/wnli?recursive=False&expand=False HTTP/1.1" 307 141
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/wnli?recursive=False&expand=False HTTP/1.1" 200 352
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140229628722112 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140229628722112 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140229628722112 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140229628722112 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140238671042016 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140238671042016 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140238671042016 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140238671042016 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
full model:
{'sciq': {'alias': 'sciq', 'acc,none': 0.94, 'acc_stderr,none': 0.023868325657594204, 'acc_norm,none': 0.91, 'acc_norm_stderr,none': 0.028762349126466136}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.969062788859705
0.9024924890572922
0.7706109127217512
0.8221264026535647
0.9190490061886575
0.9866654579796295
0.6586322754204971
0.7962110384246164
0.8195614021629236
0.7124178311176441
0.787697814339696
0.7034455022322618
0.8136386046534271
0.8174990104652458
0.6784276389594894
0.8698440245672888
0.8886492811850213
0.6541737276411673
0.6560861559753316
0.8139845219953913
0.6714741870309046
0.6164364868717988
0.8331581872497299
0.9065420049234512
0.9246185715568276
0.7477515960551026
0.574165362968651
0.8586446364199891
0.8889771415746612
Total groups 70 exceeded the threshold, stopping comparison.
The group tensor is
[7, 3, 4, 2, 6, 1, 5, 0]
tensor([7, 3, 4, 2, 6, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 5, 3, 4, 0, 7, 1]
tensor([6, 2, 5, 3, 4, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 3, 6, 2, 7, 1, 4, 0]
tensor([5, 3, 6, 2, 7, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 4, 2, 1, 3, 5, 1]
tensor([0, 0, 4, 2, 1, 3, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 3, 4, 5, 0, 1, 1]
tensor([0, 2, 3, 4, 5, 0, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 0, 2, 2, 3, 1]
tensor([0, 3, 1, 0, 2, 2, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 3, 1, 1, 2, 2, 3, 0]
tensor([0, 3, 1, 1, 2, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 1, 1.0, 1.0, 1.0, 1.0, 1]
tensor([0, 0, 1, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 2 to 4
done!
Normal merging for layer 5
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 6 to 9
done!
Normal merging for layer 10
tensor([0, 1])
tensor(0)
tensor([4, 7])
tensor(4)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 11 to 12
done!
Normal merging for layer 13
tensor([0, 5])
tensor(0)
tensor([6, 7])
tensor(6)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 14 to 19
done!
Normal merging for layer 20
tensor([0, 3])
tensor(0)
tensor([2, 7])
tensor(2)
tensor([4, 5])
tensor(4)
tensor([1, 6])
tensor(1)
done!
Normal merging for layer 21
tensor([0, 7])
tensor(0)
tensor([2, 3])
tensor(2)
tensor([4, 5])
tensor(4)
tensor([1, 6])
tensor(1)
done!
Cross-layer merge completed for layers 22 to 23
done!
Normal merging for layer 24
tensor([0, 1])
tensor(0)
tensor([2, 3, 4, 5, 6, 7])
tensor(2)
done!
Cross-layer merge completed for layers 25 to 31
done!
all done!
Model size: 12.2608 GB
[159] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2682.43it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:03,  3.00s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:31,  1.52s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:06<02:50,  1.24s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:37,  1.17s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:25,  1.09s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:12<02:15,  1.04s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:08,  1.01it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:16<02:01,  1.05it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<02:10,  1.04s/it]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:58,  1.03it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:50,  1.09it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:43,  1.15it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:25<01:37,  1.20it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:32,  1.24it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:37,  1.16it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:30<01:31,  1.21it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:31<01:27,  1.25it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:33<01:23,  1.28it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:34<01:20,  1.30it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:35<01:17,  1.32it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:37<01:15,  1.34it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:15,  1.30it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:40<01:12,  1.34it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:41<01:09,  1.37it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:43<01:08,  1.36it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:44<01:04,  1.41it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:45<01:01,  1.44it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<00:59,  1.47it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<00:59,  1.44it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:50<00:56,  1.47it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:51<00:54,  1.49it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<00:52,  1.51it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:53<00:50,  1.52it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:55<00:48,  1.54it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:56<00:47,  1.55it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:45,  1.56it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:59<00:46,  1.50it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:00<00:44,  1.52it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:42,  1.54it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:40,  1.56it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:04<00:38,  1.57it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:05<00:37,  1.58it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:36,  1.58it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:34,  1.59it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:09<00:38,  1.38it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:11<00:35,  1.45it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:12<00:32,  1.50it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:13<00:30,  1.54it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:14<00:28,  1.57it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:15<00:26,  1.60it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:17<00:25,  1.62it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:18<00:23,  1.63it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:19<00:23,  1.54it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:20<00:22,  1.59it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:22<00:20,  1.62it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:23<00:18,  1.64it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:24<00:17,  1.66it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:25<00:16,  1.68it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:26<00:14,  1.68it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:27<00:13,  1.70it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:29<00:12,  1.65it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:30<00:11,  1.64it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:31<00:10,  1.57it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:33<00:09,  1.62it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:34<00:07,  1.66it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:35<00:06,  1.69it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:36<00:05,  1.71it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:37<00:04,  1.72it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:38<00:02,  1.74it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:40<00:01,  1.66it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:41<00:00,  1.72it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:41<00:00,  1.40it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-3): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-6): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-15): 8 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-21): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (23-27): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (29-30): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-3): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-6): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-15): 8 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-21): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (23-27): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (29-30): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/159.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140214649860176 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140214649860176 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140214649860176 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140214649860176 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140214649853312 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140214649853312 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140214649853312 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140214649853312 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[159] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.43661971830985913, 'acc_stderr,none': 0.05927935558412972}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7988766190696382
0.433785810695937
0.6525315214782026
0.871250377966373
0.7790022985921221
0.6581533546839238
0.6948782189696622
0.4773368074709654
0.7967725763717977
0.6899017375628973
0.8876656342513365
0.741048035098914
0.7711231987685904
0.658722026565161
0.8415498496492992
0.7230018765402745
0.5047290951856622
0.7541475774993723
0.5858483604351846
0.9316615143976832
0.7753381144708663
0.5505312367470351
0.97693166402346
0.9600861235134092
0.9818242386505116
0.9256028941559552
0.44553043946224297
0.3299453067275913
0.5544144775455869
0.7988766190696382
0.433785810695937
0.6525315214782026
0.871250377966373
0.7790022985921221
0.6581533546839238
0.6948782189696622
0.4773368074709654
0.7967725763717977
0.6899017375628973
0.8876656342513365
0.741048035098914
0.7711231987685904
0.658722026565161
0.8415498496492992
0.7230018765402745
0.5047290951856622
0.7541475774993723
Total groups 72 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 1, 2, 0, 6, 4]
tensor([5, 3, 7, 1, 2, 0, 6, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 4, 5, 2, 0, 7, 1]
tensor([6, 3, 4, 5, 2, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 4, 7, 2, 5, 1, 6, 0]
tensor([3, 4, 7, 2, 5, 1, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 4, 7, 1, 3, 0, 5, 2]
tensor([6, 4, 7, 1, 3, 0, 5, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 6, 2, 0, 1, 7, 4]
tensor([5, 3, 6, 2, 0, 1, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[3, 5, 0, 2, 4, 0, 1, 1]
tensor([3, 5, 0, 2, 4, 0, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/159.pt
[52] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2633.31it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:46,  3.73s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:57,  1.71s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:08<03:42,  1.62s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:10<03:01,  1.34s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:13<02:59,  1.35s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:15<02:36,  1.20s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:17<02:20,  1.09s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:18<02:08,  1.01s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:20<01:59,  1.04it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<01:51,  1.10it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:23<01:48,  1.11it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:25<01:41,  1.17it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:26<01:35,  1.22it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:28<01:30,  1.26it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:29<01:27,  1.30it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:31<01:24,  1.31it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:32<01:22,  1.33it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:34<01:22,  1.30it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:35<01:19,  1.32it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:37<01:16,  1.34it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:38<01:14,  1.36it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:12,  1.37it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:41<01:14,  1.30it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:43<01:10,  1.34it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:44<01:10,  1.33it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:45<01:05,  1.38it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:47<01:02,  1.42it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:48<00:59,  1.45it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:49<00:57,  1.48it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:51<00:55,  1.50it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:52<00:53,  1.50it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:53<00:54,  1.44it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:55<00:52,  1.46it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:56<00:50,  1.49it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:57<00:48,  1.51it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:59<00:46,  1.52it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:00<00:48,  1.41it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:02<00:46,  1.45it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:03<00:43,  1.49it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:04<00:43,  1.46it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:06<00:40,  1.49it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:07<00:38,  1.52it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:08<00:37,  1.54it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:09<00:35,  1.57it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:11<00:33,  1.59it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:12<00:31,  1.60it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:13<00:30,  1.61it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:14<00:30,  1.56it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:16<00:28,  1.59it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:17<00:26,  1.60it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:18<00:25,  1.61it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:19<00:23,  1.63it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:20<00:22,  1.64it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:22<00:21,  1.65it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:23<00:19,  1.65it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:24<00:19,  1.56it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:25<00:18,  1.59it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:27<00:16,  1.61it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:28<00:15,  1.63it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:29<00:13,  1.64it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:30<00:12,  1.65it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:31<00:11,  1.67it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:33<00:10,  1.67it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:34<00:08,  1.67it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:35<00:08,  1.61it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:36<00:06,  1.63it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:37<00:05,  1.67it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:39<00:04,  1.70it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:40<00:02,  1.73it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:41<00:01,  1.76it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:42<00:00,  1.78it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:42<00:00,  1.39it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-8): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-13): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15-31): 17 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-8): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-13): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15-31): 17 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/52.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140213322335488 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213322335488 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213322335488 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213322335488 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140212903049552 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140212903049552 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140212903049552 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140212903049552 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[52] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.36326485003451675
0.9393611658494853
0.5347697273308267
0.4619639883096971
0.26830156279453604
0.8380027464731019
0.8263233574501603
0.6320977880109986
0.7843146415103402
0.2072713135482749
0.3805984042723936
0.43289830555723763
0.19273781754260347
0.8181624071033468
0.5320194969946469
0.7203219641552613
0.7789354624632999
0.3567665334531696
0.2604208944916411
0.942273921845013
0.8798126471295904
0.8327076147848976
0.5471221204152548
0.8016285641373796
0.8787721123612405
0.7314094662215913
0.733830137407364
0.7399689895148621
0.8815844426907766
0.36326485003451675
0.9393611658494853
0.5347697273308267
0.4619639883096971
0.26830156279453604
0.8380027464731019
0.8263233574501603
0.6320977880109986
0.7843146415103402
0.2072713135482749
0.3805984042723936
0.43289830555723763
0.19273781754260347
0.8181624071033468
0.5320194969946469
0.7203219641552613
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[1, 2, 5, 4, 3, 0, 7, 6]
tensor([1, 2, 5, 4, 3, 0, 7, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 2, 6, 0, 1, 4, 7]
tensor([5, 3, 2, 6, 0, 1, 4, 7], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 2, 6, 7, 4, 0, 3, 5]
tensor([1, 2, 6, 7, 4, 0, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 4, 5, 6, 2, 0, 7, 3]
tensor([1, 4, 5, 6, 2, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 5, 1, 0, 0, 2, 1, 3]
tensor([4, 5, 1, 0, 0, 2, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[1, 5, 0, 3, 4, 0, 1, 2]
tensor([1, 5, 0, 3, 4, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 2, 0, 1, 3, 2]
tensor([0, 3, 1, 2, 0, 1, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1, 0, 1.0, 1.0]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
Model saved locally at saved_models/52.pt
[7] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2621.86it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:27,  3.17s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:37,  1.57s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:54,  1.27s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:54,  1.29s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:34,  1.16s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:21,  1.08s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:10,  1.01s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:02,  1.04it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:59,  1.05it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:51,  1.11it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:44,  1.16it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:38,  1.21it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:25<01:33,  1.25it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:29,  1.28it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:26,  1.31it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:26,  1.28it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:31<01:23,  1.30it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:32<01:20,  1.33it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:18,  1.34it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:35<01:16,  1.35it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:36<01:14,  1.36it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:38<01:12,  1.37it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:39<01:12,  1.35it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:41<01:08,  1.38it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:42<01:05,  1.41it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:43<01:03,  1.44it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:45<01:00,  1.47it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:46<00:58,  1.49it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:47<00:56,  1.51it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:49<01:02,  1.32it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:50<00:58,  1.38it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<00:55,  1.42it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:53<00:52,  1.46it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:50,  1.49it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:56<00:48,  1.52it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:46,  1.53it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:44,  1.55it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:00<00:44,  1.50it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:42,  1.53it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:40,  1.55it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:38,  1.57it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:05<00:37,  1.58it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:35,  1.59it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:34,  1.60it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:08<00:32,  1.62it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:10<00:32,  1.55it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:11<00:34,  1.40it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:13<00:31,  1.47it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:14<00:29,  1.52it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:15<00:27,  1.57it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:16<00:25,  1.60it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:17<00:23,  1.63it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:19<00:22,  1.64it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:20<00:22,  1.57it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:21<00:20,  1.61it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:22<00:18,  1.64it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:24<00:20,  1.43it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:25<00:17,  1.50it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:26<00:16,  1.55it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:28<00:14,  1.60it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:29<00:12,  1.63it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:31<00:13,  1.41it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:32<00:11,  1.50it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:33<00:09,  1.55it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:34<00:08,  1.59it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:35<00:06,  1.63it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:36<00:05,  1.68it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:38<00:04,  1.72it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:39<00:02,  1.75it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:40<00:01,  1.63it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:41<00:00,  1.71it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:41<00:00,  1.40it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/7.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140215183328752 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140215183328752 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215183328752 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140215183328752 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140214649857920 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140214649857920 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140214649857920 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140214649857920 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[7] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.4249710358556562
0.875077076525312
0.8377657430496294
0.7631146361001728
0.8061134194824199
0.597293936853132
0.8993038548124084
0.40672319990150296
0.4427306135620195
0.6697754068966065
0.560869697816391
0.6365238158372074
0.7977251509942833
0.5774168099619936
0.7636367981130562
0.45204149170439595
0.2621700055892588
0.8796773875905071
0.8519712476811715
0.8067705781994788
0.5994711637356425
0.8334667984344581
0.8573465838873401
0.8381586594258955
0.8329727759300379
0.8464402476627775
0.7864641899632313
0.6699089018877085
0.7340555031678195
0.4249710358556562
0.875077076525312
0.8377657430496294
0.7631146361001728
0.8061134194824199
0.597293936853132
0.8993038548124084
0.40672319990150296
0.4427306135620195
0.6697754068966065
0.560869697816391
0.6365238158372074
0.7977251509942833
0.5774168099619936
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[2, 1, 7, 5, 6, 0, 4, 3]
tensor([2, 1, 7, 5, 6, 0, 4, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 6, 7, 1, 3, 0, 5, 4]
tensor([2, 6, 7, 1, 3, 0, 5, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 1, 7, 4, 3, 0, 5, 6]
tensor([2, 1, 7, 4, 3, 0, 5, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 7, 1, 4, 0, 6, 2]
tensor([5, 3, 7, 1, 4, 0, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 1, 3, 0, 7, 2]
tensor([5, 4, 6, 1, 3, 0, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[2, 5, 0, 4, 1, 3, 1, 0]
tensor([2, 5, 0, 4, 1, 3, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 3, 1, 1, 2, 0, 3, 2]
tensor([0, 3, 1, 1, 2, 0, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/7.pt
[47] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 1958.78it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:31,  2.78s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:34,  1.54s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:54,  1.27s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:36,  1.16s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:25,  1.09s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:12<02:16,  1.04s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:14,  1.04s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:16<02:05,  1.01it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:57,  1.06it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:49,  1.12it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:43,  1.16it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:38,  1.21it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:35,  1.22it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:32,  1.25it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:28,  1.28it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:25,  1.30it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:22,  1.32it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:32<01:19,  1.34it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:17,  1.36it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:35<01:20,  1.29it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:36<01:16,  1.32it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:38<01:13,  1.35it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:39<01:09,  1.39it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:41<01:13,  1.29it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:42<01:08,  1.35it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:43<01:04,  1.41it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:45<01:03,  1.41it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:46<01:00,  1.44it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:47<00:57,  1.48it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:49<00:55,  1.50it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:50<00:53,  1.52it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:51<00:53,  1.47it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:53<00:51,  1.50it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:49,  1.52it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:49,  1.48it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:47,  1.50it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:45,  1.53it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:43,  1.55it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:41,  1.56it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:39,  1.59it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:38,  1.59it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:36,  1.60it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:05<00:36,  1.55it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:34,  1.59it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:08<00:32,  1.62it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:09<00:31,  1.64it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:10<00:29,  1.65it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:11<00:28,  1.67it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:12<00:26,  1.68it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:14<00:25,  1.69it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:15<00:24,  1.70it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:16<00:24,  1.59it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:18<00:26,  1.37it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:19<00:23,  1.46it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:21<00:21,  1.53it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:22<00:19,  1.58it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:23<00:17,  1.62it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:24<00:16,  1.65it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:25<00:14,  1.68it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:26<00:13,  1.64it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:28<00:12,  1.67it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:29<00:11,  1.70it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:30<00:09,  1.72it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:31<00:08,  1.73it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:32<00:07,  1.74it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:33<00:06,  1.73it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:34<00:05,  1.74it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:35<00:03,  1.78it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:37<00:02,  1.73it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:38<00:01,  1.77it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:39<00:00,  1.81it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:39<00:00,  1.43it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-5): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7-9): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-26): 16 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28-30): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-5): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7-9): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-26): 16 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28-30): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/47.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140214249098080 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140214249098080 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140214249098080 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140214249098080 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140214651255504 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140214651255504 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140214651255504 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140214651255504 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[47] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7169249479288816
0.6183111372245332
0.46095560668163316
0.45412629074361927
0.5961385422474115
0.8890533054728373
0.9122749865545955
0.7722412965326397
0.28598717020359266
0.16135518374824295
0.6405733513488978
0.34881239744915987
0.02163114281968652
0.8586809120820075
0.5280864903169471
0.5549214872848108
0.8809884720847853
0.8184475126773784
0.7411818478320343
0.9204128494945557
0.8828541622108917
0.8520796846320295
0.9938665155729866
0.8532228506983649
0.8363754351071634
0.7452449391867536
0.8461045724576085
0.3693743162395131
0.3582309316878524
0.7169249479288816
0.6183111372245332
0.46095560668163316
0.45412629074361927
0.5961385422474115
0.8890533054728373
0.9122749865545955
0.7722412965326397
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[5, 4, 6, 3, 2, 0, 7, 1]
tensor([5, 4, 6, 3, 2, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 7, 2, 4, 0, 5, 3]
tensor([6, 1, 7, 2, 4, 0, 5, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 6, 3, 1, 0, 7, 5]
tensor([2, 4, 6, 3, 1, 0, 7, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 0, 6, 4, 3, 1, 5, 7]
tensor([2, 0, 6, 4, 3, 1, 5, 7], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[3, 1, 0, 1, 2, 0, 2, 3]
tensor([3, 1, 0, 1, 2, 0, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 2, 1, 0, 2, 3, 3]
tensor([0, 1, 2, 1, 0, 2, 3, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[3, 1, 0, 1, 2, 0, 3, 2]
tensor([3, 1, 0, 1, 2, 0, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 1, 2, 2, 3, 0, 3, 1]
tensor([0, 1, 2, 2, 3, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[3, 0, 1, 2, 1, 2, 3, 0]
tensor([3, 0, 1, 2, 1, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/47.pt
[89] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2642.73it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:48,  2.89s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:04<03:22,  1.45s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:06<02:42,  1.19s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:08<02:26,  1.09s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:10<02:17,  1.03s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:12<02:18,  1.06s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:09,  1.00s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:16<02:00,  1.06it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:17<01:53,  1.10it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:45,  1.16it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:20<01:39,  1.21it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:22<01:48,  1.10it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:39,  1.17it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:25<01:32,  1.24it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:27,  1.29it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:28<01:23,  1.33it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:20,  1.35it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:31<01:17,  1.38it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:36<02:13,  1.28s/it]Running loglikelihood requests:  27%|██▋       | 39/142 [00:37<01:53,  1.10s/it]Running loglikelihood requests:  29%|██▉       | 41/142 [00:39<01:38,  1.02it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:40<01:28,  1.12it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:42<01:25,  1.14it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:43<01:17,  1.23it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:45<01:12,  1.28it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:46<01:06,  1.37it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:47<01:01,  1.44it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:48<00:58,  1.49it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:50<00:55,  1.53it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:53<01:19,  1.05it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:54<01:09,  1.17it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:55<01:01,  1.28it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:57<00:56,  1.37it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:58<00:51,  1.44it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:59<00:48,  1.50it/s]Running loglikelihood requests:  50%|█████     | 71/142 [01:00<00:46,  1.54it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:01<00:43,  1.59it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:03<00:42,  1.56it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:04<00:40,  1.60it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:05<00:38,  1.63it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:06<00:36,  1.65it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:07<00:35,  1.66it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:09<00:34,  1.68it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:10<00:32,  1.67it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:11<00:31,  1.68it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:12<00:31,  1.62it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:13<00:29,  1.65it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:15<00:28,  1.67it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:16<00:26,  1.68it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:17<00:25,  1.70it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:18<00:23,  1.71it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:19<00:22,  1.72it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:20<00:21,  1.72it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:21<00:20,  1.73it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:26<00:35,  1.08s/it]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:27<00:28,  1.08it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:28<00:23,  1.23it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:29<00:19,  1.36it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:30<00:17,  1.47it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:34<00:21,  1.05it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:35<00:17,  1.19it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:36<00:14,  1.32it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:37<00:11,  1.45it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:38<00:09,  1.55it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:39<00:07,  1.63it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:40<00:06,  1.69it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:41<00:05,  1.75it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:42<00:03,  1.80it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:43<00:02,  1.77it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:45<00:01,  1.81it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:46<00:00,  1.86it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:46<00:00,  1.34it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-25): 18 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27-31): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-25): 18 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27-31): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/89.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140213854906624 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213854906624 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213854906624 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213854906624 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140229615663680 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140229615663680 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140229615663680 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140229615663680 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[89] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.4211049002945932
0.6717558353282359
0.7474911564899941
0.6826176878179091
0.7797142635267413
0.8231499933202435
0.9112841579744179
0.9329117857417774
0.3884156651377716
0.7116719161574963
0.3163186199985964
0.7579641143181478
0.7487852880300513
0.644901206148365
0.6252899485594469
0.8300709483875389
0.9686405093585702
0.4827230969630938
0.4401250454367978
0.846541236433719
0.9874080634814415
0.6954384705363965
0.33457229698994645
0.3090774332022817
0.9161115178604439
0.6466213503285708
0.719815146262638
0.8765685188625579
0.30061586460742784
0.4211049002945932
0.6717558353282359
0.7474911564899941
0.6826176878179091
0.7797142635267413
0.8231499933202435
0.9112841579744179
0.9329117857417774
0.3884156651377716
0.7116719161574963
0.3163186199985964
0.7579641143181478
0.7487852880300513
0.644901206148365
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[2, 5, 6, 1, 4, 0, 7, 3]
tensor([2, 5, 6, 1, 4, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 6, 1, 5, 0, 7, 3]
tensor([2, 4, 6, 1, 5, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 2, 7, 0, 1, 3, 6, 5]
tensor([4, 2, 7, 0, 1, 3, 6, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 3, 7, 1, 2, 0, 5, 4]
tensor([6, 3, 7, 1, 2, 0, 5, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 5, 0, 0, 2, 3, 1, 1]
tensor([4, 5, 0, 0, 2, 3, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 4, 1, 2, 1, 0, 3, 5]
tensor([0, 4, 1, 2, 1, 0, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 5, 0, 2, 1, 1, 4]
tensor([0, 3, 5, 0, 2, 1, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/89.pt
[222] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2592.14it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:04<09:49,  4.18s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:06<04:13,  1.82s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:08<03:13,  1.41s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:10<02:43,  1.21s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:26,  1.10s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:18,  1.06s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:08,  1.00it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:07,  1.01s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<01:58,  1.06it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:49,  1.12it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:43,  1.17it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<01:40,  1.19it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:26<01:47,  1.09it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:27<01:38,  1.16it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:29<01:32,  1.22it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:30<01:27,  1.27it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:31<01:23,  1.31it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:33<01:20,  1.34it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:35<01:29,  1.17it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:23,  1.24it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:38<01:18,  1.28it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:15,  1.32it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:41<01:11,  1.36it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:42<01:08,  1.39it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:44<01:08,  1.35it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:45<01:04,  1.41it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:46<01:01,  1.46it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<00:58,  1.50it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:49<00:55,  1.52it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:50<00:53,  1.54it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:51<00:51,  1.56it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<00:50,  1.57it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:54<00:54,  1.42it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:55<00:50,  1.47it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:57<00:48,  1.52it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:58<00:45,  1.54it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:59<00:43,  1.58it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:00<00:41,  1.60it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:02<00:40,  1.61it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:03<00:38,  1.63it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:04<00:41,  1.46it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:06<00:38,  1.51it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:07<00:36,  1.56it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:08<00:34,  1.60it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:09<00:32,  1.63it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:10<00:30,  1.66it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:11<00:29,  1.67it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:13<00:27,  1.68it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:14<00:27,  1.63it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:15<00:25,  1.66it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:16<00:24,  1.68it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:17<00:22,  1.70it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:19<00:21,  1.71it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:20<00:20,  1.72it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:21<00:19,  1.73it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:22<00:17,  1.74it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:23<00:16,  1.76it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:24<00:15,  1.69it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:26<00:14,  1.71it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:27<00:13,  1.73it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:28<00:12,  1.73it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:29<00:10,  1.73it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:30<00:09,  1.75it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:31<00:08,  1.78it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:32<00:07,  1.80it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:33<00:06,  1.81it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:35<00:05,  1.77it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:36<00:03,  1.81it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:37<00:02,  1.84it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:42<00:03,  1.17s/it]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:43<00:00,  1.02it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:43<00:00,  1.37it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-8): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-30): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-8): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-30): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/222.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140213322336880 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213322336880 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213322336880 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213322336880 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140212904250256 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140212904250256 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140212904250256 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140212904250256 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[222] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.891975958338049
0.3982001908036579
0.26652204443251004
0.14111026067874816
0.11763525057572936
0.8635004180455561
0.8741394655456793
0.5304527321557103
0.44617634270319845
0.7007455592903002
0.6799150362476898
0.3511365594568724
0.47145939463701925
0.4744673317050211
0.905435493752087
0.903612941640167
0.8893763849001028
0.6821933807976941
0.6796182015499347
0.7774502436639344
0.8164788896198145
0.9011095591650676
0.8564639597370403
0.9528866120532432
0.4784452972921274
0.48852492817861076
0.8397355586186119
0.7433051149723976
0.5457687574271932
0.891975958338049
0.3982001908036579
0.26652204443251004
0.14111026067874816
0.11763525057572936
0.8635004180455561
0.8741394655456793
0.5304527321557103
0.44617634270319845
0.7007455592903002
0.6799150362476898
0.3511365594568724
0.47145939463701925
0.4744673317050211
0.905435493752087
0.903612941640167
0.8893763849001028
0.6821933807976941
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[6, 5, 4, 3, 2, 1, 7, 0]
tensor([6, 5, 4, 3, 2, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 7, 4, 1, 0, 6, 3]
tensor([5, 2, 7, 4, 1, 0, 6, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 6, 5, 2, 4, 0, 7, 1]
tensor([3, 6, 5, 2, 4, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 0, 2, 1, 7, 3]
tensor([5, 4, 6, 0, 2, 1, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 0, 7, 3, 5, 1, 6, 2]
tensor([4, 0, 7, 3, 5, 1, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 5, 1, 3, 2, 0, 1, 4]
tensor([0, 5, 1, 3, 2, 0, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 0, 0, 1, 1.0, 1.0, 1.0, 1.0]
tensor([1, 0, 0, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[1, 0, 1, 1.0, 1.0, 0, 1.0, 1.0]
tensor([1, 0, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 1.0, 0, 1, 1.0, 1.0]
tensor([0, 1, 1, 1, 0, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/222.pt
[81] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2604.57it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:51,  2.92s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:04<03:28,  1.50s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:57,  1.30s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:37,  1.17s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:25,  1.10s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:12<02:15,  1.03s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:06,  1.02it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:16<01:59,  1.06it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:57,  1.07it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:49,  1.12it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:43,  1.17it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:22<01:38,  1.21it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:33,  1.25it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:34,  1.21it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:37,  1.15it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:31,  1.21it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:27,  1.25it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:32<01:23,  1.29it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:20,  1.31it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:35<01:17,  1.33it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:36<01:15,  1.34it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:38<01:16,  1.29it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:39<01:12,  1.33it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:41<01:09,  1.36it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:42<01:06,  1.39it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:43<01:04,  1.42it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:45<01:02,  1.44it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:46<00:59,  1.45it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<01:00,  1.40it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:49<00:57,  1.43it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:50<00:55,  1.46it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<00:53,  1.47it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:53<00:51,  1.49it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:49,  1.50it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:56<00:48,  1.52it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:46,  1.52it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:59<00:49,  1.38it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:00<00:46,  1.43it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:44,  1.47it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:41,  1.50it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:04<00:39,  1.53it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:05<00:38,  1.55it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:36,  1.55it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:08<00:35,  1.56it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:09<00:35,  1.49it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:10<00:33,  1.52it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:12<00:33,  1.44it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:13<00:31,  1.50it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:14<00:29,  1.53it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:16<00:27,  1.56it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:17<00:25,  1.58it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:19<00:30,  1.28it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:20<00:26,  1.37it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:21<00:24,  1.45it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:23<00:21,  1.50it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:24<00:20,  1.55it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:25<00:20,  1.44it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:27<00:18,  1.49it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:28<00:16,  1.54it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:30<00:16,  1.36it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:31<00:14,  1.45it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:32<00:12,  1.52it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:33<00:10,  1.57it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:34<00:09,  1.61it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:36<00:07,  1.65it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:37<00:06,  1.68it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:38<00:05,  1.71it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:39<00:04,  1.67it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:42<00:04,  1.18it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:44<00:02,  1.21it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:45<00:00,  1.35it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:45<00:00,  1.35it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-15): 10 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-18): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (19-20): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (21): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22-24): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (25): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26-29): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (30-31): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-15): 10 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-18): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (19-20): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (21): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22-24): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (25): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26-29): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (30-31): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/81.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140214663375008 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140214663375008 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140214663375008 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140214663375008 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140215729496640 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140215729496640 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215729496640 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140215729496640 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[81] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.14760664350766747
0.8352462310811678
0.8321308433128256
0.6921610173004006
0.8887821417891327
0.9002690003330746
0.8806289653487678
0.28445629778111486
0.40731743116347024
0.9213637317788009
0.7749865640431746
0.5716601976941454
0.9731922502549005
0.5670705490427793
0.6522486093895017
0.8529538656844128
0.2520467552169693
0.5031341434536605
0.9297274773443689
0.8347846730794728
0.5913302222348289
0.6213415287437887
0.7775301720773112
0.5931169444310043
0.46564608236144167
0.4429317777448297
0.534169366976033
0.5376905800175403
0.7666618949093333
0.14760664350766747
0.8352462310811678
0.8321308433128256
0.6921610173004006
0.8887821417891327
0.9002690003330746
0.8806289653487678
0.28445629778111486
0.40731743116347024
0.9213637317788009
0.7749865640431746
0.5716601976941454
0.9731922502549005
0.5670705490427793
0.6522486093895017
0.8529538656844128
0.2520467552169693
0.5031341434536605
0.9297274773443689
0.8347846730794728
0.5913302222348289
0.6213415287437887
0.7775301720773112
0.5931169444310043
0.46564608236144167
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[2, 6, 7, 3, 1, 0, 5, 4]
tensor([2, 6, 7, 3, 1, 0, 5, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 6, 4, 7, 1, 0, 2, 3]
tensor([5, 6, 4, 7, 1, 0, 2, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 6, 7, 3, 1, 0, 5, 2]
tensor([4, 6, 7, 3, 1, 0, 5, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 2, 5, 1, 4, 0, 6, 3]
tensor([7, 2, 5, 1, 4, 0, 6, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 7, 4, 6, 2, 0, 5, 1]
tensor([3, 7, 4, 6, 2, 0, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 6, 4, 1, 5, 0, 7, 2]
tensor([3, 6, 4, 1, 5, 0, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 1, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
Model saved locally at saved_models/81.pt
[105] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2573.77it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:57,  2.96s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:04<03:27,  1.49s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:54,  1.27s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:08<02:31,  1.12s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:10<02:18,  1.04s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:12<02:09,  1.02it/s]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:01,  1.06it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:16<02:05,  1.01it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:57,  1.07it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:49,  1.13it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:43,  1.17it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:22<01:37,  1.22it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:32,  1.26it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:25<01:28,  1.30it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:30,  1.25it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:28<01:25,  1.29it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:22,  1.32it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:31<01:19,  1.35it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:32<01:16,  1.36it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:34<01:14,  1.38it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:35<01:12,  1.39it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:37<01:13,  1.35it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:38<01:10,  1.38it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:40<01:07,  1.41it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:41<01:04,  1.43it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:42<01:02,  1.47it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:44<00:59,  1.48it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:45<00:57,  1.50it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:47<01:04,  1.31it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:48<01:00,  1.38it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:49<00:56,  1.44it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:51<00:53,  1.48it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:52<00:50,  1.52it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:53<00:48,  1.54it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:54<00:46,  1.57it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:44,  1.59it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:57<00:45,  1.52it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:58<00:43,  1.55it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [00:59<00:41,  1.57it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:01<00:39,  1.58it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:02<00:38,  1.59it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:03<00:36,  1.60it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:04<00:35,  1.61it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:06<00:33,  1.62it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:07<00:33,  1.57it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:08<00:32,  1.58it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:09<00:30,  1.61it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:11<00:28,  1.62it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:12<00:27,  1.63it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:13<00:26,  1.65it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:14<00:24,  1.66it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:15<00:23,  1.68it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:17<00:23,  1.58it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:18<00:23,  1.48it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:20<00:21,  1.53it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:21<00:19,  1.58it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:22<00:17,  1.63it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:23<00:16,  1.67it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:24<00:14,  1.68it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:25<00:13,  1.71it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:26<00:12,  1.73it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:28<00:11,  1.65it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:29<00:10,  1.68it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:30<00:08,  1.71it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:31<00:07,  1.71it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:32<00:06,  1.73it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:33<00:05,  1.72it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:35<00:03,  1.75it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:36<00:02,  1.78it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:37<00:01,  1.81it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:38<00:00,  1.74it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:38<00:00,  1.44it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-7): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9-13): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-15): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-23): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-25): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26-31): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-7): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9-13): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-15): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-23): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-25): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26-31): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/105.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140215191765072 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140215191765072 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215191765072 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140215191765072 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140215191779280 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140215191779280 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215191779280 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140215191779280 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[105] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.4006974550645674
0.553274626438677
0.7114020264319938
0.6984504610379519
0.8708568984836823
0.6210210193880212
0.7638777300395475
0.4562187856309214
0.27778416422139596
0.7174549442708792
0.8874047516186899
0.7197371862802024
0.9973953807217149
0.8197334014617479
0.9421229754064668
0.9500023700992883
0.6312941692561447
0.47890039407555335
0.4338007663161437
0.9443928308635141
0.5752272779462101
0.5023096877772295
0.7917678755247893
0.42255885423676204
0.5830463630592772
0.884102890932443
0.7935069002678894
0.6274856483424485
0.841637545221562
0.4006974550645674
0.553274626438677
0.7114020264319938
0.6984504610379519
0.8708568984836823
0.6210210193880212
0.7638777300395475
0.4562187856309214
0.27778416422139596
0.7174549442708792
0.8874047516186899
0.7197371862802024
0.9973953807217149
0.8197334014617479
0.9421229754064668
0.9500023700992883
0.6312941692561447
0.47890039407555335
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[4, 2, 5, 1, 6, 3, 7, 0]
tensor([4, 2, 5, 1, 6, 3, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 4, 7, 5, 1, 0, 6, 2]
tensor([3, 4, 7, 5, 1, 0, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[2, 5, 6, 4, 3, 1, 7, 0]
tensor([2, 5, 6, 4, 3, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 1, 4, 3, 6, 0, 5, 2]
tensor([7, 1, 4, 3, 6, 0, 5, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[0, 5, 3, 1, 1, 0, 2, 4]
tensor([0, 5, 3, 1, 1, 0, 2, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[2, 5, 1, 0, 1, 0, 4, 3]
tensor([2, 5, 1, 0, 1, 0, 4, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[4, 0, 3, 0, 2, 1, 5, 1]
tensor([4, 0, 3, 0, 2, 1, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/105.pt
[140] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2577.74it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:32,  2.78s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:04<03:24,  1.47s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:06<02:47,  1.22s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<03:03,  1.36s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:40,  1.21s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:24,  1.10s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:13,  1.03s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:03,  1.02it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:56,  1.07it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<02:00,  1.02it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:51,  1.09it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<01:43,  1.15it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:25<01:37,  1.20it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:31,  1.25it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:27,  1.29it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:30<01:30,  1.23it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:31<01:26,  1.27it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:33<01:22,  1.30it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:34<01:20,  1.31it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:16,  1.34it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:37<01:14,  1.36it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:38<01:12,  1.37it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:40<01:13,  1.33it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:41<01:09,  1.38it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:43<01:06,  1.41it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:44<01:02,  1.45it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:45<01:00,  1.48it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<00:58,  1.50it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<00:55,  1.52it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:49<00:54,  1.53it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:51<00:54,  1.49it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<00:52,  1.51it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:53<00:50,  1.51it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:49,  1.53it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:56<00:47,  1.54it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:45,  1.55it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:44,  1.57it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:42,  1.58it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:43,  1.50it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:40,  1.54it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:39,  1.56it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:05<00:37,  1.58it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:35,  1.59it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:34,  1.61it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:08<00:32,  1.61it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:10<00:31,  1.62it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:11<00:31,  1.57it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:12<00:29,  1.60it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:13<00:27,  1.61it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:14<00:26,  1.63it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:16<00:24,  1.64it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:17<00:23,  1.65it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:19<00:29,  1.27it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:21<00:26,  1.31it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:22<00:23,  1.40it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:23<00:20,  1.48it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:24<00:18,  1.55it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:25<00:16,  1.60it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:27<00:15,  1.61it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:28<00:13,  1.65it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:29<00:12,  1.68it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:30<00:11,  1.70it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:31<00:10,  1.66it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:32<00:08,  1.69it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:34<00:07,  1.72it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:35<00:06,  1.74it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:36<00:05,  1.76it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:37<00:03,  1.79it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:38<00:02,  1.81it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:39<00:01,  1.83it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:40<00:00,  1.85it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:40<00:00,  1.41it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-31): 24 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-31): 24 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/140.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140213858405168 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213858405168 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213858405168 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213858405168 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140214649856864 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140214649856864 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140214649856864 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140214649856864 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[140] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4647887323943662, 'acc_stderr,none': 0.05961305784972239}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8184046739906345
0.7274935575095154
0.7761510346641957
0.4952640991880176
0.6026187510593423
0.9057397954213379
0.8847150465408834
0.8902479029100969
0.8909574741962474
0.9547688146913799
0.8922781384249192
0.8543488371673005
0.875959532399785
0.5999975074231996
0.5166995128054149
0.9251070768460925
0.9548013456523431
0.895916642208903
0.816962777390054
0.8352073170783986
0.8879963837403804
0.7652959230128763
0.6541271822267725
0.6185613360581901
0.9254826611316034
0.6384241255415785
0.5206869695172345
0.6788316772896046
0.8168301435765506
Total groups 76 exceeded the threshold, stopping comparison.
The group tensor is
[3, 2, 6, 5, 1, 0, 7, 4]
tensor([3, 2, 6, 5, 1, 0, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 3, 6, 5, 2, 0, 7, 4]
tensor([1, 3, 6, 5, 2, 0, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 1, 4, 2, 5, 0, 7, 3]
tensor([6, 1, 4, 2, 5, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 0, 7, 4, 3, 1, 6, 5]
tensor([2, 0, 7, 4, 3, 1, 6, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 4, 1, 3, 1, 0, 5, 2]
tensor([0, 4, 1, 3, 1, 0, 5, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[3, 4, 0, 1, 1, 0, 2, 5]
tensor([3, 4, 0, 1, 1, 0, 2, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 3, 1, 2, 1, 2, 3]
tensor([0, 0, 3, 1, 2, 1, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 1, 1.0, 1.0, 1, 1.0, 1.0]
tensor([0, 0, 1, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[1, 0, 1, 1.0, 1.0, 0, 1.0, 1.0]
tensor([1, 0, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[1, 0, 1, 1.0, 1.0, 0, 1.0, 1.0]
tensor([1, 0, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/140.pt
[16] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2646.48it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:56,  2.95s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:04<03:29,  1.51s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:55,  1.28s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:37,  1.16s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:32,  1.14s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:19,  1.07s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:10,  1.01s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:16<02:05,  1.01it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:57,  1.06it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:49,  1.12it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:43,  1.17it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:38,  1.21it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:33,  1.25it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:29,  1.29it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:33,  1.20it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:28,  1.25it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:24,  1.29it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:32<01:21,  1.32it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:18,  1.34it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:35<01:15,  1.36it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:36<01:13,  1.37it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:38<01:14,  1.33it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:39<01:10,  1.37it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:40<01:07,  1.40it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:42<01:05,  1.43it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:43<01:02,  1.46it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:44<01:00,  1.48it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:46<00:58,  1.50it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:47<00:58,  1.45it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:48<00:55,  1.48it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:50<00:53,  1.51it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:51<00:51,  1.53it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:52<00:49,  1.55it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:53<00:48,  1.56it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:46,  1.58it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:45,  1.58it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:57<00:45,  1.51it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:43,  1.54it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:41,  1.55it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:01<00:39,  1.58it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:02<00:38,  1.59it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:36,  1.59it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:05<00:35,  1.60it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:06<00:33,  1.62it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:08<00:34,  1.54it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:09<00:32,  1.57it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:10<00:30,  1.60it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:11<00:28,  1.62it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:12<00:27,  1.64it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:14<00:25,  1.66it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:15<00:24,  1.66it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:16<00:23,  1.67it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:17<00:22,  1.67it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:19<00:22,  1.53it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:20<00:21,  1.52it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:21<00:19,  1.57it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:22<00:18,  1.61it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:24<00:16,  1.64it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:25<00:15,  1.66it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:26<00:13,  1.69it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:27<00:13,  1.57it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:30<00:15,  1.21it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:31<00:12,  1.34it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:32<00:10,  1.44it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:33<00:08,  1.48it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:34<00:07,  1.56it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:36<00:05,  1.64it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:37<00:04,  1.70it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:38<00:03,  1.62it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:39<00:01,  1.69it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:40<00:00,  1.75it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:40<00:00,  1.41it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-5): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7-9): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-26): 16 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28-30): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-5): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7-9): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-26): 16 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28-30): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/16.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140215196597568 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140215196597568 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215196597568 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140215196597568 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140212913107024 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140212913107024 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140212913107024 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140212913107024 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[16] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7169249479288816
0.6183111372245332
0.46095560668163316
0.45412629074361927
0.5961385422474115
0.8890533054728373
0.9122749865545955
0.7722412965326397
0.28598717020359266
0.16135518374824295
0.6405733513488978
0.34881239744915987
0.02163114281968652
0.8586809120820075
0.5280864903169471
0.5549214872848108
0.8809884720847853
0.8184475126773784
0.7411818478320343
0.9204128494945557
0.8828541622108917
0.8520796846320295
0.9938665155729866
0.8532228506983649
0.8363754351071634
0.7452449391867536
0.8461045724576085
0.3693743162395131
0.3582309316878524
0.7169249479288816
0.6183111372245332
0.46095560668163316
0.45412629074361927
0.5961385422474115
0.8890533054728373
0.9122749865545955
0.7722412965326397
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[5, 4, 6, 3, 2, 0, 7, 1]
tensor([5, 4, 6, 3, 2, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 7, 2, 4, 0, 5, 3]
tensor([6, 1, 7, 2, 4, 0, 5, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 6, 3, 1, 0, 7, 5]
tensor([2, 4, 6, 3, 1, 0, 7, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 0, 6, 4, 3, 1, 5, 7]
tensor([2, 0, 6, 4, 3, 1, 5, 7], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[3, 1, 0, 1, 2, 0, 2, 3]
tensor([3, 1, 0, 1, 2, 0, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 2, 1, 0, 2, 3, 3]
tensor([0, 1, 2, 1, 0, 2, 3, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[3, 1, 0, 1, 2, 0, 3, 2]
tensor([3, 1, 0, 1, 2, 0, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 1, 2, 2, 3, 0, 3, 1]
tensor([0, 1, 2, 2, 3, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[3, 0, 1, 2, 1, 2, 3, 0]
tensor([3, 0, 1, 2, 1, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/16.pt
[167] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2595.33it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:51,  2.92s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:04<03:23,  1.46s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:06<02:46,  1.21s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:08<02:31,  1.12s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:10<02:18,  1.04s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:12<02:09,  1.01it/s]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:07,  1.02it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:16<01:59,  1.07it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:17<01:52,  1.11it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:45,  1.17it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:20<01:40,  1.21it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:22<01:35,  1.25it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:23<01:33,  1.25it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:25<01:28,  1.30it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:26<01:24,  1.33it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:27<01:21,  1.35it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:29<01:19,  1.37it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:30<01:17,  1.39it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:32<01:15,  1.39it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:33<01:17,  1.34it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:35<01:14,  1.35it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:36<01:12,  1.36it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:38<01:09,  1.39it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:39<01:07,  1.41it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:40<01:04,  1.44it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:42<01:02,  1.46it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:43<01:00,  1.48it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:45<01:09,  1.25it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:46<01:04,  1.33it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:48<00:59,  1.39it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:49<00:56,  1.43it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:50<00:53,  1.47it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:52<00:51,  1.50it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:53<00:51,  1.46it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:51,  1.41it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:48,  1.46it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:57<00:45,  1.51it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:58<00:43,  1.54it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:41,  1.57it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:01<00:39,  1.58it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:02<00:38,  1.60it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:03<00:36,  1.61it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:05<00:37,  1.53it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:06<00:35,  1.56it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:07<00:33,  1.59it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:08<00:31,  1.60it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:10<00:30,  1.61it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:11<00:28,  1.62it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:12<00:27,  1.63it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:13<00:26,  1.64it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:15<00:25,  1.58it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:16<00:24,  1.60it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:17<00:23,  1.61it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:18<00:21,  1.64it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:19<00:19,  1.67it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:20<00:18,  1.69it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:22<00:16,  1.71it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:23<00:15,  1.72it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:24<00:14,  1.72it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:26<00:15,  1.49it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:27<00:13,  1.56it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:28<00:11,  1.62it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:29<00:10,  1.66it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:30<00:08,  1.70it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:31<00:07,  1.73it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:32<00:06,  1.76it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:33<00:05,  1.78it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:35<00:04,  1.73it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:36<00:03,  1.59it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:37<00:01,  1.67it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:38<00:00,  1.75it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:38<00:00,  1.44it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/167.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140229612666656 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140229612666656 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140229612666656 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140229612666656 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140215181962048 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140215181962048 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215181962048 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140215181962048 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[167] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4084507042253521, 'acc_stderr,none': 0.058751136942575236}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8470147925890528
0.7476864669376683
0.7057458194715966
0.9259961826035564
0.9063486644110201
0.24963708122926456
0.19827660218091203
0.8342224461031358
0.674768686362321
0.8317740713616216
0.8052770539851904
0.87411142785436
0.9423251627051763
0.6325449231299249
0.5709019604280002
0.492272078021527
0.5470094105753499
0.8106788106419917
0.8007286389438932
0.8514363351777328
0.5678533410855942
0.6964663000476322
0.43791892189616227
0.24333097709743945
0.6773756911035087
0.7468917488225975
0.9536421875896308
0.6970547096788922
0.8446279043078407
0.8470147925890528
0.7476864669376683
0.7057458194715966
0.9259961826035564
0.9063486644110201
0.24963708122926456
0.19827660218091203
0.8342224461031358
0.674768686362321
0.8317740713616216
0.8052770539851904
0.87411142785436
0.9423251627051763
0.6325449231299249
0.5709019604280002
0.492272078021527
Total groups 76 exceeded the threshold, stopping comparison.
The group tensor is
[3, 4, 6, 5, 2, 0, 7, 1]
tensor([3, 4, 6, 5, 2, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 6, 5, 3, 0, 7, 1]
tensor([2, 4, 6, 5, 3, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 3, 4, 5, 2, 0, 7, 1]
tensor([6, 3, 4, 5, 2, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 4, 7, 2, 5, 0, 6, 3]
tensor([1, 4, 7, 2, 5, 0, 6, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[0, 3, 5, 2, 0, 1, 1, 4]
tensor([0, 3, 5, 2, 0, 1, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[4, 3, 0, 5, 2, 0, 1, 1]
tensor([4, 3, 0, 5, 2, 0, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 3, 5, 1, 4, 0, 1, 2]
tensor([0, 3, 5, 1, 4, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1, 0, 1.0, 1.0, 1.0]
tensor([0, 1, 1, 1, 0, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/167.pt
[250] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2638.77it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:12,  3.07s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:04<03:29,  1.50s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:06<02:49,  1.24s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:08<02:32,  1.13s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:34,  1.16s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:21,  1.08s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:11,  1.02s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:16<02:03,  1.03it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:55,  1.08it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:48,  1.13it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:46,  1.14it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:43,  1.15it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:37,  1.20it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:31,  1.25it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:27,  1.29it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:24,  1.31it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:21,  1.33it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:32<01:31,  1.17it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:34<01:25,  1.22it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:35<01:21,  1.27it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:37<01:17,  1.30it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:38<01:14,  1.33it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:40<01:18,  1.24it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:42<01:17,  1.22it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:43<01:11,  1.30it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:44<01:07,  1.36it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:46<01:03,  1.40it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<01:00,  1.44it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<00:57,  1.47it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:50<00:55,  1.49it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:52<01:08,  1.19it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:53<01:01,  1.28it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:55<00:56,  1.35it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:56<00:53,  1.41it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:57<00:50,  1.46it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:58<00:47,  1.49it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:00<00:45,  1.52it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:01<00:43,  1.54it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:03<00:44,  1.46it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:04<00:42,  1.50it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:05<00:39,  1.53it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:06<00:37,  1.56it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:07<00:36,  1.57it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:09<00:34,  1.59it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:10<00:32,  1.61it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:11<00:31,  1.61it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:13<00:31,  1.56it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:14<00:29,  1.59it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:15<00:28,  1.59it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:16<00:26,  1.60it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:17<00:25,  1.61it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:19<00:23,  1.64it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:20<00:22,  1.65it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:21<00:20,  1.67it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:23<00:22,  1.47it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:24<00:20,  1.50it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:25<00:18,  1.56it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:26<00:16,  1.60it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:27<00:15,  1.63it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:29<00:13,  1.66it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:30<00:12,  1.69it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:31<00:11,  1.71it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:33<00:11,  1.47it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:34<00:09,  1.55it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:35<00:08,  1.61it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:36<00:06,  1.66it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:37<00:05,  1.70it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:38<00:04,  1.74it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:39<00:02,  1.77it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:40<00:01,  1.79it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:41<00:00,  1.83it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:41<00:00,  1.39it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-29): 24 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (30-31): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-29): 24 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (30-31): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/250.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140214245023360 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140214245023360 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140214245023360 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140214245023360 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140238669273744 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140238669273744 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140238669273744 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140238669273744 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[250] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4084507042253521, 'acc_stderr,none': 0.058751136942575236}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9179357562486347
0.7304758827874278
0.9028361165350093
0.9177386064031547
0.736264115539795
0.7821589386855158
0.7475448337241333
0.6622221680289488
0.7583296634655016
0.33645990832698236
0.21982307453464583
0.17345324326533693
0.35977004925853934
0.8808900006571698
0.6260065234155616
0.8246960552033293
0.6144641592309815
0.6394144929277611
0.9223063206094516
0.7309208925817541
0.5091734316973643
0.7222829855220914
0.24969394334306674
0.508931879405537
0.8097279042671086
0.5258809052220982
0.4071243454156871
0.9510154626938295
0.3256867622893698
0.9179357562486347
0.7304758827874278
0.9028361165350093
0.9177386064031547
0.736264115539795
0.7821589386855158
0.7475448337241333
0.6622221680289488
0.7583296634655016
0.33645990832698236
0.21982307453464583
0.17345324326533693
0.35977004925853934
0.8808900006571698
0.6260065234155616
0.8246960552033293
0.6144641592309815
0.6394144929277611
0.9223063206094516
0.7309208925817541
0.5091734316973643
0.7222829855220914
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[1, 4, 3, 6, 2, 0, 7, 5]
tensor([1, 4, 3, 6, 2, 0, 7, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 1, 4, 7, 2, 0, 6, 5]
tensor([3, 1, 4, 7, 2, 0, 6, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 6, 7, 0, 3, 2, 4, 1]
tensor([5, 6, 7, 0, 3, 2, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 6, 3, 5, 0, 1, 7, 2]
tensor([4, 6, 3, 5, 0, 1, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 5, 0, 4, 1, 1, 3]
tensor([0, 2, 5, 0, 4, 1, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 2, 3, 0, 2, 3, 1]
tensor([0, 1, 2, 3, 0, 2, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[2, 1, 0, 1, 2, 0, 3, 3]
tensor([2, 1, 0, 1, 2, 0, 3, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 2, 1, 3, 0, 1, 2, 3]
tensor([0, 2, 1, 3, 0, 1, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/250.pt
[102] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2611.99it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:34,  2.80s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:04<03:23,  1.46s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:06<02:46,  1.22s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:08<02:30,  1.11s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:10<02:25,  1.09s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:12<02:20,  1.08s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:10,  1.01s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:16<02:01,  1.04it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:54,  1.09it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:47,  1.14it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:42,  1.18it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<02:10,  1.10s/it]Running loglikelihood requests:  18%|█▊        | 25/142 [00:25<01:55,  1.01it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:27<01:45,  1.09it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:37,  1.16it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:30<01:31,  1.21it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:31<01:27,  1.25it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:33<01:26,  1.24it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:35<01:22,  1.27it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:19,  1.30it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:37<01:16,  1.32it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:14,  1.34it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:40<01:10,  1.37it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:42<01:08,  1.39it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:44<01:16,  1.21it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:45<01:10,  1.29it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:46<01:06,  1.35it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:48<01:02,  1.40it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:49<00:59,  1.43it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:50<00:56,  1.46it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:52<00:54,  1.48it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:53<00:55,  1.43it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:55<00:54,  1.42it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:56<00:51,  1.45it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:57<00:49,  1.48it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:59<00:47,  1.50it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:00<00:45,  1.52it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:01<00:43,  1.53it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:03<00:48,  1.33it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:04<00:44,  1.41it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:06<00:41,  1.46it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:07<00:39,  1.50it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:08<00:37,  1.52it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:09<00:35,  1.54it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:11<00:34,  1.55it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:12<00:32,  1.55it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:13<00:33,  1.48it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:15<00:30,  1.53it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:16<00:28,  1.56it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:17<00:27,  1.59it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:18<00:25,  1.61it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:19<00:23,  1.63it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:21<00:22,  1.64it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:22<00:21,  1.65it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:23<00:21,  1.54it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:24<00:19,  1.58it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:26<00:18,  1.61it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:27<00:16,  1.64it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:28<00:15,  1.66it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:29<00:13,  1.68it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:30<00:12,  1.69it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:31<00:11,  1.71it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:33<00:09,  1.72it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:35<00:11,  1.29it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:36<00:09,  1.41it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:37<00:07,  1.49it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:38<00:05,  1.57it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:40<00:04,  1.63it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:41<00:02,  1.68it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:42<00:01,  1.72it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:43<00:00,  1.75it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:43<00:00,  1.37it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-8): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-20): 11 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (21): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22-23): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-25): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27-31): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-8): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-20): 11 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (21): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22-23): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-25): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27-31): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/102.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140213322079872 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213322079872 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213322079872 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213322079872 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140214252005056 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140214252005056 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140214252005056 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140214252005056 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[102] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.43661971830985913, 'acc_stderr,none': 0.05927935558412972}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8745617680355892
0.4870053329545456
0.7039635907698274
0.13683136283338845
0.45491781836309914
0.7394007339170455
0.7100810563328822
0.5089938007827006
0.6148932447522303
0.6097500975530942
0.2557945445996536
0.6585024343395449
0.3077968033302773
0.5696663704302732
0.4357302352587323
0.8508165638532892
0.2937402676601578
0.5546720889398816
0.456887590217546
0.8515148368106646
0.6351128178266827
0.587082322906938
0.4014945886309545
0.6816643170727646
0.7647343638504926
0.15578350583970074
0.7065213224778255
0.687952127174302
0.881700230921547
0.8745617680355892
0.4870053329545456
0.7039635907698274
0.13683136283338845
0.45491781836309914
0.7394007339170455
0.7100810563328822
0.5089938007827006
0.6148932447522303
0.6097500975530942
0.2557945445996536
0.6585024343395449
0.3077968033302773
0.5696663704302732
0.4357302352587323
0.8508165638532892
0.2937402676601578
0.5546720889398816
0.456887590217546
0.8515148368106646
0.6351128178266827
0.587082322906938
0.4014945886309545
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 6, 1, 0, 4, 2]
tensor([5, 3, 7, 6, 1, 0, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 7, 1, 5, 6, 0, 4, 2]
tensor([3, 7, 1, 5, 6, 0, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 3, 2, 4, 5, 1, 7, 0]
tensor([6, 3, 2, 4, 5, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 7, 5, 1, 3, 6, 0]
tensor([2, 4, 7, 5, 1, 3, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[0, 2, 4, 6, 5, 1, 7, 3]
tensor([0, 2, 4, 6, 5, 1, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 2, 5, 3, 6, 0, 7, 1]
tensor([4, 2, 5, 3, 6, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/102.pt
[34] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 1977.59it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:40,  3.27s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:37,  1.57s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:55,  1.28s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:33,  1.14s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:27,  1.11s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:15,  1.04s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:05,  1.03it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:16<01:58,  1.07it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:51,  1.12it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:44,  1.17it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:42,  1.18it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:22<01:37,  1.23it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:32,  1.27it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:25<01:28,  1.30it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:25,  1.33it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:28<01:22,  1.35it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:29<01:20,  1.36it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:31<01:23,  1.28it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:20,  1.31it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:34<01:17,  1.34it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:36<01:14,  1.36it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:37<01:12,  1.37it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:38<01:09,  1.40it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:40<01:06,  1.42it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:42<01:23,  1.12it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:44<01:14,  1.22it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:45<01:08,  1.30it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:46<01:04,  1.36it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<01:00,  1.41it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:49<00:57,  1.45it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:50<00:54,  1.48it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<00:54,  1.44it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:53<00:52,  1.48it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:49,  1.51it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:47,  1.54it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:45,  1.56it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:43,  1.58it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:42,  1.59it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:42,  1.53it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:42,  1.49it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:40,  1.52it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:38,  1.54it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:36,  1.57it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:34,  1.59it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:08<00:32,  1.61it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:09<00:31,  1.63it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:10<00:29,  1.64it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:12<00:29,  1.60it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:13<00:27,  1.62it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:14<00:26,  1.64it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:15<00:24,  1.65it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:17<00:23,  1.66it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:18<00:22,  1.66it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:19<00:20,  1.67it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:20<00:19,  1.67it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:21<00:18,  1.68it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:23<00:18,  1.59it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:24<00:16,  1.63it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:25<00:15,  1.66it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:26<00:13,  1.68it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:27<00:12,  1.68it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:29<00:11,  1.69it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:30<00:09,  1.70it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:31<00:08,  1.71it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:33<00:09,  1.44it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:34<00:07,  1.53it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:35<00:05,  1.59it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:36<00:04,  1.58it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:37<00:03,  1.63it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:39<00:01,  1.68it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:40<00:00,  1.56it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:40<00:00,  1.41it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/34.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140213322072768 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213322072768 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213322072768 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213322072768 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140215190383568 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140215190383568 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215190383568 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140215190383568 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[34] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9416968404415913
0.7938815868474761
0.8191819727516524
0.8917669687772857
0.90570342985793
0.3043907333176312
0.4952220736758006
0.29899990072660143
0.7758711420345523
0.838301482150715
0.8086122742620726
0.9126741954079025
0.6304407611776705
0.6595297618724628
0.176689009460142
0.4424440684940646
0.43152501031897406
0.48316840881543294
0.43857258209632005
0.7021905687600761
0.2402003641072479
0.27348823957209967
0.6649171314730306
0.25117416182112623
0.9114030338716436
0.9328254651349717
0.9096786197852272
0.9107213191843687
0.8764776988845487
0.9416968404415913
0.7938815868474761
0.8191819727516524
0.8917669687772857
0.90570342985793
0.3043907333176312
0.4952220736758006
0.29899990072660143
0.7758711420345523
0.838301482150715
0.8086122742620726
0.9126741954079025
0.6304407611776705
0.6595297618724628
0.176689009460142
0.4424440684940646
0.43152501031897406
Total groups 76 exceeded the threshold, stopping comparison.
The group tensor is
[4, 3, 7, 2, 5, 1, 6, 0]
tensor([4, 3, 7, 2, 5, 1, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 5, 7, 4, 3, 0, 6, 1]
tensor([2, 5, 7, 4, 3, 0, 6, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 5, 4, 0, 1, 1, 2]
tensor([0, 3, 5, 4, 0, 1, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[4, 3, 0, 2, 5, 0, 1, 1]
tensor([4, 3, 0, 2, 5, 0, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 0, 5, 1, 4, 2, 1, 3]
tensor([0, 0, 5, 1, 4, 2, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[3, 4, 0, 1, 2, 0, 5, 1]
tensor([3, 4, 0, 1, 2, 0, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[5, 0, 4, 3, 2, 0, 1, 1]
tensor([5, 0, 4, 3, 2, 0, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 3, 4, 0, 1, 2, 1, 5]
tensor([0, 3, 4, 0, 1, 2, 1, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/34.pt
[165] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2631.17it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:20,  3.55s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:52,  1.67s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:59,  1.31s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:36,  1.16s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:22,  1.07s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:12,  1.01s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:08,  1.00it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:16<02:02,  1.04it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:55,  1.08it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:47,  1.14it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:42,  1.19it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:36,  1.23it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:34,  1.23it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:30,  1.27it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:26,  1.30it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:23,  1.33it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:21,  1.34it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:31<01:19,  1.35it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:17,  1.36it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:35<01:18,  1.30it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:36<01:15,  1.34it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:37<01:13,  1.36it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:39<01:10,  1.38it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:40<01:07,  1.41it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:41<01:04,  1.43it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:43<01:02,  1.46it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:44<01:03,  1.40it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:46<01:00,  1.44it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:47<00:57,  1.48it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:48<00:55,  1.50it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:49<00:53,  1.52it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:51<00:51,  1.52it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:52<00:49,  1.54it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:53<00:48,  1.56it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:50,  1.44it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:47,  1.49it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:57<00:45,  1.51it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:43,  1.52it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:50,  1.28it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:48,  1.30it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:04<00:44,  1.38it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:05<00:43,  1.36it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:39,  1.44it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:36,  1.51it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:09<00:34,  1.56it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:10<00:32,  1.59it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:11<00:30,  1.63it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:12<00:28,  1.66it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:13<00:26,  1.68it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:15<00:26,  1.63it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:16<00:24,  1.66it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:17<00:23,  1.68it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:18<00:21,  1.71it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:19<00:20,  1.72it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:20<00:19,  1.74it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:21<00:17,  1.75it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:23<00:16,  1.76it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:24<00:15,  1.77it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:25<00:14,  1.71it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:26<00:13,  1.73it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:28<00:13,  1.56it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:29<00:11,  1.64it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:30<00:10,  1.69it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:31<00:08,  1.74it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:32<00:07,  1.76it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:33<00:06,  1.78it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:34<00:04,  1.82it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:36<00:04,  1.67it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:37<00:02,  1.74it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:38<00:01,  1.79it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:39<00:00,  1.83it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:39<00:00,  1.43it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-8): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-13): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15-31): 17 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-8): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-13): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15-31): 17 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/165.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140215199859008 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140215199859008 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215199859008 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140215199859008 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140214652583904 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140214652583904 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140214652583904 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140214652583904 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[165] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.36326485003451675
0.9393611658494853
0.5347697273308267
0.4619639883096971
0.26830156279453604
0.8380027464731019
0.8263233574501603
0.6320977880109986
0.7843146415103402
0.2072713135482749
0.3805984042723936
0.43289830555723763
0.19273781754260347
0.8181624071033468
0.5320194969946469
0.7203219641552613
0.7789354624632999
0.3567665334531696
0.2604208944916411
0.942273921845013
0.8798126471295904
0.8327076147848976
0.5471221204152548
0.8016285641373796
0.8787721123612405
0.7314094662215913
0.733830137407364
0.7399689895148621
0.8815844426907766
0.36326485003451675
0.9393611658494853
0.5347697273308267
0.4619639883096971
0.26830156279453604
0.8380027464731019
0.8263233574501603
0.6320977880109986
0.7843146415103402
0.2072713135482749
0.3805984042723936
0.43289830555723763
0.19273781754260347
0.8181624071033468
0.5320194969946469
0.7203219641552613
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[1, 2, 5, 4, 3, 0, 7, 6]
tensor([1, 2, 5, 4, 3, 0, 7, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 2, 6, 0, 1, 4, 7]
tensor([5, 3, 2, 6, 0, 1, 4, 7], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 2, 6, 7, 4, 0, 3, 5]
tensor([1, 2, 6, 7, 4, 0, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 4, 5, 6, 2, 0, 7, 3]
tensor([1, 4, 5, 6, 2, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 5, 1, 0, 0, 2, 1, 3]
tensor([4, 5, 1, 0, 0, 2, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[1, 5, 0, 3, 4, 0, 1, 2]
tensor([1, 5, 0, 3, 4, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 2, 0, 1, 3, 2]
tensor([0, 3, 1, 2, 0, 1, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1, 0, 1.0, 1.0]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
Model saved locally at saved_models/165.pt
[183] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2639.19it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:30,  2.77s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:06<04:40,  2.02s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:08<03:24,  1.49s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:10<02:49,  1.25s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:29,  1.13s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:20,  1.07s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:10,  1.01s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:01,  1.04it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<01:54,  1.09it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:47,  1.15it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:41,  1.19it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:41,  1.17it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:25<01:35,  1.23it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:29,  1.28it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:25,  1.32it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:22,  1.34it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:31<01:20,  1.36it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:32<01:17,  1.37it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:34<01:18,  1.33it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:35<01:16,  1.35it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:36<01:13,  1.37it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:38<01:11,  1.38it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:39<01:08,  1.41it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:40<01:06,  1.43it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:42<01:03,  1.46it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:43<01:01,  1.49it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:45<01:00,  1.46it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:46<00:58,  1.49it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:47<00:56,  1.52it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:48<00:54,  1.53it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:50<00:52,  1.55it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:51<00:50,  1.56it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:52<00:48,  1.58it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:53<00:47,  1.58it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:48,  1.51it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:46,  1.53it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:57<00:44,  1.55it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:42,  1.56it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:41,  1.58it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:01<00:39,  1.60it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:02<00:37,  1.61it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:03<00:36,  1.63it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:05<00:37,  1.54it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:06<00:34,  1.58it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:07<00:32,  1.62it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:08<00:31,  1.64it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:10<00:29,  1.66it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:11<00:27,  1.68it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:12<00:26,  1.69it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:13<00:25,  1.71it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:14<00:24,  1.66it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:16<00:24,  1.60it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:17<00:22,  1.64it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:18<00:20,  1.67it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:19<00:19,  1.69it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:20<00:18,  1.72it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:21<00:16,  1.74it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:23<00:15,  1.75it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:24<00:14,  1.75it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:25<00:13,  1.70it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:26<00:12,  1.73it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:27<00:10,  1.76it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:28<00:09,  1.78it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:29<00:08,  1.79it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:30<00:07,  1.81it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:31<00:06,  1.83it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:32<00:04,  1.85it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:34<00:03,  1.87it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:35<00:02,  1.89it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:36<00:01,  1.81it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:37<00:00,  1.83it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:37<00:00,  1.46it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-8): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-30): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-8): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-30): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/183.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140212913103328 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140212913103328 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140212913103328 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140212913103328 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140215723164304 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140215723164304 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215723164304 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140215723164304 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[183] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.891975958338049
0.3982001908036579
0.26652204443251004
0.14111026067874816
0.11763525057572936
0.8635004180455561
0.8741394655456793
0.5304527321557103
0.44617634270319845
0.7007455592903002
0.6799150362476898
0.3511365594568724
0.47145939463701925
0.4744673317050211
0.905435493752087
0.903612941640167
0.8893763849001028
0.6821933807976941
0.6796182015499347
0.7774502436639344
0.8164788896198145
0.9011095591650676
0.8564639597370403
0.9528866120532432
0.4784452972921274
0.48852492817861076
0.8397355586186119
0.7433051149723976
0.5457687574271932
0.891975958338049
0.3982001908036579
0.26652204443251004
0.14111026067874816
0.11763525057572936
0.8635004180455561
0.8741394655456793
0.5304527321557103
0.44617634270319845
0.7007455592903002
0.6799150362476898
0.3511365594568724
0.47145939463701925
0.4744673317050211
0.905435493752087
0.903612941640167
0.8893763849001028
0.6821933807976941
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[6, 5, 4, 3, 2, 1, 7, 0]
tensor([6, 5, 4, 3, 2, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 7, 4, 1, 0, 6, 3]
tensor([5, 2, 7, 4, 1, 0, 6, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 6, 5, 2, 4, 0, 7, 1]
tensor([3, 6, 5, 2, 4, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 0, 2, 1, 7, 3]
tensor([5, 4, 6, 0, 2, 1, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 0, 7, 3, 5, 1, 6, 2]
tensor([4, 0, 7, 3, 5, 1, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 5, 1, 3, 2, 0, 1, 4]
tensor([0, 5, 1, 3, 2, 0, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 0, 0, 1, 1.0, 1.0, 1.0, 1.0]
tensor([1, 0, 0, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[1, 0, 1, 1.0, 1.0, 0, 1.0, 1.0]
tensor([1, 0, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 1.0, 0, 1, 1.0, 1.0]
tensor([0, 1, 1, 1, 0, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/183.pt
[56] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2640.69it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:32,  3.64s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:50,  1.66s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:08,  1.38s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:40,  1.19s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:12<02:39,  1.20s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:22,  1.09s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:11,  1.02s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:01,  1.04it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:54,  1.09it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:47,  1.15it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:45,  1.15it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:38,  1.20it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:25<01:33,  1.25it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:28,  1.29it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:25,  1.32it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:23,  1.34it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:21,  1.35it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:32<01:21,  1.31it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:18,  1.33it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:35<01:16,  1.35it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:36<01:14,  1.36it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:38<01:11,  1.38it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:39<01:09,  1.40it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:41<01:06,  1.42it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:42<01:06,  1.40it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:43<01:03,  1.44it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:45<01:00,  1.47it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:46<00:58,  1.49it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:47<00:56,  1.51it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:48<00:54,  1.53it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:50<00:52,  1.53it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<01:01,  1.29it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:53<00:56,  1.36it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:53,  1.41it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:56<00:50,  1.45it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:48,  1.47it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:45,  1.50it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:00<00:43,  1.52it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:41,  1.55it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:41,  1.52it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:39,  1.55it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:05<00:37,  1.58it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:35,  1.59it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:34,  1.61it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:08<00:32,  1.62it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:09<00:31,  1.63it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:11<00:29,  1.65it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:12<00:30,  1.56it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:13<00:28,  1.58it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:15<00:26,  1.60it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:16<00:25,  1.61it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:17<00:23,  1.63it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:18<00:22,  1.64it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:19<00:21,  1.65it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:21<00:19,  1.65it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:22<00:19,  1.58it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:23<00:17,  1.62it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:24<00:16,  1.64it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:25<00:15,  1.65it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:27<00:13,  1.68it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:28<00:12,  1.69it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:29<00:11,  1.70it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:30<00:09,  1.71it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:31<00:08,  1.72it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:33<00:07,  1.63it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:34<00:06,  1.66it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:35<00:05,  1.69it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:36<00:04,  1.73it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:37<00:02,  1.75it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:38<00:01,  1.78it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:39<00:00,  1.81it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:39<00:00,  1.42it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-11): 8 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (12): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-31): 18 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-11): 8 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (12): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-31): 18 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/56.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140215723173856 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140215723173856 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215723173856 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140215723173856 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140214650039296 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140214650039296 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140214650039296 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140214650039296 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[56] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4788732394366197, 'acc_stderr,none': 0.05970805879899505}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.6890224269714
0.6519987454814186
0.44009979043140846
0.5066155659876163
0.7217003976797293
0.12899593918836094
0.4620116191334168
0.3468065225687064
0.10107939599729882
0.5600103662507451
0.577132167051305
0.6748045019607131
0.4004557650420357
0.700349463047757
0.9095160148001629
0.6131975246160634
0.5678386109581075
0.8437866782628755
0.3536670789186643
0.44960924435079147
0.5862896519346712
0.8246697057441258
0.5484674172302575
0.3288911762125467
0.6478473442183775
0.571596663207821
0.7179565348178579
0.38406312361402845
0.42349247998810474
0.6890224269714
0.6519987454814186
0.44009979043140846
0.5066155659876163
0.7217003976797293
0.12899593918836094
0.4620116191334168
0.3468065225687064
0.10107939599729882
0.5600103662507451
0.577132167051305
0.6748045019607131
0.4004557650420357
0.700349463047757
0.9095160148001629
0.6131975246160634
0.5678386109581075
0.8437866782628755
0.3536670789186643
0.44960924435079147
0.5862896519346712
0.8246697057441258
0.5484674172302575
0.3288911762125467
0.6478473442183775
0.571596663207821
Total groups 70 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 6, 2, 1, 4, 0]
tensor([5, 3, 7, 6, 2, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 5, 1, 3, 4, 0, 6, 2]
tensor([7, 5, 1, 3, 4, 0, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 2, 1, 0, 4, 5]
tensor([6, 3, 7, 2, 1, 0, 4, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 5, 2, 1, 4, 0]
tensor([6, 3, 7, 5, 2, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 3, 2, 5, 1, 0, 4, 6]
tensor([7, 3, 2, 5, 1, 0, 4, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 2, 3, 0, 3, 2, 1]
tensor([0, 1, 2, 3, 0, 3, 2, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/56.pt
[148] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2658.01it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<09:13,  3.93s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:06<04:09,  1.80s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:08<03:10,  1.39s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:43,  1.21s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:28,  1.11s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:21,  1.08s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:11,  1.02s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:03,  1.03it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<01:56,  1.08it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:49,  1.12it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:43,  1.16it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<01:42,  1.16it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:25<01:36,  1.22it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:31,  1.26it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:27,  1.30it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:24,  1.32it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:31<01:22,  1.32it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:32<01:19,  1.34it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:34<01:23,  1.26it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:19,  1.29it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:37<01:16,  1.31it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:38<01:14,  1.33it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:40<01:11,  1.36it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:41<01:08,  1.39it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:43<01:05,  1.42it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:44<01:05,  1.38it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:45<01:02,  1.43it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<00:59,  1.45it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<00:57,  1.48it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:49<00:55,  1.49it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:51<00:53,  1.50it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<00:55,  1.44it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:54<00:55,  1.40it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:55<00:54,  1.38it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:56<00:50,  1.44it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:58<00:48,  1.47it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:59<00:45,  1.51it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:00<00:43,  1.54it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:41,  1.56it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:03<00:39,  1.58it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:04<00:38,  1.60it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:06<00:41,  1.43it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:07<00:38,  1.49it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:08<00:35,  1.54it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:09<00:33,  1.59it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:10<00:31,  1.62it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:12<00:29,  1.64it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:13<00:28,  1.66it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:14<00:26,  1.68it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:15<00:26,  1.62it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:16<00:24,  1.65it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:18<00:23,  1.67it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:19<00:21,  1.68it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:20<00:20,  1.70it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:21<00:19,  1.71it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:22<00:17,  1.73it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:23<00:16,  1.74it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:24<00:15,  1.75it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:26<00:14,  1.68it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:27<00:13,  1.72it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:28<00:12,  1.73it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:29<00:10,  1.75it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:30<00:09,  1.77it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:31<00:08,  1.78it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:32<00:07,  1.80it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:33<00:06,  1.81it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:35<00:04,  1.83it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:36<00:04,  1.72it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:37<00:02,  1.77it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:38<00:01,  1.80it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:39<00:00,  1.84it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:39<00:00,  1.43it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-8): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-30): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-8): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-30): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/148.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140215199800384 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140215199800384 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215199800384 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140215199800384 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140213846395504 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140213846395504 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213846395504 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140213846395504 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[148] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.891975958338049
0.3982001908036579
0.26652204443251004
0.14111026067874816
0.11763525057572936
0.8635004180455561
0.8741394655456793
0.5304527321557103
0.44617634270319845
0.7007455592903002
0.6799150362476898
0.3511365594568724
0.47145939463701925
0.4744673317050211
0.905435493752087
0.903612941640167
0.8893763849001028
0.6821933807976941
0.6796182015499347
0.7774502436639344
0.8164788896198145
0.9011095591650676
0.8564639597370403
0.9528866120532432
0.4784452972921274
0.48852492817861076
0.8397355586186119
0.7433051149723976
0.5457687574271932
0.891975958338049
0.3982001908036579
0.26652204443251004
0.14111026067874816
0.11763525057572936
0.8635004180455561
0.8741394655456793
0.5304527321557103
0.44617634270319845
0.7007455592903002
0.6799150362476898
0.3511365594568724
0.47145939463701925
0.4744673317050211
0.905435493752087
0.903612941640167
0.8893763849001028
0.6821933807976941
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[6, 5, 4, 3, 2, 1, 7, 0]
tensor([6, 5, 4, 3, 2, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 7, 4, 1, 0, 6, 3]
tensor([5, 2, 7, 4, 1, 0, 6, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 6, 5, 2, 4, 0, 7, 1]
tensor([3, 6, 5, 2, 4, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 0, 2, 1, 7, 3]
tensor([5, 4, 6, 0, 2, 1, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 0, 7, 3, 5, 1, 6, 2]
tensor([4, 0, 7, 3, 5, 1, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 5, 1, 3, 2, 0, 1, 4]
tensor([0, 5, 1, 3, 2, 0, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 0, 0, 1, 1.0, 1.0, 1.0, 1.0]
tensor([1, 0, 0, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[1, 0, 1, 1.0, 1.0, 0, 1.0, 1.0]
tensor([1, 0, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 1.0, 0, 1, 1.0, 1.0]
tensor([0, 1, 1, 1, 0, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/148.pt
[225] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2652.73it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:09,  3.47s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:47,  1.64s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:00,  1.32s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:37,  1.16s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:27,  1.11s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:17,  1.05s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:07,  1.01it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:16<01:59,  1.06it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:52,  1.11it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:45,  1.17it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:52,  1.08it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:43,  1.15it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:36,  1.22it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:30,  1.27it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:26,  1.31it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:22,  1.34it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:20,  1.36it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:32<01:20,  1.33it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:17,  1.35it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:35<01:15,  1.37it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:36<01:12,  1.39it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:37<01:10,  1.40it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:39<01:07,  1.43it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:40<01:05,  1.45it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:41<01:05,  1.42it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:43<01:02,  1.47it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:44<00:59,  1.50it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:45<00:57,  1.52it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:47<00:55,  1.54it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:48<00:53,  1.55it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:49<00:52,  1.56it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:50<00:50,  1.55it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:52<00:53,  1.45it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:53<00:51,  1.46it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:48,  1.51it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:45,  1.55it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:57<00:43,  1.58it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:58<00:41,  1.60it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [00:59<00:40,  1.62it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:01<00:38,  1.64it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:02<00:38,  1.57it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:03<00:36,  1.59it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:04<00:35,  1.62it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:06<00:33,  1.64it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:07<00:33,  1.59it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:08<00:31,  1.61it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:09<00:29,  1.64it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:10<00:28,  1.66it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:12<00:26,  1.68it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:13<00:26,  1.63it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:14<00:24,  1.65it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:15<00:23,  1.69it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:16<00:21,  1.70it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:17<00:20,  1.72it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:19<00:18,  1.74it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:20<00:17,  1.75it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:21<00:16,  1.76it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:22<00:15,  1.77it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:23<00:14,  1.69it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:24<00:13,  1.73it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:25<00:12,  1.75it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:27<00:10,  1.77it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:28<00:09,  1.79it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:29<00:08,  1.80it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:30<00:07,  1.82it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:31<00:06,  1.83it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:32<00:04,  1.84it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:33<00:03,  1.75it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:34<00:02,  1.80it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:35<00:01,  1.83it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:36<00:00,  1.87it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:36<00:00,  1.47it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/225.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140212917723712 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140212917723712 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140212917723712 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140212917723712 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140215198457664 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140215198457664 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140215198457664 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140215198457664 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[225] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.4249710358556562
0.875077076525312
0.8377657430496294
0.7631146361001728
0.8061134194824199
0.597293936853132
0.8993038548124084
0.40672319990150296
0.4427306135620195
0.6697754068966065
0.560869697816391
0.6365238158372074
0.7977251509942833
0.5774168099619936
0.7636367981130562
0.45204149170439595
0.2621700055892588
0.8796773875905071
0.8519712476811715
0.8067705781994788
0.5994711637356425
0.8334667984344581
0.8573465838873401
0.8381586594258955
0.8329727759300379
0.8464402476627775
0.7864641899632313
0.6699089018877085
0.7340555031678195
0.4249710358556562
0.875077076525312
0.8377657430496294
0.7631146361001728
0.8061134194824199
0.597293936853132
0.8993038548124084
0.40672319990150296
0.4427306135620195
0.6697754068966065
0.560869697816391
0.6365238158372074
0.7977251509942833
0.5774168099619936
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[2, 1, 7, 5, 6, 0, 4, 3]
tensor([2, 1, 7, 5, 6, 0, 4, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 6, 7, 1, 3, 0, 5, 4]
tensor([2, 6, 7, 1, 3, 0, 5, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 1, 7, 4, 3, 0, 5, 6]
tensor([2, 1, 7, 4, 3, 0, 5, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 7, 1, 4, 0, 6, 2]
tensor([5, 3, 7, 1, 4, 0, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 1, 3, 0, 7, 2]
tensor([5, 4, 6, 1, 3, 0, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[2, 5, 0, 4, 1, 3, 1, 0]
tensor([2, 5, 0, 4, 1, 3, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 3, 1, 1, 2, 0, 3, 2]
tensor([0, 3, 1, 1, 2, 0, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/225.pt
[211] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2618.47it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:13,  3.50s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:45,  1.62s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:57,  1.30s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:45,  1.22s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:36,  1.18s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:23,  1.09s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:10,  1.01s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:00,  1.05it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:53,  1.10it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:50,  1.12it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:42,  1.18it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:36,  1.23it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:31,  1.28it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:27,  1.32it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:24,  1.34it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:21,  1.36it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:23,  1.31it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:32<01:19,  1.35it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:16,  1.37it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:37<01:54,  1.11s/it]Running loglikelihood requests:  29%|██▉       | 41/142 [00:38<01:39,  1.01it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:40<01:31,  1.08it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:41<01:22,  1.18it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:43<01:15,  1.26it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:44<01:09,  1.34it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:45<01:04,  1.42it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:46<01:00,  1.47it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:48<00:57,  1.51it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:49<00:55,  1.54it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:50<00:56,  1.47it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:52<00:55,  1.47it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:53<00:52,  1.50it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:54<00:50,  1.53it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:55<00:47,  1.57it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:57<00:45,  1.61it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:58<00:43,  1.63it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:59<00:41,  1.64it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:00<00:42,  1.58it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:02<00:40,  1.60it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:03<00:38,  1.63it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:04<00:36,  1.65it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:05<00:35,  1.67it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:33,  1.70it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:32,  1.71it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:09<00:30,  1.72it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:14<01:05,  1.29s/it]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:16<00:52,  1.07s/it]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:17<00:43,  1.09it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:18<00:36,  1.23it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:19<00:31,  1.36it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:20<00:30,  1.33it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:22<00:27,  1.44it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:23<00:24,  1.53it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:24<00:21,  1.61it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:25<00:19,  1.67it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:26<00:18,  1.72it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:27<00:16,  1.75it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:28<00:15,  1.78it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:29<00:13,  1.79it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:31<00:13,  1.70it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:32<00:12,  1.74it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:33<00:10,  1.78it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:34<00:09,  1.81it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:35<00:08,  1.84it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:36<00:06,  1.86it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:37<00:05,  1.88it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:38<00:04,  1.90it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:39<00:03,  1.92it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:40<00:02,  1.95it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:41<00:01,  1.79it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:42<00:00,  1.85it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:42<00:00,  1.38it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-6): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-12): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15-18): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (19): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (20-23): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (25-31): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-6): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-12): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15-18): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (19): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (20-23): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (25-31): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/211.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140212917720400 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140212917720400 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140212917720400 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140212917720400 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140214242059056 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140214242059056 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140214242059056 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140214242059056 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[211] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.43661971830985913, 'acc_stderr,none': 0.05927935558412972}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.45379462613562144
0.5121987218538359
0.3805605062852953
0.9250105662738365
0.5393046917775384
0.6260326331104198
0.8626372247282749
0.4859614961709456
0.13108090074681872
0.540361134548941
0.3691796883542366
0.8419863957417434
0.3738225663436576
0.7939340566193666
0.6916928670672426
0.661840354699148
0.5510051512470766
0.5066100211995253
0.5208949147180995
0.7283079951082674
0.8857784175671931
0.553624027417834
0.6922319498836209
0.6063680148478464
0.9220941595054846
0.9172099247528117
0.5762720309944068
0.05484791873187908
0.6210078066443784
0.45379462613562144
0.5121987218538359
0.3805605062852953
0.9250105662738365
0.5393046917775384
0.6260326331104198
0.8626372247282749
0.4859614961709456
0.13108090074681872
0.540361134548941
0.3691796883542366
0.8419863957417434
0.3738225663436576
0.7939340566193666
0.6916928670672426
0.661840354699148
0.5510051512470766
0.5066100211995253
0.5208949147180995
0.7283079951082674
0.8857784175671931
0.553624027417834
Total groups 72 exceeded the threshold, stopping comparison.
The group tensor is
[1, 2, 6, 3, 5, 0, 7, 4]
tensor([1, 2, 6, 3, 5, 0, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 3, 2, 0, 6, 7, 1]
tensor([5, 4, 3, 2, 0, 6, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 4, 6, 2, 5, 0, 7, 1]
tensor([3, 4, 6, 2, 5, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 2, 4, 6, 1, 0, 3, 5]
tensor([7, 2, 4, 6, 1, 0, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 0, 7, 4, 3, 5, 6, 1]
tensor([2, 0, 7, 4, 3, 5, 6, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[3, 0, 5, 4, 1, 0, 1, 2]
tensor([3, 0, 5, 4, 1, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/211.pt
[95] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2532.53it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:48,  3.75s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<04:06,  1.78s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:08<03:12,  1.40s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:43,  1.21s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:31,  1.14s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:19,  1.06s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:09,  1.00s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:01,  1.05it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:55,  1.08it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:48,  1.13it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:55,  1.05it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<01:46,  1.11it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:25<01:40,  1.17it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:27<01:34,  1.22it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:30,  1.25it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:30<01:26,  1.28it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:32<01:32,  1.18it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:33<01:27,  1.23it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:35<01:23,  1.26it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:19,  1.29it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:38<01:17,  1.31it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:14,  1.32it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:41<01:11,  1.35it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:43<01:22,  1.15it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:44<01:17,  1.20it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:46<01:12,  1.25it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:47<01:07,  1.31it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:49<01:03,  1.37it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:50<01:00,  1.41it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:51<00:57,  1.44it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:53<00:58,  1.38it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:54<00:55,  1.42it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:55<00:52,  1.46it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:57<00:50,  1.48it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:58<00:48,  1.51it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:59<00:46,  1.52it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:01<00:44,  1.54it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:02<00:44,  1.49it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:03<00:42,  1.52it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:04<00:40,  1.54it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:06<00:39,  1.56it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:07<00:37,  1.57it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:08<00:36,  1.58it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:09<00:34,  1.60it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:11<00:32,  1.61it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:12<00:32,  1.55it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:13<00:31,  1.58it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:14<00:29,  1.60it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:16<00:27,  1.62it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:17<00:26,  1.63it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:18<00:25,  1.63it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:19<00:23,  1.64it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:21<00:22,  1.65it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:22<00:21,  1.66it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:23<00:20,  1.58it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:24<00:19,  1.61it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:25<00:17,  1.64it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:27<00:16,  1.65it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:28<00:15,  1.67it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:29<00:13,  1.68it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:30<00:12,  1.69it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:31<00:11,  1.70it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:33<00:10,  1.60it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:34<00:09,  1.64it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:35<00:07,  1.66it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:36<00:06,  1.68it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:37<00:05,  1.69it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:39<00:04,  1.49it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:40<00:03,  1.49it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:42<00:01,  1.57it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:44<00:00,  1.36it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:44<00:00,  1.37it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-8): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-20): 11 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (21): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22-23): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-25): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27-31): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-8): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-20): 11 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (21): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22-23): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-25): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27-31): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/95.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140214644542112 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140214644542112 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140214644542112 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140214644542112 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140210578258304 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140210578258304 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140210578258304 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140210578258304 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[95] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.43661971830985913, 'acc_stderr,none': 0.05927935558412972}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8745617680355892
0.4870053329545456
0.7039635907698274
0.13683136283338845
0.45491781836309914
0.7394007339170455
0.7100810563328822
0.5089938007827006
0.6148932447522303
0.6097500975530942
0.2557945445996536
0.6585024343395449
0.3077968033302773
0.5696663704302732
0.4357302352587323
0.8508165638532892
0.2937402676601578
0.5546720889398816
0.456887590217546
0.8515148368106646
0.6351128178266827
0.587082322906938
0.4014945886309545
0.6816643170727646
0.7647343638504926
0.15578350583970074
0.7065213224778255
0.687952127174302
0.881700230921547
0.8745617680355892
0.4870053329545456
0.7039635907698274
0.13683136283338845
0.45491781836309914
0.7394007339170455
0.7100810563328822
0.5089938007827006
0.6148932447522303
0.6097500975530942
0.2557945445996536
0.6585024343395449
0.3077968033302773
0.5696663704302732
0.4357302352587323
0.8508165638532892
0.2937402676601578
0.5546720889398816
0.456887590217546
0.8515148368106646
0.6351128178266827
0.587082322906938
0.4014945886309545
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 6, 1, 0, 4, 2]
tensor([5, 3, 7, 6, 1, 0, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 7, 1, 5, 6, 0, 4, 2]
tensor([3, 7, 1, 5, 6, 0, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 3, 2, 4, 5, 1, 7, 0]
tensor([6, 3, 2, 4, 5, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 7, 5, 1, 3, 6, 0]
tensor([2, 4, 7, 5, 1, 3, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[0, 2, 4, 6, 5, 1, 7, 3]
tensor([0, 2, 4, 6, 5, 1, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 2, 5, 3, 6, 0, 7, 1]
tensor([4, 2, 5, 3, 6, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/95.pt
[78] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2549.93it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:49,  3.75s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:56,  1.70s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:59,  1.31s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:35,  1.15s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:21,  1.07s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:16,  1.04s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:07,  1.01it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:16<01:59,  1.06it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:52,  1.11it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:46,  1.16it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:40,  1.20it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:38,  1.21it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:32,  1.26it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:25<01:28,  1.30it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:25,  1.33it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:28<01:22,  1.35it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:20,  1.36it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:31<01:18,  1.36it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:22,  1.27it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:35<01:26,  1.19it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:37<01:30,  1.12it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:38<01:21,  1.21it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:40<01:14,  1.30it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:41<01:09,  1.37it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:43<01:11,  1.30it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:44<01:07,  1.36it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:45<01:02,  1.42it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:46<00:58,  1.48it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<00:55,  1.52it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:49<00:53,  1.56it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:50<00:51,  1.58it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:51<00:49,  1.59it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:53<00:50,  1.54it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:47,  1.58it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:45,  1.61it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:43,  1.63it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:57<00:41,  1.66it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:40,  1.67it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:38,  1.69it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:01<00:37,  1.70it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:02<00:35,  1.71it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:39,  1.51it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:05<00:36,  1.57it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:06<00:33,  1.62it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:07<00:32,  1.66it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:08<00:30,  1.68it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:09<00:28,  1.71it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:11<00:27,  1.73it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:12<00:25,  1.74it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:13<00:26,  1.60it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:14<00:24,  1.65it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:16<00:23,  1.65it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:17<00:21,  1.69it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:18<00:20,  1.73it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:19<00:18,  1.76it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:20<00:17,  1.78it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:21<00:16,  1.80it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:22<00:14,  1.82it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:23<00:14,  1.74it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:24<00:12,  1.78it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:25<00:11,  1.81it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:27<00:10,  1.83it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:28<00:09,  1.85it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:29<00:08,  1.86it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:30<00:06,  1.86it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:31<00:05,  1.85it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:32<00:04,  1.86it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:33<00:03,  1.86it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:34<00:02,  1.72it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:35<00:01,  1.75it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:36<00:00,  1.83it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:36<00:00,  1.47it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-3): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-6): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-15): 8 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-21): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (23-27): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (29-30): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-3): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-6): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-15): 8 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-21): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (23-27): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (29-30): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/78.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140210578299824 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140210578299824 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140210578299824 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140210578299824 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140213446182624 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140213446182624 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213446182624 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140213446182624 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[78] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.43661971830985913, 'acc_stderr,none': 0.05927935558412972}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7988766190696382
0.433785810695937
0.6525315214782026
0.871250377966373
0.7790022985921221
0.6581533546839238
0.6948782189696622
0.4773368074709654
0.7967725763717977
0.6899017375628973
0.8876656342513365
0.741048035098914
0.7711231987685904
0.658722026565161
0.8415498496492992
0.7230018765402745
0.5047290951856622
0.7541475774993723
0.5858483604351846
0.9316615143976832
0.7753381144708663
0.5505312367470351
0.97693166402346
0.9600861235134092
0.9818242386505116
0.9256028941559552
0.44553043946224297
0.3299453067275913
0.5544144775455869
0.7988766190696382
0.433785810695937
0.6525315214782026
0.871250377966373
0.7790022985921221
0.6581533546839238
0.6948782189696622
0.4773368074709654
0.7967725763717977
0.6899017375628973
0.8876656342513365
0.741048035098914
0.7711231987685904
0.658722026565161
0.8415498496492992
0.7230018765402745
0.5047290951856622
0.7541475774993723
Total groups 72 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 1, 2, 0, 6, 4]
tensor([5, 3, 7, 1, 2, 0, 6, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 4, 5, 2, 0, 7, 1]
tensor([6, 3, 4, 5, 2, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 4, 7, 2, 5, 1, 6, 0]
tensor([3, 4, 7, 2, 5, 1, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 4, 7, 1, 3, 0, 5, 2]
tensor([6, 4, 7, 1, 3, 0, 5, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 6, 2, 0, 1, 7, 4]
tensor([5, 3, 6, 2, 0, 1, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[3, 5, 0, 2, 4, 0, 1, 1]
tensor([3, 5, 0, 2, 4, 0, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/78.pt
[13] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2657.37it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:04<09:26,  4.02s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<04:00,  1.73s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:58,  1.31s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:33,  1.14s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:23,  1.08s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:10,  1.00it/s]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:01,  1.06it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:16<01:53,  1.12it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:17<01:47,  1.17it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:40,  1.22it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:42,  1.18it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:22<01:36,  1.24it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:30,  1.30it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:25<01:25,  1.35it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:26<01:21,  1.38it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:28<01:18,  1.41it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:29<01:16,  1.42it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:31<01:17,  1.38it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:32<01:16,  1.37it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:33<01:14,  1.38it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:35<01:11,  1.41it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:36<01:09,  1.42it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:37<01:06,  1.46it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:39<01:03,  1.50it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:40<01:04,  1.44it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:42<01:01,  1.48it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:43<00:58,  1.53it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:44<00:55,  1.56it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:45<00:53,  1.59it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:46<00:51,  1.61it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:48<00:49,  1.63it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:49<00:48,  1.64it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:50<00:46,  1.66it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:51<00:47,  1.57it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:53<00:45,  1.61it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:54<00:43,  1.63it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:55<00:41,  1.66it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:56<00:40,  1.67it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [00:57<00:38,  1.69it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [00:58<00:36,  1.71it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [00:59<00:35,  1.72it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:01<00:37,  1.56it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:02<00:35,  1.61it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:03<00:33,  1.64it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:04<00:31,  1.68it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:06<00:30,  1.70it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:07<00:28,  1.72it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:08<00:30,  1.56it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:09<00:27,  1.63it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:11<00:25,  1.68it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:12<00:24,  1.65it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:13<00:23,  1.65it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:14<00:21,  1.70it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:15<00:20,  1.74it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:16<00:18,  1.77it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:17<00:17,  1.80it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:18<00:15,  1.82it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:19<00:14,  1.83it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:21<00:13,  1.83it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:22<00:13,  1.70it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:23<00:12,  1.75it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:24<00:10,  1.80it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:25<00:09,  1.83it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:26<00:08,  1.85it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:27<00:06,  1.86it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:28<00:05,  1.88it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:29<00:04,  1.90it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:30<00:03,  1.92it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:32<00:02,  1.75it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:33<00:01,  1.75it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:34<00:00,  1.80it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:34<00:00,  1.51it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/13.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140213848678016 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213848678016 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213848678016 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213848678016 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140213848672496 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140213848672496 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213848672496 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140213848672496 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[13] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9416968404415913
0.7938815868474761
0.8191819727516524
0.8917669687772857
0.90570342985793
0.3043907333176312
0.4952220736758006
0.29899990072660143
0.7758711420345523
0.838301482150715
0.8086122742620726
0.9126741954079025
0.6304407611776705
0.6595297618724628
0.176689009460142
0.4424440684940646
0.43152501031897406
0.48316840881543294
0.43857258209632005
0.7021905687600761
0.2402003641072479
0.27348823957209967
0.6649171314730306
0.25117416182112623
0.9114030338716436
0.9328254651349717
0.9096786197852272
0.9107213191843687
0.8764776988845487
0.9416968404415913
0.7938815868474761
0.8191819727516524
0.8917669687772857
0.90570342985793
0.3043907333176312
0.4952220736758006
0.29899990072660143
0.7758711420345523
0.838301482150715
0.8086122742620726
0.9126741954079025
0.6304407611776705
0.6595297618724628
0.176689009460142
0.4424440684940646
0.43152501031897406
Total groups 76 exceeded the threshold, stopping comparison.
The group tensor is
[4, 3, 7, 2, 5, 1, 6, 0]
tensor([4, 3, 7, 2, 5, 1, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 5, 7, 4, 3, 0, 6, 1]
tensor([2, 5, 7, 4, 3, 0, 6, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 5, 4, 0, 1, 1, 2]
tensor([0, 3, 5, 4, 0, 1, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[4, 3, 0, 2, 5, 0, 1, 1]
tensor([4, 3, 0, 2, 5, 0, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 0, 5, 1, 4, 2, 1, 3]
tensor([0, 0, 5, 1, 4, 2, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[3, 4, 0, 1, 2, 0, 5, 1]
tensor([3, 4, 0, 1, 2, 0, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[5, 0, 4, 3, 2, 0, 1, 1]
tensor([5, 0, 4, 3, 2, 0, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 3, 4, 0, 1, 2, 1, 5]
tensor([0, 3, 4, 0, 1, 2, 1, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/13.pt
[107] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2546.37it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<09:21,  3.98s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:07<05:07,  2.21s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:09<03:40,  1.61s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:11<02:57,  1.32s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:13<02:37,  1.19s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:14<02:21,  1.08s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:09,  1.01s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:18<02:01,  1.05it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<01:53,  1.10it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<01:46,  1.16it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:23<01:43,  1.17it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<01:36,  1.23it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:25<01:31,  1.28it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:27<01:27,  1.32it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:23,  1.35it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:30<01:20,  1.37it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:31<01:18,  1.38it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:33<01:21,  1.31it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:34<01:18,  1.34it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:15,  1.36it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:37<01:12,  1.39it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:38<01:09,  1.42it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:40<01:06,  1.46it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:41<01:04,  1.48it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:42<01:05,  1.42it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:44<01:05,  1.39it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:45<01:01,  1.45it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:46<00:57,  1.51it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<00:54,  1.55it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:49<00:52,  1.58it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:50<00:50,  1.61it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:51<00:48,  1.63it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:53<00:48,  1.58it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:46,  1.61it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:44,  1.64it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:49,  1.42it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:59<00:58,  1.17it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:00<00:51,  1.29it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:46,  1.40it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:03<00:44,  1.42it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:04<00:40,  1.50it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:05<00:37,  1.57it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:35,  1.62it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:32,  1.67it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:09<00:32,  1.63it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:10<00:30,  1.67it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:11<00:28,  1.70it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:12<00:27,  1.73it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:13<00:26,  1.67it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:14<00:25,  1.71it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:16<00:23,  1.72it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:17<00:22,  1.75it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:18<00:20,  1.77it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:19<00:19,  1.78it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:20<00:18,  1.79it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:21<00:17,  1.81it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:22<00:15,  1.82it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:24<00:17,  1.50it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:25<00:15,  1.57it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:26<00:13,  1.65it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:27<00:12,  1.71it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:29<00:12,  1.57it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:30<00:10,  1.66it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:31<00:08,  1.73it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:32<00:07,  1.78it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:35<00:09,  1.13it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:36<00:07,  1.23it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:38<00:05,  1.36it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:39<00:03,  1.48it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:40<00:01,  1.55it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:41<00:00,  1.67it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:41<00:00,  1.40it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-3): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9-10): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (12-13): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-31): 15 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-3): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9-10): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (12-13): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-31): 15 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/107.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140213859240464 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213859240464 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213859240464 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213859240464 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140213859243968 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140213859243968 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213859243968 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140213859243968 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[107] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.43661971830985913, 'acc_stderr,none': 0.05927935558412972}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.6384788962207051
0.4946572039289874
0.7062468225311351
0.5924118081316296
0.8055799477986278
0.4290834385090486
0.5207152243980063
0.40307410421369466
0.575849756312845
0.37833122092903004
0.559402975459772
0.9202435099718248
0.7328649512638977
0.4656521136449837
0.6226167455222542
0.6500475926241498
0.14933051029764177
0.852848973497393
0.7020336348742873
0.8036038458930006
0.25840240277743587
0.45348406473870356
0.7291236246725161
0.5999541971262276
0.7071125031923391
0.6523270915574834
0.5982105985982268
0.6520903105513685
0.39933491227192136
0.6384788962207051
0.4946572039289874
0.7062468225311351
0.5924118081316296
0.8055799477986278
0.4290834385090486
0.5207152243980063
0.40307410421369466
0.575849756312845
0.37833122092903004
0.559402975459772
0.9202435099718248
0.7328649512638977
0.4656521136449837
0.6226167455222542
0.6500475926241498
0.14933051029764177
0.852848973497393
0.7020336348742873
0.8036038458930006
0.25840240277743587
0.45348406473870356
0.7291236246725161
0.5999541971262276
0.7071125031923391
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[6, 3, 2, 1, 5, 0, 7, 4]
tensor([6, 3, 2, 1, 5, 0, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 5, 4, 2, 6, 0, 7, 1]
tensor([3, 5, 4, 2, 6, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 7, 6, 3, 4, 1, 5, 0]
tensor([2, 7, 6, 3, 4, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 2, 5, 4, 6, 0, 7, 1]
tensor([3, 2, 5, 4, 6, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 5, 7, 0, 3, 2, 6, 1]
tensor([4, 5, 7, 0, 3, 2, 6, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 6, 3, 4, 2, 1, 7, 0]
tensor([5, 6, 3, 4, 2, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/107.pt
[106] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2636.27it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:50,  3.34s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:36,  1.56s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:55,  1.28s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:31,  1.12s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:10<02:17,  1.04s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:12<02:08,  1.02it/s]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:00,  1.07it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:15<01:53,  1.12it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:17<01:50,  1.13it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:43,  1.19it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:20<01:38,  1.23it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:22<01:33,  1.28it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:23<01:28,  1.32it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:24<01:27,  1.31it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:37,  1.15it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:28<01:34,  1.18it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:27,  1.25it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:31<01:21,  1.31it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:32<01:17,  1.35it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:34<01:14,  1.39it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:35<01:11,  1.42it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:37<01:10,  1.40it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:38<01:07,  1.44it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:39<01:04,  1.47it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:40<01:01,  1.51it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:42<00:58,  1.55it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:43<00:56,  1.58it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:44<00:54,  1.60it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:45<00:52,  1.62it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:46<00:50,  1.63it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:48<00:51,  1.58it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:49<00:49,  1.60it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:50<00:47,  1.63it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:51<00:45,  1.65it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:53<00:43,  1.67it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:54<00:42,  1.68it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:55<00:40,  1.70it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:56<00:39,  1.70it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [00:57<00:39,  1.65it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [00:58<00:37,  1.69it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:00<00:35,  1.71it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:01<00:34,  1.72it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:02<00:32,  1.73it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:03<00:31,  1.75it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:04<00:30,  1.76it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:05<00:28,  1.76it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:06<00:27,  1.77it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:08<00:27,  1.71it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:09<00:25,  1.73it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:10<00:25,  1.70it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:11<00:23,  1.73it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:12<00:22,  1.76it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:13<00:20,  1.78it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:14<00:19,  1.80it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:15<00:18,  1.82it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:16<00:16,  1.83it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:18<00:16,  1.73it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:19<00:15,  1.76it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:20<00:14,  1.77it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:21<00:12,  1.81it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:22<00:11,  1.83it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:23<00:10,  1.86it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:24<00:09,  1.87it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:25<00:07,  1.89it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:26<00:06,  1.90it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:27<00:05,  1.88it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:29<00:04,  1.81it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:30<00:03,  1.85it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:31<00:02,  1.86it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:32<00:01,  1.87it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:33<00:00,  1.92it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:33<00:00,  1.52it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/106.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140213441962896 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213441962896 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213441962896 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213441962896 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140213441965872 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140213441965872 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213441965872 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140213441965872 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[106] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4084507042253521, 'acc_stderr,none': 0.058751136942575236}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8470147925890528
0.7476864669376683
0.7057458194715966
0.9259961826035564
0.9063486644110201
0.24963708122926456
0.19827660218091203
0.8342224461031358
0.674768686362321
0.8317740713616216
0.8052770539851904
0.87411142785436
0.9423251627051763
0.6325449231299249
0.5709019604280002
0.492272078021527
0.5470094105753499
0.8106788106419917
0.8007286389438932
0.8514363351777328
0.5678533410855942
0.6964663000476322
0.43791892189616227
0.24333097709743945
0.6773756911035087
0.7468917488225975
0.9536421875896308
0.6970547096788922
0.8446279043078407
0.8470147925890528
0.7476864669376683
0.7057458194715966
0.9259961826035564
0.9063486644110201
0.24963708122926456
0.19827660218091203
0.8342224461031358
0.674768686362321
0.8317740713616216
0.8052770539851904
0.87411142785436
0.9423251627051763
0.6325449231299249
0.5709019604280002
0.492272078021527
Total groups 76 exceeded the threshold, stopping comparison.
The group tensor is
[3, 4, 6, 5, 2, 0, 7, 1]
tensor([3, 4, 6, 5, 2, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 6, 5, 3, 0, 7, 1]
tensor([2, 4, 6, 5, 3, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 3, 4, 5, 2, 0, 7, 1]
tensor([6, 3, 4, 5, 2, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 4, 7, 2, 5, 0, 6, 3]
tensor([1, 4, 7, 2, 5, 0, 6, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[0, 3, 5, 2, 0, 1, 1, 4]
tensor([0, 3, 5, 2, 0, 1, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[4, 3, 0, 5, 2, 0, 1, 1]
tensor([4, 3, 0, 5, 2, 0, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 3, 5, 1, 4, 0, 1, 2]
tensor([0, 3, 5, 1, 4, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1, 0, 1.0, 1.0, 1.0]
tensor([0, 1, 1, 1, 0, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/106.pt
[137] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2645.38it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:04<11:14,  4.79s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:07<05:30,  2.38s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:09<03:44,  1.64s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:11<03:00,  1.34s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:13<02:36,  1.18s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:15<02:21,  1.08s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:09,  1.01s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:18<02:04,  1.02it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:20<01:55,  1.08it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<01:47,  1.14it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:23<01:41,  1.19it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<01:36,  1.23it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:26<01:33,  1.26it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:28<01:37,  1.18it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:29<01:32,  1.22it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:31<01:27,  1.27it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:32<01:23,  1.30it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:34<01:19,  1.34it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:35<01:17,  1.36it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:14,  1.39it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:38<01:14,  1.36it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:10,  1.40it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:41<01:07,  1.44it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:42<01:04,  1.47it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:43<01:01,  1.51it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:44<00:58,  1.55it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:46<00:56,  1.58it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<00:54,  1.61it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<00:54,  1.57it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:49<00:51,  1.60it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:50<00:49,  1.63it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<00:47,  1.65it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:53<00:46,  1.67it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:44,  1.68it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:42,  1.70it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:41,  1.70it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:57<00:40,  1.72it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:00<00:48,  1.38it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:44,  1.47it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:40,  1.55it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:37,  1.61it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:35,  1.66it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:05<00:33,  1.69it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:06<00:31,  1.72it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:07<00:30,  1.75it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:09<00:30,  1.68it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:10<00:28,  1.71it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:11<00:26,  1.74it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:12<00:25,  1.76it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:13<00:25,  1.69it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:14<00:23,  1.72it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:16<00:22,  1.75it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:17<00:20,  1.77it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:18<00:19,  1.79it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:19<00:19,  1.69it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:20<00:17,  1.73it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:21<00:16,  1.76it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:22<00:15,  1.79it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:23<00:13,  1.81it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:24<00:12,  1.83it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:26<00:11,  1.84it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:27<00:10,  1.85it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:28<00:09,  1.87it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:29<00:09,  1.57it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:32<00:11,  1.14it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:33<00:08,  1.29it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:34<00:06,  1.43it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:35<00:04,  1.55it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:36<00:03,  1.66it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:37<00:01,  1.75it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:38<00:00,  1.82it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:38<00:00,  1.44it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-3): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9-10): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (12-13): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-31): 15 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-3): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9-10): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (12-13): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-31): 15 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/137.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140214644622976 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140214644622976 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140214644622976 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140214644622976 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140213446256032 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140213446256032 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213446256032 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140213446256032 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[137] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.43661971830985913, 'acc_stderr,none': 0.05927935558412972}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.6384788962207051
0.4946572039289874
0.7062468225311351
0.5924118081316296
0.8055799477986278
0.4290834385090486
0.5207152243980063
0.40307410421369466
0.575849756312845
0.37833122092903004
0.559402975459772
0.9202435099718248
0.7328649512638977
0.4656521136449837
0.6226167455222542
0.6500475926241498
0.14933051029764177
0.852848973497393
0.7020336348742873
0.8036038458930006
0.25840240277743587
0.45348406473870356
0.7291236246725161
0.5999541971262276
0.7071125031923391
0.6523270915574834
0.5982105985982268
0.6520903105513685
0.39933491227192136
0.6384788962207051
0.4946572039289874
0.7062468225311351
0.5924118081316296
0.8055799477986278
0.4290834385090486
0.5207152243980063
0.40307410421369466
0.575849756312845
0.37833122092903004
0.559402975459772
0.9202435099718248
0.7328649512638977
0.4656521136449837
0.6226167455222542
0.6500475926241498
0.14933051029764177
0.852848973497393
0.7020336348742873
0.8036038458930006
0.25840240277743587
0.45348406473870356
0.7291236246725161
0.5999541971262276
0.7071125031923391
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[6, 3, 2, 1, 5, 0, 7, 4]
tensor([6, 3, 2, 1, 5, 0, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 5, 4, 2, 6, 0, 7, 1]
tensor([3, 5, 4, 2, 6, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 7, 6, 3, 4, 1, 5, 0]
tensor([2, 7, 6, 3, 4, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 2, 5, 4, 6, 0, 7, 1]
tensor([3, 2, 5, 4, 6, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 5, 7, 0, 3, 2, 6, 1]
tensor([4, 5, 7, 0, 3, 2, 6, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 6, 3, 4, 2, 1, 7, 0]
tensor([5, 6, 3, 4, 2, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/137.pt
[169] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2542.02it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:06<15:12,  6.47s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:08<05:37,  2.43s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:10<03:43,  1.63s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:12<02:58,  1.32s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:14<02:38,  1.19s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:15<02:20,  1.07s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:17<02:07,  1.01it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:19<02:09,  1.02s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:21<01:57,  1.06it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:22<01:49,  1.12it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:24<01:46,  1.14it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:25<01:38,  1.21it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:27<01:32,  1.26it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:28<01:26,  1.32it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:29<01:22,  1.37it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:31<01:19,  1.40it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:32<01:16,  1.42it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:34<01:16,  1.40it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:35<01:14,  1.41it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:11,  1.43it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:38<01:09,  1.44it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:08,  1.44it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:41<01:09,  1.40it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:42<01:05,  1.45it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:43<01:02,  1.49it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:45<01:02,  1.45it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:46<00:59,  1.50it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<00:56,  1.54it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<00:53,  1.58it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:49<00:51,  1.60it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:51<00:49,  1.62it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<00:48,  1.64it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:53<00:46,  1.65it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:47,  1.57it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:56<00:45,  1.61it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:43,  1.64it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:41,  1.67it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:39,  1.68it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:38,  1.70it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:01<00:36,  1.72it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:02<00:35,  1.73it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:34,  1.72it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:05<00:34,  1.66it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:06<00:32,  1.71it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:07<00:30,  1.73it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:08<00:29,  1.70it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:09<00:28,  1.73it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:11<00:26,  1.76it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:12<00:25,  1.78it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:13<00:23,  1.79it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:14<00:23,  1.75it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:15<00:21,  1.77it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:16<00:20,  1.79it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:17<00:19,  1.80it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:18<00:18,  1.82it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:19<00:16,  1.83it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:20<00:15,  1.85it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:22<00:14,  1.86it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:23<00:13,  1.86it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:24<00:12,  1.87it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:26<00:15,  1.31it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:27<00:13,  1.45it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:28<00:10,  1.56it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:29<00:09,  1.64it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:30<00:07,  1.70it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:32<00:06,  1.76it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:33<00:04,  1.81it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:34<00:03,  1.84it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:36<00:03,  1.48it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:37<00:01,  1.59it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:38<00:00,  1.68it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:38<00:00,  1.45it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/169.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140213443722224 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213443722224 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213443722224 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213443722224 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140213846180256 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140213846180256 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213846180256 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140213846180256 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[169] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.4249710358556562
0.875077076525312
0.8377657430496294
0.7631146361001728
0.8061134194824199
0.597293936853132
0.8993038548124084
0.40672319990150296
0.4427306135620195
0.6697754068966065
0.560869697816391
0.6365238158372074
0.7977251509942833
0.5774168099619936
0.7636367981130562
0.45204149170439595
0.2621700055892588
0.8796773875905071
0.8519712476811715
0.8067705781994788
0.5994711637356425
0.8334667984344581
0.8573465838873401
0.8381586594258955
0.8329727759300379
0.8464402476627775
0.7864641899632313
0.6699089018877085
0.7340555031678195
0.4249710358556562
0.875077076525312
0.8377657430496294
0.7631146361001728
0.8061134194824199
0.597293936853132
0.8993038548124084
0.40672319990150296
0.4427306135620195
0.6697754068966065
0.560869697816391
0.6365238158372074
0.7977251509942833
0.5774168099619936
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[2, 1, 7, 5, 6, 0, 4, 3]
tensor([2, 1, 7, 5, 6, 0, 4, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 6, 7, 1, 3, 0, 5, 4]
tensor([2, 6, 7, 1, 3, 0, 5, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 1, 7, 4, 3, 0, 5, 6]
tensor([2, 1, 7, 4, 3, 0, 5, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 7, 1, 4, 0, 6, 2]
tensor([5, 3, 7, 1, 4, 0, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 1, 3, 0, 7, 2]
tensor([5, 4, 6, 1, 3, 0, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[2, 5, 0, 4, 1, 3, 1, 0]
tensor([2, 5, 0, 4, 1, 3, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 3, 1, 1, 2, 0, 3, 2]
tensor([0, 3, 1, 1, 2, 0, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/169.pt
[66] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2652.33it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:04<11:11,  4.76s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:07<04:59,  2.15s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:09<03:27,  1.52s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:11<03:01,  1.34s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:13<02:35,  1.17s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:14<02:19,  1.06s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:17,  1.07s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:18<02:05,  1.01it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:20<01:59,  1.04it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:22<01:55,  1.06it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:23<01:47,  1.12it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:25<01:39,  1.20it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:26<01:32,  1.26it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:27<01:27,  1.32it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:29<01:23,  1.35it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:30<01:20,  1.38it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:32<01:24,  1.29it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:33<01:20,  1.33it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:35<01:16,  1.36it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:13,  1.39it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:37<01:11,  1.41it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:09,  1.43it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:40<01:06,  1.46it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:42<01:06,  1.44it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:43<01:02,  1.48it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:44<00:59,  1.52it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:45<00:57,  1.55it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<00:55,  1.57it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<00:53,  1.59it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:49<00:51,  1.60it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:50<00:50,  1.61it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<00:51,  1.55it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:53<00:48,  1.58it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:46,  1.60it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:45,  1.62it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:43,  1.63it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:41,  1.65it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:40,  1.66it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:38,  1.68it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:01<00:38,  1.62it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:02<00:36,  1.65it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:35,  1.67it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:05<00:33,  1.68it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:06<00:32,  1.70it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:07<00:30,  1.72it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:08<00:29,  1.72it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:09<00:28,  1.73it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:10<00:26,  1.74it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:12<00:26,  1.67it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:13<00:25,  1.70it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:14<00:23,  1.72it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:15<00:22,  1.74it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:16<00:21,  1.75it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:17<00:19,  1.77it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:19<00:18,  1.78it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:20<00:17,  1.79it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:21<00:16,  1.80it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:22<00:15,  1.73it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:23<00:14,  1.75it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:24<00:12,  1.78it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:25<00:11,  1.80it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:26<00:10,  1.82it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:27<00:09,  1.83it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:28<00:08,  1.85it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:30<00:06,  1.86it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:31<00:05,  1.87it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:32<00:04,  1.80it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:33<00:03,  1.84it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:34<00:02,  1.87it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:35<00:01,  1.89it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:36<00:00,  1.93it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:36<00:00,  1.47it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/66.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140213443718912 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213443718912 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213443718912 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213443718912 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140213443716320 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140213443716320 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213443716320 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140213443716320 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[66] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8301327615319641
0.7817720914227475
0.8113678911982368
0.2257173763133808
0.2927105469616105
0.5177793359335131
0.8145737190612207
0.7363478990038121
0.20758834533702153
0.2533018140549718
0.30764875086796684
0.29044714199656413
0.9350374080783923
0.6277994360424477
0.603553220567354
0.78279661631625
0.6718149226457006
0.5353760149732133
0.6848376228333551
0.9446001512801305
0.7972906165117697
0.5043025319455414
0.5939887608746786
0.6455402209569181
0.2505215486241291
0.321550947803521
0.8361163063826382
0.7208169010376037
0.9633188772100769
0.8301327615319641
0.7817720914227475
0.8113678911982368
0.2257173763133808
0.2927105469616105
0.5177793359335131
0.8145737190612207
0.7363478990038121
0.20758834533702153
0.2533018140549718
0.30764875086796684
0.29044714199656413
0.9350374080783923
0.6277994360424477
0.603553220567354
0.78279661631625
0.6718149226457006
0.5353760149732133
0.6848376228333551
0.9446001512801305
0.7972906165117697
0.5043025319455414
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[6, 5, 4, 1, 3, 0, 7, 2]
tensor([6, 5, 4, 1, 3, 0, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 5, 1, 0, 4, 2]
tensor([6, 3, 7, 5, 1, 0, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 4, 7, 0, 3, 1, 5, 2]
tensor([6, 4, 7, 0, 3, 1, 5, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 5, 3, 4, 1, 0, 7, 6]
tensor([2, 5, 3, 4, 1, 0, 7, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 5, 0, 2, 4, 0, 1, 1]
tensor([3, 5, 0, 2, 4, 0, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 0, 2, 1, 2, 3]
tensor([0, 3, 1, 0, 2, 1, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 3, 1, 2, 2, 1, 3, 0]
tensor([0, 3, 1, 2, 2, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 1, 2, 0, 3, 3, 2, 1]
tensor([0, 1, 2, 0, 3, 3, 2, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1, 0, 1.0, 1.0, 1.0, 1.0]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/66.pt
[18] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2601.56it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<09:15,  3.94s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:06<04:47,  2.07s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:09<04:09,  1.82s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:11<03:17,  1.46s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:13<02:45,  1.25s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:15<02:25,  1.11s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:17<02:11,  1.02s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:19<02:19,  1.10s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:23<02:48,  1.35s/it]Running loglikelihood requests:  13%|█▎        | 19/142 [00:24<02:23,  1.16s/it]Running loglikelihood requests:  15%|█▍        | 21/142 [00:26<02:05,  1.04s/it]Running loglikelihood requests:  16%|█▌        | 23/142 [00:27<01:51,  1.07it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:29<01:43,  1.13it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:30<01:34,  1.21it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:32<01:28,  1.28it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:33<01:23,  1.33it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:34<01:19,  1.37it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:36<01:16,  1.40it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:37<01:13,  1.42it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:38<01:11,  1.43it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:40<01:13,  1.38it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:41<01:10,  1.41it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:43<01:06,  1.45it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:44<01:04,  1.48it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:45<01:01,  1.52it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:46<00:58,  1.56it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:48<00:56,  1.59it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:49<00:53,  1.61it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:50<00:53,  1.58it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:51<00:51,  1.61it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:52<00:49,  1.63it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:54<00:50,  1.55it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:55<00:48,  1.60it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:56<00:45,  1.63it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:57<00:43,  1.66it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:58<00:42,  1.68it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:00<00:45,  1.53it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:01<00:42,  1.59it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:02<00:39,  1.64it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:03<00:37,  1.68it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:05<00:35,  1.71it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:06<00:34,  1.73it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:07<00:32,  1.75it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:08<00:31,  1.77it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:09<00:29,  1.78it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:10<00:29,  1.72it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:11<00:28,  1.75it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:12<00:26,  1.78it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:14<00:25,  1.79it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:15<00:23,  1.81it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:16<00:22,  1.82it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:17<00:21,  1.84it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:18<00:20,  1.85it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:19<00:18,  1.86it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:20<00:18,  1.79it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:21<00:17,  1.81it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:22<00:15,  1.83it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:23<00:14,  1.85it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:24<00:13,  1.86it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:25<00:12,  1.88it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:26<00:11,  1.88it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:27<00:09,  1.91it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:29<00:08,  1.92it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:30<00:07,  1.92it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:31<00:07,  1.74it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:32<00:06,  1.80it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:33<00:04,  1.86it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:34<00:03,  1.92it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:35<00:02,  1.96it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:36<00:01,  1.98it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:37<00:00,  2.02it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:37<00:00,  1.46it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-4): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-9): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-12): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-19): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (20-21): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22-23): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (25-31): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-4): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-9): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-12): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-19): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (20-21): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22-23): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (25-31): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/18.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140213446192704 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213446192704 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213446192704 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213446192704 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140213849158192 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140213849158192 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213849158192 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140213849158192 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[18] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.43661971830985913, 'acc_stderr,none': 0.05927935558412972}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8840011732877091
0.9530187867791682
0.922962504956303
0.7839472459406365
0.8981387177104178
0.9267264969548299
0.5821108615798573
0.2837945650872556
0.6077879555368297
0.40197237439213185
0.8904940347912964
0.7509530386402397
0.901875691813653
0.9122390055224058
0.7316046462179062
0.7968435583929763
0.548007144578676
0.5393492220671232
0.9101317167988644
0.5489553505889563
0.4310648048222707
0.3687557715800523
0.8114931082909914
0.5012706843250611
0.2095243909347347
0.3518983916697559
0.9676050865236582
0.8840253404453832
0.6389046419601837
0.8840011732877091
0.9530187867791682
0.922962504956303
0.7839472459406365
0.8981387177104178
0.9267264969548299
0.5821108615798573
0.2837945650872556
0.6077879555368297
0.40197237439213185
0.8904940347912964
0.7509530386402397
0.901875691813653
0.9122390055224058
0.7316046462179062
0.7968435583929763
0.548007144578676
0.5393492220671232
0.9101317167988644
0.5489553505889563
Total groups 72 exceeded the threshold, stopping comparison.
The group tensor is
[1, 2, 7, 4, 3, 0, 6, 5]
tensor([1, 2, 7, 4, 3, 0, 6, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 5, 4, 1, 0, 7, 2]
tensor([6, 3, 5, 4, 1, 0, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 3, 6, 4, 2, 0, 7, 5]
tensor([1, 3, 6, 4, 2, 0, 7, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 3, 6, 4, 0, 1, 7, 2]
tensor([5, 3, 6, 4, 0, 1, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 6, 5, 3, 0, 7, 1]
tensor([2, 4, 6, 5, 3, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 5, 2, 4, 0, 1, 3]
tensor([0, 1, 5, 2, 4, 0, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/18.pt
[189] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2531.24it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:04<10:21,  4.41s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:06<04:51,  2.09s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:09<04:00,  1.75s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:11<03:10,  1.41s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:13<02:45,  1.25s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:15<02:31,  1.16s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:17<02:26,  1.14s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:19<02:11,  1.03s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:21<01:59,  1.04it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:22<01:49,  1.12it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:24<01:42,  1.18it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:25<01:36,  1.23it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:26<01:30,  1.29it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:28<01:30,  1.26it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:29<01:26,  1.31it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:31<01:22,  1.35it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:32<01:21,  1.33it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:34<01:18,  1.37it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:35<01:15,  1.39it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:37<01:13,  1.41it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:38<01:13,  1.38it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:10,  1.41it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:41<01:07,  1.44it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:42<01:04,  1.47it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:43<01:01,  1.51it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:44<00:58,  1.55it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:46<00:56,  1.59it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<00:54,  1.61it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<00:55,  1.53it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:49<00:52,  1.57it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:51<00:50,  1.59it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<00:48,  1.61it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:53<00:47,  1.64it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:45,  1.65it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:43,  1.66it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:42,  1.67it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:40,  1.69it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:43,  1.53it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:40,  1.59it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:38,  1.63it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:36,  1.67it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:34,  1.69it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:05<00:33,  1.72it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:06<00:31,  1.73it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:07<00:30,  1.75it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:09<00:32,  1.58it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:10<00:29,  1.64it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:11<00:27,  1.68it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:12<00:26,  1.72it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:13<00:24,  1.74it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:14<00:23,  1.76it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:16<00:21,  1.78it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:17<00:20,  1.78it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:18<00:19,  1.80it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:19<00:20,  1.64it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:20<00:18,  1.70it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:21<00:16,  1.75it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:23<00:17,  1.53it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:24<00:15,  1.61it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:25<00:13,  1.69it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:26<00:12,  1.74it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:27<00:10,  1.78it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:28<00:09,  1.81it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:32<00:14,  1.07it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:33<00:10,  1.23it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:34<00:07,  1.38it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:35<00:06,  1.49it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:36<00:04,  1.59it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:37<00:02,  1.68it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:38<00:01,  1.74it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:40<00:00,  1.38it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:40<00:00,  1.41it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-28): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (29): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (30-31): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-28): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (29): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (30-31): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/189.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 502 0
Using the latest cached version of the dataset since glue couldn't be found on the Hugging Face Hub
WARNING:datasets.load:Using the latest cached version of the dataset since glue couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'wnli' at /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c (last modified on Wed Apr 30 15:05:20 2025).
WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'wnli' at /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c (last modified on Wed Apr 30 15:05:20 2025).
DEBUG:filelock:Attempting to acquire lock 140214255076512 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140214255076512 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140214255076512 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140214255076512 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[189] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.6024646571659534
0.8979951240825792
0.626373129339755
0.44935206242731585
0.8784896158641048
0.6837759248511185
0.6015518094762149
0.3505110226931915
0.8777011536981446
0.9326494344559366
0.9677722596476288
0.8304234396005722
0.8190920250237561
0.9903242845857883
0.8267732717549456
0.828751210253346
0.9341138981557521
0.49221062986878583
0.3069647614672379
0.9540155558764357
0.17208911684447897
0.24981632210046514
0.9635863021858757
0.7455052133761656
0.8389345141460378
0.9946030373097186
0.6620732151580052
0.6342744878528105
0.8890512085844174
Total groups 67 exceeded the threshold, stopping comparison.
The group tensor is
[2, 4, 7, 1, 3, 0, 6, 5]
tensor([2, 4, 7, 1, 3, 0, 6, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 7, 6, 5, 0, 1, 4, 3]
tensor([2, 7, 6, 5, 0, 1, 4, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 6, 4, 3, 0, 1, 7, 2]
tensor([5, 6, 4, 3, 0, 1, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 4, 1, 0, 1, 2, 5, 3]
tensor([0, 4, 1, 0, 1, 2, 5, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[2, 4, 0, 0, 5, 1, 1, 3]
tensor([2, 4, 0, 0, 5, 1, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 1, 1.0, 1.0, 1, 1.0, 1.0]
tensor([0, 0, 1, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 1.0, 0, 1, 1.0, 1.0]
tensor([0, 1, 1, 1, 0, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 1, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 0, 1.0, 1.0, 1.0, 1]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
Model saved locally at saved_models/189.pt
[210] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2602.81it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:07<17:12,  7.32s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:09<06:14,  2.70s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:11<04:14,  1.86s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:13<03:16,  1.46s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:15<02:46,  1.25s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:17<02:34,  1.18s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:19<02:17,  1.06s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:20<02:04,  1.02it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:22<01:55,  1.08it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:23<01:46,  1.15it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:25<01:40,  1.20it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:26<01:34,  1.25it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:29<01:49,  1.06it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:30<01:39,  1.15it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:31<01:32,  1.22it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:33<01:26,  1.28it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:34<01:22,  1.32it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:36<01:18,  1.36it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:37<01:19,  1.32it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:39<01:16,  1.35it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:40<01:13,  1.38it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:42<01:12,  1.37it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:43<01:08,  1.41it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:44<01:06,  1.44it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:45<01:03,  1.47it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:47<01:00,  1.50it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:48<01:00,  1.48it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:49<00:57,  1.52it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:51<00:54,  1.55it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:52<00:52,  1.57it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:53<00:51,  1.58it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:54<00:49,  1.60it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:56<00:47,  1.61it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:57<00:46,  1.61it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:58<00:50,  1.46it/s]Running loglikelihood requests:  50%|█████     | 71/142 [01:00<00:46,  1.52it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:01<00:44,  1.56it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:02<00:42,  1.59it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:03<00:40,  1.61it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:04<00:38,  1.64it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:06<00:36,  1.65it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:07<00:35,  1.67it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:08<00:36,  1.55it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:09<00:34,  1.60it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:11<00:32,  1.64it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:12<00:30,  1.67it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:13<00:28,  1.70it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:14<00:27,  1.71it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:15<00:26,  1.73it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:16<00:24,  1.73it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:17<00:23,  1.74it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:19<00:27,  1.44it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:21<00:24,  1.52it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:22<00:22,  1.59it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:23<00:20,  1.64it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:24<00:18,  1.69it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:25<00:16,  1.72it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:26<00:15,  1.75it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:27<00:14,  1.76it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:30<00:18,  1.21it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:31<00:15,  1.33it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:32<00:13,  1.44it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:33<00:11,  1.52it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:35<00:09,  1.60it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:36<00:07,  1.66it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:37<00:06,  1.70it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:38<00:05,  1.68it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:39<00:04,  1.63it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:40<00:02,  1.70it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:41<00:01,  1.78it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:42<00:00,  1.87it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:42<00:00,  1.38it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-8): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-13): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15-31): 17 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-8): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-13): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15-31): 17 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/210.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140213841144784 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213841144784 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213841144784 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140213841144784 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140213446984336 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140213446984336 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140213446984336 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140213446984336 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[210] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.36326485003451675
0.9393611658494853
0.5347697273308267
0.4619639883096971
0.26830156279453604
0.8380027464731019
0.8263233574501603
0.6320977880109986
0.7843146415103402
0.2072713135482749
0.3805984042723936
0.43289830555723763
0.19273781754260347
0.8181624071033468
0.5320194969946469
0.7203219641552613
0.7789354624632999
0.3567665334531696
0.2604208944916411
0.942273921845013
0.8798126471295904
0.8327076147848976
0.5471221204152548
0.8016285641373796
0.8787721123612405
0.7314094662215913
0.733830137407364
0.7399689895148621
0.8815844426907766
0.36326485003451675
0.9393611658494853
0.5347697273308267
0.4619639883096971
0.26830156279453604
0.8380027464731019
0.8263233574501603
0.6320977880109986
0.7843146415103402
0.2072713135482749
0.3805984042723936
0.43289830555723763
0.19273781754260347
0.8181624071033468
0.5320194969946469
0.7203219641552613
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[1, 2, 5, 4, 3, 0, 7, 6]
tensor([1, 2, 5, 4, 3, 0, 7, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 2, 6, 0, 1, 4, 7]
tensor([5, 3, 2, 6, 0, 1, 4, 7], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 2, 6, 7, 4, 0, 3, 5]
tensor([1, 2, 6, 7, 4, 0, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 4, 5, 6, 2, 0, 7, 3]
tensor([1, 4, 5, 6, 2, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 5, 1, 0, 0, 2, 1, 3]
tensor([4, 5, 1, 0, 0, 2, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[1, 5, 0, 3, 4, 0, 1, 2]
tensor([1, 5, 0, 3, 4, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 2, 0, 1, 3, 2]
tensor([0, 3, 1, 2, 0, 1, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1, 0, 1.0, 1.0]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
Model saved locally at saved_models/210.pt
[109] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 1356.74it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:04<10:29,  4.46s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:06<04:46,  2.06s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:08<03:22,  1.48s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:10<02:47,  1.24s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:12<02:33,  1.15s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:14<02:19,  1.06s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:08,  1.00it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:17<01:59,  1.06it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<01:53,  1.10it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:45,  1.16it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:49,  1.11it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<01:41,  1.18it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:25<01:34,  1.23it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:27<01:29,  1.28it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:26,  1.31it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:30<01:23,  1.33it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:32<01:28,  1.24it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:33<01:27,  1.22it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:35<01:22,  1.28it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:18,  1.32it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:37<01:14,  1.35it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:11,  1.38it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:40<01:08,  1.41it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:42<01:08,  1.39it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:43<01:04,  1.44it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:44<01:01,  1.48it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:45<00:59,  1.51it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<00:56,  1.53it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<00:54,  1.55it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:49<00:53,  1.56it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:50<00:51,  1.57it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<00:51,  1.52it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:53<00:49,  1.55it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:47,  1.57it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:56<00:45,  1.59it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:44,  1.60it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:59<00:47,  1.44it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:00<00:44,  1.49it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:42,  1.54it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:41,  1.52it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:04<00:39,  1.56it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:05<00:36,  1.60it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:35,  1.62it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:33,  1.64it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:08<00:31,  1.66it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:09<00:30,  1.67it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:11<00:29,  1.68it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:12<00:27,  1.70it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:13<00:27,  1.64it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:14<00:25,  1.66it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:15<00:24,  1.68it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:17<00:22,  1.70it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:18<00:21,  1.72it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:19<00:20,  1.73it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:20<00:18,  1.74it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:21<00:17,  1.75it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:22<00:16,  1.75it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:24<00:16,  1.66it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:25<00:14,  1.69it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:26<00:13,  1.71it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:27<00:12,  1.73it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:28<00:10,  1.75it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:29<00:09,  1.72it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:30<00:08,  1.70it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:32<00:07,  1.71it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:33<00:07,  1.51it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:35<00:05,  1.52it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:36<00:04,  1.57it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:37<00:03,  1.66it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:38<00:01,  1.70it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:39<00:00,  1.74it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:39<00:00,  1.43it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-4): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-9): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-12): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-19): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (20-21): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22-23): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (25-31): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-4): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-9): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-12): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-19): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (20-21): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22-23): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (25-31): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/109.pt
INFO:save_model:Node info successfully sent
