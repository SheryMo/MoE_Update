nohup: ignoring input
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:0'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}
网络架构：{'47': ['175', '253', '4', '66', '165', '197', '101', '217'], '122': ['48', '86', '183', '134', '254', '56', '226', '49', '141', '233', '31', '101', '217', '238', '103'], '56': ['122', '226', '204', '31', '233', '155', '253', '238'], '226': ['204', '122', '86', '133', '4', '56', '197', '134', '183', '101', '253', '66', '57'], '87': ['134', '103', '165', '86', '238', '49', '178', '233', '36'], '197': ['4', '217', '226', '47', '254', '134', '86', '178', '138', '209', '103'], '134': ['197', '48', '226', '217', '183', '122', '87', '233', '155', '36', '238', '57'], '49': ['138', '66', '183', '122', '87', '233', '204', '155', '48'], '86': ['57', '197', '238', '253', '175', '122', '226', '87', '31', '36', '165'], '141': ['217', '101', '165', '122', '48', '155'], '178': ['183', '87', '48', '31', '197', '36', '165', '254', '57'], '233': ['49', '134', '165', '122', '87', '56', '101', '175', '133'], '31': ['122', '178', '56', '86', '36', '4', '48', '66'], '138': ['57', '197', '133', '217', '209', '49', '175'], '4': ['204', '197', '155', '31', '48', '47', '226', '175'], '183': ['101', '226', '178', '134', '49', '122', '48', '57'], '204': ['57', '49', '254', '226', '66', '56', '4', '253'], '155': ['254', '56', '134', '49', '141', '4', '253'], '36': ['238', '86', '178', '134', '87', '31', '209', '165'], '101': ['122', '226', '253', '47', '233', '141', '183', '209', '175', '103'], '217': ['165', '122', '253', '47', '134', '197', '141', '138', '175', '66', '238'], '209': ['197', '133', '101', '103', '36', '138', '254', '66', '238'], '165': ['178', '133', '86', '36', '66', '47', '87', '141', '233', '217'], '254': ['204', '133', '178', '175', '209', '122', '197', '155'], '253': ['56', '155', '204', '175', '226', '47', '86', '101', '217', '133'], '175': ['101', '4', '138', '233', '217', '47', '86', '254', '253', '57'], '48': ['49', '57', '66', '183', '31', '122', '134', '141', '178', '4', '103'], '66': ['226', '217', '31', '209', '47', '49', '204', '165', '48'], '238': ['217', '56', '209', '134', '122', '87', '86', '36'], '57': ['175', '183', '134', '178', '226', '86', '138', '204', '48', '133'], '133': ['165', '233', '209', '57', '253', '226', '138', '254'], '103': ['197', '122', '101', '209', '48', '87']}
47
cuda:0
rte
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:42<00:42, 42.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:57<00:00, 26.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:57<00:00, 28.62s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/revision/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 111
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/revision/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c?recursive=False&expand=False HTTP/1.1" 307 136
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c?recursive=False&expand=False HTTP/1.1" 200 530
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/ax?recursive=False&expand=False HTTP/1.1" 307 139
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/ax?recursive=False&expand=False HTTP/1.1" 200 231
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/revision/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 111
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/revision/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/rte?recursive=False&expand=False HTTP/1.1" 307 140
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/rte?recursive=False&expand=False HTTP/1.1" 200 354
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140328337353008 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140328337353008 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140328337353008 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140328337353008 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140328364882160 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140328364882160 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140328364882160 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140328364882160 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of rte from None to 0
INFO:lm_eval.api.task:Building contexts for rte on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2412.29it/s]
DEBUG:lm_eval.evaluator:Task: rte; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:03<12:53,  3.88s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:05<05:11,  1.58s/it]Running loglikelihood requests:   2%|▎         | 5/200 [00:06<03:39,  1.12s/it]Running loglikelihood requests:   4%|▎         | 7/200 [00:08<02:58,  1.08it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:09<02:37,  1.21it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:10<02:23,  1.32it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:12<02:14,  1.39it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:13<02:07,  1.45it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:14<02:02,  1.50it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:15<01:57,  1.53it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:16<01:53,  1.58it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:18<01:49,  1.61it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:19<01:46,  1.64it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:20<01:43,  1.68it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:21<01:39,  1.71it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:22<01:37,  1.74it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:23<01:35,  1.76it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:24<01:32,  1.78it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:25<01:31,  1.78it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:27<01:29,  1.79it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:28<01:27,  1.81it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:29<01:25,  1.84it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:30<01:23,  1.87it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:31<01:19,  1.92it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:32<01:16,  1.97it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:33<01:13,  2.03it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:33<01:10,  2.09it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:34<01:07,  2.14it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:35<01:05,  2.20it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:36<01:02,  2.25it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:37<01:00,  2.29it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:38<00:59,  2.32it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:39<00:57,  2.34it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:39<00:56,  2.37it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:40<00:54,  2.39it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:41<00:53,  2.42it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:42<00:52,  2.43it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:43<00:50,  2.45it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:43<00:49,  2.48it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:44<00:48,  2.50it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:45<00:47,  2.53it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:46<00:46,  2.50it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:47<00:45,  2.53it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:47<00:44,  2.56it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:48<00:42,  2.59it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:49<00:41,  2.61it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:50<00:41,  2.58it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:50<00:40,  2.59it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:51<00:39,  2.60it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:52<00:39,  2.57it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:53<00:38,  2.56it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:53<00:37,  2.61it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:54<00:35,  2.65it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:55<00:34,  2.69it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:56<00:33,  2.71it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:56<00:32,  2.74it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:57<00:31,  2.76it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:58<00:30,  2.77it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:58<00:29,  2.78it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:59<00:29,  2.79it/s]Running loglikelihood requests:  60%|██████    | 121/200 [01:00<00:28,  2.79it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [01:01<00:27,  2.79it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [01:01<00:26,  2.79it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [01:02<00:26,  2.80it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [01:03<00:25,  2.81it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [01:03<00:24,  2.81it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [01:04<00:23,  2.83it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [01:05<00:22,  2.86it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:05<00:03, 11.88it/s]Running loglikelihood requests:  78%|███████▊  | 156/200 [01:06<00:04,  9.33it/s]Running loglikelihood requests:  79%|███████▉  | 158/200 [01:06<00:05,  7.25it/s]Running loglikelihood requests:  80%|████████  | 160/200 [01:07<00:06,  5.91it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:08<00:08,  4.55it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:08<00:09,  4.11it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:09<00:09,  3.79it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:10<00:09,  3.59it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:10<00:08,  3.45it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:11<00:08,  3.37it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:12<00:08,  3.28it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:12<00:07,  3.23it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:13<00:07,  3.20it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:14<00:06,  3.18it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:14<00:05,  3.17it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:15<00:05,  3.17it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:15<00:04,  3.17it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:16<00:04,  3.18it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:17<00:03,  3.20it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:17<00:02,  3.23it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:18<00:02,  3.26it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:19<00:01,  3.22it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:19<00:00,  3.28it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:20<00:00,  3.33it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:20<00:00,  2.49it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:1'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:1'}
full model:
{'rte': {'alias': 'rte', 'acc,none': 0.5, 'acc_stderr,none': 0.050251890762960605}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.34161229456626735
0.905233410256777
0.5205040718697735
0.4121994254524892
0.7398116665887099
0.6225415196831932
0.7923970242263771
0.7353888240887675
0.6535613357308766
0.7757058271862038
0.734046359122903
0.4471799126982846
0.773619360921301
0.7955347039939479
0.8672068064531693
0.8652880343596522
0.3302235467760883
0.6789268064017625
0.6072221471952108
0.9194446824778495
0.4812004589187253
0.5728915095234594
0.1682455054057436
0.93212414632396
0.9148362604533635
0.8268537756297094
0.7592245907029287
0.7256008379011685
0.7109756105942956
0.34161229456626735
0.905233410256777
0.5205040718697735
0.4121994254524892
0.7398116665887099
0.6225415196831932
0.7923970242263771
0.7353888240887675
0.6535613357308766
0.7757058271862038
0.734046359122903
0.4471799126982846
0.773619360921301
0.7955347039939479
0.8672068064531693
0.8652880343596522
0.3302235467760883
0.6789268064017625
0.6072221471952108
0.9194446824778495
0.4812004589187253
Total groups 73 exceeded the threshold, stopping comparison.
The group tensor is
[5, 2, 7, 1, 6, 4, 3, 0]
tensor([5, 2, 7, 1, 6, 4, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 6, 0, 7, 3, 4, 1]
tensor([5, 2, 6, 0, 7, 3, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 7, 2, 5, 4, 3, 0]
tensor([6, 1, 7, 2, 5, 4, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 2, 4, 1, 5, 0]
tensor([6, 3, 7, 2, 4, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 5, 6, 2, 3, 1, 4, 0]
tensor([7, 5, 6, 2, 3, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 5, 4, 1, 0, 1, 3]
tensor([0, 2, 5, 4, 1, 0, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1.0, 1]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 4
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 5 to 8
done!
Normal merging for layer 9
tensor([0, 5])
tensor(0)
tensor([4, 6])
tensor(4)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 10 to 30
done!
Normal merging for layer 31
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
all done!
Model size: 12.1348 GB
122
cuda:1
rte
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:43<00:43, 43.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:58<00:00, 26.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:58<00:00, 29.14s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/revision/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 111
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/revision/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140325647356352 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140325647356352 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140325647356352 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140325647356352 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140328337707024 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140328337707024 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140328337707024 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140328337707024 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of rte from None to 0
INFO:lm_eval.api.task:Building contexts for rte on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2371.93it/s]
DEBUG:lm_eval.evaluator:Task: rte; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:02<07:28,  2.26s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:03<03:50,  1.17s/it]Running loglikelihood requests:   2%|▎         | 5/200 [00:05<03:02,  1.07it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:06<02:39,  1.21it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:07<02:26,  1.30it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:09<02:18,  1.37it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:10<02:12,  1.41it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:11<02:06,  1.46it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:13<02:01,  1.51it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:14<01:57,  1.55it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:15<01:52,  1.59it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:16<01:49,  1.62it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:17<01:46,  1.64it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:18<01:42,  1.68it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:20<01:39,  1.72it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:21<01:36,  1.75it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:22<01:34,  1.77it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:23<01:32,  1.78it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:24<01:30,  1.80it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:25<01:28,  1.82it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:26<01:25,  1.85it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:27<01:23,  1.88it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:28<01:20,  1.91it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:29<01:18,  1.95it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:30<01:15,  1.99it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:31<01:13,  2.04it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:32<01:10,  2.09it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:33<01:07,  2.13it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:34<01:06,  2.16it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:35<01:04,  2.18it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:35<01:03,  2.19it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:36<01:01,  2.22it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:37<00:59,  2.27it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:38<00:57,  2.30it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:39<00:56,  2.33it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:40<00:54,  2.35it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:41<00:53,  2.38it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:41<00:51,  2.40it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:42<00:50,  2.44it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:43<00:49,  2.46it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:44<00:47,  2.48it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:44<00:46,  2.51it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:45<00:45,  2.52it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:46<00:44,  2.53it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:47<00:43,  2.55it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:48<00:42,  2.57it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:48<00:41,  2.60it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:49<00:40,  2.61it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:50<00:39,  2.63it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:51<00:38,  2.65it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:51<00:37,  2.66it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:52<00:36,  2.67it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:53<00:35,  2.69it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:54<00:34,  2.70it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:54<00:33,  2.71it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:55<00:32,  2.73it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:56<00:31,  2.74it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:56<00:30,  2.74it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:57<00:30,  2.74it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:58<00:29,  2.75it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:59<00:28,  2.75it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:59<00:27,  2.76it/s]Running loglikelihood requests:  70%|███████   | 141/200 [01:00<00:04, 12.17it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:01<00:07,  7.45it/s]Running loglikelihood requests:  74%|███████▍  | 148/200 [01:02<00:07,  6.63it/s]Running loglikelihood requests:  75%|███████▌  | 150/200 [01:02<00:08,  5.62it/s]Running loglikelihood requests:  76%|███████▌  | 152/200 [01:03<00:09,  4.87it/s]Running loglikelihood requests:  77%|███████▋  | 154/200 [01:04<00:10,  4.32it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:04<00:12,  3.50it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:05<00:12,  3.36it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:06<00:12,  3.27it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:06<00:12,  3.21it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:07<00:11,  3.16it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:08<00:11,  3.13it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:08<00:10,  3.12it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:09<00:09,  3.11it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:09<00:09,  3.12it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:10<00:08,  3.12it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:11<00:07,  3.13it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:11<00:07,  3.14it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:12<00:06,  3.15it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:13<00:06,  3.16it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:13<00:05,  3.17it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:14<00:04,  3.18it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:15<00:04,  3.19it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:15<00:03,  3.20it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:16<00:02,  3.22it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:16<00:02,  3.24it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:17<00:01,  3.26it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:18<00:00,  3.33it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:18<00:00,  3.39it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:18<00:00,  2.55it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:2'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:2'}
full model:
{'rte': {'alias': 'rte', 'acc,none': 0.5, 'acc_stderr,none': 0.050251890762960605}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.34161229456626735
0.905233410256777
0.5205040718697735
0.4121994254524892
0.7398116665887099
0.6225415196831932
0.7923970242263771
0.7353888240887675
0.6535613357308766
0.7757058271862038
0.734046359122903
0.4471799126982846
0.773619360921301
0.7955347039939479
0.8672068064531693
0.8652880343596522
0.3302235467760883
0.6789268064017625
0.6072221471952108
0.9194446824778495
0.4812004589187253
0.5728915095234594
0.1682455054057436
0.93212414632396
0.9148362604533635
0.8268537756297094
0.7592245907029287
0.7256008379011685
0.7109756105942956
0.34161229456626735
0.905233410256777
0.5205040718697735
0.4121994254524892
0.7398116665887099
0.6225415196831932
0.7923970242263771
0.7353888240887675
0.6535613357308766
0.7757058271862038
0.734046359122903
0.4471799126982846
0.773619360921301
0.7955347039939479
0.8672068064531693
0.8652880343596522
0.3302235467760883
0.6789268064017625
0.6072221471952108
0.9194446824778495
0.4812004589187253
Total groups 73 exceeded the threshold, stopping comparison.
The group tensor is
[5, 2, 7, 1, 6, 4, 3, 0]
tensor([5, 2, 7, 1, 6, 4, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 6, 0, 7, 3, 4, 1]
tensor([5, 2, 6, 0, 7, 3, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 7, 2, 5, 4, 3, 0]
tensor([6, 1, 7, 2, 5, 4, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 2, 4, 1, 5, 0]
tensor([6, 3, 7, 2, 4, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 5, 6, 2, 3, 1, 4, 0]
tensor([7, 5, 6, 2, 3, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 5, 4, 1, 0, 1, 3]
tensor([0, 2, 5, 4, 1, 0, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1.0, 1]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 4
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 5 to 8
done!
Normal merging for layer 9
tensor([0, 5])
tensor(0)
tensor([4, 6])
tensor(4)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 10 to 30
done!
Normal merging for layer 31
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
all done!
Model size: 12.1348 GB
56
cuda:2
mrpc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:42<00:42, 42.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 25.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 27.90s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: mrpc] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: mrpc] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
WARNING:lm_eval.api.task:[Task: mrpc] metric f1 is defined, but aggregation is not. using default aggregation=f1
WARNING:lm_eval.api.task:[Task: mrpc] metric f1 is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mrpc?recursive=False&expand=False HTTP/1.1" 307 141
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mrpc?recursive=False&expand=False HTTP/1.1" 200 356
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140325646152672 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140325646152672 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140325646152672 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140325646152672 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140325646152672 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140325646152672 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140325646152672 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140325646152672 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of mrpc from None to 0
INFO:lm_eval.api.task:Building contexts for mrpc on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1695.22it/s]
DEBUG:lm_eval.evaluator:Task: mrpc; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<05:37,  1.70s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:37,  1.25it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:03<02:04,  1.57it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:04<01:52,  1.72it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:05<01:43,  1.84it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:06<01:38,  1.93it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:07<01:33,  1.99it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:08<01:30,  2.04it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:09<01:28,  2.07it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:10<01:26,  2.09it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:11<01:24,  2.11it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:12<01:24,  2.10it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:13<01:22,  2.12it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:14<01:21,  2.14it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:14<01:20,  2.14it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:15<01:18,  2.16it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:16<01:16,  2.19it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:17<01:14,  2.22it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:18<01:13,  2.22it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:19<01:12,  2.23it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:20<01:10,  2.24it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:21<01:09,  2.25it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:22<01:08,  2.26it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:22<01:07,  2.27it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:23<01:06,  2.28it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:24<01:05,  2.29it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:25<01:03,  2.30it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:26<01:02,  2.31it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:27<01:01,  2.31it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:28<01:00,  2.31it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:28<01:00,  2.32it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:29<00:58,  2.32it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:30<00:57,  2.33it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:31<00:56,  2.34it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:32<00:55,  2.36it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:33<00:54,  2.38it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:33<00:53,  2.39it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:34<00:52,  2.40it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:35<00:51,  2.41it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:36<00:50,  2.42it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:37<00:49,  2.43it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:38<00:48,  2.43it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:38<00:47,  2.44it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:39<00:46,  2.44it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:40<00:45,  2.45it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:41<00:44,  2.46it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:42<00:43,  2.47it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:42<00:42,  2.48it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:43<00:41,  2.48it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:44<00:40,  2.49it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:45<00:39,  2.50it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:46<00:38,  2.51it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:46<00:37,  2.51it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:47<00:36,  2.52it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:48<00:36,  2.52it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:49<00:35,  2.53it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:50<00:34,  2.53it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:50<00:33,  2.53it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:51<00:32,  2.53it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:52<00:31,  2.54it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:53<00:31,  2.55it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:54<00:30,  2.55it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:54<00:29,  2.55it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:55<00:28,  2.56it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:56<00:27,  2.56it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:57<00:26,  2.57it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:57<00:26,  2.58it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:58<00:25,  2.58it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:59<00:24,  2.58it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [01:00<00:23,  2.59it/s]Running loglikelihood requests:  70%|███████   | 141/200 [01:00<00:22,  2.60it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [01:01<00:21,  2.61it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:02<00:20,  2.62it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [01:03<00:20,  2.63it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:03<00:03, 10.22it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:04<00:04,  7.84it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:05<00:04,  6.25it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:05<00:05,  5.19it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:06<00:06,  4.46it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:07<00:06,  3.96it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:07<00:06,  3.62it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:08<00:06,  3.40it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:09<00:05,  3.24it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:10<00:05,  3.15it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:10<00:04,  3.08it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:11<00:04,  3.04it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:12<00:03,  3.02it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:12<00:02,  3.01it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:13<00:02,  3.00it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:14<00:01,  3.01it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:14<00:00,  3.01it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:15<00:00,  3.05it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:15<00:00,  2.65it/s]
bootstrapping for stddev (sequential): f1_score
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:01<02:10,  1.32s/it]  2%|▏         | 2/100 [00:02<02:09,  1.32s/it]  3%|▎         | 3/100 [00:03<02:07,  1.32s/it]  4%|▍         | 4/100 [00:05<02:06,  1.31s/it]  5%|▌         | 5/100 [00:06<02:04,  1.31s/it]  6%|▌         | 6/100 [00:07<02:03,  1.31s/it]  7%|▋         | 7/100 [00:09<02:02,  1.31s/it]  8%|▊         | 8/100 [00:10<02:00,  1.31s/it]  9%|▉         | 9/100 [00:11<01:59,  1.31s/it] 10%|█         | 10/100 [00:13<01:57,  1.31s/it] 11%|█         | 11/100 [00:14<01:56,  1.31s/it] 12%|█▏        | 12/100 [00:15<01:55,  1.31s/it] 13%|█▎        | 13/100 [00:17<01:53,  1.31s/it] 14%|█▍        | 14/100 [00:18<01:52,  1.31s/it] 15%|█▌        | 15/100 [00:19<01:51,  1.31s/it] 16%|█▌        | 16/100 [00:20<01:49,  1.31s/it] 17%|█▋        | 17/100 [00:22<01:48,  1.31s/it] 18%|█▊        | 18/100 [00:23<01:47,  1.31s/it] 19%|█▉        | 19/100 [00:24<01:46,  1.31s/it] 20%|██        | 20/100 [00:26<01:44,  1.31s/it] 21%|██        | 21/100 [00:27<01:43,  1.31s/it] 22%|██▏       | 22/100 [00:28<01:42,  1.31s/it] 23%|██▎       | 23/100 [00:30<01:40,  1.31s/it] 24%|██▍       | 24/100 [00:31<01:39,  1.31s/it] 25%|██▌       | 25/100 [00:32<01:38,  1.31s/it] 26%|██▌       | 26/100 [00:34<01:36,  1.31s/it] 27%|██▋       | 27/100 [00:35<01:35,  1.31s/it] 28%|██▊       | 28/100 [00:36<01:34,  1.31s/it] 29%|██▉       | 29/100 [00:37<01:32,  1.31s/it] 30%|███       | 30/100 [00:39<01:31,  1.31s/it] 31%|███       | 31/100 [00:40<01:30,  1.31s/it] 32%|███▏      | 32/100 [00:41<01:29,  1.31s/it] 33%|███▎      | 33/100 [00:43<01:27,  1.31s/it] 34%|███▍      | 34/100 [00:44<01:26,  1.31s/it] 35%|███▌      | 35/100 [00:45<01:25,  1.31s/it] 36%|███▌      | 36/100 [00:47<01:23,  1.31s/it] 37%|███▋      | 37/100 [00:48<01:22,  1.31s/it] 42%|████▏     | 42/100 [00:49<00:28,  2.01it/s] 43%|████▎     | 43/100 [00:50<00:35,  1.59it/s] 44%|████▍     | 44/100 [00:51<00:42,  1.32it/s] 45%|████▌     | 45/100 [00:52<00:48,  1.14it/s] 46%|████▌     | 46/100 [00:54<00:52,  1.02it/s] 47%|████▋     | 47/100 [00:55<00:56,  1.06s/it] 48%|████▊     | 48/100 [00:56<00:58,  1.13s/it] 49%|████▉     | 49/100 [00:58<00:59,  1.17s/it] 50%|█████     | 50/100 [00:59<01:00,  1.21s/it] 51%|█████     | 51/100 [01:00<01:00,  1.24s/it] 52%|█████▏    | 52/100 [01:02<01:00,  1.26s/it] 53%|█████▎    | 53/100 [01:03<00:59,  1.27s/it] 54%|█████▍    | 54/100 [01:04<00:59,  1.28s/it] 55%|█████▌    | 55/100 [01:06<00:58,  1.29s/it] 56%|█████▌    | 56/100 [01:07<00:57,  1.30s/it] 57%|█████▋    | 57/100 [01:08<00:55,  1.30s/it] 58%|█████▊    | 58/100 [01:10<00:54,  1.30s/it] 59%|█████▉    | 59/100 [01:11<00:53,  1.30s/it] 60%|██████    | 60/100 [01:12<00:52,  1.30s/it] 61%|██████    | 61/100 [01:13<00:50,  1.31s/it] 62%|██████▏   | 62/100 [01:15<00:49,  1.31s/it] 63%|██████▎   | 63/100 [01:16<00:48,  1.31s/it] 64%|██████▍   | 64/100 [01:17<00:47,  1.31s/it] 65%|██████▌   | 65/100 [01:19<00:45,  1.31s/it] 66%|██████▌   | 66/100 [01:20<00:44,  1.31s/it] 67%|██████▋   | 67/100 [01:21<00:43,  1.31s/it] 68%|██████▊   | 68/100 [01:23<00:41,  1.31s/it] 69%|██████▉   | 69/100 [01:24<00:40,  1.31s/it] 70%|███████   | 70/100 [01:25<00:39,  1.32s/it] 71%|███████   | 71/100 [01:27<00:38,  1.32s/it] 72%|███████▏  | 72/100 [01:28<00:36,  1.32s/it] 73%|███████▎  | 73/100 [01:29<00:35,  1.32s/it] 74%|███████▍  | 74/100 [01:31<00:34,  1.32s/it] 75%|███████▌  | 75/100 [01:32<00:32,  1.32s/it] 76%|███████▌  | 76/100 [01:33<00:31,  1.31s/it] 77%|███████▋  | 77/100 [01:34<00:30,  1.31s/it] 78%|███████▊  | 78/100 [01:36<00:28,  1.31s/it] 79%|███████▉  | 79/100 [01:37<00:27,  1.31s/it] 80%|████████  | 80/100 [01:38<00:26,  1.31s/it] 81%|████████  | 81/100 [01:40<00:24,  1.31s/it] 82%|████████▏ | 82/100 [01:41<00:23,  1.31s/it] 83%|████████▎ | 83/100 [01:42<00:22,  1.31s/it] 84%|████████▍ | 84/100 [01:44<00:20,  1.31s/it] 85%|████████▌ | 85/100 [01:45<00:19,  1.31s/it] 86%|████████▌ | 86/100 [01:46<00:18,  1.31s/it] 87%|████████▋ | 87/100 [01:48<00:17,  1.31s/it] 88%|████████▊ | 88/100 [01:49<00:15,  1.31s/it] 93%|█████████▎| 93/100 [01:49<00:03,  2.32it/s] 95%|█████████▌| 95/100 [01:52<00:03,  1.48it/s] 97%|█████████▋| 97/100 [01:54<00:02,  1.17it/s] 98%|█████████▊| 98/100 [01:55<00:01,  1.07it/s] 99%|█████████▉| 99/100 [01:57<00:01,  1.01s/it]100%|██████████| 100/100 [01:58<00:00,  1.07s/it]100%|██████████| 100/100 [01:58<00:00,  1.19s/it]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:3'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:3'}
full model:
{'mrpc': {'alias': 'mrpc', 'acc,none': 0.7, 'acc_stderr,none': 0.04605661864718383, 'f1,none': np.float64(0.8214285714285714), 'f1_stderr,none': 0.0323693653386572}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.32375514242269293
0.9664141198776434
0.4263233992087649
0.29258192731768157
0.3497718342079178
0.5968480671674146
0.7870107004084098
0.7068508532201193
0.7836897379939763
0.2579819071737284
0.5774144903645764
0.7819348699339372
0.7404680063618143
0.651835611327285
0.6832243011482585
0.34680079188801705
0.6033476505145444
0.6299526106979766
0.6468468631829007
0.8458578364778718
0.39447753640727073
0.5291234791393744
0.944540808938638
0.8460909093145242
0.4915397062049404
0.5314721523201049
0.8670806018560002
0.6451914961665037
0.7380077491787214
0.32375514242269293
0.9664141198776434
0.4263233992087649
0.29258192731768157
0.3497718342079178
0.5968480671674146
0.7870107004084098
0.7068508532201193
0.7836897379939763
0.2579819071737284
0.5774144903645764
0.7819348699339372
0.7404680063618143
0.651835611327285
0.6832243011482585
0.34680079188801705
0.6033476505145444
0.6299526106979766
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[4, 5, 7, 1, 6, 2, 3, 0]
tensor([4, 5, 7, 1, 6, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 4, 5, 0, 1, 2]
tensor([6, 3, 7, 4, 5, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 1, 7, 2, 6, 0, 3, 5]
tensor([4, 1, 7, 2, 6, 0, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 5, 2, 7, 0, 4, 1]
tensor([6, 3, 5, 2, 7, 0, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 5, 6, 7, 3, 1, 0, 2]
tensor([4, 5, 6, 7, 3, 1, 0, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 2, 4, 1, 0, 5]
tensor([0, 3, 1, 2, 4, 1, 0, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1, 1.0]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 2
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 4 to 5
done!
Normal merging for layer 6
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
done!
Cross-layer merge completed for layers 7 to 9
done!
Normal merging for layer 10
tensor([0, 6])
tensor(0)
tensor([2, 5])
tensor(2)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([7])
tensor(7)
done!
Cross-layer merge completed for layers 11 to 26
done!
Normal merging for layer 27
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 28 to 30
done!
Normal merging for layer 31
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
all done!
Model size: 12.3867 GB
226
cuda:3
rte
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:37<00:37, 37.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:50<00:00, 23.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:50<00:00, 25.29s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140325646636128 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140325646636128 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140325646636128 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140325646636128 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140321890696848 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321890696848 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321890696848 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321890696848 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of rte from None to 0
INFO:lm_eval.api.task:Building contexts for rte on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2405.14it/s]
DEBUG:lm_eval.evaluator:Task: rte; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:02<07:35,  2.29s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:03<03:50,  1.17s/it]Running loglikelihood requests:   2%|▎         | 5/200 [00:05<03:00,  1.08it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:06<02:37,  1.23it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:07<02:24,  1.32it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:09<02:15,  1.39it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:10<02:09,  1.44it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:11<02:05,  1.48it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:12<02:00,  1.52it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:14<01:56,  1.55it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:15<00:39,  4.31it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:16<00:46,  3.59it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:17<00:53,  3.07it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:18<01:00,  2.70it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:19<01:05,  2.47it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:20<01:08,  2.31it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:21<01:10,  2.22it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:22<01:11,  2.16it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:23<01:11,  2.13it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:24<01:11,  2.13it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:25<01:09,  2.14it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:26<01:07,  2.17it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:27<01:05,  2.20it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:27<01:03,  2.24it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:28<01:02,  2.27it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:29<01:00,  2.30it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:30<00:59,  2.32it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:31<00:57,  2.34it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:32<00:56,  2.35it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:33<00:55,  2.37it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:33<00:53,  2.39it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:34<00:52,  2.41it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:35<00:51,  2.43it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:36<00:50,  2.45it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:37<00:48,  2.47it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:37<00:47,  2.49it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:38<00:46,  2.52it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:39<00:45,  2.53it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:40<00:44,  2.55it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:40<00:43,  2.57it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:41<00:42,  2.59it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:42<00:41,  2.60it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:43<00:40,  2.62it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:43<00:39,  2.63it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:44<00:38,  2.65it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:45<00:37,  2.66it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:46<00:36,  2.67it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:46<00:35,  2.69it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:47<00:34,  2.70it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:48<00:33,  2.71it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:49<00:32,  2.73it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:49<00:31,  2.74it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:50<00:31,  2.74it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:51<00:30,  2.75it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:52<00:29,  2.75it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:52<00:28,  2.76it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:53<00:27,  2.77it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:54<00:26,  2.78it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:54<00:26,  2.79it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:55<00:25,  2.79it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:56<00:24,  2.79it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:57<00:23,  2.81it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:57<00:22,  2.83it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:58<00:22,  2.85it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:59<00:21,  2.86it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:59<00:20,  2.87it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [01:00<00:19,  2.89it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:01<00:18,  2.91it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [01:01<00:18,  2.92it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:02<00:17,  2.93it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:03<00:16,  2.94it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:03<00:15,  2.94it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:04<00:15,  2.96it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:05<00:14,  2.97it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:05<00:13,  2.99it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:06<00:12,  3.01it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:07<00:12,  3.03it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:07<00:11,  3.03it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:08<00:10,  3.05it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:09<00:10,  3.07it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:09<00:09,  3.10it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:10<00:08,  3.11it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:10<00:07,  3.13it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:11<00:07,  3.14it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:12<00:06,  3.15it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:12<00:06,  3.16it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:13<00:05,  3.17it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:14<00:04,  3.18it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:14<00:04,  3.19it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:11<00:00,  2.78it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:4'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:4'}
full model:
{'rte': {'alias': 'rte', 'acc,none': 0.5, 'acc_stderr,none': 0.050251890762960605}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.34161229456626735
0.905233410256777
0.5205040718697735
0.4121994254524892
0.7398116665887099
0.6225415196831932
0.7923970242263771
0.7353888240887675
0.6535613357308766
0.7757058271862038
0.734046359122903
0.4471799126982846
0.773619360921301
0.7955347039939479
0.8672068064531693
0.8652880343596522
0.3302235467760883
0.6789268064017625
0.6072221471952108
0.9194446824778495
0.4812004589187253
0.5728915095234594
0.1682455054057436
0.93212414632396
0.9148362604533635
0.8268537756297094
0.7592245907029287
0.7256008379011685
0.7109756105942956
0.34161229456626735
0.905233410256777
0.5205040718697735
0.4121994254524892
0.7398116665887099
0.6225415196831932
0.7923970242263771
0.7353888240887675
0.6535613357308766
0.7757058271862038
0.734046359122903
0.4471799126982846
0.773619360921301
0.7955347039939479
0.8672068064531693
0.8652880343596522
0.3302235467760883
0.6789268064017625
0.6072221471952108
0.9194446824778495
0.4812004589187253
Total groups 73 exceeded the threshold, stopping comparison.
The group tensor is
[5, 2, 7, 1, 6, 4, 3, 0]
tensor([5, 2, 7, 1, 6, 4, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 6, 0, 7, 3, 4, 1]
tensor([5, 2, 6, 0, 7, 3, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 7, 2, 5, 4, 3, 0]
tensor([6, 1, 7, 2, 5, 4, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 2, 4, 1, 5, 0]
tensor([6, 3, 7, 2, 4, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 5, 6, 2, 3, 1, 4, 0]
tensor([7, 5, 6, 2, 3, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 5, 4, 1, 0, 1, 3]
tensor([0, 2, 5, 4, 1, 0, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1.0, 1]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 4
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 5 to 8
done!
Normal merging for layer 9
tensor([0, 5])
tensor(0)
tensor([4, 6])
tensor(4)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 10 to 30
done!
Normal merging for layer 31
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
all done!
Model size: 12.1348 GB
87
cuda:4
qnli
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:39<00:39, 39.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 24.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 26.50s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: qnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: qnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/qnli?recursive=False&expand=False HTTP/1.1" 307 141
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/qnli?recursive=False&expand=False HTTP/1.1" 200 361
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140321890687152 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321890687152 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321890687152 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321890687152 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140325646872368 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140325646872368 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140325646872368 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140325646872368 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of qnli from None to 0
INFO:lm_eval.api.task:Building contexts for qnli on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2380.08it/s]
DEBUG:lm_eval.evaluator:Task: qnli; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:02<06:45,  2.04s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:03<03:13,  1.02it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:04<02:29,  1.30it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:05<02:10,  1.48it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:06<01:59,  1.60it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:07<01:50,  1.71it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:08<01:44,  1.80it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:09<01:38,  1.88it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:10<01:32,  1.98it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:11<01:27,  2.06it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:12<01:25,  2.10it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:13<01:22,  2.14it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:14<01:20,  2.16it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:14<01:19,  2.18it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:15<01:17,  2.19it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:16<01:16,  2.21it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:17<01:15,  2.22it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:18<01:14,  2.22it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:18<00:18,  8.17it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:19<00:23,  6.37it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:20<00:28,  5.11it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:21<00:33,  4.28it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:22<00:38,  3.68it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:23<00:41,  3.32it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:23<00:44,  3.06it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:24<00:46,  2.90it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:25<00:47,  2.80it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:26<00:48,  2.73it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:27<00:48,  2.67it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:27<00:48,  2.63it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:28<00:47,  2.61it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:29<00:47,  2.61it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:30<00:46,  2.61it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:30<00:45,  2.61it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:31<00:44,  2.62it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:32<00:44,  2.60it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:33<00:43,  2.62it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:33<00:42,  2.64it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:34<00:41,  2.61it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:35<00:40,  2.63it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:36<00:39,  2.65it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:36<00:38,  2.67it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:37<00:37,  2.68it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:38<00:36,  2.69it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:39<00:35,  2.70it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:39<00:35,  2.71it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:40<00:34,  2.72it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:41<00:33,  2.72it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:42<00:32,  2.73it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:42<00:31,  2.74it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:43<00:30,  2.74it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:44<00:30,  2.75it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:44<00:29,  2.77it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:45<00:28,  2.79it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:46<00:27,  2.80it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:47<00:26,  2.83it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:47<00:25,  2.85it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:48<00:24,  2.86it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:49<00:24,  2.87it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:49<00:23,  2.88it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:50<00:22,  2.86it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:51<00:21,  2.88it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:51<00:21,  2.89it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:52<00:20,  2.91it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:53<00:19,  2.90it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [00:53<00:19,  2.89it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [00:54<00:18,  2.91it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [00:55<00:17,  2.93it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [00:56<00:16,  2.95it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [00:56<00:15,  2.96it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [00:57<00:15,  2.97it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [00:58<00:14,  2.98it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [00:58<00:13,  2.98it/s]Running loglikelihood requests:  80%|████████  | 161/200 [00:59<00:13,  2.99it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:00<00:12,  3.00it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:00<00:11,  3.02it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:01<00:10,  3.04it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:01<00:10,  3.06it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:02<00:09,  3.09it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:03<00:08,  3.10it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:03<00:08,  3.11it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:04<00:07,  3.12it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:05<00:06,  3.14it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:05<00:06,  3.16it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:06<00:05,  3.17it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:06<00:04,  3.19it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:07<00:04,  3.24it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:08<00:03,  3.27it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:08<00:02,  3.30it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:09<00:02,  3.33it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:09<00:01,  3.36it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:10<00:00,  3.40it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:11<00:00,  3.42it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:11<00:00,  2.81it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:5'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:5'}
full model:
{'qnli': {'alias': 'qnli', 'acc,none': 0.46, 'acc_stderr,none': 0.05009082659620332}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9846426504484882
0.5824664507839472
0.519615692575773
0.501155542927289
0.7945367166753022
0.308422053695068
0.8640142063914085
0.9138139850456612
0.4633856064809356
0.6590614315196575
0.6979080802987999
0.7199917897264415
0.7749024201981475
0.6702564595367945
0.48800861061165646
0.4420522818267443
0.9519161971847797
0.7484302940574024
0.5133259423607681
0.5588784666822361
0.6658633104669237
0.7569182042181317
0.5308683195195316
0.8457203515884096
0.2160432078364904
0.9025653665852331
0.9196489243365332
0.6742217686289
0.8668007010904994
0.9846426504484882
0.5824664507839472
0.519615692575773
0.501155542927289
0.7945367166753022
0.308422053695068
0.8640142063914085
0.9138139850456612
0.4633856064809356
0.6590614315196575
0.6979080802987999
0.7199917897264415
0.7749024201981475
0.6702564595367945
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[7, 5, 4, 2, 6, 1, 3, 0]
tensor([7, 5, 4, 2, 6, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 5, 1, 3, 6, 2, 7, 0]
tensor([4, 5, 1, 3, 6, 2, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 3, 4, 1, 2, 5, 0]
tensor([0, 1, 3, 4, 1, 2, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 1, 5, 2, 4, 1, 3, 0]
tensor([0, 1, 5, 2, 4, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 4, 1, 2, 1, 3, 5, 0]
tensor([0, 4, 1, 2, 1, 3, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[2, 4, 3, 0, 5, 1, 1, 0]
tensor([2, 4, 3, 0, 5, 1, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 3, 4, 2, 1, 1, 5, 0]
tensor([0, 3, 4, 2, 1, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 1, 3, 2, 0, 3, 1]
tensor([0, 2, 1, 3, 2, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 2 to 8
done!
Normal merging for layer 9
tensor([0, 7])
tensor(0)
tensor([1, 4])
tensor(1)
tensor([5])
tensor(5)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
done!
Normal merging for layer 10
tensor([0, 7])
tensor(0)
tensor([1, 5])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
done!
Normal merging for layer 11
tensor([0, 7])
tensor(0)
tensor([2, 4])
tensor(2)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
done!
Normal merging for layer 12
tensor([3, 7])
tensor(3)
tensor([5, 6])
tensor(5)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
done!
Normal merging for layer 13
tensor([0, 7])
tensor(0)
tensor([4, 5])
tensor(4)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 14 to 15
done!
Normal merging for layer 16
tensor([0, 5])
tensor(0)
tensor([2, 7])
tensor(2)
tensor([1, 4])
tensor(1)
tensor([3, 6])
tensor(3)
done!
Cross-layer merge completed for layers 17 to 31
done!
all done!
Model size: 12.3238 GB
197
cuda:5
rte
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:38<00:38, 38.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 23.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 26.10s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140321627625936 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321627625936 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321627625936 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321627625936 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140321890681248 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321890681248 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321890681248 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321890681248 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of rte from None to 0
INFO:lm_eval.api.task:Building contexts for rte on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2396.20it/s]
DEBUG:lm_eval.evaluator:Task: rte; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:02<07:46,  2.34s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:03<03:55,  1.20s/it]Running loglikelihood requests:   2%|▎         | 5/200 [00:05<03:04,  1.05it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:06<02:41,  1.20it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:08<02:28,  1.29it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:09<02:19,  1.36it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:10<02:13,  1.41it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:12<02:08,  1.44it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:13<02:03,  1.48it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:14<02:00,  1.50it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:15<01:56,  1.54it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:17<01:53,  1.56it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:18<01:50,  1.58it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:19<01:46,  1.62it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:20<01:43,  1.65it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:23<02:27,  1.15it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:24<02:10,  1.28it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:25<01:58,  1.39it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:27<01:49,  1.48it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:28<01:42,  1.57it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:29<01:36,  1.64it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:30<01:31,  1.71it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:31<01:27,  1.78it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:32<01:23,  1.83it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:33<01:20,  1.88it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:34<01:16,  1.94it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:35<01:13,  2.00it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:36<01:10,  2.05it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:36<01:07,  2.11it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:37<00:18,  6.76it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:38<00:22,  5.53it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:39<00:26,  4.65it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:40<00:30,  4.00it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:41<00:33,  3.55it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:41<00:36,  3.24it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:42<00:38,  3.02it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:43<00:39,  2.86it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:44<00:40,  2.77it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:45<00:40,  2.70it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:45<00:40,  2.66it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:46<00:39,  2.63it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:47<00:39,  2.61it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:48<00:38,  2.61it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:49<00:38,  2.60it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:49<00:37,  2.61it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:50<00:36,  2.61it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:51<00:35,  2.62it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:52<00:34,  2.63it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:52<00:33,  2.64it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:53<00:32,  2.65it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:54<00:32,  2.65it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:55<00:31,  2.66it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:55<00:30,  2.67it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:56<00:29,  2.67it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:57<00:28,  2.68it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:58<00:27,  2.69it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:58<00:27,  2.69it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:59<00:26,  2.70it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [01:00<00:25,  2.70it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [01:00<00:24,  2.72it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [01:01<00:23,  2.75it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [01:02<00:22,  2.76it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [01:03<00:21,  2.77it/s]Running loglikelihood requests:  70%|███████   | 141/200 [01:03<00:21,  2.79it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [01:04<00:20,  2.80it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:05<00:19,  2.81it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [01:05<00:18,  2.81it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:06<00:18,  2.83it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:07<00:17,  2.84it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:08<00:16,  2.85it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:08<00:15,  2.86it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:09<00:14,  2.87it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:10<00:14,  2.90it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:10<00:13,  2.92it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:11<00:12,  2.93it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:12<00:11,  2.94it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:12<00:11,  2.96it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:13<00:10,  2.97it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:14<00:09,  3.00it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:14<00:08,  3.02it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:15<00:08,  3.03it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:16<00:07,  3.05it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:16<00:06,  3.06it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:17<00:06,  3.06it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:17<00:05,  3.08it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:18<00:04,  3.09it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:19<00:04,  3.10it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:19<00:03,  3.11it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:20<00:02,  3.13it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:21<00:02,  3.14it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:21<00:01,  3.16it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:22<00:00,  3.23it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:22<00:00,  3.29it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:22<00:00,  2.41it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:6'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:6'}
full model:
{'rte': {'alias': 'rte', 'acc,none': 0.5, 'acc_stderr,none': 0.050251890762960605}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.34161229456626735
0.905233410256777
0.5205040718697735
0.4121994254524892
0.7398116665887099
0.6225415196831932
0.7923970242263771
0.7353888240887675
0.6535613357308766
0.7757058271862038
0.734046359122903
0.4471799126982846
0.773619360921301
0.7955347039939479
0.8672068064531693
0.8652880343596522
0.3302235467760883
0.6789268064017625
0.6072221471952108
0.9194446824778495
0.4812004589187253
0.5728915095234594
0.1682455054057436
0.93212414632396
0.9148362604533635
0.8268537756297094
0.7592245907029287
0.7256008379011685
0.7109756105942956
0.34161229456626735
0.905233410256777
0.5205040718697735
0.4121994254524892
0.7398116665887099
0.6225415196831932
0.7923970242263771
0.7353888240887675
0.6535613357308766
0.7757058271862038
0.734046359122903
0.4471799126982846
0.773619360921301
0.7955347039939479
0.8672068064531693
0.8652880343596522
0.3302235467760883
0.6789268064017625
0.6072221471952108
0.9194446824778495
0.4812004589187253
Total groups 73 exceeded the threshold, stopping comparison.
The group tensor is
[5, 2, 7, 1, 6, 4, 3, 0]
tensor([5, 2, 7, 1, 6, 4, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 6, 0, 7, 3, 4, 1]
tensor([5, 2, 6, 0, 7, 3, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 7, 2, 5, 4, 3, 0]
tensor([6, 1, 7, 2, 5, 4, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 2, 4, 1, 5, 0]
tensor([6, 3, 7, 2, 4, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 5, 6, 2, 3, 1, 4, 0]
tensor([7, 5, 6, 2, 3, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 5, 4, 1, 0, 1, 3]
tensor([0, 2, 5, 4, 1, 0, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1.0, 1]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 4
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 5 to 8
done!
Normal merging for layer 9
tensor([0, 5])
tensor(0)
tensor([4, 6])
tensor(4)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 10 to 30
done!
Normal merging for layer 31
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
all done!
Model size: 12.1348 GB
134
cuda:6
fda
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:43<00:43, 43.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 22.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 25.59s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:None: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/hazyresearch/based-fda HTTP/1.1" 200 1043
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/hazyresearch/based-fda/hazyresearch/based-fda.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/hazyresearch/based-fda HTTP/1.1" 200 1043
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/hazyresearch/based-fda/resolve/42569d301e12fbcf8d5a69e04e892aa013e20314/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/hazyresearch/based-fda/revision/42569d301e12fbcf8d5a69e04e892aa013e20314 HTTP/1.1" 200 1043
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/hazyresearch/based-fda/tree/42569d301e12fbcf8d5a69e04e892aa013e20314?recursive=False&expand=False HTTP/1.1" 200 291
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/hazyresearch/based-fda/paths-info/42569d301e12fbcf8d5a69e04e892aa013e20314 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/hazyresearch/based-fda/tree/42569d301e12fbcf8d5a69e04e892aa013e20314/data?recursive=False&expand=False HTTP/1.1" 200 484
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/hazyresearch/based-fda/paths-info/42569d301e12fbcf8d5a69e04e892aa013e20314 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/hazyresearch/based-fda/revision/42569d301e12fbcf8d5a69e04e892aa013e20314 HTTP/1.1" 200 1043
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/hazyresearch/based-fda/resolve/42569d301e12fbcf8d5a69e04e892aa013e20314/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/hazyresearch/based-fda/paths-info/42569d301e12fbcf8d5a69e04e892aa013e20314 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/hazyresearch/based-fda/paths-info/42569d301e12fbcf8d5a69e04e892aa013e20314 HTTP/1.1" 200 218
DEBUG:filelock:Attempting to acquire lock 140325646872512 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_hazyresearch___based-fda_default_0.0.0_42569d301e12fbcf8d5a69e04e892aa013e20314.lock
DEBUG:filelock:Lock 140325646872512 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_hazyresearch___based-fda_default_0.0.0_42569d301e12fbcf8d5a69e04e892aa013e20314.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/hazyresearch___based-fda/default/0.0.0/42569d301e12fbcf8d5a69e04e892aa013e20314/dataset_info.json
DEBUG:filelock:Attempting to release lock 140325646872512 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_hazyresearch___based-fda_default_0.0.0_42569d301e12fbcf8d5a69e04e892aa013e20314.lock
DEBUG:filelock:Lock 140325646872512 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_hazyresearch___based-fda_default_0.0.0_42569d301e12fbcf8d5a69e04e892aa013e20314.lock
DEBUG:filelock:Attempting to acquire lock 140325647194432 on /public/home/zouyifei001/.cache/huggingface/datasets/hazyresearch___based-fda/default/0.0.0/42569d301e12fbcf8d5a69e04e892aa013e20314_builder.lock
DEBUG:filelock:Lock 140325647194432 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/hazyresearch___based-fda/default/0.0.0/42569d301e12fbcf8d5a69e04e892aa013e20314_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/hazyresearch___based-fda/default/0.0.0/42569d301e12fbcf8d5a69e04e892aa013e20314/dataset_info.json
DEBUG:filelock:Attempting to release lock 140325647194432 on /public/home/zouyifei001/.cache/huggingface/datasets/hazyresearch___based-fda/default/0.0.0/42569d301e12fbcf8d5a69e04e892aa013e20314_builder.lock
DEBUG:filelock:Lock 140325647194432 released on /public/home/zouyifei001/.cache/huggingface/datasets/hazyresearch___based-fda/default/0.0.0/42569d301e12fbcf8d5a69e04e892aa013e20314_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
INFO:lm_eval.evaluator:fda: Using gen_kwargs: {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of fda from None to 0
INFO:lm_eval.api.task:Building contexts for fda on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 281875.27it/s]
DEBUG:lm_eval.evaluator:Task: fda; number of requests on this rank: 100
INFO:lm_eval.evaluator:Running generate_until requests
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/100 [00:21<35:12, 21.34s/it]Running generate_until requests:   2%|▏         | 2/100 [00:35<27:56, 17.11s/it]Running generate_until requests:   3%|▎         | 3/100 [00:49<25:11, 15.58s/it]Running generate_until requests:   4%|▍         | 4/100 [01:01<23:04, 14.42s/it]Running generate_until requests:   5%|▌         | 5/100 [01:18<24:05, 15.22s/it]Running generate_until requests:   6%|▌         | 6/100 [01:33<23:50, 15.22s/it]Running generate_until requests:   7%|▋         | 7/100 [01:46<22:21, 14.43s/it]Running generate_until requests:   8%|▊         | 8/100 [02:05<24:21, 15.88s/it]Running generate_until requests:   9%|▉         | 9/100 [02:18<22:56, 15.12s/it]Running generate_until requests:  10%|█         | 10/100 [02:31<21:20, 14.23s/it]Running generate_until requests:  11%|█         | 11/100 [02:43<20:10, 13.60s/it]Running generate_until requests:  12%|█▏        | 12/100 [02:55<19:30, 13.30s/it]Running generate_until requests:  13%|█▎        | 13/100 [03:09<19:14, 13.27s/it]Running generate_until requests:  14%|█▍        | 14/100 [03:28<21:43, 15.16s/it]Running generate_until requests:  15%|█▌        | 15/100 [03:42<20:42, 14.62s/it]Running generate_until requests:  16%|█▌        | 16/100 [03:49<17:25, 12.45s/it]Running generate_until requests:  17%|█▋        | 17/100 [04:02<17:22, 12.56s/it]Running generate_until requests:  18%|█▊        | 18/100 [04:20<19:40, 14.39s/it]Running generate_until requests:  19%|█▉        | 19/100 [04:33<18:40, 13.83s/it]Running generate_until requests:  20%|██        | 20/100 [04:52<20:21, 15.27s/it]Running generate_until requests:  21%|██        | 21/100 [05:05<19:09, 14.55s/it]Running generate_until requests:  22%|██▏       | 22/100 [05:20<19:13, 14.79s/it]Running generate_until requests:  23%|██▎       | 23/100 [05:39<20:33, 16.02s/it]Running generate_until requests:  24%|██▍       | 24/100 [05:52<19:09, 15.13s/it]Running generate_until requests:  25%|██▌       | 25/100 [06:04<17:48, 14.25s/it]Running generate_until requests:  26%|██▌       | 26/100 [06:23<19:10, 15.54s/it]Running generate_until requests:  27%|██▋       | 27/100 [06:41<20:01, 16.45s/it]Running generate_until requests:  28%|██▊       | 28/100 [06:55<18:45, 15.64s/it]Running generate_until requests:  29%|██▉       | 29/100 [07:07<17:23, 14.70s/it]Running generate_until requests:  30%|███       | 30/100 [07:22<17:04, 14.64s/it]Running generate_until requests:  31%|███       | 31/100 [07:41<18:17, 15.91s/it]Running generate_until requests:  32%|███▏      | 32/100 [07:55<17:26, 15.39s/it]Running generate_until requests:  33%|███▎      | 33/100 [08:08<16:19, 14.62s/it]Running generate_until requests:  34%|███▍      | 34/100 [08:26<17:26, 15.85s/it]Running generate_until requests:  35%|███▌      | 35/100 [08:41<16:42, 15.42s/it]Running generate_until requests:  36%|███▌      | 36/100 [08:54<15:50, 14.85s/it]Running generate_until requests:  37%|███▋      | 37/100 [09:01<12:59, 12.37s/it]Running generate_until requests:  38%|███▊      | 38/100 [09:15<13:26, 13.01s/it]Running generate_until requests:  39%|███▉      | 39/100 [09:34<15:01, 14.77s/it]Running generate_until requests:  40%|████      | 40/100 [09:48<14:24, 14.41s/it]Running generate_until requests:  41%|████      | 41/100 [10:00<13:34, 13.81s/it]Running generate_until requests:  42%|████▏     | 42/100 [10:13<13:01, 13.48s/it]Running generate_until requests:  43%|████▎     | 43/100 [10:26<12:36, 13.27s/it]Running generate_until requests:  44%|████▍     | 44/100 [10:39<12:16, 13.16s/it]Running generate_until requests:  45%|████▌     | 45/100 [10:51<11:42, 12.77s/it]Running generate_until requests:  46%|████▌     | 46/100 [11:03<11:22, 12.64s/it]Running generate_until requests:  47%|████▋     | 47/100 [11:21<12:42, 14.39s/it]Running generate_until requests:  48%|████▊     | 48/100 [11:35<12:09, 14.03s/it]Running generate_until requests:  49%|████▉     | 49/100 [11:47<11:27, 13.48s/it]Running generate_until requests:  50%|█████     | 50/100 [11:55<09:51, 11.83s/it]Running generate_until requests:  51%|█████     | 51/100 [12:08<09:55, 12.16s/it]Running generate_until requests:  52%|█████▏    | 52/100 [12:26<11:13, 14.04s/it]Running generate_until requests:  53%|█████▎    | 53/100 [12:39<10:45, 13.74s/it]Running generate_until requests:  54%|█████▍    | 54/100 [12:52<10:20, 13.50s/it]Running generate_until requests:  55%|█████▌    | 55/100 [13:05<09:53, 13.20s/it]Running generate_until requests:  56%|█████▌    | 56/100 [13:17<09:29, 12.94s/it]Running generate_until requests:  57%|█████▋    | 57/100 [13:34<10:05, 14.07s/it]Running generate_until requests:  58%|█████▊    | 58/100 [13:49<10:03, 14.38s/it]Running generate_until requests:  59%|█████▉    | 59/100 [14:01<09:27, 13.83s/it]Running generate_until requests:  60%|██████    | 60/100 [14:20<10:10, 15.26s/it]Running generate_until requests:  61%|██████    | 61/100 [14:38<10:27, 16.10s/it]Running generate_until requests:  62%|██████▏   | 62/100 [14:51<09:39, 15.25s/it]Running generate_until requests:  63%|██████▎   | 63/100 [14:57<07:37, 12.36s/it]Running generate_until requests:  64%|██████▍   | 64/100 [15:09<07:24, 12.36s/it]Running generate_until requests:  65%|██████▌   | 65/100 [15:28<08:19, 14.26s/it]Running generate_until requests:  66%|██████▌   | 66/100 [15:47<08:51, 15.63s/it]Running generate_until requests:  67%|██████▋   | 67/100 [16:05<09:05, 16.54s/it]Running generate_until requests:  68%|██████▊   | 68/100 [16:18<08:11, 15.35s/it]Running generate_until requests:  69%|██████▉   | 69/100 [16:37<08:27, 16.39s/it]Running generate_until requests:  70%|███████   | 70/100 [16:52<07:58, 15.94s/it]Running generate_until requests:  71%|███████   | 71/100 [17:04<07:11, 14.86s/it]Running generate_until requests:  72%|███████▏  | 72/100 [17:23<07:26, 15.95s/it]Running generate_until requests:  73%|███████▎  | 73/100 [17:35<06:43, 14.95s/it]Running generate_until requests:  74%|███████▍  | 74/100 [17:54<06:56, 16.01s/it]Running generate_until requests:  75%|███████▌  | 75/100 [18:07<06:17, 15.10s/it]Running generate_until requests:  76%|███████▌  | 76/100 [18:12<04:55, 12.31s/it]Running generate_until requests:  77%|███████▋  | 77/100 [18:25<04:41, 12.25s/it]Running generate_until requests:  78%|███████▊  | 78/100 [18:37<04:29, 12.26s/it]Running generate_until requests:  79%|███████▉  | 79/100 [18:55<04:56, 14.10s/it]Running generate_until requests:  80%|████████  | 80/100 [19:09<04:42, 14.15s/it]Running generate_until requests:  81%|████████  | 81/100 [19:22<04:19, 13.64s/it]Running generate_until requests:  82%|████████▏ | 82/100 [19:40<04:31, 15.07s/it]Running generate_until requests:  83%|████████▎ | 83/100 [19:55<04:11, 14.82s/it]Running generate_until requests:  84%|████████▍ | 84/100 [20:08<03:48, 14.30s/it]Running generate_until requests:  85%|████████▌ | 85/100 [20:14<02:57, 11.85s/it]Running generate_until requests:  86%|████████▌ | 86/100 [20:30<03:06, 13.30s/it]Running generate_until requests:  87%|████████▋ | 87/100 [20:43<02:50, 13.12s/it]Running generate_until requests:  88%|████████▊ | 88/100 [20:56<02:35, 12.96s/it]Running generate_until requests:  89%|████████▉ | 89/100 [21:11<02:31, 13.77s/it]Running generate_until requests:  90%|█████████ | 90/100 [21:17<01:52, 11.23s/it]Running generate_until requests:  91%|█████████ | 91/100 [21:24<01:29,  9.95s/it]Running generate_until requests:  92%|█████████▏| 92/100 [21:32<01:15,  9.41s/it]Running generate_until requests:  93%|█████████▎| 93/100 [21:43<01:09,  9.98s/it]Running generate_until requests:  94%|█████████▍| 94/100 [21:52<00:57,  9.58s/it]Running generate_until requests:  95%|█████████▌| 95/100 [21:58<00:43,  8.68s/it]Running generate_until requests:  96%|█████████▌| 96/100 [22:09<00:37,  9.36s/it]Running generate_until requests:  97%|█████████▋| 97/100 [22:14<00:23,  7.90s/it]Running generate_until requests:  98%|█████████▊| 98/100 [22:16<00:12,  6.31s/it]Running generate_until requests:  99%|█████████▉| 99/100 [22:25<00:07,  7.02s/it]Running generate_until requests: 100%|██████████| 100/100 [22:30<00:00,  6.25s/it]Running generate_until requests: 100%|██████████| 100/100 [22:30<00:00, 13.50s/it]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:7'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:7'}
full model:
{'fda': {'alias': 'fda', 'contains,none': np.float64(0.87), 'contains_stderr,none': 'N/A'}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.21659225770002863
0.3676799923890141
0.7718276709674655
0.684178265870135
0.5285482451872934
0.273816514287302
0.7566340923804737
0.4195302615712732
0.7048918175177445
0.8024335694665047
0.2596571673087474
0.2906758673048459
0.6559956854150746
0.2288937672529506
0.2941846927212091
0.14545644431597915
0.27237013713290037
0.1954083722558289
0.191889756656264
0.3231769309584375
0.763391177198558
0.7591908569872912
0.47699609290977873
0.24147181909203896
0.6276241833035655
0.5039903698909594
0.506775271104323
0.17806387173399968
0.3609346134453604
0.21659225770002863
0.3676799923890141
0.7718276709674655
0.684178265870135
0.5285482451872934
0.273816514287302
0.7566340923804737
0.4195302615712732
0.7048918175177445
0.8024335694665047
0.2596571673087474
0.2906758673048459
0.6559956854150746
0.2288937672529506
0.2941846927212091
0.14545644431597915
0.27237013713290037
0.1954083722558289
0.191889756656264
0.3231769309584375
0.763391177198558
0.7591908569872912
0.47699609290977873
0.24147181909203896
0.6276241833035655
0.5039903698909594
0.506775271104323
0.17806387173399968
0.3609346134453604
0.21659225770002863
0.3676799923890141
0.7718276709674655
0.684178265870135
0.5285482451872934
0.273816514287302
0.7566340923804737
0.4195302615712732
0.7048918175177445
0.8024335694665047
0.2596571673087474
0.2906758673048459
0.6559956854150746
0.2288937672529506
0.2941846927212091
0.14545644431597915
0.27237013713290037
Total groups 70 exceeded the threshold, stopping comparison.
The group tensor is
[6, 2, 7, 0, 5, 1, 4, 3]
tensor([6, 2, 7, 0, 5, 1, 4, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 0, 1, 5, 2, 7, 6, 3]
tensor([4, 0, 1, 5, 2, 7, 6, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 0, 4, 2, 6, 3, 7, 5]
tensor([1, 0, 4, 2, 6, 3, 7, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 0, 6, 3, 5, 1, 4, 2]
tensor([7, 0, 6, 3, 5, 1, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 0, 2, 5, 1, 1, 4]
tensor([0, 3, 0, 2, 5, 1, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 3, 1, 4, 1, 5, 2, 0]
tensor([0, 3, 1, 4, 1, 5, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
done!
Normal merging for layer 2
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
done!
Normal merging for layer 3
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 4 to 11
done!
Normal merging for layer 12
tensor([0, 2])
tensor(0)
tensor([5, 6])
tensor(5)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
done!
Normal merging for layer 13
tensor([0, 7])
tensor(0)
tensor([2, 4])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
done!
Cross-layer merge completed for layers 14 to 31
done!
all done!
Model size: 11.8828 GB
49
cuda:7
coqa
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:40<00:40, 40.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 24.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 27.26s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/coqa HTTP/1.1" 200 857
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/EleutherAI/coqa/EleutherAI/coqa.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/coqa HTTP/1.1" 200 857
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/coqa/resolve/82e11af842af6c1396f5e9a5c7de260107c50cf1/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/coqa/revision/82e11af842af6c1396f5e9a5c7de260107c50cf1 HTTP/1.1" 200 857
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/coqa/tree/82e11af842af6c1396f5e9a5c7de260107c50cf1?recursive=False&expand=False HTTP/1.1" 200 489
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/coqa/tree/82e11af842af6c1396f5e9a5c7de260107c50cf1/data?recursive=False&expand=False HTTP/1.1" 404 79
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/coqa/tree/82e11af842af6c1396f5e9a5c7de260107c50cf1/data?recursive=False&expand=False HTTP/1.1" 404 79
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/coqa/revision/82e11af842af6c1396f5e9a5c7de260107c50cf1 HTTP/1.1" 200 857
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/coqa/resolve/82e11af842af6c1396f5e9a5c7de260107c50cf1/dataset_infos.json HTTP/1.1" 200 0
DEBUG:filelock:Attempting to acquire lock 140325253250800 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___coqa_default_0.0.0_82e11af842af6c1396f5e9a5c7de260107c50cf1.lock
DEBUG:filelock:Lock 140325253250800 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___coqa_default_0.0.0_82e11af842af6c1396f5e9a5c7de260107c50cf1.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___coqa/default/0.0.0/82e11af842af6c1396f5e9a5c7de260107c50cf1/dataset_info.json
DEBUG:filelock:Attempting to release lock 140325253250800 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___coqa_default_0.0.0_82e11af842af6c1396f5e9a5c7de260107c50cf1.lock
DEBUG:filelock:Lock 140325253250800 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___coqa_default_0.0.0_82e11af842af6c1396f5e9a5c7de260107c50cf1.lock
DEBUG:filelock:Attempting to acquire lock 140325646133024 on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___coqa/default/0.0.0/82e11af842af6c1396f5e9a5c7de260107c50cf1_builder.lock
DEBUG:filelock:Lock 140325646133024 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___coqa/default/0.0.0/82e11af842af6c1396f5e9a5c7de260107c50cf1_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___coqa/default/0.0.0/82e11af842af6c1396f5e9a5c7de260107c50cf1/dataset_info.json
DEBUG:filelock:Attempting to release lock 140325646133024 on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___coqa/default/0.0.0/82e11af842af6c1396f5e9a5c7de260107c50cf1_builder.lock
DEBUG:filelock:Lock 140325646133024 released on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___coqa/default/0.0.0/82e11af842af6c1396f5e9a5c7de260107c50cf1_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
DEBUG:lm_eval.api.task:doc_to_target returned a list. Assuming multiple targets.
INFO:lm_eval.evaluator:coqa: Using gen_kwargs: {'until': ['\nQ:']}
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of coqa from None to 0
INFO:lm_eval.api.task:Building contexts for coqa on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 69928.38it/s]
DEBUG:lm_eval.evaluator:Task: coqa; number of requests on this rank: 100
INFO:lm_eval.evaluator:Running generate_until requests
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/100 [00:06<10:50,  6.57s/it]Running generate_until requests:   2%|▏         | 2/100 [00:12<09:57,  6.10s/it]Running generate_until requests:   3%|▎         | 3/100 [00:19<10:27,  6.47s/it]Running generate_until requests:   4%|▍         | 4/100 [00:24<09:47,  6.12s/it]Running generate_until requests:   5%|▌         | 5/100 [00:30<09:29,  5.99s/it]Running generate_until requests:   6%|▌         | 6/100 [00:35<08:57,  5.72s/it]Running generate_until requests:   7%|▋         | 7/100 [00:42<09:11,  5.93s/it]Running generate_until requests:   8%|▊         | 8/100 [00:47<08:42,  5.67s/it]Running generate_until requests:   9%|▉         | 9/100 [00:52<08:35,  5.67s/it]Running generate_until requests:  10%|█         | 10/100 [00:59<08:47,  5.86s/it]Running generate_until requests:  12%|█▏        | 12/100 [01:05<06:44,  4.59s/it]Running generate_until requests:  13%|█▎        | 13/100 [01:10<06:49,  4.71s/it]Running generate_until requests:  14%|█▍        | 14/100 [01:15<06:48,  4.75s/it]Running generate_until requests:  15%|█▌        | 15/100 [01:20<06:51,  4.85s/it]Running generate_until requests:  16%|█▌        | 16/100 [01:25<06:44,  4.82s/it]Running generate_until requests:  17%|█▋        | 17/100 [01:30<06:42,  4.84s/it]Running generate_until requests:  18%|█▊        | 18/100 [01:34<06:31,  4.77s/it]Running generate_until requests:  19%|█▉        | 19/100 [01:39<06:31,  4.83s/it]Running generate_until requests:  20%|██        | 20/100 [01:44<06:32,  4.90s/it]Running generate_until requests:  21%|██        | 21/100 [01:50<06:34,  4.99s/it]Running generate_until requests:  22%|██▏       | 22/100 [01:55<06:30,  5.00s/it]Running generate_until requests:  23%|██▎       | 23/100 [02:01<06:54,  5.38s/it]Running generate_until requests:  25%|██▌       | 25/100 [02:06<05:00,  4.01s/it]Running generate_until requests:  26%|██▌       | 26/100 [02:11<05:18,  4.30s/it]Running generate_until requests:  27%|██▋       | 27/100 [02:16<05:34,  4.58s/it]Running generate_until requests:  28%|██▊       | 28/100 [02:21<05:43,  4.77s/it]Running generate_until requests:  29%|██▉       | 29/100 [02:27<05:48,  4.90s/it]Running generate_until requests:  30%|███       | 30/100 [02:32<05:48,  4.98s/it]Running generate_until requests:  31%|███       | 31/100 [02:37<05:36,  4.87s/it]Running generate_until requests:  32%|███▏      | 32/100 [02:42<05:40,  5.01s/it]Running generate_until requests:  33%|███▎      | 33/100 [02:47<05:34,  5.00s/it]Running generate_until requests:  34%|███▍      | 34/100 [02:52<05:26,  4.95s/it]Running generate_until requests:  35%|███▌      | 35/100 [02:56<05:16,  4.87s/it]Running generate_until requests:  36%|███▌      | 36/100 [03:03<05:39,  5.31s/it]Running generate_until requests:  38%|███▊      | 38/100 [03:06<03:42,  3.59s/it]Running generate_until requests:  39%|███▉      | 39/100 [03:11<03:58,  3.91s/it]Running generate_until requests:  40%|████      | 40/100 [03:15<04:04,  4.07s/it]Running generate_until requests:  41%|████      | 41/100 [03:20<04:11,  4.27s/it]Running generate_until requests:  42%|████▏     | 42/100 [03:26<04:37,  4.79s/it]Running generate_until requests:  43%|████▎     | 43/100 [03:31<04:30,  4.74s/it]Running generate_until requests:  44%|████▍     | 44/100 [03:36<04:27,  4.77s/it]Running generate_until requests:  45%|████▌     | 45/100 [03:40<04:20,  4.73s/it]Running generate_until requests:  46%|████▌     | 46/100 [03:44<04:07,  4.58s/it]Running generate_until requests:  47%|████▋     | 47/100 [03:49<03:59,  4.51s/it]Running generate_until requests:  48%|████▊     | 48/100 [03:54<03:59,  4.60s/it]Running generate_until requests:  49%|████▉     | 49/100 [03:59<04:06,  4.84s/it]Running generate_until requests:  50%|█████     | 50/100 [04:03<03:53,  4.67s/it]Running generate_until requests:  52%|█████▏    | 52/100 [04:06<02:35,  3.24s/it]Running generate_until requests:  53%|█████▎    | 53/100 [04:11<02:50,  3.63s/it]Running generate_until requests:  54%|█████▍    | 54/100 [04:15<02:52,  3.76s/it]Running generate_until requests:  55%|█████▌    | 55/100 [04:20<02:58,  3.97s/it]Running generate_until requests:  56%|█████▌    | 56/100 [04:25<03:06,  4.24s/it]Running generate_until requests:  57%|█████▋    | 57/100 [04:29<03:07,  4.36s/it]Running generate_until requests:  58%|█████▊    | 58/100 [04:34<02:59,  4.28s/it]Running generate_until requests:  59%|█████▉    | 59/100 [04:39<03:09,  4.63s/it]Running generate_until requests:  60%|██████    | 60/100 [04:43<02:58,  4.46s/it]Running generate_until requests:  61%|██████    | 61/100 [04:49<03:05,  4.76s/it]Running generate_until requests:  62%|██████▏   | 62/100 [04:54<03:04,  4.86s/it]Running generate_until requests:  63%|██████▎   | 63/100 [04:58<02:49,  4.59s/it]Running generate_until requests:  64%|██████▍   | 64/100 [05:02<02:38,  4.40s/it]Running generate_until requests:  65%|██████▌   | 65/100 [05:05<02:28,  4.25s/it]Running generate_until requests:  67%|██████▋   | 67/100 [05:07<01:30,  2.75s/it]Running generate_until requests:  68%|██████▊   | 68/100 [05:12<01:42,  3.20s/it]Running generate_until requests:  69%|██████▉   | 69/100 [05:18<02:02,  3.95s/it]Running generate_until requests:  70%|███████   | 70/100 [05:23<02:02,  4.07s/it]Running generate_until requests:  71%|███████   | 71/100 [05:32<02:38,  5.48s/it]Running generate_until requests:  72%|███████▏  | 72/100 [05:35<02:20,  5.01s/it]Running generate_until requests:  73%|███████▎  | 73/100 [05:39<02:06,  4.69s/it]Running generate_until requests:  74%|███████▍  | 74/100 [05:44<01:58,  4.57s/it]Running generate_until requests:  75%|███████▌  | 75/100 [05:47<01:48,  4.35s/it]Running generate_until requests:  76%|███████▌  | 76/100 [05:51<01:40,  4.19s/it]Running generate_until requests:  77%|███████▋  | 77/100 [05:56<01:40,  4.37s/it]Running generate_until requests:  78%|███████▊  | 78/100 [06:00<01:33,  4.24s/it]Running generate_until requests:  79%|███████▉  | 79/100 [06:04<01:24,  4.04s/it]Running generate_until requests:  81%|████████  | 81/100 [06:08<00:59,  3.15s/it]Running generate_until requests:  82%|████████▏ | 82/100 [06:11<00:58,  3.26s/it]Running generate_until requests:  83%|████████▎ | 83/100 [06:15<00:58,  3.44s/it]Running generate_until requests:  84%|████████▍ | 84/100 [06:19<00:55,  3.46s/it]Running generate_until requests:  85%|████████▌ | 85/100 [06:22<00:51,  3.45s/it]Running generate_until requests:  86%|████████▌ | 86/100 [06:26<00:49,  3.52s/it]Running generate_until requests:  87%|████████▋ | 87/100 [06:31<00:50,  3.86s/it]Running generate_until requests:  88%|████████▊ | 88/100 [06:34<00:46,  3.84s/it]Running generate_until requests:  89%|████████▉ | 89/100 [06:38<00:40,  3.67s/it]Running generate_until requests:  90%|█████████ | 90/100 [06:41<00:35,  3.51s/it]Running generate_until requests:  91%|█████████ | 91/100 [06:44<00:30,  3.35s/it]Running generate_until requests:  92%|█████████▏| 92/100 [06:47<00:26,  3.26s/it]Running generate_until requests:  93%|█████████▎| 93/100 [06:50<00:22,  3.23s/it]Running generate_until requests:  94%|█████████▍| 94/100 [06:53<00:19,  3.24s/it]Running generate_until requests:  95%|█████████▌| 95/100 [06:57<00:16,  3.32s/it]Running generate_until requests:  96%|█████████▌| 96/100 [07:00<00:13,  3.33s/it]Running generate_until requests:  97%|█████████▋| 97/100 [07:03<00:09,  3.15s/it]Running generate_until requests:  98%|█████████▊| 98/100 [07:06<00:06,  3.02s/it]Running generate_until requests: 100%|██████████| 100/100 [07:05<00:00,  4.26s/it]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:0'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}
full model:
{'coqa': {'alias': 'coqa', 'em,none': 0.595, 'em_stderr,none': 0.044774970461162564, 'f1,none': 0.7211574141733987, 'f1_stderr,none': 0.037128235455690536}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.6057853926542468
0.4195568875297668
0.5244744113321889
0.5202703028806769
0.6150870974034927
0.5299634576457063
0.9336763524510373
0.23649940737178063
0.388911200696845
0.6478041116722705
0.5517233675449297
0.6723258763091353
0.7175526480521238
0.8411089149883405
0.7404554224148189
0.26376935916880817
0.9373006475493478
0.5360566853939598
0.38729358133282565
0.4541602442018795
0.8623573205888978
0.7318340566806717
0.6643209906079897
0.8122565195147101
0.4707270481319977
0.9785001455445378
0.17075087907531752
0.489625917805058
0.7595051272431785
0.6057853926542468
0.4195568875297668
0.5244744113321889
0.5202703028806769
0.6150870974034927
0.5299634576457063
0.9336763524510373
0.23649940737178063
0.388911200696845
0.6478041116722705
0.5517233675449297
0.6723258763091353
0.7175526480521238
0.8411089149883405
0.7404554224148189
0.26376935916880817
0.9373006475493478
0.5360566853939598
0.38729358133282565
0.4541602442018795
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[5, 4, 3, 2, 0, 1, 7, 6]
tensor([5, 4, 3, 2, 0, 1, 7, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 4, 0, 2, 1, 7, 6]
tensor([5, 3, 4, 0, 2, 1, 7, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 1, 7, 0, 6, 2, 3, 4]
tensor([5, 1, 7, 0, 6, 2, 3, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 3, 6, 0, 5, 2, 4, 1]
tensor([7, 3, 6, 0, 5, 2, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 5, 7, 2, 4, 0, 3, 1]
tensor([6, 5, 7, 2, 4, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 5, 0, 0, 1, 2, 1, 3]
tensor([4, 5, 0, 0, 1, 2, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1, 2, 2, 0, 3, 3]
tensor([0, 1, 1, 2, 2, 0, 3, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([7])
tensor(7)
tensor([6])
tensor(6)
done!
Normal merging for layer 2
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
done!
Normal merging for layer 4
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 5 to 8
done!
Normal merging for layer 9
tensor([2, 3])
tensor(2)
tensor([4, 6])
tensor(4)
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
done!
Cross-layer merge completed for layers 10 to 22
done!
Normal merging for layer 23
tensor([0, 5])
tensor(0)
tensor([1, 2])
tensor(1)
tensor([3, 4])
tensor(3)
tensor([6, 7])
tensor(6)
done!
Cross-layer merge completed for layers 24 to 31
done!
all done!
Model size: 12.3238 GB
86
cuda:0
wnli
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:39<00:39, 39.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 23.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 26.26s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/wnli?recursive=False&expand=False HTTP/1.1" 307 141
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/wnli?recursive=False&expand=False HTTP/1.1" 200 352
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140325647362256 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140325647362256 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140325647362256 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140325647362256 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140321636683984 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321636683984 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321636683984 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321636683984 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2419.41it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:01<03:49,  1.63s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:02<01:43,  1.35it/s]Running loglikelihood requests:   4%|▎         | 5/142 [00:03<01:18,  1.74it/s]Running loglikelihood requests:   5%|▍         | 7/142 [00:04<01:08,  1.97it/s]Running loglikelihood requests:   6%|▋         | 9/142 [00:04<01:02,  2.13it/s]Running loglikelihood requests:   8%|▊         | 11/142 [00:05<00:57,  2.26it/s]Running loglikelihood requests:   9%|▉         | 13/142 [00:06<00:54,  2.35it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:07<00:51,  2.45it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:08<00:49,  2.53it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:08<00:46,  2.63it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:09<00:44,  2.70it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:10<00:42,  2.77it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:10<00:41,  2.83it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:11<00:39,  2.88it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:12<00:38,  2.92it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:12<00:38,  2.91it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:13<00:37,  2.94it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:14<00:36,  2.96it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:14<00:35,  2.98it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:15<00:34,  3.00it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:16<00:33,  3.01it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:16<00:32,  3.03it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:16<00:05, 13.73it/s]Running loglikelihood requests:  46%|████▋     | 66/142 [00:17<00:06, 10.90it/s]Running loglikelihood requests:  48%|████▊     | 68/142 [00:18<00:08,  8.51it/s]Running loglikelihood requests:  49%|████▉     | 70/142 [00:18<00:10,  6.92it/s]Running loglikelihood requests:  51%|█████     | 72/142 [00:19<00:12,  5.83it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:19<00:15,  4.58it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:20<00:15,  4.22it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [00:21<00:16,  3.97it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [00:21<00:16,  3.80it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [00:22<00:16,  3.69it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [00:22<00:16,  3.61it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [00:23<00:16,  3.55it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [00:24<00:15,  3.52it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [00:24<00:15,  3.50it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [00:25<00:14,  3.48it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [00:25<00:14,  3.48it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [00:26<00:13,  3.48it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [00:26<00:12,  3.48it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [00:27<00:12,  3.48it/s]Running loglikelihood requests:  71%|███████   | 101/142 [00:28<00:11,  3.49it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [00:28<00:11,  3.50it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [00:29<00:10,  3.51it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [00:29<00:09,  3.52it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [00:30<00:09,  3.53it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [00:30<00:08,  3.54it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [00:31<00:08,  3.55it/s]Running loglikelihood requests:  81%|████████  | 115/142 [00:32<00:07,  3.56it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [00:32<00:07,  3.56it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [00:33<00:06,  3.57it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [00:33<00:05,  3.58it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [00:34<00:05,  3.60it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [00:34<00:04,  3.61it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [00:35<00:04,  3.62it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [00:35<00:03,  3.63it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [00:36<00:03,  3.64it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [00:37<00:02,  3.66it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [00:37<00:01,  3.68it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [00:38<00:01,  3.65it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [00:38<00:00,  3.69it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [00:39<00:00,  3.74it/s]Running loglikelihood requests: 100%|██████████| 142/142 [00:39<00:00,  3.63it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:1'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:1'}
full model:
{'wnli': {'alias': 'wnli', 'acc,none': 0.5352112676056338, 'acc_stderr,none': 0.0596130578497224}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.31190044950254875
0.7306280553786256
0.6985925052465822
0.5843516732063875
0.8124235844589114
0.820438203921003
0.6253838264581936
0.7600037006947045
0.8225872978677496
0.708398461303215
0.8646089269479391
0.8239362351853257
0.7608099850331435
0.6657423857513638
0.7943257460202938
0.7511476003698512
0.9073696655228775
0.8741838353767599
0.7945799099309127
0.9323691001541556
0.865243808509542
0.8176606226311932
0.6785099625983169
0.9579534328203848
0.788928884938056
0.9833718962298513
0.5933012307657521
0.7829988799240639
0.7823073743206628
0.31190044950254875
0.7306280553786256
0.6985925052465822
0.5843516732063875
0.8124235844589114
0.820438203921003
0.6253838264581936
0.7600037006947045
0.8225872978677496
0.708398461303215
0.8646089269479391
0.8239362351853257
0.7608099850331435
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[7, 4, 5, 1, 6, 3, 2, 0]
tensor([7, 4, 5, 1, 6, 3, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 2, 4, 5, 6, 1, 0, 3]
tensor([7, 2, 4, 5, 6, 1, 0, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 5, 4, 1, 2, 0]
tensor([6, 3, 7, 5, 4, 1, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 6, 4, 7, 1, 2, 0]
tensor([5, 3, 6, 4, 7, 1, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 3, 6, 5, 7, 2, 1, 0]
tensor([4, 3, 6, 5, 7, 2, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1, 2, 3, 2, 3, 0]
tensor([0, 1, 1, 2, 3, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 1, 2, 2, 3, 0, 3, 1]
tensor([0, 1, 2, 2, 3, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 1, 1, 1.0, 1.0, 1.0, 1.0]
tensor([0, 0, 1, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 4
tensor([7])
tensor(7)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 5 to 15
done!
Normal merging for layer 16
tensor([0, 7])
tensor(0)
tensor([1, 2])
tensor(1)
tensor([3, 5])
tensor(3)
tensor([4, 6])
tensor(4)
done!
Normal merging for layer 17
tensor([0, 5])
tensor(0)
tensor([1, 7])
tensor(1)
tensor([2, 3])
tensor(2)
tensor([4, 6])
tensor(4)
done!
Cross-layer merge completed for layers 18 to 30
done!
Normal merging for layer 31
tensor([0, 1])
tensor(0)
tensor([2, 3, 4, 5, 6, 7])
tensor(2)
done!
all done!
Model size: 12.2608 GB
141
cuda:1
winogrande
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:37<00:37, 37.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 23.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 25.92s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/winogrande HTTP/1.1" 307 67
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/winogrande HTTP/1.1" 200 1036
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/winogrande/winogrande.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): datasets-server.hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://datasets-server.hf-mirror.com:443 "GET /parquet?dataset=winogrande HTTP/1.1" 302 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET / HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/winogrande/winogrande.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/winogrande/resolve/main/winogrande.py HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/winogrande/resolve/main/winogrande.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/winogrande/resolve/main/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/winogrande/resolve/main/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/winogrande/resolve/main/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/winogrande/resolve/main/README.md HTTP/1.1" 200 0
DEBUG:filelock:Attempting to acquire lock 140325645859584 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_winogrande_winogrande_xl_1.1.0_a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2.lock
DEBUG:filelock:Lock 140325645859584 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_winogrande_winogrande_xl_1.1.0_a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2/dataset_info.json
DEBUG:filelock:Attempting to release lock 140325645859584 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_winogrande_winogrande_xl_1.1.0_a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2.lock
DEBUG:filelock:Lock 140325645859584 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_winogrande_winogrande_xl_1.1.0_a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2.lock
DEBUG:filelock:Attempting to acquire lock 140326448100848 on /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2_builder.lock
DEBUG:filelock:Lock 140326448100848 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2/dataset_info.json
DEBUG:filelock:Attempting to release lock 140326448100848 on /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2_builder.lock
DEBUG:filelock:Lock 140326448100848 released on /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
DEBUG:lm_eval.api.task:doc_to_text returned an int. Assuming multiple inputs.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of winogrande from None to 0
INFO:lm_eval.api.task:Building contexts for winogrande on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 127215.77it/s]
DEBUG:lm_eval.evaluator:Task: winogrande; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<03:48,  1.15s/it]Running loglikelihood requests:   1%|          | 2/200 [00:01<02:52,  1.15it/s]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:25,  1.35it/s]Running loglikelihood requests:   2%|▏         | 4/200 [00:02<02:10,  1.50it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:03<02:02,  1.60it/s]Running loglikelihood requests:   3%|▎         | 6/200 [00:04<01:56,  1.67it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:04<01:52,  1.72it/s]Running loglikelihood requests:   4%|▍         | 8/200 [00:05<01:49,  1.75it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:05<01:47,  1.78it/s]Running loglikelihood requests:   5%|▌         | 10/200 [00:06<01:45,  1.80it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:06<01:44,  1.82it/s]Running loglikelihood requests:   6%|▌         | 12/200 [00:07<01:42,  1.83it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:07<01:41,  1.84it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:08<00:18,  9.23it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:09<00:30,  5.58it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:10<00:42,  4.04it/s]Running loglikelihood requests:  15%|█▌        | 30/200 [00:10<00:47,  3.56it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:11<00:53,  3.15it/s]Running loglikelihood requests:  16%|█▌        | 32/200 [00:11<00:59,  2.83it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:12<01:04,  2.58it/s]Running loglikelihood requests:  17%|█▋        | 34/200 [00:12<01:09,  2.39it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:13<01:13,  2.25it/s]Running loglikelihood requests:  18%|█▊        | 36/200 [00:13<01:16,  2.15it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:14<01:20,  2.03it/s]Running loglikelihood requests:  19%|█▉        | 38/200 [00:15<01:21,  2.00it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:15<01:21,  1.98it/s]Running loglikelihood requests:  20%|██        | 40/200 [00:16<01:21,  1.96it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:16<01:21,  1.95it/s]Running loglikelihood requests:  21%|██        | 42/200 [00:17<01:21,  1.94it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:17<01:20,  1.94it/s]Running loglikelihood requests:  22%|██▏       | 44/200 [00:18<01:20,  1.93it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:18<01:20,  1.93it/s]Running loglikelihood requests:  23%|██▎       | 46/200 [00:19<01:19,  1.93it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:19<01:19,  1.92it/s]Running loglikelihood requests:  24%|██▍       | 48/200 [00:20<01:19,  1.92it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:20<01:18,  1.92it/s]Running loglikelihood requests:  25%|██▌       | 50/200 [00:21<01:18,  1.92it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:21<01:17,  1.92it/s]Running loglikelihood requests:  26%|██▌       | 52/200 [00:22<01:17,  1.92it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:22<01:16,  1.92it/s]Running loglikelihood requests:  27%|██▋       | 54/200 [00:23<01:16,  1.92it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:23<01:15,  1.92it/s]Running loglikelihood requests:  28%|██▊       | 56/200 [00:24<01:14,  1.93it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:24<01:14,  1.93it/s]Running loglikelihood requests:  29%|██▉       | 58/200 [00:25<01:13,  1.93it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:25<01:12,  1.93it/s]Running loglikelihood requests:  30%|███       | 60/200 [00:26<01:12,  1.94it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:27<01:11,  1.94it/s]Running loglikelihood requests:  31%|███       | 62/200 [00:27<01:11,  1.94it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:28<01:10,  1.94it/s]Running loglikelihood requests:  32%|███▏      | 64/200 [00:28<01:10,  1.94it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:29<01:09,  1.94it/s]Running loglikelihood requests:  33%|███▎      | 66/200 [00:29<01:09,  1.94it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:30<01:08,  1.94it/s]Running loglikelihood requests:  34%|███▍      | 68/200 [00:30<01:08,  1.94it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:31<01:07,  1.94it/s]Running loglikelihood requests:  35%|███▌      | 70/200 [00:31<01:07,  1.94it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:32<01:06,  1.93it/s]Running loglikelihood requests:  36%|███▌      | 72/200 [00:32<01:06,  1.93it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:33<01:05,  1.93it/s]Running loglikelihood requests:  37%|███▋      | 74/200 [00:33<01:04,  1.94it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:34<01:04,  1.94it/s]Running loglikelihood requests:  38%|███▊      | 76/200 [00:34<01:03,  1.94it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:35<01:03,  1.95it/s]Running loglikelihood requests:  39%|███▉      | 78/200 [00:35<01:02,  1.94it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:36<01:02,  1.95it/s]Running loglikelihood requests:  40%|████      | 80/200 [00:36<01:01,  1.95it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:37<01:00,  1.95it/s]Running loglikelihood requests:  41%|████      | 82/200 [00:37<01:00,  1.95it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:38<00:59,  1.95it/s]Running loglikelihood requests:  42%|████▏     | 84/200 [00:38<00:59,  1.96it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:39<00:58,  1.96it/s]Running loglikelihood requests:  43%|████▎     | 86/200 [00:39<00:57,  1.97it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:40<00:57,  1.97it/s]Running loglikelihood requests:  44%|████▍     | 88/200 [00:40<00:56,  1.97it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:41<00:56,  1.98it/s]Running loglikelihood requests:  45%|████▌     | 90/200 [00:41<00:55,  1.97it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:42<00:55,  1.97it/s]Running loglikelihood requests:  46%|████▌     | 92/200 [00:42<00:54,  1.97it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:43<00:54,  1.97it/s]Running loglikelihood requests:  47%|████▋     | 94/200 [00:43<00:53,  1.97it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:44<00:53,  1.97it/s]Running loglikelihood requests:  48%|████▊     | 96/200 [00:44<00:52,  1.97it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:45<00:52,  1.97it/s]Running loglikelihood requests:  49%|████▉     | 98/200 [00:45<00:51,  1.97it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:46<00:51,  1.97it/s]Running loglikelihood requests:  50%|█████     | 100/200 [00:46<00:50,  1.97it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:47<00:50,  1.98it/s]Running loglikelihood requests:  51%|█████     | 102/200 [00:47<00:49,  1.98it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:48<00:48,  1.99it/s]Running loglikelihood requests:  52%|█████▏    | 104/200 [00:48<00:48,  1.99it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:49<00:47,  1.99it/s]Running loglikelihood requests:  53%|█████▎    | 106/200 [00:49<00:47,  2.00it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:50<00:46,  2.00it/s]Running loglikelihood requests:  54%|█████▍    | 108/200 [00:50<00:45,  2.00it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:51<00:45,  2.00it/s]Running loglikelihood requests:  55%|█████▌    | 110/200 [00:51<00:45,  2.00it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:52<00:44,  2.00it/s]Running loglikelihood requests:  56%|█████▌    | 112/200 [00:52<00:44,  2.00it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:53<00:43,  2.00it/s]Running loglikelihood requests:  57%|█████▋    | 114/200 [00:53<00:42,  2.00it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:54<00:42,  2.01it/s]Running loglikelihood requests:  58%|█████▊    | 116/200 [00:54<00:41,  2.01it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:55<00:41,  2.01it/s]Running loglikelihood requests:  59%|█████▉    | 118/200 [00:55<00:40,  2.01it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:56<00:40,  2.01it/s]Running loglikelihood requests:  60%|██████    | 120/200 [00:56<00:39,  2.01it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:57<00:39,  2.01it/s]Running loglikelihood requests:  61%|██████    | 122/200 [00:57<00:38,  2.01it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:58<00:38,  2.01it/s]Running loglikelihood requests:  62%|██████▏   | 124/200 [00:58<00:37,  2.01it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:59<00:37,  2.01it/s]Running loglikelihood requests:  63%|██████▎   | 126/200 [00:59<00:36,  2.01it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [01:00<00:36,  2.01it/s]Running loglikelihood requests:  64%|██████▍   | 128/200 [01:00<00:35,  2.01it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [01:01<00:35,  2.01it/s]Running loglikelihood requests:  65%|██████▌   | 130/200 [01:01<00:34,  2.01it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [01:02<00:34,  2.01it/s]Running loglikelihood requests:  66%|██████▌   | 132/200 [01:02<00:33,  2.02it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [01:03<00:33,  2.03it/s]Running loglikelihood requests:  67%|██████▋   | 134/200 [01:03<00:32,  2.03it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [01:04<00:32,  2.03it/s]Running loglikelihood requests:  68%|██████▊   | 136/200 [01:04<00:31,  2.02it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [01:05<00:31,  2.02it/s]Running loglikelihood requests:  69%|██████▉   | 138/200 [01:05<00:30,  2.03it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [01:06<00:30,  2.03it/s]Running loglikelihood requests:  70%|███████   | 140/200 [01:06<00:29,  2.03it/s]Running loglikelihood requests:  70%|███████   | 141/200 [01:07<00:29,  2.03it/s]Running loglikelihood requests:  71%|███████   | 142/200 [01:07<00:28,  2.03it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [01:08<00:28,  2.03it/s]Running loglikelihood requests:  72%|███████▏  | 144/200 [01:08<00:27,  2.04it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:08<00:03, 11.34it/s]Running loglikelihood requests:  80%|████████  | 160/200 [01:10<00:07,  5.70it/s]Running loglikelihood requests:  81%|████████  | 162/200 [01:11<00:08,  4.37it/s]Running loglikelihood requests:  82%|████████▏ | 164/200 [01:12<00:10,  3.58it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:12<00:10,  3.29it/s]Running loglikelihood requests:  83%|████████▎ | 166/200 [01:13<00:11,  3.04it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:13<00:11,  2.81it/s]Running loglikelihood requests:  84%|████████▍ | 168/200 [01:14<00:12,  2.62it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:14<00:12,  2.47it/s]Running loglikelihood requests:  85%|████████▌ | 170/200 [01:15<00:12,  2.36it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:15<00:12,  2.29it/s]Running loglikelihood requests:  86%|████████▌ | 172/200 [01:16<00:12,  2.23it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:16<00:12,  2.19it/s]Running loglikelihood requests:  87%|████████▋ | 174/200 [01:17<00:12,  2.17it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:17<00:11,  2.14it/s]Running loglikelihood requests:  88%|████████▊ | 176/200 [01:18<00:11,  2.13it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:18<00:10,  2.12it/s]Running loglikelihood requests:  89%|████████▉ | 178/200 [01:19<00:10,  2.11it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:19<00:09,  2.11it/s]Running loglikelihood requests:  90%|█████████ | 180/200 [01:20<00:09,  2.12it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:20<00:08,  2.12it/s]Running loglikelihood requests:  91%|█████████ | 182/200 [01:20<00:08,  2.13it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:21<00:07,  2.13it/s]Running loglikelihood requests:  92%|█████████▏| 184/200 [01:21<00:07,  2.08it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:22<00:07,  2.08it/s]Running loglikelihood requests:  93%|█████████▎| 186/200 [01:22<00:06,  2.09it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:23<00:06,  2.10it/s]Running loglikelihood requests:  94%|█████████▍| 188/200 [01:23<00:05,  2.11it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:24<00:05,  2.12it/s]Running loglikelihood requests:  95%|█████████▌| 190/200 [01:24<00:04,  2.13it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:25<00:04,  2.15it/s]Running loglikelihood requests:  96%|█████████▌| 192/200 [01:25<00:03,  2.16it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:26<00:03,  2.16it/s]Running loglikelihood requests:  97%|█████████▋| 194/200 [01:26<00:02,  2.17it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:27<00:02,  2.17it/s]Running loglikelihood requests:  98%|█████████▊| 196/200 [01:27<00:01,  2.17it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:27<00:01,  2.17it/s]Running loglikelihood requests:  99%|█████████▉| 198/200 [01:28<00:00,  2.17it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:28<00:00,  2.17it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:29<00:00,  2.18it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:29<00:00,  2.24it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:2'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:2'}
full model:
{'winogrande': {'alias': 'winogrande', 'acc,none': 0.69, 'acc_stderr,none': 0.046482319871173176}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9565474555896152
0.9256436871263655
0.919777800952949
0.8502516827829548
0.9788217808920475
0.7842184683361061
0.5847175538207384
0.7741211354638314
0.8734705119073022
0.9731317123565076
0.9046875380381674
0.8858535931048256
0.9314298098713261
0.8442403312485581
0.7166029053748131
0.7061450250233585
0.7563059483803782
0.6877563559653158
0.8738989590810852
0.784285727893607
0.8409178900131131
0.8425380927376759
0.6782655591765769
0.7228968829640015
0.8418686487797186
0.9141168065903919
0.8562481461255738
0.6139553840231665
0.934088538459943
Total groups 66 exceeded the threshold, stopping comparison.
The group tensor is
[6, 2, 4, 3, 5, 1, 7, 0]
tensor([6, 2, 4, 3, 5, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 3, 1, 5, 4, 7, 0]
tensor([6, 2, 3, 1, 5, 4, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 4, 1, 0, 3, 0, 1, 2]
tensor([5, 4, 1, 0, 3, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 3, 1, 4, 5, 1, 0]
tensor([0, 2, 3, 1, 4, 5, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[5, 1, 2, 3, 4, 0, 1, 0]
tensor([5, 1, 2, 3, 4, 0, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 2, 1, 1, 2, 3, 3, 0]
tensor([0, 2, 1, 1, 2, 3, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1, 1.0, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 0, 1.0, 1.0, 1.0, 1]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 2 to 7
done!
Normal merging for layer 8
tensor([3, 5])
tensor(3)
tensor([2, 6])
tensor(2)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 9 to 13
done!
Normal merging for layer 14
tensor([0, 7])
tensor(0)
tensor([3, 6])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
done!
Normal merging for layer 15
tensor([5, 7])
tensor(5)
tensor([1, 6])
tensor(1)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 16
tensor([0, 7])
tensor(0)
tensor([2, 3])
tensor(2)
tensor([1, 4])
tensor(1)
tensor([5, 6])
tensor(5)
done!
Cross-layer merge completed for layers 17 to 23
done!
Normal merging for layer 24
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
Normal merging for layer 25
tensor([0, 3])
tensor(0)
tensor([1, 2, 4, 5, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 26 to 31
done!
all done!
Model size: 11.8828 GB
178
cuda:2
cb
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:39<00:39, 39.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 24.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.69s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: cb] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: cb] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
WARNING:lm_eval.api.task:[Task: cb] metric f1 is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/revision/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/revision/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646?recursive=False&expand=False HTTP/1.1" 307 138
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646?recursive=False&expand=False HTTP/1.1" 200 501
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/axb?recursive=False&expand=False HTTP/1.1" 307 142
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/axb?recursive=False&expand=False HTTP/1.1" 200 232
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/revision/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/revision/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/cb?recursive=False&expand=False HTTP/1.1" 307 141
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/cb?recursive=False&expand=False HTTP/1.1" 200 347
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:filelock:Attempting to acquire lock 140321495715664 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_cb_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140321495715664 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_cb_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/cb/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321495715664 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_cb_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140321495715664 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_cb_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140325646618544 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/cb/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140325646618544 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/cb/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/cb/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140325646618544 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/cb/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140325646618544 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/cb/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of cb from None to 0
INFO:lm_eval.api.task:Building contexts for cb on rank 0...
  0%|          | 0/56 [00:00<?, ?it/s]100%|██████████| 56/56 [00:00<00:00, 1317.53it/s]
DEBUG:lm_eval.evaluator:Task: cb; number of requests on this rank: 168
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/168 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/168 [00:02<06:40,  2.40s/it]Running loglikelihood requests:   1%|          | 2/168 [00:04<05:49,  2.10s/it]Running loglikelihood requests:   2%|▏         | 4/168 [00:06<03:41,  1.35s/it]Running loglikelihood requests:   3%|▎         | 5/168 [00:07<04:02,  1.49s/it]Running loglikelihood requests:   4%|▍         | 7/168 [00:09<03:07,  1.16s/it]Running loglikelihood requests:   5%|▍         | 8/168 [00:11<03:23,  1.27s/it]Running loglikelihood requests:   6%|▌         | 10/168 [00:12<02:40,  1.02s/it]Running loglikelihood requests:   7%|▋         | 11/168 [00:13<02:51,  1.09s/it]Running loglikelihood requests:   8%|▊         | 13/168 [00:15<02:21,  1.10it/s]Running loglikelihood requests:   8%|▊         | 14/168 [00:16<02:32,  1.01it/s]Running loglikelihood requests:  10%|▉         | 16/168 [00:17<02:10,  1.17it/s]Running loglikelihood requests:  10%|█         | 17/168 [00:18<02:22,  1.06it/s]Running loglikelihood requests:  11%|█▏        | 19/168 [00:20<02:01,  1.23it/s]Running loglikelihood requests:  12%|█▏        | 20/168 [00:21<02:12,  1.12it/s]Running loglikelihood requests:  13%|█▎        | 22/168 [00:22<01:52,  1.30it/s]Running loglikelihood requests:  14%|█▎        | 23/168 [00:23<02:02,  1.18it/s]Running loglikelihood requests:  15%|█▍        | 25/168 [00:24<01:45,  1.36it/s]Running loglikelihood requests:  15%|█▌        | 26/168 [00:25<01:55,  1.23it/s]Running loglikelihood requests:  17%|█▋        | 28/168 [00:26<01:39,  1.41it/s]Running loglikelihood requests:  17%|█▋        | 29/168 [00:27<01:49,  1.27it/s]Running loglikelihood requests:  18%|█▊        | 31/168 [00:28<01:35,  1.44it/s]Running loglikelihood requests:  19%|█▉        | 32/168 [00:30<01:45,  1.29it/s]Running loglikelihood requests:  20%|█▉        | 33/168 [00:31<01:53,  1.18it/s]Running loglikelihood requests:  21%|██        | 35/168 [00:32<01:35,  1.39it/s]Running loglikelihood requests:  22%|██▏       | 37/168 [00:33<01:25,  1.53it/s]Running loglikelihood requests:  23%|██▎       | 38/168 [00:34<01:35,  1.36it/s]Running loglikelihood requests:  24%|██▍       | 40/168 [00:35<01:23,  1.53it/s]Running loglikelihood requests:  24%|██▍       | 41/168 [00:36<01:33,  1.36it/s]Running loglikelihood requests:  26%|██▌       | 43/168 [00:37<01:21,  1.54it/s]Running loglikelihood requests:  26%|██▌       | 44/168 [00:38<01:30,  1.37it/s]Running loglikelihood requests:  27%|██▋       | 46/168 [00:39<01:18,  1.56it/s]Running loglikelihood requests:  28%|██▊       | 47/168 [00:40<01:25,  1.41it/s]Running loglikelihood requests:  29%|██▉       | 49/168 [00:41<01:14,  1.61it/s]Running loglikelihood requests:  30%|██▉       | 50/168 [00:42<01:21,  1.45it/s]Running loglikelihood requests:  30%|███       | 51/168 [00:43<01:27,  1.33it/s]Running loglikelihood requests:  31%|███       | 52/168 [00:44<01:32,  1.25it/s]Running loglikelihood requests:  38%|███▊      | 64/168 [00:44<00:18,  5.50it/s]Running loglikelihood requests:  39%|███▊      | 65/168 [00:45<00:25,  4.04it/s]Running loglikelihood requests:  40%|███▉      | 67/168 [00:46<00:29,  3.45it/s]Running loglikelihood requests:  40%|████      | 68/168 [00:47<00:36,  2.71it/s]Running loglikelihood requests:  42%|████▏     | 70/168 [00:48<00:38,  2.55it/s]Running loglikelihood requests:  42%|████▏     | 71/168 [00:49<00:46,  2.11it/s]Running loglikelihood requests:  43%|████▎     | 73/168 [00:50<00:44,  2.14it/s]Running loglikelihood requests:  44%|████▍     | 74/168 [00:51<00:51,  1.82it/s]Running loglikelihood requests:  45%|████▍     | 75/168 [00:51<00:57,  1.61it/s]Running loglikelihood requests:  45%|████▌     | 76/168 [00:52<01:03,  1.46it/s]Running loglikelihood requests:  46%|████▋     | 78/168 [00:53<00:53,  1.68it/s]Running loglikelihood requests:  48%|████▊     | 80/168 [00:54<00:47,  1.85it/s]Running loglikelihood requests:  49%|████▉     | 82/168 [00:55<00:43,  1.96it/s]Running loglikelihood requests:  49%|████▉     | 83/168 [00:56<00:49,  1.71it/s]Running loglikelihood requests:  51%|█████     | 85/168 [00:57<00:44,  1.88it/s]Running loglikelihood requests:  51%|█████     | 86/168 [00:58<00:49,  1.66it/s]Running loglikelihood requests:  52%|█████▏    | 88/168 [00:59<00:43,  1.85it/s]Running loglikelihood requests:  53%|█████▎    | 89/168 [00:59<00:47,  1.65it/s]Running loglikelihood requests:  54%|█████▎    | 90/168 [01:00<00:51,  1.50it/s]Running loglikelihood requests:  55%|█████▍    | 92/168 [01:01<00:43,  1.75it/s]Running loglikelihood requests:  56%|█████▌    | 94/168 [01:02<00:38,  1.93it/s]Running loglikelihood requests:  57%|█████▋    | 95/168 [01:03<00:42,  1.71it/s]Running loglikelihood requests:  58%|█████▊    | 97/168 [01:04<00:37,  1.91it/s]Running loglikelihood requests:  58%|█████▊    | 98/168 [01:04<00:41,  1.69it/s]Running loglikelihood requests:  60%|█████▉    | 100/168 [01:05<00:35,  1.91it/s]Running loglikelihood requests:  60%|██████    | 101/168 [01:06<00:39,  1.70it/s]Running loglikelihood requests:  61%|██████▏   | 103/168 [01:07<00:33,  1.92it/s]Running loglikelihood requests:  62%|██████▏   | 104/168 [01:08<00:37,  1.72it/s]Running loglikelihood requests:  62%|██████▎   | 105/168 [01:09<00:40,  1.57it/s]Running loglikelihood requests:  63%|██████▎   | 106/168 [01:09<00:42,  1.47it/s]Running loglikelihood requests:  64%|██████▍   | 108/168 [01:10<00:33,  1.77it/s]Running loglikelihood requests:  65%|██████▌   | 110/168 [01:11<00:29,  1.98it/s]Running loglikelihood requests:  66%|██████▌   | 111/168 [01:12<00:32,  1.76it/s]Running loglikelihood requests:  67%|██████▋   | 113/168 [01:13<00:27,  1.98it/s]Running loglikelihood requests:  68%|██████▊   | 114/168 [01:13<00:30,  1.76it/s]Running loglikelihood requests:  69%|██████▉   | 116/168 [01:14<00:26,  1.98it/s]Running loglikelihood requests:  70%|███████   | 118/168 [01:15<00:23,  2.14it/s]Running loglikelihood requests:  71%|███████   | 119/168 [01:16<00:26,  1.87it/s]Running loglikelihood requests:  71%|███████▏  | 120/168 [01:17<00:28,  1.69it/s]Running loglikelihood requests:  73%|███████▎  | 122/168 [01:17<00:23,  1.94it/s]Running loglikelihood requests:  74%|███████▍  | 124/168 [01:18<00:20,  2.11it/s]Running loglikelihood requests:  74%|███████▍  | 125/168 [01:19<00:23,  1.86it/s]Running loglikelihood requests:  76%|███████▌  | 127/168 [01:20<00:19,  2.06it/s]Running loglikelihood requests:  76%|███████▌  | 128/168 [01:21<00:21,  1.83it/s]Running loglikelihood requests:  77%|███████▋  | 129/168 [01:21<00:23,  1.67it/s]Running loglikelihood requests:  78%|███████▊  | 131/168 [01:22<00:19,  1.94it/s]Running loglikelihood requests:  79%|███████▉  | 133/168 [01:23<00:16,  2.16it/s]Running loglikelihood requests:  80%|███████▉  | 134/168 [01:24<00:17,  1.94it/s]Running loglikelihood requests:  81%|████████  | 136/168 [01:24<00:14,  2.18it/s]Running loglikelihood requests:  82%|████████▏ | 137/168 [01:25<00:15,  1.94it/s]Running loglikelihood requests:  82%|████████▏ | 138/168 [01:26<00:16,  1.78it/s]Running loglikelihood requests:  83%|████████▎ | 139/168 [01:26<00:17,  1.66it/s]Running loglikelihood requests:  83%|████████▎ | 140/168 [01:27<00:17,  1.58it/s]Running loglikelihood requests:  85%|████████▍ | 142/168 [01:28<00:13,  1.94it/s]Running loglikelihood requests:  86%|████████▌ | 144/168 [01:29<00:10,  2.19it/s]Running loglikelihood requests:  87%|████████▋ | 146/168 [01:29<00:09,  2.37it/s]Running loglikelihood requests:  88%|████████▊ | 148/168 [01:30<00:07,  2.51it/s]Running loglikelihood requests:  89%|████████▊ | 149/168 [01:31<00:08,  2.19it/s]Running loglikelihood requests:  89%|████████▉ | 150/168 [01:31<00:09,  1.97it/s]Running loglikelihood requests:  90%|█████████ | 152/168 [01:32<00:07,  2.24it/s]Running loglikelihood requests:  92%|█████████▏| 154/168 [01:33<00:05,  2.43it/s]Running loglikelihood requests:  92%|█████████▏| 155/168 [01:34<00:06,  2.14it/s]Running loglikelihood requests:  93%|█████████▎| 157/168 [01:34<00:04,  2.37it/s]Running loglikelihood requests:  94%|█████████▍| 158/168 [01:35<00:04,  2.11it/s]Running loglikelihood requests:  95%|█████████▌| 160/168 [01:36<00:03,  2.41it/s]Running loglikelihood requests:  96%|█████████▌| 161/168 [01:36<00:03,  2.17it/s]Running loglikelihood requests:  97%|█████████▋| 163/168 [01:37<00:02,  2.48it/s]Running loglikelihood requests:  98%|█████████▊| 164/168 [01:37<00:01,  2.24it/s]Running loglikelihood requests:  99%|█████████▉| 166/168 [01:38<00:00,  2.55it/s]Running loglikelihood requests:  99%|█████████▉| 167/168 [01:39<00:00,  2.30it/s]Running loglikelihood requests: 100%|██████████| 168/168 [01:39<00:00,  1.70it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:3'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:3'}
full model:
{'cb': {'alias': 'cb', 'acc,none': 0.44642857142857145, 'acc_stderr,none': 0.06703189227942397, 'f1,none': np.float64(0.2946127946127946), 'f1_stderr,none': 'N/A'}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8754479021250823
0.8035282713865031
0.9500865550679058
0.9469110267801961
0.9212663689239236
0.9365178765256775
0.671589453090406
0.6744336218334667
0.8076345549626031
0.7976470386087894
0.807072393829993
0.6600400889592184
0.7484904863798857
0.9363063771008698
0.6357711114598822
0.9166089836828051
0.6715835426958833
0.7409884813902188
0.412428535140908
0.8747283305135303
0.8491219158212996
0.9125101690232402
0.8366676259845086
0.705936129020536
0.7920385356495101
0.9243830461584077
0.9274604399644588
0.7835428130351293
0.8119111717866335
Total groups 67 exceeded the threshold, stopping comparison.
The group tensor is
[6, 3, 5, 2, 7, 1, 4, 0]
tensor([6, 3, 5, 2, 7, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 7, 3, 6, 1, 2, 0]
tensor([5, 4, 7, 3, 6, 1, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 1, 6, 3, 7, 2, 4, 0]
tensor([5, 1, 6, 3, 7, 2, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 2, 0, 1, 5, 0, 1, 3]
tensor([4, 2, 0, 1, 5, 0, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[4, 3, 0, 2, 1, 1, 5, 0]
tensor([4, 3, 0, 2, 1, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 2, 2, 1, 3, 0]
tensor([0, 3, 1, 2, 2, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1.0, 1]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 2 to 6
done!
Normal merging for layer 7
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 8 to 12
done!
Normal merging for layer 13
tensor([2, 5])
tensor(2)
tensor([3, 6])
tensor(3)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Normal merging for layer 14
tensor([2, 7])
tensor(2)
tensor([4, 5])
tensor(4)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 15 to 18
done!
Normal merging for layer 19
tensor([0, 7])
tensor(0)
tensor([2, 5])
tensor(2)
tensor([3, 4])
tensor(3)
tensor([1, 6])
tensor(1)
done!
Cross-layer merge completed for layers 20 to 23
done!
Normal merging for layer 24
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 25 to 31
done!
all done!
Model size: 11.9458 GB
233
cuda:3
qnli
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:38<00:38, 38.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 24.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 26.22s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: qnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: qnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140321890200000 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321890200000 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321890200000 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321890200000 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140325646869968 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140325646869968 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140325646869968 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140325646869968 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of qnli from None to 0
INFO:lm_eval.api.task:Building contexts for qnli on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2371.06it/s]
DEBUG:lm_eval.evaluator:Task: qnli; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<05:32,  1.67s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:51,  1.15it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:03<02:16,  1.43it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:04<02:01,  1.59it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:06<01:52,  1.70it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:07<01:45,  1.80it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:07<01:39,  1.89it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:08<01:34,  1.96it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:09<01:29,  2.04it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:10<01:25,  2.12it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:11<01:22,  2.17it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:12<01:19,  2.22it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:13<01:17,  2.25it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:14<01:16,  2.28it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:14<01:14,  2.29it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:15<01:13,  2.31it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:16<01:11,  2.33it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:17<00:19,  7.73it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:18<00:24,  6.16it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:18<00:29,  5.06it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:19<00:33,  4.30it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:20<00:37,  3.77it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:21<00:41,  3.39it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:22<00:44,  3.13it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:22<00:46,  2.95it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:23<00:47,  2.83it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:24<00:48,  2.77it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:25<00:48,  2.73it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:25<00:47,  2.70it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:26<00:47,  2.69it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:27<00:46,  2.68it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:28<00:45,  2.69it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:28<00:44,  2.70it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:29<00:44,  2.70it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:30<00:43,  2.71it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:31<00:42,  2.72it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:31<00:41,  2.73it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:32<00:40,  2.74it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:33<00:39,  2.74it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:34<00:38,  2.75it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:34<00:38,  2.76it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:35<00:37,  2.78it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:36<00:36,  2.79it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:36<00:35,  2.79it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:37<00:34,  2.80it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:38<00:33,  2.80it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:39<00:33,  2.81it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:39<00:32,  2.81it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:40<00:31,  2.81it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:41<00:30,  2.82it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:41<00:30,  2.83it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:42<00:29,  2.84it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:43<00:28,  2.85it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:43<00:27,  2.87it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:44<00:26,  2.89it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:45<00:25,  2.91it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:45<00:24,  2.93it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:46<00:24,  2.94it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:47<00:23,  2.96it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:47<00:22,  2.97it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:48<00:21,  2.98it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:49<00:21,  2.99it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:49<00:20,  3.00it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:50<00:19,  3.01it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:51<00:18,  3.03it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [00:51<00:18,  3.04it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [00:52<00:17,  3.05it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [00:53<00:16,  3.06it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [00:53<00:16,  3.06it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [00:54<00:15,  3.06it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [00:55<00:14,  3.07it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [00:55<00:13,  3.08it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [00:56<00:13,  3.09it/s]Running loglikelihood requests:  80%|████████  | 161/200 [00:57<00:12,  3.09it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [00:57<00:11,  3.10it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [00:58<00:11,  3.12it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [00:59<00:10,  3.13it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [00:59<00:09,  3.14it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:00<00:09,  3.18it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:00<00:08,  3.20it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:01<00:07,  3.21it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:02<00:07,  3.22it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:02<00:06,  3.24it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:03<00:05,  3.25it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:03<00:05,  3.25it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:04<00:04,  3.28it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:05<00:03,  3.32it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:05<00:03,  3.35it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:06<00:02,  3.37it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:06<00:02,  3.40it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:07<00:01,  3.43it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:08<00:00,  3.46it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:08<00:00,  3.49it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:08<00:00,  2.92it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:4'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:4'}
full model:
{'qnli': {'alias': 'qnli', 'acc,none': 0.46, 'acc_stderr,none': 0.05009082659620332}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9846426504484882
0.5824664507839472
0.519615692575773
0.501155542927289
0.7945367166753022
0.308422053695068
0.8640142063914085
0.9138139850456612
0.4633856064809356
0.6590614315196575
0.6979080802987999
0.7199917897264415
0.7749024201981475
0.6702564595367945
0.48800861061165646
0.4420522818267443
0.9519161971847797
0.7484302940574024
0.5133259423607681
0.5588784666822361
0.6658633104669237
0.7569182042181317
0.5308683195195316
0.8457203515884096
0.2160432078364904
0.9025653665852331
0.9196489243365332
0.6742217686289
0.8668007010904994
0.9846426504484882
0.5824664507839472
0.519615692575773
0.501155542927289
0.7945367166753022
0.308422053695068
0.8640142063914085
0.9138139850456612
0.4633856064809356
0.6590614315196575
0.6979080802987999
0.7199917897264415
0.7749024201981475
0.6702564595367945
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[7, 5, 4, 2, 6, 1, 3, 0]
tensor([7, 5, 4, 2, 6, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 5, 1, 3, 6, 2, 7, 0]
tensor([4, 5, 1, 3, 6, 2, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 3, 4, 1, 2, 5, 0]
tensor([0, 1, 3, 4, 1, 2, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 1, 5, 2, 4, 1, 3, 0]
tensor([0, 1, 5, 2, 4, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 4, 1, 2, 1, 3, 5, 0]
tensor([0, 4, 1, 2, 1, 3, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[2, 4, 3, 0, 5, 1, 1, 0]
tensor([2, 4, 3, 0, 5, 1, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 3, 4, 2, 1, 1, 5, 0]
tensor([0, 3, 4, 2, 1, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 1, 3, 2, 0, 3, 1]
tensor([0, 2, 1, 3, 2, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 2 to 8
done!
Normal merging for layer 9
tensor([0, 7])
tensor(0)
tensor([1, 4])
tensor(1)
tensor([5])
tensor(5)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
done!
Normal merging for layer 10
tensor([0, 7])
tensor(0)
tensor([1, 5])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
done!
Normal merging for layer 11
tensor([0, 7])
tensor(0)
tensor([2, 4])
tensor(2)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
done!
Normal merging for layer 12
tensor([3, 7])
tensor(3)
tensor([5, 6])
tensor(5)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
done!
Normal merging for layer 13
tensor([0, 7])
tensor(0)
tensor([4, 5])
tensor(4)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 14 to 15
done!
Normal merging for layer 16
tensor([0, 5])
tensor(0)
tensor([2, 7])
tensor(2)
tensor([1, 4])
tensor(1)
tensor([3, 6])
tensor(3)
done!
Cross-layer merge completed for layers 17 to 31
done!
all done!
Model size: 12.3238 GB
31
cuda:4
mrpc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:39<00:39, 39.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 24.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.74s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: mrpc] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: mrpc] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
WARNING:lm_eval.api.task:[Task: mrpc] metric f1 is defined, but aggregation is not. using default aggregation=f1
WARNING:lm_eval.api.task:[Task: mrpc] metric f1 is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140321884476512 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321884476512 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321884476512 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321884476512 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140328373703728 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140328373703728 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140328373703728 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140328373703728 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of mrpc from None to 0
INFO:lm_eval.api.task:Building contexts for mrpc on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2247.08it/s]
DEBUG:lm_eval.evaluator:Task: mrpc; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<05:02,  1.52s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:31,  1.30it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:03<02:03,  1.58it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:04<01:50,  1.74it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:05<01:44,  1.84it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:06<01:39,  1.90it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:07<01:35,  1.95it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:08<01:33,  1.98it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:09<01:31,  2.01it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:10<01:29,  2.03it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:11<01:27,  2.04it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:12<01:26,  2.06it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:13<01:24,  2.07it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:14<01:23,  2.08it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:15<01:21,  2.09it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:16<01:20,  2.10it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:17<01:18,  2.11it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:17<01:17,  2.13it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:18<01:16,  2.13it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:19<01:15,  2.14it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:20<01:13,  2.15it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:21<01:12,  2.16it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:22<01:11,  2.16it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:23<01:10,  2.17it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:24<01:09,  2.18it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:25<01:08,  2.18it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:26<01:07,  2.19it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:27<01:05,  2.20it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:28<01:04,  2.21it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:28<00:18,  6.75it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:29<00:22,  5.46it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:30<00:27,  4.54it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:31<00:31,  3.89it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:32<00:34,  3.43it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:33<00:37,  3.10it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:34<00:40,  2.87it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:35<00:41,  2.71it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:35<00:42,  2.60it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:36<00:43,  2.53it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:37<00:43,  2.48it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:38<00:42,  2.45it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:39<00:42,  2.43it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:40<00:41,  2.42it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:40<00:40,  2.42it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:41<00:40,  2.41it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:42<00:39,  2.41it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:43<00:38,  2.41it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:44<00:37,  2.41it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:45<00:36,  2.41it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:45<00:36,  2.41it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:46<00:35,  2.41it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:47<00:34,  2.41it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:48<00:33,  2.42it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:49<00:32,  2.42it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:50<00:31,  2.42it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:50<00:30,  2.43it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:51<00:30,  2.43it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:52<00:29,  2.43it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:53<00:28,  2.44it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:54<00:27,  2.45it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:54<00:26,  2.45it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:55<00:25,  2.45it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:56<00:24,  2.46it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:57<00:23,  2.47it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:58<00:23,  2.48it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [00:58<00:22,  2.49it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [00:59<00:21,  2.50it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:00<00:20,  2.51it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:01<00:19,  2.51it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:02<00:18,  2.51it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:02<00:17,  2.52it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:03<00:17,  2.53it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:04<00:16,  2.54it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:05<00:15,  2.54it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:06<00:14,  2.55it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:06<00:13,  2.57it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:07<00:12,  2.59it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:08<00:11,  2.60it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:09<00:11,  2.62it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:09<00:10,  2.63it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:10<00:09,  2.64it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:11<00:08,  2.66it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:12<00:07,  2.69it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:12<00:07,  2.71it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:13<00:06,  2.73it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:14<00:05,  2.75it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:14<00:04,  2.77it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:15<00:03,  2.79it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:16<00:03,  2.80it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:17<00:02,  2.82it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:17<00:01,  2.83it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:18<00:01,  2.85it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:19<00:00,  2.90it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:19<00:00,  2.53it/s]
bootstrapping for stddev (sequential): f1_score
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:01<02:11,  1.32s/it]  2%|▏         | 2/100 [00:02<02:09,  1.32s/it]  3%|▎         | 3/100 [00:03<02:08,  1.32s/it]  4%|▍         | 4/100 [00:05<02:06,  1.32s/it]  5%|▌         | 5/100 [00:06<02:05,  1.32s/it]  6%|▌         | 6/100 [00:07<02:04,  1.32s/it]  7%|▋         | 7/100 [00:09<02:02,  1.32s/it] 12%|█▏        | 12/100 [00:09<00:43,  2.05it/s] 13%|█▎        | 13/100 [00:11<00:54,  1.60it/s] 14%|█▍        | 14/100 [00:12<01:05,  1.32it/s] 15%|█▌        | 15/100 [00:13<01:14,  1.14it/s] 16%|█▌        | 16/100 [00:15<01:22,  1.02it/s] 17%|█▋        | 17/100 [00:16<01:28,  1.06s/it] 18%|█▊        | 18/100 [00:17<01:32,  1.13s/it] 19%|█▉        | 19/100 [00:19<01:35,  1.18s/it] 20%|██        | 20/100 [00:20<01:37,  1.22s/it] 21%|██        | 21/100 [00:21<01:38,  1.24s/it] 22%|██▏       | 22/100 [00:23<01:38,  1.26s/it] 23%|██▎       | 23/100 [00:24<01:38,  1.28s/it] 24%|██▍       | 24/100 [00:25<01:37,  1.29s/it] 25%|██▌       | 25/100 [00:26<01:37,  1.30s/it] 26%|██▌       | 26/100 [00:28<01:36,  1.30s/it] 27%|██▋       | 27/100 [00:29<01:35,  1.31s/it] 28%|██▊       | 28/100 [00:30<01:34,  1.31s/it] 29%|██▉       | 29/100 [00:32<01:33,  1.31s/it] 30%|███       | 30/100 [00:33<01:32,  1.31s/it] 31%|███       | 31/100 [00:34<01:30,  1.32s/it] 32%|███▏      | 32/100 [00:36<01:29,  1.32s/it] 33%|███▎      | 33/100 [00:37<01:28,  1.32s/it] 34%|███▍      | 34/100 [00:38<01:26,  1.32s/it] 35%|███▌      | 35/100 [00:40<01:25,  1.32s/it] 36%|███▌      | 36/100 [00:41<01:24,  1.32s/it] 37%|███▋      | 37/100 [00:42<01:23,  1.32s/it] 38%|███▊      | 38/100 [00:44<01:21,  1.32s/it] 39%|███▉      | 39/100 [00:45<01:20,  1.32s/it] 40%|████      | 40/100 [00:46<01:19,  1.32s/it] 41%|████      | 41/100 [00:48<01:17,  1.32s/it] 42%|████▏     | 42/100 [00:49<01:16,  1.32s/it] 43%|████▎     | 43/100 [00:50<01:15,  1.32s/it] 44%|████▍     | 44/100 [00:52<01:13,  1.32s/it] 45%|████▌     | 45/100 [00:53<01:12,  1.32s/it] 46%|████▌     | 46/100 [00:54<01:11,  1.32s/it] 47%|████▋     | 47/100 [00:55<01:09,  1.32s/it] 48%|████▊     | 48/100 [00:57<01:08,  1.32s/it] 49%|████▉     | 49/100 [00:58<01:07,  1.32s/it] 50%|█████     | 50/100 [00:59<01:05,  1.32s/it] 51%|█████     | 51/100 [01:01<01:04,  1.32s/it] 52%|█████▏    | 52/100 [01:02<01:03,  1.32s/it] 53%|█████▎    | 53/100 [01:03<01:01,  1.32s/it] 54%|█████▍    | 54/100 [01:05<01:00,  1.32s/it] 55%|█████▌    | 55/100 [01:06<00:59,  1.32s/it] 56%|█████▌    | 56/100 [01:07<00:57,  1.32s/it] 57%|█████▋    | 57/100 [01:09<00:56,  1.32s/it] 58%|█████▊    | 58/100 [01:10<00:55,  1.32s/it] 63%|██████▎   | 63/100 [01:10<00:18,  2.05it/s] 64%|██████▍   | 64/100 [01:12<00:22,  1.61it/s] 65%|██████▌   | 65/100 [01:13<00:26,  1.33it/s] 66%|██████▌   | 66/100 [01:14<00:29,  1.14it/s] 67%|██████▋   | 67/100 [01:16<00:32,  1.02it/s] 68%|██████▊   | 68/100 [01:17<00:33,  1.06s/it] 69%|██████▉   | 69/100 [01:18<00:35,  1.13s/it] 70%|███████   | 70/100 [01:20<00:35,  1.18s/it] 71%|███████   | 71/100 [01:21<00:35,  1.22s/it] 72%|███████▏  | 72/100 [01:22<00:34,  1.25s/it] 73%|███████▎  | 73/100 [01:24<00:34,  1.27s/it] 74%|███████▍  | 74/100 [01:25<00:33,  1.28s/it] 75%|███████▌  | 75/100 [01:26<00:32,  1.29s/it] 76%|███████▌  | 76/100 [01:28<00:31,  1.30s/it] 77%|███████▋  | 77/100 [01:29<00:30,  1.31s/it] 78%|███████▊  | 78/100 [01:30<00:28,  1.31s/it] 79%|███████▉  | 79/100 [01:32<00:27,  1.31s/it] 80%|████████  | 80/100 [01:33<00:26,  1.31s/it] 81%|████████  | 81/100 [01:34<00:24,  1.31s/it] 82%|████████▏ | 82/100 [01:36<00:23,  1.32s/it] 83%|████████▎ | 83/100 [01:37<00:22,  1.32s/it] 84%|████████▍ | 84/100 [01:38<00:21,  1.32s/it] 85%|████████▌ | 85/100 [01:39<00:19,  1.32s/it] 86%|████████▌ | 86/100 [01:41<00:18,  1.32s/it] 87%|████████▋ | 87/100 [01:42<00:17,  1.32s/it] 88%|████████▊ | 88/100 [01:43<00:15,  1.32s/it] 89%|████████▉ | 89/100 [01:45<00:14,  1.32s/it] 90%|█████████ | 90/100 [01:46<00:13,  1.32s/it] 91%|█████████ | 91/100 [01:47<00:11,  1.32s/it] 92%|█████████▏| 92/100 [01:49<00:10,  1.32s/it] 93%|█████████▎| 93/100 [01:50<00:09,  1.32s/it] 94%|█████████▍| 94/100 [01:51<00:07,  1.32s/it] 95%|█████████▌| 95/100 [01:53<00:06,  1.32s/it] 96%|█████████▌| 96/100 [01:54<00:05,  1.31s/it] 97%|█████████▋| 97/100 [01:55<00:03,  1.32s/it] 98%|█████████▊| 98/100 [01:57<00:02,  1.32s/it] 99%|█████████▉| 99/100 [01:58<00:01,  1.32s/it]100%|██████████| 100/100 [01:59<00:00,  1.32s/it]100%|██████████| 100/100 [01:59<00:00,  1.20s/it]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:5'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:5'}
full model:
{'mrpc': {'alias': 'mrpc', 'acc,none': 0.7, 'acc_stderr,none': 0.04605661864718383, 'f1,none': np.float64(0.8214285714285714), 'f1_stderr,none': 0.0323693653386572}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.32375514242269293
0.9664141198776434
0.4263233992087649
0.29258192731768157
0.3497718342079178
0.5968480671674146
0.7870107004084098
0.7068508532201193
0.7836897379939763
0.2579819071737284
0.5774144903645764
0.7819348699339372
0.7404680063618143
0.651835611327285
0.6832243011482585
0.34680079188801705
0.6033476505145444
0.6299526106979766
0.6468468631829007
0.8458578364778718
0.39447753640727073
0.5291234791393744
0.944540808938638
0.8460909093145242
0.4915397062049404
0.5314721523201049
0.8670806018560002
0.6451914961665037
0.7380077491787214
0.32375514242269293
0.9664141198776434
0.4263233992087649
0.29258192731768157
0.3497718342079178
0.5968480671674146
0.7870107004084098
0.7068508532201193
0.7836897379939763
0.2579819071737284
0.5774144903645764
0.7819348699339372
0.7404680063618143
0.651835611327285
0.6832243011482585
0.34680079188801705
0.6033476505145444
0.6299526106979766
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[4, 5, 7, 1, 6, 2, 3, 0]
tensor([4, 5, 7, 1, 6, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 4, 5, 0, 1, 2]
tensor([6, 3, 7, 4, 5, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 1, 7, 2, 6, 0, 3, 5]
tensor([4, 1, 7, 2, 6, 0, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 5, 2, 7, 0, 4, 1]
tensor([6, 3, 5, 2, 7, 0, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 5, 6, 7, 3, 1, 0, 2]
tensor([4, 5, 6, 7, 3, 1, 0, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 2, 4, 1, 0, 5]
tensor([0, 3, 1, 2, 4, 1, 0, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1, 1.0]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 2
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 4 to 5
done!
Normal merging for layer 6
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
done!
Cross-layer merge completed for layers 7 to 9
done!
Normal merging for layer 10
tensor([0, 6])
tensor(0)
tensor([2, 5])
tensor(2)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([7])
tensor(7)
done!
Cross-layer merge completed for layers 11 to 26
done!
Normal merging for layer 27
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 28 to 30
done!
Normal merging for layer 31
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
all done!
Model size: 12.3867 GB
138
cuda:5
copa
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:40<00:40, 40.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 24.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 27.14s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: copa] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: copa] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/copa?recursive=False&expand=False HTTP/1.1" 307 143
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/copa?recursive=False&expand=False HTTP/1.1" 200 348
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140325646866128 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_copa_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140325646866128 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_copa_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140325646866128 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_copa_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140325646866128 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_copa_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140321079885264 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140321079885264 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321079885264 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140321079885264 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
DEBUG:lm_eval.api.task:Both target_delimiter " " and target choice: " the toilet filled with water." have whitespace
DEBUG:lm_eval.api.task:Both target_delimiter " " and target choice: " water flowed from the spout." have whitespace
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of copa from None to 0
INFO:lm_eval.api.task:Building contexts for copa on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 122604.62it/s]
DEBUG:lm_eval.evaluator:Task: copa; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<03:30,  1.06s/it]Running loglikelihood requests:   1%|          | 2/200 [00:01<02:28,  1.33it/s]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:06,  1.55it/s]Running loglikelihood requests:   2%|▏         | 4/200 [00:02<01:56,  1.69it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:03<01:49,  1.78it/s]Running loglikelihood requests:   3%|▎         | 6/200 [00:03<01:46,  1.83it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:04<01:43,  1.87it/s]Running loglikelihood requests:   4%|▍         | 8/200 [00:04<01:40,  1.90it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:05<01:39,  1.92it/s]Running loglikelihood requests:   5%|▌         | 10/200 [00:05<01:38,  1.94it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:06<01:36,  1.95it/s]Running loglikelihood requests:   6%|▌         | 12/200 [00:06<01:35,  1.97it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:07<01:34,  1.97it/s]Running loglikelihood requests:   7%|▋         | 14/200 [00:07<01:33,  1.98it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:08<01:32,  1.99it/s]Running loglikelihood requests:   8%|▊         | 16/200 [00:08<01:32,  2.00it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:09<01:31,  2.00it/s]Running loglikelihood requests:   9%|▉         | 18/200 [00:09<01:30,  2.01it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:10<01:29,  2.01it/s]Running loglikelihood requests:  10%|█         | 20/200 [00:10<01:29,  2.01it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:11<01:28,  2.01it/s]Running loglikelihood requests:  11%|█         | 22/200 [00:11<01:28,  2.02it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:12<01:27,  2.02it/s]Running loglikelihood requests:  12%|█▏        | 24/200 [00:12<01:26,  2.02it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:13<01:26,  2.03it/s]Running loglikelihood requests:  13%|█▎        | 26/200 [00:13<01:25,  2.03it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:14<01:25,  2.02it/s]Running loglikelihood requests:  14%|█▍        | 28/200 [00:14<01:24,  2.02it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:15<01:24,  2.03it/s]Running loglikelihood requests:  15%|█▌        | 30/200 [00:15<01:23,  2.04it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:16<01:22,  2.04it/s]Running loglikelihood requests:  16%|█▌        | 32/200 [00:16<01:22,  2.05it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:17<01:21,  2.05it/s]Running loglikelihood requests:  17%|█▋        | 34/200 [00:17<01:20,  2.05it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:18<01:20,  2.05it/s]Running loglikelihood requests:  18%|█▊        | 36/200 [00:18<01:19,  2.05it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:18<01:19,  2.05it/s]Running loglikelihood requests:  19%|█▉        | 38/200 [00:19<01:19,  2.05it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:19<01:18,  2.05it/s]Running loglikelihood requests:  20%|██        | 40/200 [00:20<01:17,  2.05it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:20<01:17,  2.05it/s]Running loglikelihood requests:  21%|██        | 42/200 [00:21<01:17,  2.05it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:21<01:16,  2.05it/s]Running loglikelihood requests:  22%|██▏       | 44/200 [00:22<01:15,  2.05it/s]Running loglikelihood requests:  29%|██▉       | 58/200 [00:22<00:12, 10.97it/s]Running loglikelihood requests:  30%|███       | 60/200 [00:23<00:20,  6.70it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:24<00:25,  5.52it/s]Running loglikelihood requests:  31%|███       | 62/200 [00:24<00:29,  4.60it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:25<00:35,  3.91it/s]Running loglikelihood requests:  32%|███▏      | 64/200 [00:25<00:40,  3.39it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:26<00:44,  3.01it/s]Running loglikelihood requests:  33%|███▎      | 66/200 [00:26<00:48,  2.74it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:27<00:52,  2.55it/s]Running loglikelihood requests:  34%|███▍      | 68/200 [00:27<00:54,  2.42it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:28<00:56,  2.32it/s]Running loglikelihood requests:  35%|███▌      | 70/200 [00:28<00:57,  2.24it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:29<00:58,  2.19it/s]Running loglikelihood requests:  36%|███▌      | 72/200 [00:29<00:59,  2.16it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:29<00:59,  2.15it/s]Running loglikelihood requests:  37%|███▋      | 74/200 [00:30<00:59,  2.13it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:30<00:58,  2.12it/s]Running loglikelihood requests:  38%|███▊      | 76/200 [00:31<00:58,  2.12it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:31<00:58,  2.12it/s]Running loglikelihood requests:  39%|███▉      | 78/200 [00:32<00:57,  2.12it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:32<00:57,  2.12it/s]Running loglikelihood requests:  40%|████      | 80/200 [00:33<00:56,  2.11it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:33<00:56,  2.11it/s]Running loglikelihood requests:  41%|████      | 82/200 [00:34<00:56,  2.11it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:34<00:55,  2.11it/s]Running loglikelihood requests:  42%|████▏     | 84/200 [00:35<00:54,  2.11it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:35<00:54,  2.11it/s]Running loglikelihood requests:  43%|████▎     | 86/200 [00:36<00:54,  2.11it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:36<00:53,  2.11it/s]Running loglikelihood requests:  44%|████▍     | 88/200 [00:37<00:53,  2.11it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:37<00:52,  2.11it/s]Running loglikelihood requests:  45%|████▌     | 90/200 [00:38<00:52,  2.11it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:38<00:51,  2.11it/s]Running loglikelihood requests:  46%|████▌     | 92/200 [00:38<00:51,  2.10it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:39<00:50,  2.10it/s]Running loglikelihood requests:  47%|████▋     | 94/200 [00:39<00:50,  2.10it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:40<00:49,  2.10it/s]Running loglikelihood requests:  48%|████▊     | 96/200 [00:40<00:49,  2.10it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:41<00:48,  2.10it/s]Running loglikelihood requests:  49%|████▉     | 98/200 [00:41<00:48,  2.11it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:42<00:47,  2.12it/s]Running loglikelihood requests:  50%|█████     | 100/200 [00:42<00:47,  2.12it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:43<00:46,  2.12it/s]Running loglikelihood requests:  51%|█████     | 102/200 [00:43<00:46,  2.12it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:44<00:45,  2.13it/s]Running loglikelihood requests:  52%|█████▏    | 104/200 [00:44<00:44,  2.13it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:45<00:44,  2.14it/s]Running loglikelihood requests:  53%|█████▎    | 106/200 [00:45<00:44,  2.13it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:46<00:43,  2.14it/s]Running loglikelihood requests:  54%|█████▍    | 108/200 [00:46<00:43,  2.13it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:46<00:42,  2.13it/s]Running loglikelihood requests:  55%|█████▌    | 110/200 [00:47<00:42,  2.13it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:47<00:41,  2.13it/s]Running loglikelihood requests:  56%|█████▌    | 112/200 [00:48<00:41,  2.13it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:48<00:40,  2.13it/s]Running loglikelihood requests:  57%|█████▋    | 114/200 [00:49<00:41,  2.06it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:49<00:40,  2.09it/s]Running loglikelihood requests:  58%|█████▊    | 116/200 [00:50<00:39,  2.12it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:50<00:38,  2.13it/s]Running loglikelihood requests:  59%|█████▉    | 118/200 [00:51<00:38,  2.14it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:51<00:37,  2.15it/s]Running loglikelihood requests:  60%|██████    | 120/200 [00:52<00:37,  2.14it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:52<00:36,  2.15it/s]Running loglikelihood requests:  61%|██████    | 122/200 [00:53<00:36,  2.15it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:53<00:35,  2.15it/s]Running loglikelihood requests:  62%|██████▏   | 124/200 [00:54<00:35,  2.16it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:54<00:34,  2.16it/s]Running loglikelihood requests:  63%|██████▎   | 126/200 [00:54<00:34,  2.16it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:55<00:33,  2.16it/s]Running loglikelihood requests:  64%|██████▍   | 128/200 [00:55<00:33,  2.16it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:56<00:33,  2.15it/s]Running loglikelihood requests:  65%|██████▌   | 130/200 [00:56<00:32,  2.16it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:57<00:31,  2.16it/s]Running loglikelihood requests:  66%|██████▌   | 132/200 [00:57<00:31,  2.16it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:58<00:31,  2.16it/s]Running loglikelihood requests:  67%|██████▋   | 134/200 [00:58<00:30,  2.15it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:59<00:30,  2.15it/s]Running loglikelihood requests:  68%|██████▊   | 136/200 [00:59<00:29,  2.15it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [01:00<00:29,  2.16it/s]Running loglikelihood requests:  69%|██████▉   | 138/200 [01:00<00:28,  2.16it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [01:00<00:28,  2.16it/s]Running loglikelihood requests:  70%|███████   | 140/200 [01:01<00:27,  2.16it/s]Running loglikelihood requests:  70%|███████   | 141/200 [01:01<00:27,  2.17it/s]Running loglikelihood requests:  71%|███████   | 142/200 [01:02<00:26,  2.17it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [01:02<00:26,  2.18it/s]Running loglikelihood requests:  72%|███████▏  | 144/200 [01:03<00:25,  2.18it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:03<00:25,  2.18it/s]Running loglikelihood requests:  73%|███████▎  | 146/200 [01:04<00:24,  2.19it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [01:04<00:24,  2.20it/s]Running loglikelihood requests:  74%|███████▍  | 148/200 [01:05<00:23,  2.19it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:05<00:23,  2.19it/s]Running loglikelihood requests:  75%|███████▌  | 150/200 [01:05<00:22,  2.19it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:06<00:22,  2.19it/s]Running loglikelihood requests:  76%|███████▌  | 152/200 [01:06<00:22,  2.18it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:07<00:21,  2.18it/s]Running loglikelihood requests:  77%|███████▋  | 154/200 [01:07<00:21,  2.19it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:08<00:20,  2.19it/s]Running loglikelihood requests:  78%|███████▊  | 156/200 [01:08<00:20,  2.19it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:09<00:19,  2.20it/s]Running loglikelihood requests:  79%|███████▉  | 158/200 [01:09<00:19,  2.19it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:10<00:18,  2.20it/s]Running loglikelihood requests:  80%|████████  | 160/200 [01:10<00:18,  2.21it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:10<00:17,  2.21it/s]Running loglikelihood requests:  81%|████████  | 162/200 [01:11<00:17,  2.22it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:11<00:16,  2.22it/s]Running loglikelihood requests:  82%|████████▏ | 164/200 [01:12<00:16,  2.21it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:12<00:15,  2.21it/s]Running loglikelihood requests:  83%|████████▎ | 166/200 [01:13<00:15,  2.22it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:13<00:14,  2.23it/s]Running loglikelihood requests:  84%|████████▍ | 168/200 [01:14<00:14,  2.23it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:14<00:13,  2.22it/s]Running loglikelihood requests:  85%|████████▌ | 170/200 [01:15<00:13,  2.22it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:15<00:13,  2.22it/s]Running loglikelihood requests:  86%|████████▌ | 172/200 [01:15<00:12,  2.22it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:16<00:12,  2.23it/s]Running loglikelihood requests:  87%|████████▋ | 174/200 [01:16<00:11,  2.24it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:17<00:11,  2.24it/s]Running loglikelihood requests:  88%|████████▊ | 176/200 [01:17<00:10,  2.24it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:18<00:10,  2.24it/s]Running loglikelihood requests:  89%|████████▉ | 178/200 [01:18<00:09,  2.25it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:19<00:09,  2.25it/s]Running loglikelihood requests:  90%|█████████ | 180/200 [01:19<00:08,  2.26it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:19<00:08,  2.25it/s]Running loglikelihood requests:  91%|█████████ | 182/200 [01:20<00:08,  2.24it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:20<00:07,  2.27it/s]Running loglikelihood requests:  92%|█████████▏| 184/200 [01:21<00:07,  2.27it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:21<00:06,  2.28it/s]Running loglikelihood requests:  93%|█████████▎| 186/200 [01:22<00:06,  2.27it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:22<00:05,  2.28it/s]Running loglikelihood requests:  94%|█████████▍| 188/200 [01:23<00:05,  2.28it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:23<00:04,  2.28it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:6'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:6'}
full model:
{'copa': {'alias': 'copa', 'acc,none': 0.82, 'acc_stderr,none': 0.03861229196653691}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7827148355678417
0.8244609672079803
0.649635438690672
0.6067147398962018
0.806517251330611
0.9304450428937726
0.9492767057467371
0.8146777207922447
0.5943920512469448
0.6904992046065734
0.898578027138337
0.9772061768478973
0.9053772479641496
0.8020143483317842
0.5125553000556808
0.7018400400551881
0.8957865573767195
0.6076519427757794
0.975563476608663
0.9639212062070597
0.9296418237714587
0.9150145595079396
0.9197448761365332
0.7066536936990038
0.6639302750778917
0.912306872752127
0.7698141900267782
0.6230420491138425
0.8289959673107662
Total groups 68 exceeded the threshold, stopping comparison.
The group tensor is
[7, 3, 4, 1, 2, 6, 5, 0]
tensor([7, 3, 4, 1, 2, 6, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 4, 1, 0, 5, 7, 2]
tensor([6, 3, 4, 1, 0, 5, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 4, 3, 2, 0, 5, 6, 1]
tensor([7, 4, 3, 2, 0, 5, 6, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 6, 4, 2, 1, 3, 5, 0]
tensor([7, 6, 4, 2, 1, 3, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 3, 0, 1, 2, 3, 2]
tensor([0, 1, 3, 0, 1, 2, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 3, 1, 0, 2, 3, 2]
tensor([0, 1, 3, 1, 0, 2, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 0, 1.0, 1.0, 1.0, 1]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 2 to 3
done!
Normal merging for layer 4
tensor([4])
tensor(4)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 5 to 6
done!
Normal merging for layer 7
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 8 to 15
done!
Normal merging for layer 16
tensor([0, 3])
tensor(0)
tensor([1, 4])
tensor(1)
tensor([5, 7])
tensor(5)
tensor([2, 6])
tensor(2)
done!
Cross-layer merge completed for layers 17 to 21
done!
Normal merging for layer 22
tensor([0, 4])
tensor(0)
tensor([1, 3])
tensor(1)
tensor([5, 7])
tensor(5)
tensor([2, 6])
tensor(2)
done!
Cross-layer merge completed for layers 23 to 27
done!
Normal merging for layer 28
tensor([0, 3])
tensor(0)
tensor([1, 2, 4, 5, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 29 to 30
done!
Normal merging for layer 31
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
all done!
Model size: 12.1348 GB
4
cuda:6
mastermind_46_easy
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:38<00:38, 38.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 23.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 25.71s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_46_mcq_random HTTP/1.1" 200 778
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/flair/mastermind_46_mcq_random/flair/mastermind_46_mcq_random.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_46_mcq_random HTTP/1.1" 200 786
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/flair/mastermind_46_mcq_random/resolve/544d077942975b1664c0bc4fd54df026050329a4/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_46_mcq_random/revision/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 786
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_46_mcq_random/tree/544d077942975b1664c0bc4fd54df026050329a4?recursive=False&expand=False HTTP/1.1" 200 290
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_46_mcq_random/tree/544d077942975b1664c0bc4fd54df026050329a4/data?recursive=False&expand=False HTTP/1.1" 200 361
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_46_mcq_random/revision/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 786
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/flair/mastermind_46_mcq_random/resolve/544d077942975b1664c0bc4fd54df026050329a4/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_46_mcq_random/paths-info/544d077942975b1664c0bc4fd54df026050329a4 HTTP/1.1" 200 218
DEBUG:filelock:Attempting to acquire lock 140321758583776 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_46_mcq_random_default_0.0.0_544d077942975b1664c0bc4fd54df026050329a4.lock
DEBUG:filelock:Lock 140321758583776 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_46_mcq_random_default_0.0.0_544d077942975b1664c0bc4fd54df026050329a4.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_46_mcq_random/default/0.0.0/544d077942975b1664c0bc4fd54df026050329a4/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321758583776 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_46_mcq_random_default_0.0.0_544d077942975b1664c0bc4fd54df026050329a4.lock
DEBUG:filelock:Lock 140321758583776 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_46_mcq_random_default_0.0.0_544d077942975b1664c0bc4fd54df026050329a4.lock
DEBUG:filelock:Attempting to acquire lock 140328371796688 on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_46_mcq_random/default/0.0.0/544d077942975b1664c0bc4fd54df026050329a4_builder.lock
DEBUG:filelock:Lock 140328371796688 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_46_mcq_random/default/0.0.0/544d077942975b1664c0bc4fd54df026050329a4_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_46_mcq_random/default/0.0.0/544d077942975b1664c0bc4fd54df026050329a4/dataset_info.json
DEBUG:filelock:Attempting to release lock 140328371796688 on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_46_mcq_random/default/0.0.0/544d077942975b1664c0bc4fd54df026050329a4_builder.lock
DEBUG:filelock:Lock 140328371796688 released on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_46_mcq_random/default/0.0.0/544d077942975b1664c0bc4fd54df026050329a4_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of mastermind_46_easy from None to 0
INFO:lm_eval.api.task:Building contexts for mastermind_46_easy on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1380.14it/s]
DEBUG:lm_eval.evaluator:Task: mastermind_46_easy; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:02<14:42,  2.21s/it]Running loglikelihood requests:   0%|          | 2/400 [00:03<12:21,  1.86s/it]Running loglikelihood requests:   1%|          | 3/400 [00:05<11:33,  1.75s/it]Running loglikelihood requests:   1%|          | 4/400 [00:07<11:10,  1.69s/it]Running loglikelihood requests:   1%|▏         | 5/400 [00:08<10:57,  1.66s/it]Running loglikelihood requests:   2%|▏         | 6/400 [00:10<10:47,  1.64s/it]Running loglikelihood requests:   2%|▏         | 7/400 [00:11<10:40,  1.63s/it]Running loglikelihood requests:   2%|▏         | 8/400 [00:13<10:35,  1.62s/it]Running loglikelihood requests:   2%|▏         | 9/400 [00:15<10:30,  1.61s/it]Running loglikelihood requests:   2%|▎         | 10/400 [00:16<10:26,  1.61s/it]Running loglikelihood requests:   3%|▎         | 11/400 [00:18<10:23,  1.60s/it]Running loglikelihood requests:   3%|▎         | 12/400 [00:19<10:20,  1.60s/it]Running loglikelihood requests:   3%|▎         | 13/400 [00:21<10:17,  1.60s/it]Running loglikelihood requests:   4%|▎         | 14/400 [00:23<10:15,  1.59s/it]Running loglikelihood requests:   4%|▍         | 15/400 [00:24<10:13,  1.59s/it]Running loglikelihood requests:   4%|▍         | 16/400 [00:26<10:10,  1.59s/it]Running loglikelihood requests:   4%|▍         | 17/400 [00:27<10:08,  1.59s/it]Running loglikelihood requests:   4%|▍         | 18/400 [00:29<10:05,  1.59s/it]Running loglikelihood requests:   5%|▍         | 19/400 [00:30<10:03,  1.58s/it]Running loglikelihood requests:   5%|▌         | 20/400 [00:32<10:00,  1.58s/it]Running loglikelihood requests:   5%|▌         | 21/400 [00:34<09:57,  1.58s/it]Running loglikelihood requests:   6%|▌         | 22/400 [00:35<09:55,  1.58s/it]Running loglikelihood requests:   6%|▌         | 23/400 [00:37<09:54,  1.58s/it]Running loglikelihood requests:   6%|▌         | 24/400 [00:38<09:52,  1.57s/it]Running loglikelihood requests:   6%|▋         | 25/400 [00:40<09:50,  1.57s/it]Running loglikelihood requests:   6%|▋         | 26/400 [00:41<09:48,  1.57s/it]Running loglikelihood requests:   7%|▋         | 27/400 [00:43<09:46,  1.57s/it]Running loglikelihood requests:   7%|▋         | 28/400 [00:45<09:43,  1.57s/it]Running loglikelihood requests:   7%|▋         | 29/400 [00:46<09:40,  1.57s/it]Running loglikelihood requests:   8%|▊         | 30/400 [00:48<09:38,  1.56s/it]Running loglikelihood requests:   8%|▊         | 31/400 [00:49<09:37,  1.56s/it]Running loglikelihood requests:   8%|▊         | 32/400 [00:51<09:35,  1.56s/it]Running loglikelihood requests:   8%|▊         | 33/400 [00:52<09:32,  1.56s/it]Running loglikelihood requests:   8%|▊         | 34/400 [00:54<09:30,  1.56s/it]Running loglikelihood requests:   9%|▉         | 35/400 [00:55<09:28,  1.56s/it]Running loglikelihood requests:  10%|▉         | 39/400 [00:56<03:34,  1.69it/s]Running loglikelihood requests:  10%|█         | 41/400 [00:59<05:19,  1.12it/s]Running loglikelihood requests:  10%|█         | 42/400 [01:00<06:01,  1.01s/it]Running loglikelihood requests:  11%|█         | 43/400 [01:02<06:40,  1.12s/it]Running loglikelihood requests:  11%|█         | 44/400 [01:03<07:14,  1.22s/it]Running loglikelihood requests:  11%|█▏        | 45/400 [01:05<07:41,  1.30s/it]Running loglikelihood requests:  12%|█▏        | 46/400 [01:06<08:03,  1.37s/it]Running loglikelihood requests:  12%|█▏        | 47/400 [01:08<08:19,  1.42s/it]Running loglikelihood requests:  12%|█▏        | 48/400 [01:10<08:31,  1.45s/it]Running loglikelihood requests:  12%|█▏        | 49/400 [01:11<08:40,  1.48s/it]Running loglikelihood requests:  12%|█▎        | 50/400 [01:13<08:46,  1.51s/it]Running loglikelihood requests:  13%|█▎        | 51/400 [01:14<08:49,  1.52s/it]Running loglikelihood requests:  13%|█▎        | 52/400 [01:16<08:51,  1.53s/it]Running loglikelihood requests:  13%|█▎        | 53/400 [01:17<08:52,  1.53s/it]Running loglikelihood requests:  14%|█▎        | 54/400 [01:19<08:51,  1.54s/it]Running loglikelihood requests:  14%|█▍        | 55/400 [01:20<08:51,  1.54s/it]Running loglikelihood requests:  14%|█▍        | 56/400 [01:22<08:50,  1.54s/it]Running loglikelihood requests:  14%|█▍        | 57/400 [01:24<08:49,  1.54s/it]Running loglikelihood requests:  14%|█▍        | 58/400 [01:25<08:47,  1.54s/it]Running loglikelihood requests:  15%|█▍        | 59/400 [01:27<08:46,  1.54s/it]Running loglikelihood requests:  15%|█▌        | 60/400 [01:28<08:44,  1.54s/it]Running loglikelihood requests:  15%|█▌        | 61/400 [01:30<08:42,  1.54s/it]Running loglikelihood requests:  16%|█▌        | 62/400 [01:31<08:40,  1.54s/it]Running loglikelihood requests:  16%|█▌        | 63/400 [01:33<08:39,  1.54s/it]Running loglikelihood requests:  16%|█▌        | 64/400 [01:34<08:37,  1.54s/it]Running loglikelihood requests:  16%|█▋        | 65/400 [01:36<08:36,  1.54s/it]Running loglikelihood requests:  16%|█▋        | 66/400 [01:37<08:34,  1.54s/it]Running loglikelihood requests:  17%|█▋        | 67/400 [01:39<08:32,  1.54s/it]Running loglikelihood requests:  17%|█▋        | 68/400 [01:40<08:30,  1.54s/it]Running loglikelihood requests:  17%|█▋        | 69/400 [01:42<08:29,  1.54s/it]Running loglikelihood requests:  18%|█▊        | 71/400 [01:44<06:29,  1.18s/it]Running loglikelihood requests:  18%|█▊        | 72/400 [01:45<07:01,  1.28s/it]Running loglikelihood requests:  18%|█▊        | 73/400 [01:47<07:24,  1.36s/it]Running loglikelihood requests:  18%|█▊        | 74/400 [01:48<07:43,  1.42s/it]Running loglikelihood requests:  19%|█▉        | 75/400 [01:50<07:55,  1.46s/it]Running loglikelihood requests:  19%|█▉        | 76/400 [01:51<08:01,  1.48s/it]Running loglikelihood requests:  19%|█▉        | 77/400 [01:53<08:04,  1.50s/it]Running loglikelihood requests:  20%|█▉        | 78/400 [01:54<08:06,  1.51s/it]Running loglikelihood requests:  20%|█▉        | 79/400 [01:56<08:07,  1.52s/it]Running loglikelihood requests:  20%|██        | 80/400 [01:58<08:06,  1.52s/it]Running loglikelihood requests:  21%|██▏       | 85/400 [01:59<03:39,  1.44it/s]Running loglikelihood requests:  22%|██▏       | 86/400 [02:01<04:20,  1.20it/s]Running loglikelihood requests:  22%|██▏       | 87/400 [02:02<05:00,  1.04it/s]Running loglikelihood requests:  22%|██▏       | 88/400 [02:04<05:36,  1.08s/it]Running loglikelihood requests:  22%|██▏       | 89/400 [02:05<06:07,  1.18s/it]Running loglikelihood requests:  22%|██▎       | 90/400 [02:07<06:32,  1.27s/it]Running loglikelihood requests:  23%|██▎       | 91/400 [02:08<06:51,  1.33s/it]Running loglikelihood requests:  23%|██▎       | 92/400 [02:10<07:05,  1.38s/it]Running loglikelihood requests:  23%|██▎       | 93/400 [02:11<07:15,  1.42s/it]Running loglikelihood requests:  24%|██▎       | 94/400 [02:13<07:22,  1.45s/it]Running loglikelihood requests:  24%|██▍       | 95/400 [02:14<07:27,  1.47s/it]Running loglikelihood requests:  24%|██▍       | 96/400 [02:16<07:29,  1.48s/it]Running loglikelihood requests:  24%|██▍       | 97/400 [02:17<07:31,  1.49s/it]Running loglikelihood requests:  24%|██▍       | 98/400 [02:19<07:32,  1.50s/it]Running loglikelihood requests:  25%|██▍       | 99/400 [02:20<07:32,  1.50s/it]Running loglikelihood requests:  25%|██▌       | 100/400 [02:22<07:32,  1.51s/it]Running loglikelihood requests:  25%|██▌       | 101/400 [02:23<07:31,  1.51s/it]Running loglikelihood requests:  26%|██▌       | 102/400 [02:25<07:31,  1.51s/it]Running loglikelihood requests:  26%|██▌       | 103/400 [02:26<07:30,  1.52s/it]Running loglikelihood requests:  26%|██▌       | 104/400 [02:28<07:29,  1.52s/it]Running loglikelihood requests:  26%|██▋       | 105/400 [02:29<07:27,  1.52s/it]Running loglikelihood requests:  26%|██▋       | 106/400 [02:31<07:25,  1.51s/it]Running loglikelihood requests:  27%|██▋       | 107/400 [02:32<07:23,  1.51s/it]Running loglikelihood requests:  27%|██▋       | 108/400 [02:34<07:21,  1.51s/it]Running loglikelihood requests:  27%|██▋       | 109/400 [02:35<07:20,  1.51s/it]Running loglikelihood requests:  28%|██▊       | 110/400 [02:37<07:18,  1.51s/it]Running loglikelihood requests:  28%|██▊       | 111/400 [02:38<07:17,  1.51s/it]Running loglikelihood requests:  28%|██▊       | 112/400 [02:40<07:15,  1.51s/it]Running loglikelihood requests:  28%|██▊       | 113/400 [02:41<07:13,  1.51s/it]Running loglikelihood requests:  28%|██▊       | 114/400 [02:43<07:11,  1.51s/it]Running loglikelihood requests:  29%|██▉       | 115/400 [02:45<07:09,  1.51s/it]Running loglikelihood requests:  29%|██▉       | 116/400 [02:46<07:07,  1.51s/it]Running loglikelihood requests:  29%|██▉       | 117/400 [02:48<07:06,  1.51s/it]Running loglikelihood requests:  30%|██▉       | 118/400 [02:49<07:04,  1.51s/it]Running loglikelihood requests:  30%|██▉       | 119/400 [02:51<07:03,  1.51s/it]Running loglikelihood requests:  30%|███       | 120/400 [02:52<07:01,  1.50s/it]Running loglikelihood requests:  30%|███       | 121/400 [02:54<06:59,  1.50s/it]Running loglikelihood requests:  30%|███       | 122/400 [02:55<06:57,  1.50s/it]Running loglikelihood requests:  31%|███       | 123/400 [02:57<06:56,  1.50s/it]Running loglikelihood requests:  31%|███       | 124/400 [02:58<06:54,  1.50s/it]Running loglikelihood requests:  32%|███▏      | 129/400 [02:59<03:02,  1.49it/s]Running loglikelihood requests:  32%|███▎      | 130/400 [03:01<03:37,  1.24it/s]Running loglikelihood requests:  33%|███▎      | 131/400 [03:02<04:12,  1.07it/s]Running loglikelihood requests:  33%|███▎      | 132/400 [03:04<04:43,  1.06s/it]Running loglikelihood requests:  33%|███▎      | 133/400 [03:05<05:09,  1.16s/it]Running loglikelihood requests:  34%|███▎      | 134/400 [03:07<05:31,  1.24s/it]Running loglikelihood requests:  34%|███▍      | 135/400 [03:08<05:47,  1.31s/it]Running loglikelihood requests:  34%|███▍      | 136/400 [03:10<05:59,  1.36s/it]Running loglikelihood requests:  34%|███▍      | 137/400 [03:11<06:08,  1.40s/it]Running loglikelihood requests:  34%|███▍      | 138/400 [03:13<06:13,  1.43s/it]Running loglikelihood requests:  35%|███▍      | 139/400 [03:14<06:17,  1.45s/it]Running loglikelihood requests:  35%|███▌      | 140/400 [03:16<06:19,  1.46s/it]Running loglikelihood requests:  35%|███▌      | 141/400 [03:17<06:20,  1.47s/it]Running loglikelihood requests:  36%|███▌      | 142/400 [03:19<06:20,  1.48s/it]Running loglikelihood requests:  36%|███▌      | 143/400 [03:20<06:20,  1.48s/it]Running loglikelihood requests:  36%|███▌      | 144/400 [03:22<06:19,  1.48s/it]Running loglikelihood requests:  36%|███▋      | 145/400 [03:23<06:18,  1.48s/it]Running loglikelihood requests:  36%|███▋      | 146/400 [03:25<06:16,  1.48s/it]Running loglikelihood requests:  37%|███▋      | 147/400 [03:26<06:15,  1.48s/it]Running loglikelihood requests:  37%|███▋      | 148/400 [03:28<06:13,  1.48s/it]Running loglikelihood requests:  37%|███▋      | 149/400 [03:29<06:12,  1.48s/it]Running loglikelihood requests:  38%|███▊      | 150/400 [03:31<06:10,  1.48s/it]Running loglikelihood requests:  38%|███▊      | 151/400 [03:32<06:09,  1.48s/it]Running loglikelihood requests:  38%|███▊      | 152/400 [03:34<06:07,  1.48s/it]Running loglikelihood requests:  38%|███▊      | 153/400 [03:35<06:06,  1.48s/it]Running loglikelihood requests:  38%|███▊      | 154/400 [03:37<06:05,  1.49s/it]Running loglikelihood requests:  39%|███▉      | 155/400 [03:38<06:04,  1.49s/it]Running loglikelihood requests:  39%|███▉      | 156/400 [03:40<06:02,  1.49s/it]Running loglikelihood requests:  39%|███▉      | 157/400 [03:41<06:00,  1.48s/it]Running loglikelihood requests:  40%|███▉      | 158/400 [03:43<05:58,  1.48s/it]Running loglikelihood requests:  40%|███▉      | 159/400 [03:44<05:57,  1.48s/it]Running loglikelihood requests:  40%|████      | 160/400 [03:46<05:54,  1.48s/it]Running loglikelihood requests:  40%|████      | 161/400 [03:47<05:52,  1.48s/it]Running loglikelihood requests:  40%|████      | 162/400 [03:49<05:51,  1.48s/it]Running loglikelihood requests:  41%|████      | 163/400 [03:50<05:49,  1.47s/it]Running loglikelihood requests:  41%|████      | 164/400 [03:51<05:47,  1.47s/it]Running loglikelihood requests:  41%|████▏     | 165/400 [03:53<05:46,  1.47s/it]Running loglikelihood requests:  42%|████▏     | 166/400 [03:54<05:44,  1.47s/it]Running loglikelihood requests:  42%|████▏     | 167/400 [03:56<05:42,  1.47s/it]Running loglikelihood requests:  42%|████▏     | 168/400 [03:57<05:41,  1.47s/it]Running loglikelihood requests:  42%|████▏     | 169/400 [03:59<05:40,  1.47s/it]Running loglikelihood requests:  44%|████▎     | 174/400 [04:00<02:24,  1.57it/s]Running loglikelihood requests:  44%|████▍     | 175/400 [04:02<02:54,  1.29it/s]Running loglikelihood requests:  44%|████▍     | 176/400 [04:03<03:22,  1.11it/s]Running loglikelihood requests:  44%|████▍     | 177/400 [04:05<03:48,  1.02s/it]Running loglikelihood requests:  44%|████▍     | 178/400 [04:06<04:10,  1.13s/it]Running loglikelihood requests:  45%|████▍     | 179/400 [04:07<04:28,  1.21s/it]Running loglikelihood requests:  45%|████▌     | 180/400 [04:09<04:42,  1.28s/it]Running loglikelihood requests:  45%|████▌     | 181/400 [04:10<04:52,  1.33s/it]Running loglikelihood requests:  46%|████▌     | 182/400 [04:12<04:59,  1.37s/it]Running loglikelihood requests:  46%|████▌     | 183/400 [04:13<05:03,  1.40s/it]Running loglikelihood requests:  46%|████▌     | 184/400 [04:15<05:06,  1.42s/it]Running loglikelihood requests:  46%|████▋     | 185/400 [04:16<05:07,  1.43s/it]Running loglikelihood requests:  46%|████▋     | 186/400 [04:18<05:08,  1.44s/it]Running loglikelihood requests:  47%|████▋     | 187/400 [04:19<05:08,  1.45s/it]Running loglikelihood requests:  47%|████▋     | 188/400 [04:21<05:07,  1.45s/it]Running loglikelihood requests:  47%|████▋     | 189/400 [04:22<05:06,  1.45s/it]Running loglikelihood requests:  48%|████▊     | 190/400 [04:24<05:05,  1.45s/it]Running loglikelihood requests:  48%|████▊     | 191/400 [04:25<05:04,  1.46s/it]Running loglikelihood requests:  48%|████▊     | 192/400 [04:26<05:02,  1.46s/it]Running loglikelihood requests:  48%|████▊     | 193/400 [04:28<05:01,  1.46s/it]Running loglikelihood requests:  48%|████▊     | 194/400 [04:29<05:00,  1.46s/it]Running loglikelihood requests:  49%|████▉     | 195/400 [04:31<04:58,  1.46s/it]Running loglikelihood requests:  49%|████▉     | 196/400 [04:32<04:57,  1.46s/it]Running loglikelihood requests:  49%|████▉     | 197/400 [04:34<04:55,  1.46s/it]Running loglikelihood requests:  50%|████▉     | 198/400 [04:35<04:53,  1.45s/it]Running loglikelihood requests:  50%|████▉     | 199/400 [04:37<04:52,  1.45s/it]Running loglikelihood requests:  50%|█████     | 200/400 [04:38<04:50,  1.45s/it]Running loglikelihood requests:  50%|█████     | 201/400 [04:40<04:49,  1.45s/it]Running loglikelihood requests:  50%|█████     | 202/400 [04:41<04:47,  1.45s/it]Running loglikelihood requests:  51%|█████     | 203/400 [04:42<04:45,  1.45s/it]Running loglikelihood requests:  51%|█████     | 204/400 [04:44<04:44,  1.45s/it]Running loglikelihood requests:  51%|█████▏    | 205/400 [04:45<04:42,  1.45s/it]Running loglikelihood requests:  52%|█████▏    | 206/400 [04:47<04:41,  1.45s/it]Running loglikelihood requests:  52%|█████▏    | 207/400 [04:48<04:40,  1.45s/it]Running loglikelihood requests:  52%|█████▏    | 208/400 [04:50<04:39,  1.46s/it]Running loglikelihood requests:  52%|█████▏    | 209/400 [04:51<04:37,  1.45s/it]Running loglikelihood requests:  52%|█████▎    | 210/400 [04:53<04:36,  1.46s/it]Running loglikelihood requests:  53%|█████▎    | 211/400 [04:54<04:34,  1.45s/it]Running loglikelihood requests:  53%|█████▎    | 212/400 [04:56<04:33,  1.45s/it]Running loglikelihood requests:  53%|█████▎    | 213/400 [04:57<04:30,  1.45s/it]Running loglikelihood requests:  54%|█████▎    | 214/400 [04:58<04:28,  1.44s/it]Running loglikelihood requests:  54%|█████▍    | 215/400 [05:00<04:26,  1.44s/it]Running loglikelihood requests:  55%|█████▌    | 220/400 [05:01<01:53,  1.59it/s]Running loglikelihood requests:  55%|█████▌    | 221/400 [05:03<02:16,  1.32it/s]Running loglikelihood requests:  56%|█████▌    | 222/400 [05:04<02:37,  1.13it/s]Running loglikelihood requests:  56%|█████▌    | 223/400 [05:05<02:57,  1.00s/it]Running loglikelihood requests:  56%|█████▌    | 224/400 [05:07<03:13,  1.10s/it]Running loglikelihood requests:  56%|█████▋    | 225/400 [05:08<03:26,  1.18s/it]Running loglikelihood requests:  56%|█████▋    | 226/400 [05:10<03:36,  1.25s/it]Running loglikelihood requests:  57%|█████▋    | 227/400 [05:11<03:44,  1.30s/it]Running loglikelihood requests:  57%|█████▋    | 228/400 [05:13<03:49,  1.33s/it]Running loglikelihood requests:  57%|█████▋    | 229/400 [05:14<03:51,  1.36s/it]Running loglikelihood requests:  57%|█████▊    | 230/400 [05:15<03:53,  1.37s/it]Running loglikelihood requests:  58%|█████▊    | 231/400 [05:17<03:53,  1.38s/it]Running loglikelihood requests:  58%|█████▊    | 232/400 [05:18<03:53,  1.39s/it]Running loglikelihood requests:  58%|█████▊    | 233/400 [05:20<03:53,  1.40s/it]Running loglikelihood requests:  58%|█████▊    | 234/400 [05:21<03:53,  1.41s/it]Running loglikelihood requests:  59%|█████▉    | 235/400 [05:22<03:52,  1.41s/it]Running loglikelihood requests:  59%|█████▉    | 236/400 [05:24<03:51,  1.41s/it]Running loglikelihood requests:  59%|█████▉    | 237/400 [05:25<03:50,  1.41s/it]Running loglikelihood requests:  60%|█████▉    | 238/400 [05:27<03:48,  1.41s/it]Running loglikelihood requests:  60%|█████▉    | 239/400 [05:28<03:47,  1.41s/it]Running loglikelihood requests:  60%|██████    | 240/400 [05:29<03:45,  1.41s/it]Running loglikelihood requests:  60%|██████    | 241/400 [05:31<03:43,  1.41s/it]Running loglikelihood requests:  60%|██████    | 242/400 [05:32<03:41,  1.40s/it]Running loglikelihood requests:  61%|██████    | 243/400 [05:34<03:40,  1.40s/it]Running loglikelihood requests:  61%|██████    | 244/400 [05:35<03:38,  1.40s/it]Running loglikelihood requests:  61%|██████▏   | 245/400 [05:36<03:36,  1.40s/it]Running loglikelihood requests:  62%|██████▏   | 246/400 [05:38<03:34,  1.40s/it]Running loglikelihood requests:  62%|██████▏   | 247/400 [05:39<03:33,  1.39s/it]Running loglikelihood requests:  62%|██████▏   | 248/400 [05:41<03:31,  1.39s/it]Running loglikelihood requests:  62%|██████▏   | 249/400 [05:42<03:29,  1.39s/it]Running loglikelihood requests:  62%|██████▎   | 250/400 [05:43<03:28,  1.39s/it]Running loglikelihood requests:  63%|██████▎   | 251/400 [05:45<03:26,  1.39s/it]Running loglikelihood requests:  63%|██████▎   | 252/400 [05:46<03:25,  1.39s/it]Running loglikelihood requests:  63%|██████▎   | 253/400 [05:48<03:23,  1.39s/it]Running loglikelihood requests:  64%|██████▎   | 254/400 [05:49<03:22,  1.38s/it]Running loglikelihood requests:  64%|██████▍   | 255/400 [05:50<03:23,  1.41s/it]Running loglikelihood requests:  64%|██████▍   | 256/400 [05:52<03:22,  1.41s/it]Running loglikelihood requests:  64%|██████▍   | 257/400 [05:53<03:20,  1.40s/it]Running loglikelihood requests:  64%|██████▍   | 258/400 [05:55<03:18,  1.40s/it]Running loglikelihood requests:  65%|██████▍   | 259/400 [05:56<03:16,  1.39s/it]Running loglikelihood requests:  65%|██████▌   | 260/400 [05:57<03:14,  1.39s/it]Running loglikelihood requests:  65%|██████▌   | 261/400 [05:59<03:13,  1.39s/it]Running loglikelihood requests:  66%|██████▌   | 262/400 [06:00<03:11,  1.39s/it]Running loglikelihood requests:  66%|██████▌   | 263/400 [06:02<03:09,  1.39s/it]Running loglikelihood requests:  67%|██████▋   | 268/400 [06:02<01:12,  1.81it/s]Running loglikelihood requests:  67%|██████▋   | 269/400 [06:04<01:29,  1.46it/s]Running loglikelihood requests:  68%|██████▊   | 270/400 [06:05<01:46,  1.22it/s]Running loglikelihood requests:  68%|██████▊   | 271/400 [06:06<02:00,  1.07it/s]Running loglikelihood requests:  68%|██████▊   | 272/400 [06:08<02:12,  1.04s/it]Running loglikelihood requests:  68%|██████▊   | 273/400 [06:09<02:22,  1.12s/it]Running loglikelihood requests:  68%|██████▊   | 274/400 [06:11<02:29,  1.19s/it]Running loglikelihood requests:  69%|██████▉   | 275/400 [06:12<02:34,  1.24s/it]Running loglikelihood requests:  69%|██████▉   | 276/400 [06:13<02:38,  1.28s/it]Running loglikelihood requests:  69%|██████▉   | 277/400 [06:15<02:40,  1.30s/it]Running loglikelihood requests:  70%|██████▉   | 278/400 [06:16<02:41,  1.32s/it]Running loglikelihood requests:  70%|██████▉   | 279/400 [06:17<02:41,  1.34s/it]Running loglikelihood requests:  70%|███████   | 280/400 [06:19<02:42,  1.36s/it]Running loglikelihood requests:  70%|███████   | 281/400 [06:20<02:42,  1.37s/it]Running loglikelihood requests:  70%|███████   | 282/400 [06:22<02:42,  1.38s/it]Running loglikelihood requests:  71%|███████   | 283/400 [06:23<02:40,  1.37s/it]Running loglikelihood requests:  71%|███████   | 284/400 [06:24<02:38,  1.37s/it]Running loglikelihood requests:  71%|███████▏  | 285/400 [06:26<02:36,  1.36s/it]Running loglikelihood requests:  72%|███████▏  | 286/400 [06:27<02:35,  1.36s/it]Running loglikelihood requests:  72%|███████▏  | 287/400 [06:28<02:33,  1.36s/it]Running loglikelihood requests:  72%|███████▏  | 288/400 [06:30<02:32,  1.36s/it]Running loglikelihood requests:  72%|███████▏  | 289/400 [06:31<02:31,  1.36s/it]Running loglikelihood requests:  72%|███████▎  | 290/400 [06:33<02:29,  1.36s/it]Running loglikelihood requests:  73%|███████▎  | 291/400 [06:34<02:28,  1.36s/it]Running loglikelihood requests:  73%|███████▎  | 292/400 [06:35<02:27,  1.36s/it]Running loglikelihood requests:  73%|███████▎  | 293/400 [06:37<02:26,  1.37s/it]Running loglikelihood requests:  74%|███████▎  | 294/400 [06:38<02:26,  1.38s/it]Running loglikelihood requests:  74%|███████▍  | 295/400 [06:39<02:23,  1.37s/it]Running loglikelihood requests:  74%|███████▍  | 296/400 [06:41<02:21,  1.36s/it]Running loglikelihood requests:  74%|███████▍  | 297/400 [06:42<02:19,  1.35s/it]Running loglikelihood requests:  74%|███████▍  | 298/400 [06:43<02:17,  1.35s/it]Running loglikelihood requests:  75%|███████▍  | 299/400 [06:45<02:15,  1.34s/it]Running loglikelihood requests:  75%|███████▌  | 300/400 [06:46<02:14,  1.34s/it]Running loglikelihood requests:  75%|███████▌  | 301/400 [06:47<02:12,  1.34s/it]Running loglikelihood requests:  76%|███████▌  | 302/400 [06:49<02:10,  1.34s/it]Running loglikelihood requests:  76%|███████▌  | 303/400 [06:50<02:09,  1.33s/it]Running loglikelihood requests:  76%|███████▌  | 304/400 [06:51<02:07,  1.33s/it]Running loglikelihood requests:  76%|███████▋  | 305/400 [06:53<02:06,  1.33s/it]Running loglikelihood requests:  76%|███████▋  | 306/400 [06:54<02:05,  1.33s/it]Running loglikelihood requests:  77%|███████▋  | 307/400 [06:55<02:03,  1.33s/it]Running loglikelihood requests:  77%|███████▋  | 308/400 [06:57<02:02,  1.33s/it]Running loglikelihood requests:  77%|███████▋  | 309/400 [06:58<02:00,  1.33s/it]Running loglikelihood requests:  78%|███████▊  | 310/400 [06:59<01:59,  1.33s/it]Running loglikelihood requests:  78%|███████▊  | 311/400 [07:01<01:57,  1.33s/it]Running loglikelihood requests:  78%|███████▊  | 312/400 [07:02<01:56,  1.32s/it]Running loglikelihood requests:  79%|███████▉  | 317/400 [07:03<00:42,  1.95it/s]Running loglikelihood requests:  80%|███████▉  | 318/400 [07:04<00:52,  1.55it/s]Running loglikelihood requests:  80%|███████▉  | 319/400 [07:05<01:02,  1.29it/s]Running loglikelihood requests:  80%|████████  | 320/400 [07:07<01:11,  1.12it/s]Running loglikelihood requests:  80%|████████  | 321/400 [07:08<01:18,  1.01it/s]Running loglikelihood requests:  80%|████████  | 322/400 [07:09<01:23,  1.07s/it]Running loglikelihood requests:  81%|████████  | 323/400 [07:11<01:27,  1.14s/it]Running loglikelihood requests:  81%|████████  | 324/400 [07:12<01:30,  1.19s/it]Running loglikelihood requests:  81%|████████▏ | 325/400 [07:13<01:31,  1.22s/it]Running loglikelihood requests:  82%|████████▏ | 326/400 [07:15<01:32,  1.25s/it]Running loglikelihood requests:  82%|████████▏ | 327/400 [07:16<01:32,  1.27s/it]Running loglikelihood requests:  82%|████████▏ | 328/400 [07:17<01:32,  1.28s/it]Running loglikelihood requests:  82%|████████▏ | 329/400 [07:18<01:31,  1.29s/it]Running loglikelihood requests:  82%|████████▎ | 330/400 [07:20<01:30,  1.30s/it]Running loglikelihood requests:  83%|████████▎ | 331/400 [07:21<01:29,  1.30s/it]Running loglikelihood requests:  83%|████████▎ | 332/400 [07:22<01:28,  1.30s/it]Running loglikelihood requests:  83%|████████▎ | 333/400 [07:24<01:27,  1.30s/it]Running loglikelihood requests:  84%|████████▎ | 334/400 [07:25<01:26,  1.30s/it]Running loglikelihood requests:  84%|████████▍ | 335/400 [07:26<01:24,  1.30s/it]Running loglikelihood requests:  84%|████████▍ | 336/400 [07:28<01:23,  1.30s/it]Running loglikelihood requests:  84%|████████▍ | 337/400 [07:29<01:22,  1.30s/it]Running loglikelihood requests:  84%|████████▍ | 338/400 [07:30<01:20,  1.30s/it]Running loglikelihood requests:  85%|████████▍ | 339/400 [07:32<01:19,  1.30s/it]Running loglikelihood requests:  85%|████████▌ | 340/400 [07:33<01:18,  1.30s/it]Running loglikelihood requests:  85%|████████▌ | 341/400 [07:34<01:16,  1.30s/it]Running loglikelihood requests:  86%|████████▌ | 342/400 [07:35<01:15,  1.30s/it]Running loglikelihood requests:  86%|████████▌ | 343/400 [07:37<01:14,  1.30s/it]Running loglikelihood requests:  86%|████████▌ | 344/400 [07:38<01:12,  1.30s/it]Running loglikelihood requests:  86%|████████▋ | 345/400 [07:39<01:11,  1.30s/it]Running loglikelihood requests:  86%|████████▋ | 346/400 [07:41<01:10,  1.30s/it]Running loglikelihood requests:  87%|████████▋ | 347/400 [07:42<01:08,  1.30s/it]Running loglikelihood requests:  87%|████████▋ | 348/400 [07:43<01:07,  1.30s/it]Running loglikelihood requests:  87%|████████▋ | 349/400 [07:45<01:06,  1.30s/it]Running loglikelihood requests:  88%|████████▊ | 350/400 [07:46<01:05,  1.30s/it]Running loglikelihood requests:  88%|████████▊ | 351/400 [07:47<01:03,  1.30s/it]Running loglikelihood requests:  88%|████████▊ | 352/400 [07:48<01:02,  1.30s/it]Running loglikelihood requests:  88%|████████▊ | 353/400 [07:50<01:01,  1.31s/it]Running loglikelihood requests:  88%|████████▊ | 354/400 [07:51<01:00,  1.32s/it]Running loglikelihood requests:  89%|████████▉ | 355/400 [07:52<00:59,  1.33s/it]Running loglikelihood requests:  89%|████████▉ | 356/400 [07:54<00:58,  1.34s/it]Running loglikelihood requests:  89%|████████▉ | 357/400 [07:55<00:57,  1.34s/it]Running loglikelihood requests:  90%|████████▉ | 358/400 [07:57<00:55,  1.33s/it]Running loglikelihood requests:  90%|████████▉ | 359/400 [07:58<00:54,  1.32s/it]Running loglikelihood requests:  90%|█████████ | 360/400 [07:59<00:52,  1.32s/it]Running loglikelihood requests:  90%|█████████ | 361/400 [08:00<00:51,  1.32s/it]Running loglikelihood requests:  90%|█████████ | 362/400 [08:02<00:49,  1.31s/it]Running loglikelihood requests:  91%|█████████ | 363/400 [08:03<00:48,  1.31s/it]Running loglikelihood requests:  92%|█████████▏| 369/400 [08:04<00:16,  1.89it/s]Running loglikelihood requests:  92%|█████████▎| 370/400 [08:06<00:19,  1.56it/s]Running loglikelihood requests:  93%|█████████▎| 371/400 [08:07<00:21,  1.32it/s]Running loglikelihood requests:  93%|█████████▎| 372/400 [08:08<00:24,  1.15it/s]Running loglikelihood requests:  93%|█████████▎| 373/400 [08:10<00:25,  1.04it/s]Running loglikelihood requests:  94%|█████████▎| 374/400 [08:11<00:27,  1.04s/it]Running loglikelihood requests:  94%|█████████▍| 375/400 [08:12<00:27,  1.10s/it]Running loglikelihood requests:  94%|█████████▍| 376/400 [08:13<00:27,  1.15s/it]Running loglikelihood requests:  94%|█████████▍| 377/400 [08:15<00:27,  1.19s/it]Running loglikelihood requests:  94%|█████████▍| 378/400 [08:16<00:26,  1.21s/it]Running loglikelihood requests:  95%|█████████▍| 379/400 [08:17<00:25,  1.23s/it]Running loglikelihood requests:  95%|█████████▌| 380/400 [08:19<00:24,  1.25s/it]Running loglikelihood requests:  95%|█████████▌| 381/400 [08:20<00:23,  1.26s/it]Running loglikelihood requests:  96%|█████████▌| 382/400 [08:21<00:22,  1.26s/it]Running loglikelihood requests:  96%|█████████▌| 383/400 [08:22<00:21,  1.27s/it]Running loglikelihood requests:  96%|█████████▌| 384/400 [08:24<00:20,  1.27s/it]Running loglikelihood requests:  96%|█████████▋| 385/400 [08:25<00:18,  1.26s/it]Running loglikelihood requests:  96%|█████████▋| 386/400 [08:26<00:17,  1.25s/it]Running loglikelihood requests:  97%|█████████▋| 387/400 [08:27<00:16,  1.24s/it]Running loglikelihood requests:  97%|█████████▋| 388/400 [08:29<00:14,  1.24s/it]Running loglikelihood requests:  97%|█████████▋| 389/400 [08:30<00:13,  1.21s/it]Running loglikelihood requests:  98%|█████████▊| 390/400 [08:31<00:11,  1.19s/it]Running loglikelihood requests:  98%|█████████▊| 391/400 [08:32<00:10,  1.18s/it]Running loglikelihood requests:  98%|█████████▊| 392/400 [08:33<00:09,  1.17s/it]Running loglikelihood requests:  98%|█████████▊| 393/400 [08:34<00:08,  1.16s/it]Running loglikelihood requests:  98%|█████████▊| 394/400 [08:35<00:06,  1.15s/it]Running loglikelihood requests:  99%|█████████▉| 395/400 [08:37<00:05,  1.14s/it]Running loglikelihood requests:  99%|█████████▉| 396/400 [08:38<00:04,  1.13s/it]Running loglikelihood requests:  99%|█████████▉| 397/400 [08:39<00:03,  1.11s/it]Running loglikelihood requests: 100%|█████████▉| 398/400 [08:40<00:02,  1.10s/it]Running loglikelihood requests: 100%|█████████▉| 399/400 [08:41<00:01,  1.09s/it]Running loglikelihood requests: 100%|██████████| 400/400 [08:42<00:00,  1.08s/it]Running loglikelihood requests: 100%|██████████| 400/400 [08:42<00:00,  1.31s/it]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:7'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:7'}
full model:
{'mastermind_46_easy': {'alias': 'mastermind_46_easy', 'acc,none': 0.75, 'acc_stderr,none': 0.04351941398892446}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9900730655060652
0.9975621416797388
0.9971758007187725
0.9980230222224056
0.9889479987910904
0.9764864248764976
0.9903944102306983
0.9944743493554844
0.9959595842537624
0.9872350810596702
0.9630442697748985
0.9836939129473328
0.9680042833072414
0.9739989664157891
0.9945965755000291
0.9874567967365809
0.9910190996776087
0.9888895436517939
0.9820307305549418
0.9881803990776891
0.9859944271811626
0.9931460182904469
0.984259193892523
0.99673879588459
0.9800909214289261
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[5, 2, 0, 3, 7, 6, 1, 4]
tensor([5, 2, 0, 3, 7, 6, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 0, 3, 7, 6, 1, 4]
tensor([5, 2, 0, 3, 7, 6, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 0, 3, 7, 6, 1, 4]
tensor([5, 2, 0, 3, 7, 6, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 0, 3, 7, 6, 1, 4]
tensor([5, 2, 0, 3, 7, 6, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 0, 3, 7, 6, 1, 4]
tensor([5, 2, 0, 3, 7, 6, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 0, 3, 7, 6, 1, 4]
tensor([5, 2, 0, 3, 7, 6, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
done!
Normal merging for layer 3
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
done!
Normal merging for layer 4
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
done!
Normal merging for layer 5
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 6 to 31
done!
all done!
Model size: 12.0718 GB
183
cuda:7
mastermind_24_easy
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:43<00:43, 43.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 22.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 25.52s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_24_mcq_random HTTP/1.1" 200 772
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/flair/mastermind_24_mcq_random/flair/mastermind_24_mcq_random.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_24_mcq_random HTTP/1.1" 200 779
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/flair/mastermind_24_mcq_random/resolve/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_24_mcq_random/revision/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 779
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_24_mcq_random/tree/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5?recursive=False&expand=False HTTP/1.1" 200 290
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_24_mcq_random/tree/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5/data?recursive=False&expand=False HTTP/1.1" 200 358
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_24_mcq_random/revision/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 779
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/flair/mastermind_24_mcq_random/resolve/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:filelock:Attempting to acquire lock 140321082113344 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_24_mcq_random_default_0.0.0_cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5.lock
DEBUG:filelock:Lock 140321082113344 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_24_mcq_random_default_0.0.0_cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321082113344 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_24_mcq_random_default_0.0.0_cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5.lock
DEBUG:filelock:Lock 140321082113344 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_24_mcq_random_default_0.0.0_cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5.lock
DEBUG:filelock:Attempting to acquire lock 140321090021104 on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5_builder.lock
DEBUG:filelock:Lock 140321090021104 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321090021104 on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5_builder.lock
DEBUG:filelock:Lock 140321090021104 released on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of mastermind_24_easy from None to 0
INFO:lm_eval.api.task:Building contexts for mastermind_24_easy on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1397.65it/s]
DEBUG:lm_eval.evaluator:Task: mastermind_24_easy; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:01<10:36,  1.60s/it]Running loglikelihood requests:   0%|          | 2/400 [00:02<08:49,  1.33s/it]Running loglikelihood requests:   1%|          | 3/400 [00:03<08:13,  1.24s/it]Running loglikelihood requests:   1%|          | 4/400 [00:05<07:55,  1.20s/it]Running loglikelihood requests:   1%|▏         | 5/400 [00:06<07:44,  1.18s/it]Running loglikelihood requests:   2%|▏         | 6/400 [00:07<07:36,  1.16s/it]Running loglikelihood requests:   2%|▏         | 7/400 [00:08<07:30,  1.15s/it]Running loglikelihood requests:   2%|▏         | 8/400 [00:09<07:26,  1.14s/it]Running loglikelihood requests:   2%|▏         | 9/400 [00:10<07:23,  1.13s/it]Running loglikelihood requests:   2%|▎         | 10/400 [00:11<07:21,  1.13s/it]Running loglikelihood requests:   3%|▎         | 12/400 [00:12<05:40,  1.14it/s]Running loglikelihood requests:   3%|▎         | 13/400 [00:14<06:03,  1.07it/s]Running loglikelihood requests:   4%|▎         | 14/400 [00:15<06:20,  1.02it/s]Running loglikelihood requests:   4%|▍         | 16/400 [00:16<05:08,  1.25it/s]Running loglikelihood requests:   4%|▍         | 17/400 [00:17<05:35,  1.14it/s]Running loglikelihood requests:   4%|▍         | 18/400 [00:18<05:57,  1.07it/s]Running loglikelihood requests:   5%|▍         | 19/400 [00:19<06:15,  1.02it/s]Running loglikelihood requests:   5%|▌         | 20/400 [00:20<06:28,  1.02s/it]Running loglikelihood requests:   5%|▌         | 21/400 [00:21<06:37,  1.05s/it]Running loglikelihood requests:   6%|▌         | 22/400 [00:23<06:44,  1.07s/it]Running loglikelihood requests:   6%|▌         | 23/400 [00:24<06:48,  1.08s/it]Running loglikelihood requests:   6%|▋         | 25/400 [00:25<05:16,  1.18it/s]Running loglikelihood requests:   6%|▋         | 26/400 [00:26<05:40,  1.10it/s]Running loglikelihood requests:   7%|▋         | 27/400 [00:27<05:59,  1.04it/s]Running loglikelihood requests:   7%|▋         | 29/400 [00:28<04:53,  1.26it/s]Running loglikelihood requests:   8%|▊         | 30/400 [00:29<05:20,  1.16it/s]Running loglikelihood requests:   8%|▊         | 31/400 [00:30<05:41,  1.08it/s]Running loglikelihood requests:   8%|▊         | 32/400 [00:31<05:58,  1.03it/s]Running loglikelihood requests:   8%|▊         | 34/400 [00:33<04:50,  1.26it/s]Running loglikelihood requests:   9%|▉         | 35/400 [00:34<05:16,  1.15it/s]Running loglikelihood requests:  10%|▉         | 38/400 [00:35<03:43,  1.62it/s]Running loglikelihood requests:  11%|█         | 44/400 [00:36<02:00,  2.95it/s]Running loglikelihood requests:  11%|█▏        | 45/400 [00:37<02:33,  2.31it/s]Running loglikelihood requests:  12%|█▏        | 47/400 [00:38<02:43,  2.16it/s]Running loglikelihood requests:  12%|█▏        | 48/400 [00:39<03:17,  1.78it/s]Running loglikelihood requests:  12%|█▏        | 49/400 [00:40<03:51,  1.52it/s]Running loglikelihood requests:  12%|█▎        | 50/400 [00:41<04:22,  1.33it/s]Running loglikelihood requests:  13%|█▎        | 52/400 [00:42<03:54,  1.48it/s]Running loglikelihood requests:  13%|█▎        | 53/400 [00:43<04:24,  1.31it/s]Running loglikelihood requests:  14%|█▎        | 54/400 [00:44<04:53,  1.18it/s]Running loglikelihood requests:  14%|█▍        | 56/400 [00:46<04:16,  1.34it/s]Running loglikelihood requests:  14%|█▍        | 57/400 [00:47<04:47,  1.19it/s]Running loglikelihood requests:  14%|█▍        | 58/400 [00:48<05:12,  1.09it/s]Running loglikelihood requests:  15%|█▍        | 59/400 [00:49<05:32,  1.03it/s]Running loglikelihood requests:  15%|█▌        | 60/400 [00:50<05:47,  1.02s/it]Running loglikelihood requests:  15%|█▌        | 61/400 [00:51<05:57,  1.06s/it]Running loglikelihood requests:  16%|█▌        | 62/400 [00:52<06:00,  1.07s/it]Running loglikelihood requests:  16%|█▌        | 63/400 [00:54<06:01,  1.07s/it]Running loglikelihood requests:  16%|█▌        | 64/400 [00:55<06:01,  1.07s/it]Running loglikelihood requests:  16%|█▋        | 65/400 [00:56<06:00,  1.08s/it]Running loglikelihood requests:  16%|█▋        | 66/400 [00:57<06:00,  1.08s/it]Running loglikelihood requests:  17%|█▋        | 67/400 [00:58<05:59,  1.08s/it]Running loglikelihood requests:  17%|█▋        | 68/400 [00:59<05:58,  1.08s/it]Running loglikelihood requests:  17%|█▋        | 69/400 [01:00<05:58,  1.08s/it]Running loglikelihood requests:  18%|█▊        | 70/400 [01:01<05:57,  1.08s/it]Running loglikelihood requests:  18%|█▊        | 71/400 [01:02<05:57,  1.09s/it]Running loglikelihood requests:  18%|█▊        | 72/400 [01:03<05:55,  1.09s/it]Running loglikelihood requests:  18%|█▊        | 73/400 [01:04<05:54,  1.08s/it]Running loglikelihood requests:  18%|█▊        | 74/400 [01:05<05:53,  1.08s/it]Running loglikelihood requests:  19%|█▉        | 76/400 [01:07<04:30,  1.20it/s]Running loglikelihood requests:  19%|█▉        | 77/400 [01:08<04:49,  1.12it/s]Running loglikelihood requests:  20%|█▉        | 78/400 [01:09<05:04,  1.06it/s]Running loglikelihood requests:  20%|█▉        | 79/400 [01:10<05:15,  1.02it/s]Running loglikelihood requests:  20%|██        | 80/400 [01:11<05:22,  1.01s/it]Running loglikelihood requests:  20%|██        | 81/400 [01:12<05:27,  1.03s/it]Running loglikelihood requests:  20%|██        | 82/400 [01:13<05:31,  1.04s/it]Running loglikelihood requests:  21%|██        | 83/400 [01:14<05:33,  1.05s/it]Running loglikelihood requests:  21%|██        | 84/400 [01:15<05:34,  1.06s/it]Running loglikelihood requests:  21%|██▏       | 85/400 [01:16<05:34,  1.06s/it]Running loglikelihood requests:  22%|██▏       | 87/400 [01:17<04:17,  1.22it/s]Running loglikelihood requests:  22%|██▏       | 88/400 [01:18<04:35,  1.13it/s]Running loglikelihood requests:  22%|██▏       | 89/400 [01:19<04:49,  1.07it/s]Running loglikelihood requests:  22%|██▎       | 90/400 [01:21<05:00,  1.03it/s]Running loglikelihood requests:  23%|██▎       | 91/400 [01:22<05:08,  1.00it/s]Running loglikelihood requests:  23%|██▎       | 92/400 [01:23<05:13,  1.02s/it]Running loglikelihood requests:  24%|██▎       | 94/400 [01:24<04:05,  1.25it/s]Running loglikelihood requests:  24%|██▍       | 95/400 [01:25<04:24,  1.15it/s]Running loglikelihood requests:  24%|██▍       | 96/400 [01:26<04:39,  1.09it/s]Running loglikelihood requests:  24%|██▍       | 98/400 [01:27<03:48,  1.32it/s]Running loglikelihood requests:  25%|██▌       | 100/400 [01:28<03:21,  1.49it/s]Running loglikelihood requests:  26%|██▌       | 102/400 [01:29<03:05,  1.60it/s]Running loglikelihood requests:  26%|██▌       | 103/400 [01:30<03:31,  1.40it/s]Running loglikelihood requests:  26%|██▌       | 104/400 [01:31<03:54,  1.26it/s]Running loglikelihood requests:  27%|██▋       | 107/400 [01:32<02:49,  1.73it/s]Running loglikelihood requests:  27%|██▋       | 108/400 [01:33<03:15,  1.49it/s]Running loglikelihood requests:  27%|██▋       | 109/400 [01:34<03:39,  1.33it/s]Running loglikelihood requests:  28%|██▊       | 110/400 [01:35<03:59,  1.21it/s]Running loglikelihood requests:  30%|██▉       | 118/400 [01:36<01:29,  3.16it/s]Running loglikelihood requests:  30%|███       | 121/400 [01:38<01:31,  3.06it/s]Running loglikelihood requests:  30%|███       | 122/400 [01:39<01:54,  2.42it/s]Running loglikelihood requests:  31%|███       | 123/400 [01:40<02:20,  1.98it/s]Running loglikelihood requests:  31%|███       | 124/400 [01:41<02:45,  1.67it/s]Running loglikelihood requests:  31%|███▏      | 125/400 [01:42<03:08,  1.46it/s]Running loglikelihood requests:  32%|███▏      | 126/400 [01:43<03:28,  1.31it/s]Running loglikelihood requests:  32%|███▏      | 127/400 [01:44<03:45,  1.21it/s]Running loglikelihood requests:  32%|███▏      | 128/400 [01:45<03:58,  1.14it/s]Running loglikelihood requests:  32%|███▎      | 130/400 [01:46<03:15,  1.38it/s]Running loglikelihood requests:  33%|███▎      | 132/400 [01:47<02:53,  1.55it/s]Running loglikelihood requests:  33%|███▎      | 133/400 [01:48<03:13,  1.38it/s]Running loglikelihood requests:  34%|███▎      | 134/400 [01:49<03:31,  1.26it/s]Running loglikelihood requests:  34%|███▍      | 135/400 [01:50<03:45,  1.17it/s]Running loglikelihood requests:  34%|███▍      | 136/400 [01:51<03:56,  1.12it/s]Running loglikelihood requests:  34%|███▍      | 137/400 [01:52<04:04,  1.07it/s]Running loglikelihood requests:  34%|███▍      | 138/400 [01:53<04:10,  1.05it/s]Running loglikelihood requests:  35%|███▍      | 139/400 [01:54<04:14,  1.02it/s]Running loglikelihood requests:  35%|███▌      | 140/400 [01:55<04:16,  1.01it/s]Running loglikelihood requests:  35%|███▌      | 141/400 [01:56<04:18,  1.00it/s]Running loglikelihood requests:  36%|███▌      | 142/400 [01:57<04:18,  1.00s/it]Running loglikelihood requests:  36%|███▌      | 143/400 [01:58<04:18,  1.01s/it]Running loglikelihood requests:  36%|███▋      | 145/400 [01:59<03:18,  1.28it/s]Running loglikelihood requests:  36%|███▋      | 146/400 [02:00<03:32,  1.19it/s]Running loglikelihood requests:  37%|███▋      | 147/400 [02:01<03:43,  1.13it/s]Running loglikelihood requests:  37%|███▋      | 148/400 [02:02<03:51,  1.09it/s]Running loglikelihood requests:  37%|███▋      | 149/400 [02:03<03:57,  1.06it/s]Running loglikelihood requests:  38%|███▊      | 150/400 [02:04<04:01,  1.03it/s]Running loglikelihood requests:  38%|███▊      | 152/400 [02:05<03:08,  1.31it/s]Running loglikelihood requests:  38%|███▊      | 153/400 [02:06<03:23,  1.21it/s]Running loglikelihood requests:  38%|███▊      | 154/400 [02:07<03:34,  1.14it/s]Running loglikelihood requests:  39%|███▉      | 155/400 [02:08<03:43,  1.10it/s]Running loglikelihood requests:  39%|███▉      | 157/400 [02:09<02:59,  1.36it/s]Running loglikelihood requests:  40%|███▉      | 158/400 [02:10<03:14,  1.25it/s]Running loglikelihood requests:  40%|████      | 160/400 [02:11<02:44,  1.46it/s]Running loglikelihood requests:  40%|████      | 161/400 [02:12<03:00,  1.32it/s]Running loglikelihood requests:  40%|████      | 162/400 [02:13<03:14,  1.22it/s]Running loglikelihood requests:  41%|████      | 163/400 [02:14<03:25,  1.15it/s]Running loglikelihood requests:  41%|████▏     | 165/400 [02:15<02:48,  1.40it/s]Running loglikelihood requests:  42%|████▏     | 166/400 [02:16<03:03,  1.27it/s]Running loglikelihood requests:  42%|████▏     | 167/400 [02:17<03:16,  1.19it/s]Running loglikelihood requests:  42%|████▏     | 168/400 [02:18<03:25,  1.13it/s]Running loglikelihood requests:  42%|████▎     | 170/400 [02:19<02:46,  1.38it/s]Running loglikelihood requests:  43%|████▎     | 171/400 [02:20<03:01,  1.26it/s]Running loglikelihood requests:  43%|████▎     | 172/400 [02:22<03:13,  1.18it/s]Running loglikelihood requests:  43%|████▎     | 173/400 [02:23<03:22,  1.12it/s]Running loglikelihood requests:  44%|████▎     | 174/400 [02:24<03:29,  1.08it/s]Running loglikelihood requests:  44%|████▍     | 175/400 [02:25<03:34,  1.05it/s]Running loglikelihood requests:  44%|████▍     | 176/400 [02:26<03:37,  1.03it/s]Running loglikelihood requests:  44%|████▍     | 178/400 [02:27<02:49,  1.31it/s]Running loglikelihood requests:  45%|████▍     | 179/400 [02:28<03:02,  1.21it/s]Running loglikelihood requests:  45%|████▌     | 180/400 [02:29<03:11,  1.15it/s]Running loglikelihood requests:  45%|████▌     | 181/400 [02:30<03:19,  1.10it/s]Running loglikelihood requests:  46%|████▌     | 183/400 [02:31<02:39,  1.36it/s]Running loglikelihood requests:  46%|████▌     | 184/400 [02:32<02:52,  1.25it/s]Running loglikelihood requests:  47%|████▋     | 187/400 [02:33<02:00,  1.76it/s]Running loglikelihood requests:  47%|████▋     | 188/400 [02:34<02:18,  1.53it/s]Running loglikelihood requests:  47%|████▋     | 189/400 [02:35<02:34,  1.37it/s]Running loglikelihood requests:  48%|████▊     | 190/400 [02:36<02:47,  1.26it/s]Running loglikelihood requests:  48%|████▊     | 191/400 [02:37<02:57,  1.18it/s]Running loglikelihood requests:  50%|████▉     | 198/400 [02:37<00:54,  3.72it/s]Running loglikelihood requests:  50%|████▉     | 199/400 [02:38<01:12,  2.75it/s]Running loglikelihood requests:  50%|█████     | 200/400 [02:39<01:32,  2.16it/s]Running loglikelihood requests:  50%|█████     | 202/400 [02:40<01:34,  2.11it/s]Running loglikelihood requests:  51%|█████     | 203/400 [02:41<01:52,  1.75it/s]Running loglikelihood requests:  51%|█████     | 204/400 [02:42<02:09,  1.52it/s]Running loglikelihood requests:  51%|█████▏    | 205/400 [02:43<02:23,  1.36it/s]Running loglikelihood requests:  52%|█████▏    | 206/400 [02:44<02:35,  1.24it/s]Running loglikelihood requests:  52%|█████▏    | 207/400 [02:45<02:45,  1.17it/s]Running loglikelihood requests:  52%|█████▎    | 210/400 [02:46<01:50,  1.72it/s]Running loglikelihood requests:  53%|█████▎    | 212/400 [02:47<01:44,  1.80it/s]Running loglikelihood requests:  53%|█████▎    | 213/400 [02:48<02:00,  1.56it/s]Running loglikelihood requests:  54%|█████▍    | 215/400 [02:49<01:49,  1.69it/s]Running loglikelihood requests:  54%|█████▍    | 216/400 [02:50<02:04,  1.48it/s]Running loglikelihood requests:  54%|█████▍    | 217/400 [02:51<02:17,  1.33it/s]Running loglikelihood requests:  55%|█████▍    | 219/400 [02:52<01:58,  1.53it/s]Running loglikelihood requests:  55%|█████▌    | 220/400 [02:53<02:11,  1.37it/s]Running loglikelihood requests:  55%|█████▌    | 221/400 [02:54<02:22,  1.26it/s]Running loglikelihood requests:  56%|█████▌    | 222/400 [02:55<02:31,  1.18it/s]Running loglikelihood requests:  56%|█████▌    | 223/400 [02:56<02:37,  1.12it/s]Running loglikelihood requests:  56%|█████▌    | 224/400 [02:57<02:42,  1.08it/s]Running loglikelihood requests:  56%|█████▋    | 225/400 [02:58<02:45,  1.06it/s]Running loglikelihood requests:  56%|█████▋    | 226/400 [02:59<02:47,  1.04it/s]Running loglikelihood requests:  57%|█████▋    | 227/400 [03:00<02:49,  1.02it/s]Running loglikelihood requests:  57%|█████▋    | 228/400 [03:01<02:49,  1.01it/s]Running loglikelihood requests:  57%|█████▋    | 229/400 [03:02<02:49,  1.01it/s]Running loglikelihood requests:  57%|█████▊    | 230/400 [03:03<02:49,  1.00it/s]Running loglikelihood requests:  58%|█████▊    | 231/400 [03:04<02:49,  1.00s/it]Running loglikelihood requests:  58%|█████▊    | 232/400 [03:05<02:48,  1.00s/it]Running loglikelihood requests:  58%|█████▊    | 233/400 [03:06<02:47,  1.00s/it]Running loglikelihood requests:  59%|█████▉    | 235/400 [03:07<02:07,  1.29it/s]Running loglikelihood requests:  59%|█████▉    | 236/400 [03:08<02:16,  1.21it/s]Running loglikelihood requests:  59%|█████▉    | 237/400 [03:09<02:22,  1.14it/s]Running loglikelihood requests:  60%|█████▉    | 238/400 [03:10<02:27,  1.10it/s]Running loglikelihood requests:  60%|█████▉    | 239/400 [03:11<02:30,  1.07it/s]Running loglikelihood requests:  60%|██████    | 240/400 [03:12<02:32,  1.05it/s]Running loglikelihood requests:  60%|██████    | 242/400 [03:13<01:58,  1.33it/s]Running loglikelihood requests:  61%|██████    | 243/400 [03:14<02:07,  1.23it/s]Running loglikelihood requests:  61%|██████    | 244/400 [03:15<02:14,  1.16it/s]Running loglikelihood requests:  61%|██████▏   | 245/400 [03:16<02:19,  1.11it/s]Running loglikelihood requests:  62%|██████▏   | 246/400 [03:17<02:23,  1.08it/s]Running loglikelihood requests:  62%|██████▏   | 247/400 [03:18<02:25,  1.05it/s]Running loglikelihood requests:  62%|██████▏   | 249/400 [03:19<01:52,  1.34it/s]Running loglikelihood requests:  62%|██████▎   | 250/400 [03:20<02:01,  1.24it/s]Running loglikelihood requests:  63%|██████▎   | 251/400 [03:21<02:07,  1.17it/s]Running loglikelihood requests:  63%|██████▎   | 252/400 [03:22<02:12,  1.12it/s]Running loglikelihood requests:  63%|██████▎   | 253/400 [03:23<02:15,  1.08it/s]Running loglikelihood requests:  64%|██████▎   | 254/400 [03:24<02:18,  1.06it/s]Running loglikelihood requests:  64%|██████▍   | 255/400 [03:25<02:19,  1.04it/s]Running loglikelihood requests:  64%|██████▍   | 256/400 [03:26<02:20,  1.03it/s]Running loglikelihood requests:  64%|██████▍   | 257/400 [03:27<02:20,  1.02it/s]Running loglikelihood requests:  64%|██████▍   | 258/400 [03:28<02:20,  1.01it/s]Running loglikelihood requests:  65%|██████▌   | 260/400 [03:29<01:46,  1.31it/s]Running loglikelihood requests:  65%|██████▌   | 261/400 [03:30<01:54,  1.22it/s]Running loglikelihood requests:  66%|██████▌   | 264/400 [03:31<01:17,  1.75it/s]Running loglikelihood requests:  66%|██████▋   | 265/400 [03:32<01:28,  1.53it/s]Running loglikelihood requests:  66%|██████▋   | 266/400 [03:33<01:37,  1.37it/s]Running loglikelihood requests:  67%|██████▋   | 267/400 [03:34<01:45,  1.26it/s]Running loglikelihood requests:  67%|██████▋   | 268/400 [03:35<01:51,  1.18it/s]Running loglikelihood requests:  67%|██████▋   | 269/400 [03:36<01:56,  1.13it/s]Running loglikelihood requests:  68%|██████▊   | 270/400 [03:37<01:59,  1.09it/s]Running loglikelihood requests:  70%|███████   | 281/400 [03:38<00:27,  4.30it/s]Running loglikelihood requests:  71%|███████   | 284/400 [03:39<00:30,  3.85it/s]Running loglikelihood requests:  71%|███████▏  | 285/400 [03:40<00:38,  2.98it/s]Running loglikelihood requests:  72%|███████▏  | 286/400 [03:41<00:47,  2.39it/s]Running loglikelihood requests:  72%|███████▏  | 287/400 [03:42<00:57,  1.97it/s]Running loglikelihood requests:  72%|███████▏  | 288/400 [03:43<01:06,  1.68it/s]Running loglikelihood requests:  72%|███████▏  | 289/400 [03:44<01:14,  1.48it/s]Running loglikelihood requests:  72%|███████▎  | 290/400 [03:45<01:21,  1.34it/s]Running loglikelihood requests:  73%|███████▎  | 291/400 [03:46<01:27,  1.24it/s]Running loglikelihood requests:  73%|███████▎  | 293/400 [03:47<01:12,  1.48it/s]Running loglikelihood requests:  74%|███████▎  | 294/400 [03:48<01:19,  1.34it/s]Running loglikelihood requests:  74%|███████▍  | 295/400 [03:49<01:24,  1.24it/s]Running loglikelihood requests:  74%|███████▍  | 296/400 [03:50<01:28,  1.18it/s]Running loglikelihood requests:  74%|███████▍  | 297/400 [03:51<01:31,  1.13it/s]Running loglikelihood requests:  75%|███████▍  | 299/400 [03:52<01:12,  1.40it/s]Running loglikelihood requests:  75%|███████▌  | 300/400 [03:53<01:17,  1.28it/s]Running loglikelihood requests:  75%|███████▌  | 301/400 [03:54<01:22,  1.20it/s]Running loglikelihood requests:  76%|███████▌  | 302/400 [03:55<01:25,  1.15it/s]Running loglikelihood requests:  76%|███████▌  | 304/400 [03:56<01:07,  1.41it/s]Running loglikelihood requests:  76%|███████▋  | 305/400 [03:57<01:13,  1.30it/s]Running loglikelihood requests:  77%|███████▋  | 307/400 [03:58<01:01,  1.52it/s]Running loglikelihood requests:  77%|███████▋  | 309/400 [03:59<00:54,  1.68it/s]Running loglikelihood requests:  78%|███████▊  | 310/400 [04:00<01:00,  1.48it/s]Running loglikelihood requests:  78%|███████▊  | 311/400 [04:01<01:06,  1.34it/s]Running loglikelihood requests:  78%|███████▊  | 312/400 [04:02<01:10,  1.25it/s]Running loglikelihood requests:  78%|███████▊  | 313/400 [04:03<01:13,  1.18it/s]Running loglikelihood requests:  79%|███████▉  | 315/400 [04:04<00:59,  1.44it/s]Running loglikelihood requests:  79%|███████▉  | 317/400 [04:05<00:51,  1.62it/s]Running loglikelihood requests:  80%|███████▉  | 318/400 [04:06<00:57,  1.44it/s]Running loglikelihood requests:  80%|███████▉  | 319/400 [04:07<01:01,  1.31it/s]Running loglikelihood requests:  80%|████████  | 321/400 [04:08<00:51,  1.53it/s]Running loglikelihood requests:  81%|████████  | 323/400 [04:09<00:45,  1.68it/s]Running loglikelihood requests:  82%|████████▏ | 326/400 [04:10<00:35,  2.10it/s]Running loglikelihood requests:  82%|████████▏ | 327/400 [04:11<00:41,  1.78it/s]Running loglikelihood requests:  82%|████████▏ | 328/400 [04:11<00:46,  1.55it/s]Running loglikelihood requests:  82%|████████▏ | 329/400 [04:12<00:50,  1.39it/s]Running loglikelihood requests:  82%|████████▎ | 330/400 [04:13<00:54,  1.28it/s]Running loglikelihood requests:  83%|████████▎ | 332/400 [04:14<00:45,  1.51it/s]Running loglikelihood requests:  83%|████████▎ | 333/400 [04:15<00:49,  1.36it/s]Running loglikelihood requests:  84%|████████▎ | 334/400 [04:16<00:52,  1.26it/s]Running loglikelihood requests:  84%|████████▍ | 335/400 [04:17<00:54,  1.19it/s]Running loglikelihood requests:  84%|████████▍ | 336/400 [04:18<00:56,  1.14it/s]Running loglikelihood requests:  84%|████████▍ | 337/400 [04:19<00:57,  1.10it/s]Running loglikelihood requests:  84%|████████▍ | 338/400 [04:20<00:57,  1.08it/s]Running loglikelihood requests:  85%|████████▍ | 339/400 [04:21<00:57,  1.06it/s]Running loglikelihood requests:  85%|████████▌ | 340/400 [04:22<00:57,  1.05it/s]Running loglikelihood requests:  85%|████████▌ | 341/400 [04:23<00:55,  1.07it/s]Running loglikelihood requests:  86%|████████▌ | 342/400 [04:24<00:53,  1.08it/s]Running loglikelihood requests:  86%|████████▌ | 343/400 [04:25<00:52,  1.10it/s]Running loglikelihood requests:  86%|████████▋ | 346/400 [04:26<00:30,  1.76it/s]Running loglikelihood requests:  87%|████████▋ | 348/400 [04:27<00:27,  1.91it/s]Running loglikelihood requests:  87%|████████▋ | 349/400 [04:28<00:30,  1.68it/s]Running loglikelihood requests:  88%|████████▊ | 350/400 [04:28<00:33,  1.51it/s]Running loglikelihood requests:  88%|████████▊ | 351/400 [04:29<00:35,  1.40it/s]Running loglikelihood requests:  88%|████████▊ | 352/400 [04:30<00:36,  1.32it/s]Running loglikelihood requests:  88%|████████▊ | 353/400 [04:31<00:37,  1.26it/s]Running loglikelihood requests:  88%|████████▊ | 354/400 [04:32<00:37,  1.22it/s]Running loglikelihood requests:  89%|████████▉ | 355/400 [04:33<00:37,  1.19it/s]Running loglikelihood requests:  89%|████████▉ | 356/400 [04:34<00:37,  1.18it/s]Running loglikelihood requests:  90%|████████▉ | 358/400 [04:35<00:27,  1.50it/s]Running loglikelihood requests:  90%|████████▉ | 359/400 [04:36<00:29,  1.39it/s]Running loglikelihood requests:  90%|█████████ | 360/400 [04:36<00:30,  1.31it/s]Running loglikelihood requests:  90%|█████████ | 361/400 [04:37<00:31,  1.26it/s]Running loglikelihood requests:  90%|█████████ | 362/400 [04:38<00:31,  1.22it/s]Running loglikelihood requests:  93%|█████████▎| 373/400 [04:39<00:06,  4.49it/s]Running loglikelihood requests:  94%|█████████▍| 375/400 [04:40<00:06,  3.83it/s]Running loglikelihood requests:  94%|█████████▍| 376/400 [04:41<00:07,  3.03it/s]Running loglikelihood requests:  95%|█████████▍| 379/400 [04:42<00:06,  3.14it/s]Running loglikelihood requests:  95%|█████████▌| 381/400 [04:43<00:06,  2.89it/s]Running loglikelihood requests:  96%|█████████▌| 382/400 [04:44<00:07,  2.36it/s]Running loglikelihood requests:  96%|█████████▌| 384/400 [04:44<00:06,  2.34it/s]Running loglikelihood requests:  96%|█████████▋| 386/400 [04:45<00:06,  2.32it/s]Running loglikelihood requests:  97%|█████████▋| 387/400 [04:46<00:06,  1.96it/s]Running loglikelihood requests:  97%|█████████▋| 389/400 [04:47<00:05,  2.06it/s]Running loglikelihood requests:  98%|█████████▊| 391/400 [04:48<00:04,  2.12it/s]Running loglikelihood requests:  98%|█████████▊| 393/400 [04:49<00:03,  2.17it/s]Running loglikelihood requests:  99%|█████████▉| 395/400 [04:50<00:02,  2.20it/s]Running loglikelihood requests:  99%|█████████▉| 397/400 [04:51<00:01,  2.23it/s]Running loglikelihood requests: 100%|█████████▉| 398/400 [04:51<00:01,  1.90it/s]Running loglikelihood requests: 100%|██████████| 400/400 [04:52<00:00,  2.01it/s]Running loglikelihood requests: 100%|██████████| 400/400 [04:52<00:00,  1.37it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:0'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}
full model:
{'mastermind_24_easy': {'alias': 'mastermind_24_easy', 'acc,none': 0.33, 'acc_stderr,none': 0.04725815626252609}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9705388716727775
0.9880227828623074
0.982119607496995
0.9941469535767887
0.984917378949356
0.9883609317461776
0.9752442385272352
0.9858149677985593
0.9966001900863091
0.9954965780195978
0.9982374916142641
0.987839947084121
0.9740425263242771
0.9802324544963317
0.9965978314947238
0.9890806289155061
0.971808392501808
0.9767123038440666
0.9820534501654181
0.9672299205809463
0.9753579231968917
0.9973837437819523
0.9951994747100236
0.9465102818948206
0.9757933184251313
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[7, 0, 1, 3, 6, 5, 4, 2]
tensor([7, 0, 1, 3, 6, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 0, 1, 3, 7, 5, 4, 2]
tensor([6, 0, 1, 3, 7, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 0, 1, 3, 6, 5, 4, 2]
tensor([7, 0, 1, 3, 6, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 0, 1, 3, 7, 5, 4, 2]
tensor([6, 0, 1, 3, 7, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 0, 1, 3, 6, 5, 4, 2]
tensor([7, 0, 1, 3, 6, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 0, 2, 3, 6, 5, 4, 1]
tensor([7, 0, 2, 3, 6, 5, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 3
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Normal merging for layer 4
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 5
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 6 to 31
done!
all done!
Model size: 12.0718 GB
204
cuda:0
openbookqa
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:39<00:39, 39.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 24.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.89s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa HTTP/1.1" 307 67
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa HTTP/1.1" 200 1409
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/openbookqa/openbookqa.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa HTTP/1.1" 307 67
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa HTTP/1.1" 200 1409
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/openbookqa/resolve/388097ea7776314e93a529163e0fea805b8a6454/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/openbookqa/resolve/388097ea7776314e93a529163e0fea805b8a6454/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa/revision/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 117
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa/revision/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 1409
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa/tree/388097ea7776314e93a529163e0fea805b8a6454?recursive=False&expand=False HTTP/1.1" 307 142
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa/tree/388097ea7776314e93a529163e0fea805b8a6454?recursive=False&expand=False HTTP/1.1" 200 390
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa/tree/388097ea7776314e93a529163e0fea805b8a6454/additional?recursive=False&expand=False HTTP/1.1" 307 153
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa/tree/388097ea7776314e93a529163e0fea805b8a6454/additional?recursive=False&expand=False HTTP/1.1" 200 363
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa/revision/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 117
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa/revision/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 1409
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/openbookqa/resolve/388097ea7776314e93a529163e0fea805b8a6454/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/openbookqa/resolve/388097ea7776314e93a529163e0fea805b8a6454/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa/tree/388097ea7776314e93a529163e0fea805b8a6454/main?recursive=False&expand=False HTTP/1.1" 307 147
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa/tree/388097ea7776314e93a529163e0fea805b8a6454/main?recursive=False&expand=False HTTP/1.1" 200 359
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140321884477424 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_openbookqa_main_0.0.0_388097ea7776314e93a529163e0fea805b8a6454.lock
DEBUG:filelock:Lock 140321884477424 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_openbookqa_main_0.0.0_388097ea7776314e93a529163e0fea805b8a6454.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321884477424 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_openbookqa_main_0.0.0_388097ea7776314e93a529163e0fea805b8a6454.lock
DEBUG:filelock:Lock 140321884477424 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_openbookqa_main_0.0.0_388097ea7776314e93a529163e0fea805b8a6454.lock
DEBUG:filelock:Attempting to acquire lock 140321081601504 on /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454_builder.lock
DEBUG:filelock:Lock 140321081601504 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321081601504 on /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454_builder.lock
DEBUG:filelock:Lock 140321081601504 released on /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of openbookqa from None to 0
INFO:lm_eval.api.task:Building contexts for openbookqa on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2586.43it/s]
DEBUG:lm_eval.evaluator:Task: openbookqa; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:01<08:33,  1.29s/it]Running loglikelihood requests:   0%|          | 2/400 [00:02<06:33,  1.01it/s]Running loglikelihood requests:   1%|          | 3/400 [00:02<05:50,  1.13it/s]Running loglikelihood requests:   1%|          | 4/400 [00:03<05:29,  1.20it/s]Running loglikelihood requests:   1%|▏         | 5/400 [00:04<04:58,  1.32it/s]Running loglikelihood requests:   2%|▏         | 6/400 [00:04<04:39,  1.41it/s]Running loglikelihood requests:   2%|▏         | 7/400 [00:05<04:25,  1.48it/s]Running loglikelihood requests:   2%|▏         | 8/400 [00:06<04:16,  1.53it/s]Running loglikelihood requests:   2%|▏         | 9/400 [00:06<04:08,  1.57it/s]Running loglikelihood requests:   2%|▎         | 10/400 [00:07<04:03,  1.60it/s]Running loglikelihood requests:   3%|▎         | 11/400 [00:07<03:59,  1.63it/s]Running loglikelihood requests:   3%|▎         | 12/400 [00:08<03:56,  1.64it/s]Running loglikelihood requests:   3%|▎         | 13/400 [00:09<03:53,  1.66it/s]Running loglikelihood requests:   4%|▍         | 15/400 [00:09<02:57,  2.17it/s]Running loglikelihood requests:   4%|▍         | 16/400 [00:10<03:09,  2.02it/s]Running loglikelihood requests:   4%|▍         | 17/400 [00:10<03:18,  1.93it/s]Running loglikelihood requests:   4%|▍         | 18/400 [00:11<03:24,  1.87it/s]Running loglikelihood requests:   5%|▍         | 19/400 [00:11<03:28,  1.83it/s]Running loglikelihood requests:   5%|▌         | 20/400 [00:12<03:30,  1.81it/s]Running loglikelihood requests:   5%|▌         | 21/400 [00:13<03:31,  1.79it/s]Running loglikelihood requests:   6%|▌         | 22/400 [00:13<03:31,  1.78it/s]Running loglikelihood requests:   6%|▌         | 23/400 [00:14<03:31,  1.78it/s]Running loglikelihood requests:   6%|▌         | 24/400 [00:14<03:31,  1.78it/s]Running loglikelihood requests:   6%|▋         | 25/400 [00:15<03:31,  1.77it/s]Running loglikelihood requests:   6%|▋         | 26/400 [00:15<03:34,  1.74it/s]Running loglikelihood requests:   7%|▋         | 27/400 [00:16<03:32,  1.75it/s]Running loglikelihood requests:   7%|▋         | 28/400 [00:17<03:31,  1.76it/s]Running loglikelihood requests:   7%|▋         | 29/400 [00:17<03:29,  1.77it/s]Running loglikelihood requests:   8%|▊         | 33/400 [00:18<01:49,  3.36it/s]Running loglikelihood requests:   8%|▊         | 34/400 [00:18<02:06,  2.89it/s]Running loglikelihood requests:   9%|▉         | 35/400 [00:19<02:19,  2.61it/s]Running loglikelihood requests:   9%|▉         | 36/400 [00:19<02:31,  2.40it/s]Running loglikelihood requests:   9%|▉         | 37/400 [00:20<02:40,  2.26it/s]Running loglikelihood requests:  10%|▉         | 38/400 [00:20<02:47,  2.16it/s]Running loglikelihood requests:  10%|▉         | 39/400 [00:21<02:51,  2.10it/s]Running loglikelihood requests:  10%|█         | 40/400 [00:21<02:55,  2.06it/s]Running loglikelihood requests:  10%|█         | 41/400 [00:22<02:56,  2.03it/s]Running loglikelihood requests:  10%|█         | 42/400 [00:22<02:57,  2.01it/s]Running loglikelihood requests:  11%|█         | 43/400 [00:23<02:58,  2.00it/s]Running loglikelihood requests:  11%|█         | 44/400 [00:23<02:58,  1.99it/s]Running loglikelihood requests:  11%|█▏        | 45/400 [00:24<02:59,  1.98it/s]Running loglikelihood requests:  15%|█▌        | 61/400 [00:24<00:29, 11.58it/s]Running loglikelihood requests:  16%|█▌        | 63/400 [00:25<00:47,  7.09it/s]Running loglikelihood requests:  16%|█▋        | 65/400 [00:26<01:06,  5.05it/s]Running loglikelihood requests:  16%|█▋        | 66/400 [00:27<01:16,  4.38it/s]Running loglikelihood requests:  17%|█▋        | 67/400 [00:27<01:27,  3.81it/s]Running loglikelihood requests:  17%|█▋        | 68/400 [00:28<01:40,  3.32it/s]Running loglikelihood requests:  17%|█▋        | 69/400 [00:28<01:51,  2.98it/s]Running loglikelihood requests:  18%|█▊        | 70/400 [00:29<02:01,  2.72it/s]Running loglikelihood requests:  18%|█▊        | 71/400 [00:29<02:10,  2.52it/s]Running loglikelihood requests:  18%|█▊        | 72/400 [00:30<02:19,  2.35it/s]Running loglikelihood requests:  18%|█▊        | 73/400 [00:30<02:24,  2.27it/s]Running loglikelihood requests:  18%|█▊        | 74/400 [00:31<02:27,  2.21it/s]Running loglikelihood requests:  19%|█▉        | 75/400 [00:31<02:30,  2.17it/s]Running loglikelihood requests:  19%|█▉        | 76/400 [00:32<02:31,  2.13it/s]Running loglikelihood requests:  19%|█▉        | 77/400 [00:32<02:33,  2.11it/s]Running loglikelihood requests:  20%|█▉        | 78/400 [00:33<02:33,  2.09it/s]Running loglikelihood requests:  20%|█▉        | 79/400 [00:33<02:33,  2.09it/s]Running loglikelihood requests:  20%|██        | 81/400 [00:34<01:57,  2.70it/s]Running loglikelihood requests:  20%|██        | 82/400 [00:34<02:06,  2.52it/s]Running loglikelihood requests:  21%|██        | 83/400 [00:35<02:12,  2.38it/s]Running loglikelihood requests:  21%|██        | 84/400 [00:35<02:17,  2.29it/s]Running loglikelihood requests:  22%|██▏       | 86/400 [00:36<01:50,  2.85it/s]Running loglikelihood requests:  22%|██▏       | 87/400 [00:36<01:59,  2.63it/s]Running loglikelihood requests:  22%|██▏       | 88/400 [00:37<02:06,  2.46it/s]Running loglikelihood requests:  22%|██▏       | 89/400 [00:37<02:12,  2.35it/s]Running loglikelihood requests:  22%|██▎       | 90/400 [00:37<02:16,  2.28it/s]Running loglikelihood requests:  23%|██▎       | 91/400 [00:38<02:19,  2.22it/s]Running loglikelihood requests:  23%|██▎       | 92/400 [00:38<02:21,  2.17it/s]Running loglikelihood requests:  23%|██▎       | 93/400 [00:39<02:22,  2.15it/s]Running loglikelihood requests:  24%|██▎       | 94/400 [00:39<02:23,  2.13it/s]Running loglikelihood requests:  24%|██▍       | 95/400 [00:40<02:24,  2.12it/s]Running loglikelihood requests:  24%|██▍       | 96/400 [00:40<02:24,  2.11it/s]Running loglikelihood requests:  24%|██▍       | 97/400 [00:41<02:24,  2.10it/s]Running loglikelihood requests:  24%|██▍       | 98/400 [00:41<02:24,  2.09it/s]Running loglikelihood requests:  25%|██▍       | 99/400 [00:42<02:24,  2.09it/s]Running loglikelihood requests:  25%|██▌       | 100/400 [00:42<02:23,  2.08it/s]Running loglikelihood requests:  25%|██▌       | 101/400 [00:43<02:23,  2.09it/s]Running loglikelihood requests:  26%|██▌       | 102/400 [00:43<02:22,  2.09it/s]Running loglikelihood requests:  26%|██▌       | 103/400 [00:44<02:21,  2.10it/s]Running loglikelihood requests:  26%|██▋       | 106/400 [00:44<01:27,  3.37it/s]Running loglikelihood requests:  27%|██▋       | 107/400 [00:45<01:37,  2.99it/s]Running loglikelihood requests:  27%|██▋       | 108/400 [00:45<01:47,  2.73it/s]Running loglikelihood requests:  27%|██▋       | 109/400 [00:46<01:54,  2.53it/s]Running loglikelihood requests:  28%|██▊       | 110/400 [00:46<02:00,  2.41it/s]Running loglikelihood requests:  28%|██▊       | 111/400 [00:47<02:04,  2.31it/s]Running loglikelihood requests:  28%|██▊       | 112/400 [00:47<02:08,  2.25it/s]Running loglikelihood requests:  28%|██▊       | 113/400 [00:48<02:10,  2.21it/s]Running loglikelihood requests:  28%|██▊       | 114/400 [00:48<02:11,  2.17it/s]Running loglikelihood requests:  29%|██▉       | 115/400 [00:48<02:11,  2.16it/s]Running loglikelihood requests:  29%|██▉       | 116/400 [00:49<02:11,  2.16it/s]Running loglikelihood requests:  29%|██▉       | 117/400 [00:49<02:11,  2.15it/s]Running loglikelihood requests:  30%|██▉       | 118/400 [00:50<02:11,  2.15it/s]Running loglikelihood requests:  30%|██▉       | 119/400 [00:50<02:10,  2.15it/s]Running loglikelihood requests:  30%|███       | 120/400 [00:51<02:11,  2.14it/s]Running loglikelihood requests:  30%|███       | 121/400 [00:51<02:10,  2.14it/s]Running loglikelihood requests:  30%|███       | 122/400 [00:52<02:10,  2.13it/s]Running loglikelihood requests:  31%|███       | 123/400 [00:52<02:09,  2.13it/s]Running loglikelihood requests:  31%|███       | 124/400 [00:53<02:09,  2.13it/s]Running loglikelihood requests:  31%|███▏      | 125/400 [00:53<02:09,  2.12it/s]Running loglikelihood requests:  32%|███▏      | 126/400 [00:54<02:09,  2.12it/s]Running loglikelihood requests:  32%|███▏      | 127/400 [00:54<02:08,  2.13it/s]Running loglikelihood requests:  32%|███▏      | 128/400 [00:55<02:08,  2.12it/s]Running loglikelihood requests:  32%|███▏      | 129/400 [00:55<02:08,  2.12it/s]Running loglikelihood requests:  32%|███▎      | 130/400 [00:55<02:07,  2.13it/s]Running loglikelihood requests:  33%|███▎      | 131/400 [00:56<02:06,  2.13it/s]Running loglikelihood requests:  33%|███▎      | 132/400 [00:56<02:05,  2.14it/s]Running loglikelihood requests:  33%|███▎      | 133/400 [00:57<02:04,  2.14it/s]Running loglikelihood requests:  34%|███▎      | 134/400 [00:57<02:04,  2.14it/s]Running loglikelihood requests:  34%|███▍      | 138/400 [00:58<01:04,  4.05it/s]Running loglikelihood requests:  35%|███▍      | 139/400 [00:58<01:15,  3.47it/s]Running loglikelihood requests:  35%|███▌      | 140/400 [00:59<01:24,  3.07it/s]Running loglikelihood requests:  35%|███▌      | 141/400 [00:59<01:32,  2.80it/s]Running loglikelihood requests:  36%|███▌      | 142/400 [01:00<01:39,  2.60it/s]Running loglikelihood requests:  36%|███▌      | 143/400 [01:00<01:44,  2.46it/s]Running loglikelihood requests:  36%|███▌      | 144/400 [01:01<01:48,  2.36it/s]Running loglikelihood requests:  36%|███▋      | 145/400 [01:01<01:50,  2.30it/s]Running loglikelihood requests:  37%|███▋      | 147/400 [01:02<01:27,  2.90it/s]Running loglikelihood requests:  37%|███▋      | 148/400 [01:02<01:33,  2.68it/s]Running loglikelihood requests:  37%|███▋      | 149/400 [01:02<01:38,  2.54it/s]Running loglikelihood requests:  38%|███▊      | 151/400 [01:03<01:20,  3.08it/s]Running loglikelihood requests:  38%|███▊      | 152/400 [01:03<01:28,  2.80it/s]Running loglikelihood requests:  38%|███▊      | 153/400 [01:04<01:34,  2.62it/s]Running loglikelihood requests:  38%|███▊      | 154/400 [01:04<01:38,  2.49it/s]Running loglikelihood requests:  39%|███▉      | 155/400 [01:05<01:42,  2.39it/s]Running loglikelihood requests:  39%|███▉      | 156/400 [01:05<01:44,  2.33it/s]Running loglikelihood requests:  39%|███▉      | 157/400 [01:06<01:46,  2.29it/s]Running loglikelihood requests:  40%|███▉      | 158/400 [01:06<01:47,  2.25it/s]Running loglikelihood requests:  40%|███▉      | 159/400 [01:07<01:47,  2.23it/s]Running loglikelihood requests:  40%|████      | 160/400 [01:07<01:48,  2.22it/s]Running loglikelihood requests:  40%|████      | 161/400 [01:08<01:48,  2.21it/s]Running loglikelihood requests:  40%|████      | 162/400 [01:08<01:48,  2.20it/s]Running loglikelihood requests:  41%|████      | 163/400 [01:08<01:47,  2.20it/s]Running loglikelihood requests:  42%|████▏     | 166/400 [01:09<01:06,  3.52it/s]Running loglikelihood requests:  42%|████▏     | 167/400 [01:09<01:14,  3.12it/s]Running loglikelihood requests:  43%|████▎     | 171/400 [01:10<00:47,  4.82it/s]Running loglikelihood requests:  43%|████▎     | 172/400 [01:10<00:56,  4.04it/s]Running loglikelihood requests:  43%|████▎     | 173/400 [01:11<01:05,  3.48it/s]Running loglikelihood requests:  44%|████▎     | 174/400 [01:11<01:13,  3.10it/s]Running loglikelihood requests:  44%|████▍     | 175/400 [01:12<01:19,  2.83it/s]Running loglikelihood requests:  44%|████▍     | 176/400 [01:12<01:24,  2.65it/s]Running loglikelihood requests:  44%|████▍     | 177/400 [01:13<01:28,  2.52it/s]Running loglikelihood requests:  44%|████▍     | 178/400 [01:13<01:31,  2.44it/s]Running loglikelihood requests:  45%|████▍     | 179/400 [01:13<01:33,  2.36it/s]Running loglikelihood requests:  45%|████▌     | 180/400 [01:14<01:34,  2.32it/s]Running loglikelihood requests:  45%|████▌     | 181/400 [01:14<01:35,  2.29it/s]Running loglikelihood requests:  46%|████▌     | 182/400 [01:15<01:36,  2.26it/s]Running loglikelihood requests:  46%|████▌     | 183/400 [01:15<01:36,  2.24it/s]Running loglikelihood requests:  46%|████▌     | 184/400 [01:16<01:36,  2.25it/s]Running loglikelihood requests:  46%|████▋     | 185/400 [01:16<01:35,  2.24it/s]Running loglikelihood requests:  46%|████▋     | 186/400 [01:17<01:35,  2.24it/s]Running loglikelihood requests:  47%|████▋     | 187/400 [01:17<01:35,  2.24it/s]Running loglikelihood requests:  47%|████▋     | 188/400 [01:17<01:34,  2.24it/s]Running loglikelihood requests:  48%|████▊     | 190/400 [01:18<01:11,  2.92it/s]Running loglikelihood requests:  48%|████▊     | 192/400 [01:18<01:01,  3.39it/s]Running loglikelihood requests:  48%|████▊     | 193/400 [01:19<01:08,  3.04it/s]Running loglikelihood requests:  48%|████▊     | 194/400 [01:19<01:16,  2.71it/s]Running loglikelihood requests:  49%|████▉     | 195/400 [01:20<01:20,  2.56it/s]Running loglikelihood requests:  49%|████▉     | 196/400 [01:20<01:23,  2.46it/s]Running loglikelihood requests:  49%|████▉     | 197/400 [01:21<01:25,  2.38it/s]Running loglikelihood requests:  50%|████▉     | 198/400 [01:21<01:27,  2.32it/s]Running loglikelihood requests:  50%|█████     | 202/400 [01:22<00:46,  4.28it/s]Running loglikelihood requests:  51%|█████     | 203/400 [01:22<00:53,  3.68it/s]Running loglikelihood requests:  51%|█████     | 204/400 [01:22<00:59,  3.27it/s]Running loglikelihood requests:  51%|█████▏    | 205/400 [01:23<01:05,  2.98it/s]Running loglikelihood requests:  52%|█████▏    | 206/400 [01:23<01:10,  2.76it/s]Running loglikelihood requests:  52%|█████▏    | 207/400 [01:24<01:14,  2.60it/s]Running loglikelihood requests:  52%|█████▏    | 209/400 [01:24<00:59,  3.19it/s]Running loglikelihood requests:  52%|█████▎    | 210/400 [01:25<01:05,  2.91it/s]Running loglikelihood requests:  56%|█████▌    | 224/400 [01:25<00:13, 12.88it/s]Running loglikelihood requests:  56%|█████▋    | 226/400 [01:26<00:22,  7.71it/s]Running loglikelihood requests:  57%|█████▋    | 228/400 [01:27<00:31,  5.50it/s]Running loglikelihood requests:  57%|█████▋    | 229/400 [01:27<00:35,  4.79it/s]Running loglikelihood requests:  57%|█████▊    | 230/400 [01:28<00:40,  4.17it/s]Running loglikelihood requests:  58%|█████▊    | 231/400 [01:28<00:46,  3.67it/s]Running loglikelihood requests:  58%|█████▊    | 232/400 [01:28<00:50,  3.30it/s]Running loglikelihood requests:  58%|█████▊    | 233/400 [01:29<00:55,  3.02it/s]Running loglikelihood requests:  58%|█████▊    | 234/400 [01:29<00:59,  2.81it/s]Running loglikelihood requests:  59%|█████▉    | 235/400 [01:30<01:03,  2.58it/s]Running loglikelihood requests:  59%|█████▉    | 237/400 [01:30<00:51,  3.16it/s]Running loglikelihood requests:  60%|█████▉    | 239/400 [01:31<00:44,  3.60it/s]Running loglikelihood requests:  60%|██████    | 240/400 [01:31<00:49,  3.22it/s]Running loglikelihood requests:  60%|██████    | 241/400 [01:32<00:53,  2.96it/s]Running loglikelihood requests:  60%|██████    | 242/400 [01:32<00:57,  2.77it/s]Running loglikelihood requests:  61%|██████    | 243/400 [01:32<00:59,  2.63it/s]Running loglikelihood requests:  61%|██████    | 244/400 [01:33<01:01,  2.53it/s]Running loglikelihood requests:  61%|██████▏   | 245/400 [01:33<01:03,  2.45it/s]Running loglikelihood requests:  62%|██████▏   | 246/400 [01:34<01:04,  2.40it/s]Running loglikelihood requests:  62%|██████▏   | 247/400 [01:34<01:04,  2.35it/s]Running loglikelihood requests:  62%|██████▏   | 248/400 [01:35<01:04,  2.34it/s]Running loglikelihood requests:  62%|██████▏   | 249/400 [01:35<01:04,  2.33it/s]Running loglikelihood requests:  62%|██████▎   | 250/400 [01:35<01:04,  2.32it/s]Running loglikelihood requests:  63%|██████▎   | 251/400 [01:36<01:04,  2.31it/s]Running loglikelihood requests:  63%|██████▎   | 252/400 [01:36<01:04,  2.31it/s]Running loglikelihood requests:  63%|██████▎   | 253/400 [01:37<01:03,  2.30it/s]Running loglikelihood requests:  64%|██████▎   | 254/400 [01:37<01:03,  2.32it/s]Running loglikelihood requests:  64%|██████▍   | 255/400 [01:38<01:02,  2.33it/s]Running loglikelihood requests:  64%|██████▍   | 256/400 [01:38<01:01,  2.35it/s]Running loglikelihood requests:  64%|██████▍   | 257/400 [01:38<01:00,  2.34it/s]Running loglikelihood requests:  64%|██████▍   | 258/400 [01:39<01:00,  2.34it/s]Running loglikelihood requests:  65%|██████▍   | 259/400 [01:39<01:00,  2.34it/s]Running loglikelihood requests:  65%|██████▌   | 260/400 [01:40<00:59,  2.34it/s]Running loglikelihood requests:  65%|██████▌   | 261/400 [01:40<00:59,  2.33it/s]Running loglikelihood requests:  66%|██████▌   | 262/400 [01:41<00:59,  2.33it/s]Running loglikelihood requests:  66%|██████▌   | 263/400 [01:41<00:58,  2.36it/s]Running loglikelihood requests:  66%|██████▋   | 265/400 [01:41<00:43,  3.08it/s]Running loglikelihood requests:  66%|██████▋   | 266/400 [01:42<00:46,  2.87it/s]Running loglikelihood requests:  67%|██████▋   | 267/400 [01:42<00:49,  2.71it/s]Running loglikelihood requests:  67%|██████▋   | 268/400 [01:43<00:50,  2.61it/s]Running loglikelihood requests:  67%|██████▋   | 269/400 [01:43<00:51,  2.54it/s]Running loglikelihood requests:  68%|██████▊   | 270/400 [01:44<00:51,  2.51it/s]Running loglikelihood requests:  68%|██████▊   | 271/400 [01:44<00:52,  2.47it/s]Running loglikelihood requests:  68%|██████▊   | 272/400 [01:44<00:52,  2.44it/s]Running loglikelihood requests:  68%|██████▊   | 273/400 [01:45<00:52,  2.44it/s]Running loglikelihood requests:  68%|██████▊   | 274/400 [01:45<00:52,  2.41it/s]Running loglikelihood requests:  69%|██████▉   | 275/400 [01:46<00:52,  2.38it/s]Running loglikelihood requests:  69%|██████▉   | 276/400 [01:46<00:52,  2.37it/s]Running loglikelihood requests:  69%|██████▉   | 277/400 [01:47<00:52,  2.36it/s]Running loglikelihood requests:  70%|██████▉   | 278/400 [01:47<00:51,  2.37it/s]Running loglikelihood requests:  70%|██████▉   | 279/400 [01:47<00:51,  2.36it/s]Running loglikelihood requests:  70%|███████   | 280/400 [01:48<00:51,  2.35it/s]Running loglikelihood requests:  70%|███████   | 281/400 [01:48<00:50,  2.37it/s]Running loglikelihood requests:  70%|███████   | 282/400 [01:49<00:49,  2.37it/s]Running loglikelihood requests:  71%|███████   | 283/400 [01:49<00:48,  2.39it/s]Running loglikelihood requests:  71%|███████   | 284/400 [01:49<00:48,  2.40it/s]Running loglikelihood requests:  71%|███████▏  | 285/400 [01:50<00:47,  2.40it/s]Running loglikelihood requests:  72%|███████▏  | 286/400 [01:50<00:47,  2.41it/s]Running loglikelihood requests:  72%|███████▏  | 287/400 [01:51<00:46,  2.41it/s]Running loglikelihood requests:  72%|███████▏  | 288/400 [01:51<00:46,  2.41it/s]Running loglikelihood requests:  72%|███████▏  | 289/400 [01:52<00:45,  2.42it/s]Running loglikelihood requests:  72%|███████▎  | 290/400 [01:52<00:45,  2.44it/s]Running loglikelihood requests:  73%|███████▎  | 291/400 [01:52<00:44,  2.45it/s]Running loglikelihood requests:  73%|███████▎  | 292/400 [01:53<00:44,  2.44it/s]Running loglikelihood requests:  73%|███████▎  | 293/400 [01:53<00:44,  2.43it/s]Running loglikelihood requests:  74%|███████▎  | 294/400 [01:54<00:43,  2.42it/s]Running loglikelihood requests:  74%|███████▍  | 297/400 [01:54<00:26,  3.91it/s]Running loglikelihood requests:  74%|███████▍  | 298/400 [01:54<00:29,  3.43it/s]Running loglikelihood requests:  75%|███████▍  | 299/400 [01:55<00:32,  3.12it/s]Running loglikelihood requests:  75%|███████▌  | 300/400 [01:55<00:34,  2.91it/s]Running loglikelihood requests:  75%|███████▌  | 301/400 [01:56<00:35,  2.75it/s]Running loglikelihood requests:  76%|███████▌  | 302/400 [01:56<00:37,  2.65it/s]Running loglikelihood requests:  76%|███████▌  | 303/400 [01:56<00:38,  2.55it/s]Running loglikelihood requests:  77%|███████▋  | 307/400 [01:57<00:20,  4.65it/s]Running loglikelihood requests:  77%|███████▋  | 308/400 [01:57<00:23,  3.98it/s]Running loglikelihood requests:  77%|███████▋  | 309/400 [01:58<00:25,  3.52it/s]Running loglikelihood requests:  78%|███████▊  | 310/400 [01:58<00:28,  3.19it/s]Running loglikelihood requests:  78%|███████▊  | 311/400 [01:59<00:29,  2.97it/s]Running loglikelihood requests:  78%|███████▊  | 312/400 [01:59<00:31,  2.79it/s]Running loglikelihood requests:  78%|███████▊  | 313/400 [01:59<00:32,  2.66it/s]Running loglikelihood requests:  78%|███████▊  | 314/400 [02:00<00:33,  2.58it/s]Running loglikelihood requests:  79%|███████▉  | 315/400 [02:00<00:33,  2.54it/s]Running loglikelihood requests:  79%|███████▉  | 316/400 [02:01<00:33,  2.52it/s]Running loglikelihood requests:  79%|███████▉  | 317/400 [02:01<00:33,  2.51it/s]Running loglikelihood requests:  80%|███████▉  | 318/400 [02:01<00:33,  2.48it/s]Running loglikelihood requests:  80%|████████  | 321/400 [02:02<00:19,  3.96it/s]Running loglikelihood requests:  80%|████████  | 322/400 [02:02<00:22,  3.51it/s]Running loglikelihood requests:  81%|████████  | 323/400 [02:03<00:24,  3.20it/s]Running loglikelihood requests:  81%|████████  | 324/400 [02:03<00:25,  3.00it/s]Running loglikelihood requests:  81%|████████▏ | 325/400 [02:03<00:26,  2.83it/s]Running loglikelihood requests:  82%|████████▏ | 326/400 [02:04<00:27,  2.73it/s]Running loglikelihood requests:  82%|████████▏ | 327/400 [02:04<00:27,  2.64it/s]Running loglikelihood requests:  82%|████████▏ | 328/400 [02:05<00:27,  2.60it/s]Running loglikelihood requests:  82%|████████▏ | 329/400 [02:05<00:27,  2.55it/s]Running loglikelihood requests:  82%|████████▎ | 330/400 [02:05<00:27,  2.51it/s]Running loglikelihood requests:  83%|████████▎ | 331/400 [02:06<00:27,  2.52it/s]Running loglikelihood requests:  83%|████████▎ | 332/400 [02:06<00:27,  2.49it/s]Running loglikelihood requests:  83%|████████▎ | 333/400 [02:07<00:26,  2.54it/s]Running loglikelihood requests:  84%|████████▎ | 334/400 [02:07<00:26,  2.50it/s]Running loglikelihood requests:  84%|████████▍ | 336/400 [02:07<00:19,  3.25it/s]Running loglikelihood requests:  84%|████████▍ | 337/400 [02:08<00:20,  3.01it/s]Running loglikelihood requests:  84%|████████▍ | 338/400 [02:08<00:21,  2.86it/s]Running loglikelihood requests:  85%|████████▍ | 339/400 [02:09<00:22,  2.73it/s]Running loglikelihood requests:  85%|████████▌ | 340/400 [02:09<00:22,  2.65it/s]Running loglikelihood requests:  85%|████████▌ | 341/400 [02:10<00:22,  2.59it/s]Running loglikelihood requests:  86%|████████▌ | 342/400 [02:10<00:22,  2.54it/s]Running loglikelihood requests:  86%|████████▌ | 343/400 [02:10<00:22,  2.49it/s]Running loglikelihood requests:  86%|████████▌ | 344/400 [02:11<00:22,  2.49it/s]Running loglikelihood requests:  86%|████████▋ | 345/400 [02:11<00:23,  2.33it/s]Running loglikelihood requests:  86%|████████▋ | 346/400 [02:12<00:22,  2.44it/s]Running loglikelihood requests:  87%|████████▋ | 347/400 [02:12<00:21,  2.45it/s]Running loglikelihood requests:  87%|████████▋ | 348/400 [02:12<00:21,  2.45it/s]Running loglikelihood requests:  87%|████████▋ | 349/400 [02:13<00:20,  2.47it/s]Running loglikelihood requests:  88%|████████▊ | 350/400 [02:13<00:20,  2.48it/s]Running loglikelihood requests:  88%|████████▊ | 351/400 [02:14<00:19,  2.48it/s]Running loglikelihood requests:  88%|████████▊ | 352/400 [02:14<00:18,  2.54it/s]Running loglikelihood requests:  88%|████████▊ | 353/400 [02:14<00:18,  2.57it/s]Running loglikelihood requests:  88%|████████▊ | 354/400 [02:15<00:18,  2.55it/s]Running loglikelihood requests:  89%|████████▉ | 355/400 [02:15<00:17,  2.53it/s]Running loglikelihood requests:  89%|████████▉ | 356/400 [02:16<00:17,  2.56it/s]Running loglikelihood requests:  89%|████████▉ | 357/400 [02:16<00:16,  2.55it/s]Running loglikelihood requests:  90%|████████▉ | 358/400 [02:16<00:16,  2.54it/s]Running loglikelihood requests:  90%|████████▉ | 359/400 [02:17<00:16,  2.54it/s]Running loglikelihood requests:  90%|█████████ | 360/400 [02:17<00:15,  2.56it/s]Running loglikelihood requests:  91%|█████████ | 363/400 [02:18<00:09,  4.10it/s]Running loglikelihood requests:  91%|█████████ | 364/400 [02:18<00:09,  3.63it/s]Running loglikelihood requests:  91%|█████████▏| 365/400 [02:18<00:10,  3.20it/s]Running loglikelihood requests:  92%|█████████▏| 366/400 [02:19<00:11,  3.05it/s]Running loglikelihood requests:  92%|█████████▏| 367/400 [02:19<00:11,  2.94it/s]Running loglikelihood requests:  92%|█████████▏| 368/400 [02:19<00:11,  2.82it/s]Running loglikelihood requests:  92%|█████████▏| 369/400 [02:20<00:11,  2.74it/s]Running loglikelihood requests:  92%|█████████▎| 370/400 [02:20<00:11,  2.70it/s]Running loglikelihood requests:  93%|█████████▎| 371/400 [02:21<00:10,  2.69it/s]Running loglikelihood requests:  93%|█████████▎| 372/400 [02:21<00:10,  2.71it/s]Running loglikelihood requests:  93%|█████████▎| 373/400 [02:21<00:09,  2.71it/s]Running loglikelihood requests:  94%|█████████▎| 374/400 [02:22<00:09,  2.68it/s]Running loglikelihood requests:  94%|█████████▍| 377/400 [02:22<00:05,  4.35it/s]Running loglikelihood requests:  94%|█████████▍| 378/400 [02:22<00:05,  3.87it/s]Running loglikelihood requests:  95%|█████████▍| 379/400 [02:23<00:05,  3.51it/s]Running loglikelihood requests:  95%|█████████▌| 380/400 [02:23<00:06,  3.30it/s]Running loglikelihood requests:  95%|█████████▌| 381/400 [02:24<00:05,  3.17it/s]Running loglikelihood requests:  96%|█████████▌| 382/400 [02:24<00:05,  3.06it/s]Running loglikelihood requests:  96%|█████████▌| 383/400 [02:24<00:05,  2.97it/s]Running loglikelihood requests:  96%|█████████▌| 384/400 [02:25<00:05,  2.90it/s]Running loglikelihood requests:  97%|█████████▋| 387/400 [02:25<00:02,  4.53it/s]Running loglikelihood requests:  97%|█████████▋| 388/400 [02:25<00:02,  4.01it/s]Running loglikelihood requests:  98%|█████████▊| 390/400 [02:26<00:02,  4.42it/s]Running loglikelihood requests: 100%|██████████| 400/400 [02:22<00:00,  2.80it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:1'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:1'}
full model:
{'openbookqa': {'alias': 'openbookqa', 'acc,none': 0.25, 'acc_stderr,none': 0.04351941398892446, 'acc_norm,none': 0.39, 'acc_norm_stderr,none': 0.04902071300001973}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7187319796021299
0.7542307644255121
0.9218102409329888
0.9507846064925026
0.9244473932111009
0.9697851265705995
0.9088369054586506
0.9555908644563589
0.6065244449438403
0.8380982140772407
0.9227293008726757
0.940339967265193
0.8763319017557598
0.8956221731459367
0.8537768995934096
0.9486125995141542
0.9673324054839537
0.9137514724447725
0.9379677722073356
0.9822722776537973
0.9198472199854921
0.886666948473494
0.9000260607563779
0.9759633945879138
0.9869744580755335
Total groups 76 exceeded the threshold, stopping comparison.
The group tensor is
[6, 2, 3, 1, 5, 4, 7, 0]
tensor([6, 2, 3, 1, 5, 4, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 3, 1, 4, 5, 7, 0]
tensor([6, 2, 3, 1, 4, 5, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 3, 1, 4, 5, 7, 0]
tensor([6, 2, 3, 1, 4, 5, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 5, 2, 4, 3, 7, 0]
tensor([6, 1, 5, 2, 4, 3, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 1, 3, 2, 5, 4, 6, 0]
tensor([7, 1, 3, 2, 5, 4, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 3, 1, 5, 4, 7, 0]
tensor([6, 2, 3, 1, 5, 4, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1, 1.0, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 1, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Normal merging for layer 4
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
done!
Normal merging for layer 5
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 6 to 29
done!
Normal merging for layer 30
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
Normal merging for layer 31
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
all done!
Model size: 12.3238 GB
155
cuda:1
copa
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:37<00:37, 37.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:50<00:00, 23.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:50<00:00, 25.22s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: copa] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: copa] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140336578480752 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_copa_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140336578480752 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_copa_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140336578480752 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_copa_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140336578480752 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_copa_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140322151734048 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140322151734048 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140322151734048 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140322151734048 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
DEBUG:lm_eval.api.task:Both target_delimiter " " and target choice: " the toilet filled with water." have whitespace
DEBUG:lm_eval.api.task:Both target_delimiter " " and target choice: " water flowed from the spout." have whitespace
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of copa from None to 0
INFO:lm_eval.api.task:Building contexts for copa on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 131896.35it/s]
DEBUG:lm_eval.evaluator:Task: copa; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:00<03:13,  1.03it/s]Running loglikelihood requests:   1%|          | 2/200 [00:01<02:19,  1.42it/s]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:01,  1.62it/s]Running loglikelihood requests:   2%|▏         | 4/200 [00:02<01:52,  1.75it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:03<01:46,  1.83it/s]Running loglikelihood requests:   3%|▎         | 6/200 [00:03<01:43,  1.88it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:04<01:40,  1.92it/s]Running loglikelihood requests:   4%|▍         | 8/200 [00:04<01:38,  1.94it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:05<01:37,  1.96it/s]Running loglikelihood requests:   5%|▌         | 10/200 [00:05<01:36,  1.98it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:05<01:34,  1.99it/s]Running loglikelihood requests:   6%|▌         | 12/200 [00:06<01:33,  2.00it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:06<01:33,  2.01it/s]Running loglikelihood requests:   7%|▋         | 14/200 [00:07<01:32,  2.02it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:07<01:32,  1.99it/s]Running loglikelihood requests:   8%|▊         | 16/200 [00:08<01:31,  2.00it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:08<01:30,  2.01it/s]Running loglikelihood requests:   9%|▉         | 18/200 [00:09<01:31,  1.99it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:09<01:30,  2.01it/s]Running loglikelihood requests:  10%|█         | 20/200 [00:10<01:29,  2.01it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:10<01:28,  2.02it/s]Running loglikelihood requests:  11%|█         | 22/200 [00:11<01:27,  2.03it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:11<01:26,  2.04it/s]Running loglikelihood requests:  12%|█▏        | 24/200 [00:12<01:26,  2.04it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:12<01:25,  2.04it/s]Running loglikelihood requests:  13%|█▎        | 26/200 [00:13<01:25,  2.04it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:13<01:24,  2.05it/s]Running loglikelihood requests:  14%|█▍        | 28/200 [00:14<01:23,  2.05it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:14<01:23,  2.05it/s]Running loglikelihood requests:  15%|█▌        | 30/200 [00:15<01:22,  2.06it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:15<01:21,  2.07it/s]Running loglikelihood requests:  16%|█▌        | 32/200 [00:16<01:21,  2.07it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:16<01:20,  2.07it/s]Running loglikelihood requests:  17%|█▋        | 34/200 [00:17<01:19,  2.08it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:17<01:19,  2.08it/s]Running loglikelihood requests:  18%|█▊        | 36/200 [00:18<01:18,  2.08it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:18<01:18,  2.08it/s]Running loglikelihood requests:  19%|█▉        | 38/200 [00:19<01:17,  2.08it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:19<01:17,  2.08it/s]Running loglikelihood requests:  20%|██        | 40/200 [00:20<01:16,  2.09it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:20<01:16,  2.08it/s]Running loglikelihood requests:  21%|██        | 42/200 [00:21<01:15,  2.08it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:21<01:15,  2.09it/s]Running loglikelihood requests:  22%|██▏       | 44/200 [00:22<01:14,  2.09it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:22<01:14,  2.09it/s]Running loglikelihood requests:  23%|██▎       | 46/200 [00:23<01:13,  2.09it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:23<01:13,  2.09it/s]Running loglikelihood requests:  24%|██▍       | 48/200 [00:23<01:12,  2.08it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:24<01:12,  2.08it/s]Running loglikelihood requests:  25%|██▌       | 50/200 [00:24<01:12,  2.08it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:25<01:11,  2.08it/s]Running loglikelihood requests:  26%|██▌       | 52/200 [00:25<01:11,  2.08it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:26<01:10,  2.08it/s]Running loglikelihood requests:  27%|██▋       | 54/200 [00:26<01:09,  2.09it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:27<01:09,  2.09it/s]Running loglikelihood requests:  28%|██▊       | 56/200 [00:27<01:08,  2.10it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:28<01:07,  2.10it/s]Running loglikelihood requests:  29%|██▉       | 58/200 [00:28<01:07,  2.10it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:29<01:06,  2.11it/s]Running loglikelihood requests:  30%|███       | 60/200 [00:29<01:06,  2.10it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:30<01:06,  2.10it/s]Running loglikelihood requests:  31%|███       | 62/200 [00:30<01:05,  2.11it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:31<01:05,  2.11it/s]Running loglikelihood requests:  32%|███▏      | 64/200 [00:31<01:04,  2.11it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:32<01:04,  2.11it/s]Running loglikelihood requests:  33%|███▎      | 66/200 [00:32<01:03,  2.11it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:33<01:03,  2.11it/s]Running loglikelihood requests:  34%|███▍      | 68/200 [00:33<01:02,  2.11it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:33<01:01,  2.11it/s]Running loglikelihood requests:  35%|███▌      | 70/200 [00:34<01:01,  2.11it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:34<01:01,  2.11it/s]Running loglikelihood requests:  36%|███▌      | 72/200 [00:35<01:00,  2.11it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:35<00:59,  2.12it/s]Running loglikelihood requests:  37%|███▋      | 74/200 [00:36<00:59,  2.12it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:36<00:58,  2.13it/s]Running loglikelihood requests:  38%|███▊      | 76/200 [00:37<00:58,  2.13it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:37<00:57,  2.13it/s]Running loglikelihood requests:  39%|███▉      | 78/200 [00:38<00:57,  2.14it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:38<00:56,  2.14it/s]Running loglikelihood requests:  40%|████      | 80/200 [00:39<00:56,  2.14it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:39<00:55,  2.13it/s]Running loglikelihood requests:  41%|████      | 82/200 [00:40<00:55,  2.13it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:40<00:54,  2.13it/s]Running loglikelihood requests:  42%|████▏     | 84/200 [00:41<00:54,  2.14it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:41<00:53,  2.14it/s]Running loglikelihood requests:  43%|████▎     | 86/200 [00:41<00:53,  2.14it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:42<00:52,  2.14it/s]Running loglikelihood requests:  44%|████▍     | 88/200 [00:42<00:52,  2.14it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:43<00:51,  2.14it/s]Running loglikelihood requests:  45%|████▌     | 90/200 [00:43<00:51,  2.14it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:44<00:51,  2.13it/s]Running loglikelihood requests:  46%|████▌     | 92/200 [00:44<00:50,  2.13it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:45<00:50,  2.13it/s]Running loglikelihood requests:  47%|████▋     | 94/200 [00:45<00:49,  2.13it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:46<00:49,  2.13it/s]Running loglikelihood requests:  48%|████▊     | 96/200 [00:46<00:48,  2.13it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:47<00:48,  2.13it/s]Running loglikelihood requests:  49%|████▉     | 98/200 [00:47<00:47,  2.14it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:48<00:47,  2.15it/s]Running loglikelihood requests:  50%|█████     | 100/200 [00:48<00:46,  2.15it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:48<00:46,  2.15it/s]Running loglikelihood requests:  51%|█████     | 102/200 [00:49<00:45,  2.15it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:49<00:45,  2.15it/s]Running loglikelihood requests:  52%|█████▏    | 104/200 [00:50<00:44,  2.16it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:50<00:43,  2.16it/s]Running loglikelihood requests:  53%|█████▎    | 106/200 [00:51<00:43,  2.16it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:51<00:43,  2.16it/s]Running loglikelihood requests:  54%|█████▍    | 108/200 [00:52<00:42,  2.16it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:52<00:42,  2.15it/s]Running loglikelihood requests:  55%|█████▌    | 110/200 [00:53<00:41,  2.15it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:53<00:41,  2.15it/s]Running loglikelihood requests:  56%|█████▌    | 112/200 [00:54<00:40,  2.15it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:54<00:40,  2.15it/s]Running loglikelihood requests:  57%|█████▋    | 114/200 [00:55<00:39,  2.16it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:55<00:39,  2.16it/s]Running loglikelihood requests:  58%|█████▊    | 116/200 [00:55<00:38,  2.17it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:56<00:38,  2.18it/s]Running loglikelihood requests:  59%|█████▉    | 118/200 [00:56<00:37,  2.18it/s]Running loglikelihood requests:  66%|██████▌   | 132/200 [00:57<00:06, 10.28it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:57<00:08,  7.91it/s]Running loglikelihood requests:  67%|██████▋   | 134/200 [00:58<00:10,  6.22it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:58<00:12,  5.03it/s]Running loglikelihood requests:  68%|██████▊   | 136/200 [00:59<00:15,  4.19it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:59<00:17,  3.59it/s]Running loglikelihood requests:  69%|██████▉   | 138/200 [01:00<00:19,  3.18it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [01:00<00:21,  2.88it/s]Running loglikelihood requests:  70%|███████   | 140/200 [01:01<00:22,  2.68it/s]Running loglikelihood requests:  70%|███████   | 141/200 [01:01<00:23,  2.53it/s]Running loglikelihood requests:  71%|███████   | 142/200 [01:01<00:23,  2.44it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [01:02<00:24,  2.37it/s]Running loglikelihood requests:  72%|███████▏  | 144/200 [01:02<00:24,  2.33it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:03<00:24,  2.29it/s]Running loglikelihood requests:  73%|███████▎  | 146/200 [01:03<00:23,  2.27it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [01:04<00:23,  2.26it/s]Running loglikelihood requests:  74%|███████▍  | 148/200 [01:04<00:23,  2.25it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:05<00:22,  2.24it/s]Running loglikelihood requests:  75%|███████▌  | 150/200 [01:05<00:22,  2.23it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:05<00:22,  2.22it/s]Running loglikelihood requests:  76%|███████▌  | 152/200 [01:06<00:21,  2.21it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:06<00:21,  2.21it/s]Running loglikelihood requests:  77%|███████▋  | 154/200 [01:07<00:20,  2.22it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:07<00:20,  2.22it/s]Running loglikelihood requests:  78%|███████▊  | 156/200 [01:08<00:19,  2.22it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:08<00:19,  2.23it/s]Running loglikelihood requests:  79%|███████▉  | 158/200 [01:09<00:18,  2.22it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:09<00:18,  2.22it/s]Running loglikelihood requests:  80%|████████  | 160/200 [01:10<00:17,  2.23it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:10<00:17,  2.24it/s]Running loglikelihood requests:  81%|████████  | 162/200 [01:10<00:16,  2.24it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:11<00:16,  2.24it/s]Running loglikelihood requests:  82%|████████▏ | 164/200 [01:11<00:16,  2.24it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:12<00:15,  2.24it/s]Running loglikelihood requests:  83%|████████▎ | 166/200 [01:12<00:15,  2.20it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:13<00:14,  2.22it/s]Running loglikelihood requests:  84%|████████▍ | 168/200 [01:13<00:14,  2.23it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:14<00:13,  2.23it/s]Running loglikelihood requests:  85%|████████▌ | 170/200 [01:14<00:13,  2.22it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:14<00:13,  2.23it/s]Running loglikelihood requests:  86%|████████▌ | 172/200 [01:15<00:12,  2.23it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:15<00:12,  2.25it/s]Running loglikelihood requests:  87%|████████▋ | 174/200 [01:16<00:11,  2.25it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:16<00:11,  2.26it/s]Running loglikelihood requests:  88%|████████▊ | 176/200 [01:17<00:10,  2.22it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:17<00:10,  2.24it/s]Running loglikelihood requests:  89%|████████▉ | 178/200 [01:18<00:09,  2.26it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:18<00:09,  2.27it/s]Running loglikelihood requests:  90%|█████████ | 180/200 [01:18<00:08,  2.28it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:19<00:08,  2.27it/s]Running loglikelihood requests:  91%|█████████ | 182/200 [01:19<00:07,  2.27it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:20<00:07,  2.30it/s]Running loglikelihood requests:  92%|█████████▏| 184/200 [01:20<00:06,  2.30it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:21<00:06,  2.31it/s]Running loglikelihood requests:  93%|█████████▎| 186/200 [01:21<00:06,  2.31it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:21<00:05,  2.32it/s]Running loglikelihood requests:  94%|█████████▍| 188/200 [01:22<00:05,  2.31it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:22<00:04,  2.32it/s]Running loglikelihood requests:  95%|█████████▌| 190/200 [01:23<00:04,  2.34it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:23<00:03,  2.33it/s]Running loglikelihood requests:  96%|█████████▌| 192/200 [01:24<00:03,  2.33it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:24<00:03,  2.33it/s]Running loglikelihood requests:  97%|█████████▋| 194/200 [01:24<00:02,  2.33it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:25<00:02,  2.36it/s]Running loglikelihood requests:  98%|█████████▊| 196/200 [01:25<00:01,  2.36it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:26<00:01,  2.38it/s]Running loglikelihood requests:  99%|█████████▉| 198/200 [01:26<00:00,  2.39it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:27<00:00,  2.39it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:27<00:00,  2.41it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:27<00:00,  2.29it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:2'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:2'}
full model:
{'copa': {'alias': 'copa', 'acc,none': 0.82, 'acc_stderr,none': 0.03861229196653691}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7827148355678417
0.8244609672079803
0.649635438690672
0.6067147398962018
0.806517251330611
0.9304450428937726
0.9492767057467371
0.8146777207922447
0.5943920512469448
0.6904992046065734
0.898578027138337
0.9772061768478973
0.9053772479641496
0.8020143483317842
0.5125553000556808
0.7018400400551881
0.8957865573767195
0.6076519427757794
0.975563476608663
0.9639212062070597
0.9296418237714587
0.9150145595079396
0.9197448761365332
0.7066536936990038
0.6639302750778917
0.912306872752127
0.7698141900267782
0.6230420491138425
0.8289959673107662
Total groups 68 exceeded the threshold, stopping comparison.
The group tensor is
[7, 3, 4, 1, 2, 6, 5, 0]
tensor([7, 3, 4, 1, 2, 6, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 4, 1, 0, 5, 7, 2]
tensor([6, 3, 4, 1, 0, 5, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 4, 3, 2, 0, 5, 6, 1]
tensor([7, 4, 3, 2, 0, 5, 6, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 6, 4, 2, 1, 3, 5, 0]
tensor([7, 6, 4, 2, 1, 3, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 3, 0, 1, 2, 3, 2]
tensor([0, 1, 3, 0, 1, 2, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 3, 1, 0, 2, 3, 2]
tensor([0, 1, 3, 1, 0, 2, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 0, 1.0, 1.0, 1.0, 1]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 2 to 3
done!
Normal merging for layer 4
tensor([4])
tensor(4)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 5 to 6
done!
Normal merging for layer 7
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 8 to 15
done!
Normal merging for layer 16
tensor([0, 3])
tensor(0)
tensor([1, 4])
tensor(1)
tensor([5, 7])
tensor(5)
tensor([2, 6])
tensor(2)
done!
Cross-layer merge completed for layers 17 to 21
done!
Normal merging for layer 22
tensor([0, 4])
tensor(0)
tensor([1, 3])
tensor(1)
tensor([5, 7])
tensor(5)
tensor([2, 6])
tensor(2)
done!
Cross-layer merge completed for layers 23 to 27
done!
Normal merging for layer 28
tensor([0, 3])
tensor(0)
tensor([1, 2, 4, 5, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 29 to 30
done!
Normal merging for layer 31
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
all done!
Model size: 12.1348 GB
36
cuda:2
mrpc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:42<00:42, 42.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 25.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 28.39s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: mrpc] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: mrpc] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
WARNING:lm_eval.api.task:[Task: mrpc] metric f1 is defined, but aggregation is not. using default aggregation=f1
WARNING:lm_eval.api.task:[Task: mrpc] metric f1 is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140321622809904 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321622809904 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321622809904 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321622809904 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140321082435744 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321082435744 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321082435744 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321082435744 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of mrpc from None to 0
INFO:lm_eval.api.task:Building contexts for mrpc on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2286.51it/s]
DEBUG:lm_eval.evaluator:Task: mrpc; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<04:58,  1.50s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:26,  1.34it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:03<01:58,  1.64it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:04<01:46,  1.81it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:05<01:39,  1.92it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:06<01:35,  1.99it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:07<01:31,  2.04it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:08<01:29,  2.07it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:09<01:27,  2.10it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:09<01:25,  2.12it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:10<01:23,  2.14it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:11<01:22,  2.15it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:12<01:20,  2.16it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:13<01:19,  2.17it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:14<01:18,  2.19it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:15<01:16,  2.20it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:16<01:15,  2.21it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:17<01:14,  2.22it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:18<01:12,  2.23it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:18<01:11,  2.24it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:19<01:10,  2.25it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:20<01:09,  2.26it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:21<01:08,  2.27it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:22<01:07,  2.28it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:23<01:06,  2.28it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:24<01:04,  2.29it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:25<01:03,  2.30it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:25<01:02,  2.31it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:26<01:01,  2.31it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:27<01:00,  2.32it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:28<00:59,  2.33it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:29<00:58,  2.33it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:30<00:57,  2.34it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:31<00:56,  2.35it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:31<00:55,  2.35it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:32<00:54,  2.37it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:33<00:53,  2.38it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:34<00:52,  2.38it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:35<00:51,  2.39it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:36<00:50,  2.40it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:36<00:49,  2.41it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:37<00:48,  2.41it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:38<00:47,  2.42it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:39<00:11,  8.34it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:39<00:14,  6.51it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:40<00:18,  5.27it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:41<00:21,  4.42it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:42<00:23,  3.85it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:43<00:25,  3.45it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:43<00:27,  3.17it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:44<00:28,  2.97it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:45<00:29,  2.84it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:46<00:29,  2.74it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:47<00:29,  2.68it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:47<00:29,  2.65it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:48<00:28,  2.62it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:49<00:28,  2.60it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:50<00:27,  2.59it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:50<00:26,  2.58it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:51<00:25,  2.58it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:52<00:25,  2.58it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:53<00:24,  2.58it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:54<00:23,  2.59it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:54<00:22,  2.59it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:55<00:21,  2.60it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [00:56<00:21,  2.61it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [00:57<00:20,  2.62it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [00:57<00:19,  2.63it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [00:58<00:18,  2.63it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [00:59<00:17,  2.64it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:00<00:17,  2.63it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:00<00:16,  2.65it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:01<00:15,  2.65it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:02<00:14,  2.66it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:03<00:13,  2.67it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:03<00:13,  2.69it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:04<00:12,  2.71it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:05<00:11,  2.72it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:05<00:10,  2.74it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:06<00:09,  2.75it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:07<00:09,  2.76it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:08<00:08,  2.78it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:08<00:07,  2.80it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:09<00:06,  2.82it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:10<00:05,  2.85it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:10<00:05,  2.87it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:11<00:04,  2.89it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:12<00:03,  2.91it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:12<00:03,  2.92it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:13<00:02,  2.94it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:14<00:01,  2.95it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:14<00:01,  2.97it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:15<00:00,  3.02it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:15<00:00,  2.65it/s]
bootstrapping for stddev (sequential): f1_score
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:01<02:08,  1.30s/it]  2%|▏         | 2/100 [00:02<02:07,  1.30s/it]  3%|▎         | 3/100 [00:03<02:06,  1.30s/it]  4%|▍         | 4/100 [00:05<02:04,  1.30s/it]  5%|▌         | 5/100 [00:06<02:03,  1.30s/it]  6%|▌         | 6/100 [00:07<02:02,  1.30s/it]  7%|▋         | 7/100 [00:09<02:00,  1.30s/it]  8%|▊         | 8/100 [00:10<01:59,  1.30s/it]  9%|▉         | 9/100 [00:11<01:58,  1.30s/it] 10%|█         | 10/100 [00:12<01:56,  1.30s/it] 11%|█         | 11/100 [00:14<01:55,  1.30s/it] 12%|█▏        | 12/100 [00:15<01:54,  1.30s/it] 13%|█▎        | 13/100 [00:16<01:53,  1.30s/it] 14%|█▍        | 14/100 [00:18<01:51,  1.30s/it] 15%|█▌        | 15/100 [00:19<01:50,  1.30s/it] 16%|█▌        | 16/100 [00:20<01:49,  1.30s/it] 17%|█▋        | 17/100 [00:22<01:48,  1.30s/it] 18%|█▊        | 18/100 [00:23<01:46,  1.30s/it] 23%|██▎       | 23/100 [00:23<00:36,  2.12it/s] 24%|██▍       | 24/100 [00:25<00:46,  1.65it/s] 25%|██▌       | 25/100 [00:26<00:55,  1.35it/s] 26%|██▌       | 26/100 [00:27<01:03,  1.17it/s] 27%|██▋       | 27/100 [00:29<01:10,  1.04it/s] 28%|██▊       | 28/100 [00:30<01:15,  1.05s/it] 29%|██▉       | 29/100 [00:31<01:19,  1.11s/it] 30%|███       | 30/100 [00:32<01:21,  1.16s/it] 31%|███       | 31/100 [00:34<01:22,  1.20s/it] 32%|███▏      | 32/100 [00:35<01:23,  1.23s/it] 33%|███▎      | 33/100 [00:36<01:23,  1.25s/it] 34%|███▍      | 34/100 [00:38<01:23,  1.26s/it] 35%|███▌      | 35/100 [00:39<01:22,  1.27s/it] 36%|███▌      | 36/100 [00:40<01:22,  1.28s/it] 37%|███▋      | 37/100 [00:42<01:21,  1.29s/it] 38%|███▊      | 38/100 [00:43<01:20,  1.29s/it] 39%|███▉      | 39/100 [00:44<01:19,  1.30s/it] 40%|████      | 40/100 [00:46<01:18,  1.31s/it] 41%|████      | 41/100 [00:47<01:17,  1.31s/it] 42%|████▏     | 42/100 [00:48<01:15,  1.31s/it] 43%|████▎     | 43/100 [00:49<01:14,  1.31s/it] 44%|████▍     | 44/100 [00:51<01:13,  1.31s/it] 45%|████▌     | 45/100 [00:52<01:11,  1.30s/it] 46%|████▌     | 46/100 [00:53<01:10,  1.30s/it] 47%|████▋     | 47/100 [00:55<01:09,  1.30s/it] 48%|████▊     | 48/100 [00:56<01:07,  1.30s/it] 49%|████▉     | 49/100 [00:57<01:06,  1.31s/it] 50%|█████     | 50/100 [00:59<01:05,  1.31s/it] 51%|█████     | 51/100 [01:00<01:03,  1.31s/it] 52%|█████▏    | 52/100 [01:01<01:02,  1.31s/it] 53%|█████▎    | 53/100 [01:02<01:01,  1.30s/it] 54%|█████▍    | 54/100 [01:04<01:00,  1.31s/it] 55%|█████▌    | 55/100 [01:05<00:58,  1.31s/it] 56%|█████▌    | 56/100 [01:06<00:57,  1.31s/it] 57%|█████▋    | 57/100 [01:08<00:56,  1.31s/it] 58%|█████▊    | 58/100 [01:09<00:54,  1.31s/it] 59%|█████▉    | 59/100 [01:10<00:53,  1.31s/it] 60%|██████    | 60/100 [01:12<00:52,  1.31s/it] 61%|██████    | 61/100 [01:13<00:50,  1.30s/it] 62%|██████▏   | 62/100 [01:14<00:49,  1.30s/it] 63%|██████▎   | 63/100 [01:16<00:48,  1.31s/it] 64%|██████▍   | 64/100 [01:17<00:47,  1.31s/it] 65%|██████▌   | 65/100 [01:18<00:45,  1.31s/it] 66%|██████▌   | 66/100 [01:19<00:44,  1.31s/it] 67%|██████▋   | 67/100 [01:21<00:43,  1.30s/it] 68%|██████▊   | 68/100 [01:22<00:41,  1.30s/it] 69%|██████▉   | 69/100 [01:23<00:40,  1.31s/it] 70%|███████   | 70/100 [01:25<00:39,  1.31s/it] 75%|███████▌  | 75/100 [01:25<00:11,  2.23it/s] 76%|███████▌  | 76/100 [01:26<00:14,  1.70it/s] 77%|███████▋  | 77/100 [01:28<00:16,  1.38it/s] 78%|███████▊  | 78/100 [01:29<00:18,  1.18it/s] 79%|███████▉  | 79/100 [01:30<00:20,  1.05it/s] 80%|████████  | 80/100 [01:31<00:20,  1.04s/it] 81%|████████  | 81/100 [01:33<00:21,  1.11s/it] 82%|████████▏ | 82/100 [01:34<00:20,  1.16s/it] 83%|████████▎ | 83/100 [01:35<00:20,  1.20s/it] 84%|████████▍ | 84/100 [01:37<00:19,  1.23s/it] 85%|████████▌ | 85/100 [01:38<00:18,  1.25s/it] 86%|████████▌ | 86/100 [01:39<00:17,  1.27s/it] 87%|████████▋ | 87/100 [01:41<00:16,  1.28s/it] 88%|████████▊ | 88/100 [01:42<00:15,  1.29s/it] 89%|████████▉ | 89/100 [01:43<00:14,  1.29s/it] 90%|█████████ | 90/100 [01:44<00:12,  1.30s/it] 91%|█████████ | 91/100 [01:46<00:11,  1.30s/it] 92%|█████████▏| 92/100 [01:47<00:10,  1.30s/it] 93%|█████████▎| 93/100 [01:48<00:09,  1.30s/it] 94%|█████████▍| 94/100 [01:50<00:07,  1.30s/it] 95%|█████████▌| 95/100 [01:51<00:06,  1.30s/it] 96%|█████████▌| 96/100 [01:52<00:05,  1.30s/it] 97%|█████████▋| 97/100 [01:54<00:03,  1.30s/it] 98%|█████████▊| 98/100 [01:55<00:02,  1.30s/it] 99%|█████████▉| 99/100 [01:56<00:01,  1.30s/it]100%|██████████| 100/100 [01:57<00:00,  1.30s/it]100%|██████████| 100/100 [01:57<00:00,  1.18s/it]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:3'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:3'}
full model:
{'mrpc': {'alias': 'mrpc', 'acc,none': 0.7, 'acc_stderr,none': 0.04605661864718383, 'f1,none': np.float64(0.8214285714285714), 'f1_stderr,none': 0.0323693653386572}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.32375514242269293
0.9664141198776434
0.4263233992087649
0.29258192731768157
0.3497718342079178
0.5968480671674146
0.7870107004084098
0.7068508532201193
0.7836897379939763
0.2579819071737284
0.5774144903645764
0.7819348699339372
0.7404680063618143
0.651835611327285
0.6832243011482585
0.34680079188801705
0.6033476505145444
0.6299526106979766
0.6468468631829007
0.8458578364778718
0.39447753640727073
0.5291234791393744
0.944540808938638
0.8460909093145242
0.4915397062049404
0.5314721523201049
0.8670806018560002
0.6451914961665037
0.7380077491787214
0.32375514242269293
0.9664141198776434
0.4263233992087649
0.29258192731768157
0.3497718342079178
0.5968480671674146
0.7870107004084098
0.7068508532201193
0.7836897379939763
0.2579819071737284
0.5774144903645764
0.7819348699339372
0.7404680063618143
0.651835611327285
0.6832243011482585
0.34680079188801705
0.6033476505145444
0.6299526106979766
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[4, 5, 7, 1, 6, 2, 3, 0]
tensor([4, 5, 7, 1, 6, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 4, 5, 0, 1, 2]
tensor([6, 3, 7, 4, 5, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 1, 7, 2, 6, 0, 3, 5]
tensor([4, 1, 7, 2, 6, 0, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 5, 2, 7, 0, 4, 1]
tensor([6, 3, 5, 2, 7, 0, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 5, 6, 7, 3, 1, 0, 2]
tensor([4, 5, 6, 7, 3, 1, 0, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 2, 4, 1, 0, 5]
tensor([0, 3, 1, 2, 4, 1, 0, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1, 1.0]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 2
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 4 to 5
done!
Normal merging for layer 6
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
done!
Cross-layer merge completed for layers 7 to 9
done!
Normal merging for layer 10
tensor([0, 6])
tensor(0)
tensor([2, 5])
tensor(2)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([7])
tensor(7)
done!
Cross-layer merge completed for layers 11 to 26
done!
Normal merging for layer 27
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 28 to 30
done!
Normal merging for layer 31
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
all done!
Model size: 12.3867 GB
101
cuda:3
multirc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:43<00:43, 43.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 22.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 25.90s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: multirc] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: multirc] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/multirc?recursive=False&expand=False HTTP/1.1" 307 146
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/multirc?recursive=False&expand=False HTTP/1.1" 200 365
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:filelock:Attempting to acquire lock 140321890301136 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140321890301136 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321890301136 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140321890301136 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140321890296432 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140321890296432 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321890296432 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140321890296432 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of multirc from None to 0
INFO:lm_eval.api.task:Building contexts for multirc on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1151.95it/s]
DEBUG:lm_eval.evaluator:Task: multirc; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:03<10:59,  3.31s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:06<06:11,  1.89s/it]Running loglikelihood requests:   2%|▎         | 5/200 [00:08<05:16,  1.62s/it]Running loglikelihood requests:   4%|▎         | 7/200 [00:11<04:52,  1.51s/it]Running loglikelihood requests:   4%|▍         | 9/200 [00:14<04:38,  1.46s/it]Running loglikelihood requests:   6%|▌         | 11/200 [00:17<04:28,  1.42s/it]Running loglikelihood requests:   6%|▋         | 13/200 [00:19<04:22,  1.40s/it]Running loglikelihood requests:   8%|▊         | 15/200 [00:22<04:16,  1.39s/it]Running loglikelihood requests:   8%|▊         | 17/200 [00:25<04:12,  1.38s/it]Running loglikelihood requests:  10%|▉         | 19/200 [00:27<04:08,  1.37s/it]Running loglikelihood requests:  10%|█         | 21/200 [00:30<04:04,  1.37s/it]Running loglikelihood requests:  12%|█▏        | 23/200 [00:33<04:00,  1.36s/it]Running loglikelihood requests:  12%|█▎        | 25/200 [00:35<03:57,  1.36s/it]Running loglikelihood requests:  14%|█▎        | 27/200 [00:38<03:53,  1.35s/it]Running loglikelihood requests:  16%|█▋        | 33/200 [00:40<02:02,  1.37it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:42<02:21,  1.16it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:45<02:37,  1.03it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:48<02:50,  1.06s/it]Running loglikelihood requests:  20%|██        | 41/200 [00:50<02:59,  1.13s/it]Running loglikelihood requests:  22%|██▏       | 43/200 [00:53<03:06,  1.19s/it]Running loglikelihood requests:  22%|██▎       | 45/200 [00:56<03:09,  1.22s/it]Running loglikelihood requests:  24%|██▎       | 47/200 [00:58<03:11,  1.25s/it]Running loglikelihood requests:  24%|██▍       | 49/200 [01:01<03:12,  1.27s/it]Running loglikelihood requests:  26%|██▌       | 51/200 [01:04<03:11,  1.29s/it]Running loglikelihood requests:  26%|██▋       | 53/200 [01:06<03:10,  1.30s/it]Running loglikelihood requests:  28%|██▊       | 55/200 [01:09<03:09,  1.30s/it]Running loglikelihood requests:  28%|██▊       | 57/200 [01:11<03:06,  1.31s/it]Running loglikelihood requests:  30%|██▉       | 59/200 [01:14<03:04,  1.31s/it]Running loglikelihood requests:  30%|███       | 61/200 [01:17<03:02,  1.31s/it]Running loglikelihood requests:  32%|███▏      | 63/200 [01:19<02:59,  1.31s/it]Running loglikelihood requests:  32%|███▎      | 65/200 [01:22<02:56,  1.31s/it]Running loglikelihood requests:  34%|███▎      | 67/200 [01:25<02:54,  1.31s/it]Running loglikelihood requests:  34%|███▍      | 69/200 [01:27<02:51,  1.31s/it]Running loglikelihood requests:  36%|███▌      | 71/200 [01:30<02:49,  1.31s/it]Running loglikelihood requests:  36%|███▋      | 73/200 [01:32<02:46,  1.31s/it]Running loglikelihood requests:  38%|███▊      | 75/200 [01:35<02:43,  1.31s/it]Running loglikelihood requests:  38%|███▊      | 77/200 [01:38<02:41,  1.31s/it]Running loglikelihood requests:  42%|████▏     | 83/200 [01:40<01:28,  1.33it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [01:42<01:40,  1.15it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [01:45<01:49,  1.03it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [01:47<01:57,  1.05s/it]Running loglikelihood requests:  46%|████▌     | 91/200 [01:50<02:02,  1.12s/it]Running loglikelihood requests:  46%|████▋     | 93/200 [01:53<02:05,  1.17s/it]Running loglikelihood requests:  48%|████▊     | 95/200 [01:55<02:06,  1.21s/it]Running loglikelihood requests:  48%|████▊     | 97/200 [01:58<02:07,  1.24s/it]Running loglikelihood requests:  50%|████▉     | 99/200 [02:01<02:06,  1.26s/it]Running loglikelihood requests:  50%|█████     | 101/200 [02:03<02:05,  1.27s/it]Running loglikelihood requests:  52%|█████▏    | 103/200 [02:06<02:03,  1.28s/it]Running loglikelihood requests:  52%|█████▎    | 105/200 [02:08<02:01,  1.28s/it]Running loglikelihood requests:  54%|█████▎    | 107/200 [02:11<01:59,  1.29s/it]Running loglikelihood requests:  55%|█████▍    | 109/200 [02:13<01:57,  1.29s/it]Running loglikelihood requests:  56%|█████▌    | 111/200 [02:16<01:54,  1.29s/it]Running loglikelihood requests:  56%|█████▋    | 113/200 [02:19<01:52,  1.29s/it]Running loglikelihood requests:  57%|█████▊    | 115/200 [02:21<01:49,  1.29s/it]Running loglikelihood requests:  58%|█████▊    | 117/200 [02:24<01:47,  1.29s/it]Running loglikelihood requests:  60%|█████▉    | 119/200 [02:26<01:44,  1.29s/it]Running loglikelihood requests:  60%|██████    | 121/200 [02:29<01:41,  1.29s/it]Running loglikelihood requests:  62%|██████▏   | 123/200 [02:32<01:38,  1.29s/it]Running loglikelihood requests:  62%|██████▎   | 125/200 [02:34<01:36,  1.28s/it]Running loglikelihood requests:  64%|██████▎   | 127/200 [02:37<01:33,  1.28s/it]Running loglikelihood requests:  64%|██████▍   | 129/200 [02:39<01:31,  1.28s/it]Running loglikelihood requests:  68%|██████▊   | 135/200 [02:40<00:43,  1.50it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [02:42<00:44,  1.43it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [02:44<00:44,  1.37it/s]Running loglikelihood requests:  70%|███████   | 141/200 [02:45<00:44,  1.33it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [02:47<00:44,  1.30it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [02:49<00:43,  1.28it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [02:50<00:41,  1.26it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [02:52<00:40,  1.26it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [02:53<00:39,  1.25it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [02:55<00:38,  1.23it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [02:57<00:37,  1.21it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [02:59<00:35,  1.20it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [03:00<00:34,  1.19it/s]Running loglikelihood requests:  80%|████████  | 161/200 [03:02<00:32,  1.20it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [03:03<00:30,  1.22it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [03:05<00:27,  1.27it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [03:06<00:25,  1.30it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [03:08<00:23,  1.32it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [03:09<00:21,  1.34it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [03:11<00:19,  1.36it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [03:12<00:18,  1.37it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [03:14<00:16,  1.39it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [03:15<00:15,  1.40it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [03:16<00:13,  1.40it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [03:18<00:12,  1.41it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [03:19<00:10,  1.42it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [03:21<00:09,  1.42it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [03:22<00:07,  1.43it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [03:23<00:06,  1.43it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [03:25<00:04,  1.43it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [03:26<00:03,  1.43it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [03:27<00:02,  1.44it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [03:29<00:00,  1.44it/s]Running loglikelihood requests: 100%|██████████| 200/200 [03:29<00:00,  1.05s/it]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:4'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:4'}
full model:
{'multirc': {'alias': 'multirc', 'acc,none': 0.54, 'acc_stderr,none': 0.05009082659620331}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.6089030413152259
0.13723554564825818
0.32639298774801107
0.39554004962513223
0.17838079452737773
0.18172395498612623
0.4458149326815675
0.2599923469162762
0.44953845901781375
0.264228421159195
0.28453253935496087
0.5178234519727917
0.18549509579986273
0.3164231435223059
0.47622435667997154
0.5789910219470605
0.38414027299756903
0.6648306636715198
0.2288657593104162
0.30014630918632723
0.28954340172694804
0.7956189982678256
0.789982900191189
0.3644425339397572
0.37373995160413354
0.19965826892219962
0.3752330861186013
0.6279352732581062
0.3474029418882723
0.6089030413152259
0.13723554564825818
0.32639298774801107
0.39554004962513223
0.17838079452737773
0.18172395498612623
0.4458149326815675
0.2599923469162762
0.44953845901781375
0.264228421159195
0.28453253935496087
0.5178234519727917
0.18549509579986273
0.3164231435223059
0.47622435667997154
0.5789910219470605
0.38414027299756903
0.6648306636715198
0.2288657593104162
0.30014630918632723
0.28954340172694804
0.7956189982678256
0.789982900191189
0.3644425339397572
0.37373995160413354
0.19965826892219962
0.3752330861186013
0.6279352732581062
0.3474029418882723
0.6089030413152259
0.13723554564825818
0.32639298774801107
0.39554004962513223
0.17838079452737773
0.18172395498612623
0.4458149326815675
0.2599923469162762
0.44953845901781375
0.264228421159195
0.28453253935496087
0.5178234519727917
0.18549509579986273
0.3164231435223059
0.47622435667997154
0.5789910219470605
0.38414027299756903
0.6648306636715198
0.2288657593104162
0.30014630918632723
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 0, 6, 2, 4, 1]
tensor([5, 3, 7, 0, 6, 2, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 7, 4, 2, 1, 3, 6, 0]
tensor([5, 7, 4, 2, 1, 3, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 1, 3, 6, 0, 5, 7, 4]
tensor([2, 1, 3, 6, 0, 5, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 4, 1, 6, 0, 2, 3, 5]
tensor([7, 4, 1, 6, 0, 2, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 6, 3, 2, 5, 1, 7, 0]
tensor([4, 6, 3, 2, 5, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 0, 2, 1, 4, 7, 3, 6]
tensor([5, 0, 2, 1, 4, 7, 3, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 0, 1, 1.0, 1.0, 1.0]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
done!
Normal merging for layer 2
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 3 to 4
done!
Normal merging for layer 5
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
done!
Normal merging for layer 6
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
done!
Normal merging for layer 7
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
done!
Cross-layer merge completed for layers 8 to 25
done!
Normal merging for layer 26
tensor([0, 3])
tensor(0)
tensor([1, 2, 4, 5, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 27 to 31
done!
all done!
Model size: 12.3238 GB
217
cuda:4
rte
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:38<00:38, 38.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 23.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 26.13s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140326447568064 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140326447568064 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140326447568064 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140326447568064 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140325252114784 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140325252114784 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140325252114784 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140325252114784 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of rte from None to 0
INFO:lm_eval.api.task:Building contexts for rte on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2386.90it/s]
DEBUG:lm_eval.evaluator:Task: rte; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:02<07:26,  2.25s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:03<03:57,  1.21s/it]Running loglikelihood requests:   2%|▎         | 5/200 [00:05<03:08,  1.03it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:06<02:45,  1.17it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:08<02:32,  1.25it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:09<02:22,  1.32it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:10<02:16,  1.37it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:12<02:11,  1.40it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:13<02:07,  1.44it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:14<02:03,  1.46it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:16<01:59,  1.50it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:17<01:56,  1.52it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:18<01:53,  1.54it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:19<01:49,  1.58it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:21<01:45,  1.61it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:22<01:42,  1.64it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:23<01:40,  1.66it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:24<01:38,  1.68it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:25<01:36,  1.69it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:26<01:34,  1.71it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:27<01:31,  1.74it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:29<01:28,  1.77it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:30<01:25,  1.81it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:31<01:22,  1.84it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:32<01:20,  1.88it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:33<01:17,  1.93it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:34<01:14,  1.98it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:35<01:11,  2.03it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:35<01:08,  2.08it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:36<00:19,  6.56it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:37<00:23,  5.40it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:38<00:26,  4.56it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:39<00:30,  3.95it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:40<00:33,  3.51it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:40<00:36,  3.21it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:41<00:38,  2.99it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:42<00:39,  2.83it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:43<00:40,  2.74it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:44<00:40,  2.67it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:44<00:40,  2.63it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:45<00:40,  2.60it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:46<00:39,  2.59it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:47<00:39,  2.59it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:48<00:38,  2.58it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:48<00:37,  2.58it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:49<00:36,  2.59it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:50<00:35,  2.60it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:51<00:34,  2.61it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:51<00:33,  2.62it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:52<00:33,  2.63it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:53<00:32,  2.64it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:54<00:31,  2.64it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:54<00:30,  2.64it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:55<00:29,  2.65it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:56<00:28,  2.66it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:57<00:28,  2.67it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:57<00:27,  2.67it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:58<00:26,  2.67it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:59<00:25,  2.68it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [01:00<00:24,  2.70it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [01:00<00:23,  2.73it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [01:01<00:22,  2.74it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [01:02<00:22,  2.75it/s]Running loglikelihood requests:  70%|███████   | 141/200 [01:03<00:21,  2.77it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [01:03<00:20,  2.78it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:04<00:19,  2.79it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [01:05<00:18,  2.80it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:05<00:18,  2.81it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:06<00:17,  2.83it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:07<00:16,  2.84it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:07<00:15,  2.85it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:08<00:15,  2.86it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:09<00:14,  2.89it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:09<00:13,  2.90it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:10<00:12,  2.91it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:11<00:11,  2.92it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:12<00:11,  2.94it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:12<00:10,  2.96it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:13<00:09,  2.98it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:14<00:08,  3.00it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:14<00:08,  3.01it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:15<00:07,  3.03it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:15<00:06,  3.04it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:16<00:06,  3.04it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:17<00:05,  3.06it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:17<00:04,  3.06it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:18<00:04,  3.07it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:19<00:03,  3.08it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:19<00:02,  3.10it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:20<00:02,  3.13it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:21<00:01,  3.14it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:21<00:00,  3.21it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:22<00:00,  3.27it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:5'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:5'}
full model:
{'rte': {'alias': 'rte', 'acc,none': 0.5, 'acc_stderr,none': 0.050251890762960605}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.34161229456626735
0.905233410256777
0.5205040718697735
0.4121994254524892
0.7398116665887099
0.6225415196831932
0.7923970242263771
0.7353888240887675
0.6535613357308766
0.7757058271862038
0.734046359122903
0.4471799126982846
0.773619360921301
0.7955347039939479
0.8672068064531693
0.8652880343596522
0.3302235467760883
0.6789268064017625
0.6072221471952108
0.9194446824778495
0.4812004589187253
0.5728915095234594
0.1682455054057436
0.93212414632396
0.9148362604533635
0.8268537756297094
0.7592245907029287
0.7256008379011685
0.7109756105942956
0.34161229456626735
0.905233410256777
0.5205040718697735
0.4121994254524892
0.7398116665887099
0.6225415196831932
0.7923970242263771
0.7353888240887675
0.6535613357308766
0.7757058271862038
0.734046359122903
0.4471799126982846
0.773619360921301
0.7955347039939479
0.8672068064531693
0.8652880343596522
0.3302235467760883
0.6789268064017625
0.6072221471952108
0.9194446824778495
0.4812004589187253
Total groups 73 exceeded the threshold, stopping comparison.
The group tensor is
[5, 2, 7, 1, 6, 4, 3, 0]
tensor([5, 2, 7, 1, 6, 4, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 6, 0, 7, 3, 4, 1]
tensor([5, 2, 6, 0, 7, 3, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 7, 2, 5, 4, 3, 0]
tensor([6, 1, 7, 2, 5, 4, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 2, 4, 1, 5, 0]
tensor([6, 3, 7, 2, 4, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 5, 6, 2, 3, 1, 4, 0]
tensor([7, 5, 6, 2, 3, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 5, 4, 1, 0, 1, 3]
tensor([0, 2, 5, 4, 1, 0, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1.0, 1]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 4
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 5 to 8
done!
Normal merging for layer 9
tensor([0, 5])
tensor(0)
tensor([4, 6])
tensor(4)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 10 to 30
done!
Normal merging for layer 31
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
all done!
Model size: 12.1348 GB
209
cuda:5
wnli
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:44<00:44, 44.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 22.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 26.20s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140321081999776 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321081999776 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321081999776 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321081999776 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140325252110656 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140325252110656 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140325252110656 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140325252110656 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2402.88it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:01<03:26,  1.46s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:02<01:37,  1.42it/s]Running loglikelihood requests:   4%|▎         | 5/142 [00:03<01:17,  1.77it/s]Running loglikelihood requests:   5%|▍         | 7/142 [00:04<01:08,  1.96it/s]Running loglikelihood requests:   6%|▋         | 9/142 [00:04<01:03,  2.10it/s]Running loglikelihood requests:   8%|▊         | 11/142 [00:05<00:59,  2.21it/s]Running loglikelihood requests:   9%|▉         | 13/142 [00:06<00:56,  2.29it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:07<00:53,  2.37it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:08<00:51,  2.45it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:08<00:48,  2.54it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:09<00:46,  2.61it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:10<00:44,  2.68it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:10<00:42,  2.74it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:11<00:41,  2.79it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:12<00:39,  2.83it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:12<00:38,  2.86it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:13<00:37,  2.88it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:14<00:36,  2.90it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:15<00:36,  2.91it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:15<00:35,  2.92it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:16<00:34,  2.94it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:17<00:33,  2.95it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:17<00:32,  2.98it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:18<00:31,  3.01it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:18<00:30,  3.05it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:19<00:29,  3.09it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:20<00:28,  3.12it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:20<00:27,  3.15it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:21<00:26,  3.17it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:22<00:26,  3.18it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:22<00:25,  3.19it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:23<00:24,  3.20it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:23<00:23,  3.22it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:24<00:23,  3.23it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:25<00:22,  3.24it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:25<00:21,  3.25it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:26<00:21,  3.27it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:26<00:20,  3.27it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [00:27<00:19,  3.28it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [00:28<00:19,  3.29it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [00:28<00:18,  3.30it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [00:29<00:17,  3.31it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [00:30<00:17,  3.32it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [00:30<00:16,  3.33it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [00:31<00:15,  3.34it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [00:31<00:15,  3.34it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [00:32<00:14,  3.36it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [00:32<00:13,  3.37it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [00:33<00:13,  3.38it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [00:34<00:12,  3.39it/s]Running loglikelihood requests:  71%|███████   | 101/142 [00:34<00:12,  3.40it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [00:35<00:11,  3.41it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [00:35<00:00, 15.85it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [00:36<00:01, 12.10it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [00:36<00:01,  9.51it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [00:37<00:01,  7.75it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [00:37<00:01,  6.53it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [00:38<00:00,  5.69it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [00:39<00:00,  5.10it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [00:39<00:00,  4.72it/s]Running loglikelihood requests: 100%|██████████| 142/142 [00:39<00:00,  3.59it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:6'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:6'}
full model:
{'wnli': {'alias': 'wnli', 'acc,none': 0.5352112676056338, 'acc_stderr,none': 0.0596130578497224}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.31190044950254875
0.7306280553786256
0.6985925052465822
0.5843516732063875
0.8124235844589114
0.820438203921003
0.6253838264581936
0.7600037006947045
0.8225872978677496
0.708398461303215
0.8646089269479391
0.8239362351853257
0.7608099850331435
0.6657423857513638
0.7943257460202938
0.7511476003698512
0.9073696655228775
0.8741838353767599
0.7945799099309127
0.9323691001541556
0.865243808509542
0.8176606226311932
0.6785099625983169
0.9579534328203848
0.788928884938056
0.9833718962298513
0.5933012307657521
0.7829988799240639
0.7823073743206628
0.31190044950254875
0.7306280553786256
0.6985925052465822
0.5843516732063875
0.8124235844589114
0.820438203921003
0.6253838264581936
0.7600037006947045
0.8225872978677496
0.708398461303215
0.8646089269479391
0.8239362351853257
0.7608099850331435
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[7, 4, 5, 1, 6, 3, 2, 0]
tensor([7, 4, 5, 1, 6, 3, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 2, 4, 5, 6, 1, 0, 3]
tensor([7, 2, 4, 5, 6, 1, 0, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 5, 4, 1, 2, 0]
tensor([6, 3, 7, 5, 4, 1, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 6, 4, 7, 1, 2, 0]
tensor([5, 3, 6, 4, 7, 1, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 3, 6, 5, 7, 2, 1, 0]
tensor([4, 3, 6, 5, 7, 2, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1, 2, 3, 2, 3, 0]
tensor([0, 1, 1, 2, 3, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 1, 2, 2, 3, 0, 3, 1]
tensor([0, 1, 2, 2, 3, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 1, 1, 1.0, 1.0, 1.0, 1.0]
tensor([0, 0, 1, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 4
tensor([7])
tensor(7)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 5 to 15
done!
Normal merging for layer 16
tensor([0, 7])
tensor(0)
tensor([1, 2])
tensor(1)
tensor([3, 5])
tensor(3)
tensor([4, 6])
tensor(4)
done!
Normal merging for layer 17
tensor([0, 5])
tensor(0)
tensor([1, 7])
tensor(1)
tensor([2, 3])
tensor(2)
tensor([4, 6])
tensor(4)
done!
Cross-layer merge completed for layers 18 to 30
done!
Normal merging for layer 31
tensor([0, 1])
tensor(0)
tensor([2, 3, 4, 5, 6, 7])
tensor(2)
done!
all done!
Model size: 12.2608 GB
165
cuda:6
mnli
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:36<00:36, 36.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:50<00:00, 22.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:50<00:00, 25.03s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: mnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: mnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mnli?recursive=False&expand=False HTTP/1.1" 307 141
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mnli?recursive=False&expand=False HTTP/1.1" 200 512
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140321093269216 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321093269216 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321093269216 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321093269216 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140321622935216 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321622935216 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321622935216 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321622935216 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of mnli from None to 0
INFO:lm_eval.api.task:Building contexts for mnli on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 118550.14it/s]
DEBUG:lm_eval.evaluator:Task: mnli; number of requests on this rank: 300
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/300 [00:00<?, ?it/s]Running loglikelihood requests:   2%|▏         | 7/300 [00:00<00:15, 18.33it/s]Running loglikelihood requests:   3%|▎         | 9/300 [00:01<00:51,  5.60it/s]Running loglikelihood requests:   3%|▎         | 10/300 [00:02<01:27,  3.30it/s]Running loglikelihood requests:   4%|▎         | 11/300 [00:03<02:01,  2.38it/s]Running loglikelihood requests:   4%|▍         | 13/300 [00:04<02:02,  2.35it/s]Running loglikelihood requests:   5%|▍         | 14/300 [00:04<02:27,  1.94it/s]Running loglikelihood requests:   5%|▌         | 16/300 [00:05<02:15,  2.10it/s]Running loglikelihood requests:   6%|▌         | 17/300 [00:06<02:35,  1.82it/s]Running loglikelihood requests:   6%|▌         | 18/300 [00:07<02:52,  1.64it/s]Running loglikelihood requests:   7%|▋         | 20/300 [00:08<02:28,  1.89it/s]Running loglikelihood requests:   7%|▋         | 21/300 [00:08<02:45,  1.69it/s]Running loglikelihood requests:   8%|▊         | 23/300 [00:09<02:23,  1.92it/s]Running loglikelihood requests:   8%|▊         | 25/300 [00:10<02:11,  2.10it/s]Running loglikelihood requests:   9%|▊         | 26/300 [00:11<02:28,  1.85it/s]Running loglikelihood requests:   9%|▉         | 28/300 [00:12<02:12,  2.05it/s]Running loglikelihood requests:  10%|▉         | 29/300 [00:12<02:29,  1.81it/s]Running loglikelihood requests:  10%|█         | 30/300 [00:13<02:43,  1.65it/s]Running loglikelihood requests:  10%|█         | 31/300 [00:14<02:54,  1.54it/s]Running loglikelihood requests:  11%|█         | 32/300 [00:15<03:03,  1.46it/s]Running loglikelihood requests:  11%|█▏        | 34/300 [00:16<02:28,  1.79it/s]Running loglikelihood requests:  12%|█▏        | 36/300 [00:16<02:10,  2.02it/s]Running loglikelihood requests:  13%|█▎        | 38/300 [00:17<02:00,  2.18it/s]Running loglikelihood requests:  13%|█▎        | 40/300 [00:18<01:52,  2.31it/s]Running loglikelihood requests:  14%|█▎        | 41/300 [00:19<02:08,  2.02it/s]Running loglikelihood requests:  14%|█▍        | 42/300 [00:19<02:22,  1.81it/s]Running loglikelihood requests:  14%|█▍        | 43/300 [00:20<02:34,  1.66it/s]Running loglikelihood requests:  15%|█▌        | 45/300 [00:21<02:10,  1.96it/s]Running loglikelihood requests:  16%|█▌        | 47/300 [00:22<01:56,  2.16it/s]Running loglikelihood requests:  16%|█▌        | 48/300 [00:22<02:11,  1.91it/s]Running loglikelihood requests:  17%|█▋        | 50/300 [00:23<01:57,  2.13it/s]Running loglikelihood requests:  17%|█▋        | 52/300 [00:24<01:47,  2.30it/s]Running loglikelihood requests:  18%|█▊        | 53/300 [00:25<02:02,  2.01it/s]Running loglikelihood requests:  18%|█▊        | 54/300 [00:25<02:15,  1.81it/s]Running loglikelihood requests:  18%|█▊        | 55/300 [00:26<02:26,  1.68it/s]Running loglikelihood requests:  19%|█▉        | 57/300 [00:27<02:02,  1.98it/s]Running loglikelihood requests:  20%|█▉        | 59/300 [00:28<01:49,  2.20it/s]Running loglikelihood requests:  20%|██        | 61/300 [00:28<01:41,  2.36it/s]Running loglikelihood requests:  21%|██        | 62/300 [00:29<01:55,  2.07it/s]Running loglikelihood requests:  21%|██▏       | 64/300 [00:30<01:44,  2.27it/s]Running loglikelihood requests:  22%|██▏       | 65/300 [00:31<01:57,  2.00it/s]Running loglikelihood requests:  22%|██▏       | 66/300 [00:31<02:08,  1.82it/s]Running loglikelihood requests:  22%|██▏       | 67/300 [00:32<02:18,  1.69it/s]Running loglikelihood requests:  23%|██▎       | 69/300 [00:33<01:54,  2.01it/s]Running loglikelihood requests:  24%|██▎       | 71/300 [00:33<01:42,  2.24it/s]Running loglikelihood requests:  24%|██▍       | 72/300 [00:34<01:54,  1.99it/s]Running loglikelihood requests:  24%|██▍       | 73/300 [00:35<02:05,  1.81it/s]Running loglikelihood requests:  25%|██▌       | 75/300 [00:36<01:46,  2.11it/s]Running loglikelihood requests:  26%|██▌       | 77/300 [00:36<01:36,  2.32it/s]Running loglikelihood requests:  26%|██▋       | 79/300 [00:37<01:29,  2.47it/s]Running loglikelihood requests:  27%|██▋       | 80/300 [00:38<01:42,  2.15it/s]Running loglikelihood requests:  27%|██▋       | 81/300 [00:38<01:53,  1.93it/s]Running loglikelihood requests:  27%|██▋       | 82/300 [00:39<02:02,  1.77it/s]Running loglikelihood requests:  28%|██▊       | 83/300 [00:40<02:10,  1.67it/s]Running loglikelihood requests:  28%|██▊       | 85/300 [00:41<01:46,  2.02it/s]Running loglikelihood requests:  29%|██▊       | 86/300 [00:41<01:56,  1.84it/s]Running loglikelihood requests:  29%|██▉       | 88/300 [00:42<01:38,  2.14it/s]Running loglikelihood requests:  30%|██▉       | 89/300 [00:43<01:49,  1.92it/s]Running loglikelihood requests:  30%|███       | 91/300 [00:43<01:34,  2.20it/s]Running loglikelihood requests:  31%|███       | 93/300 [00:44<01:26,  2.40it/s]Running loglikelihood requests:  32%|███▏      | 95/300 [00:45<01:20,  2.54it/s]Running loglikelihood requests:  32%|███▏      | 96/300 [00:45<01:32,  2.21it/s]Running loglikelihood requests:  33%|███▎      | 98/300 [00:46<01:23,  2.41it/s]Running loglikelihood requests:  33%|███▎      | 99/300 [00:47<01:34,  2.12it/s]Running loglikelihood requests:  34%|███▎      | 101/300 [00:48<01:24,  2.35it/s]Running loglikelihood requests:  34%|███▍      | 102/300 [00:48<01:35,  2.08it/s]Running loglikelihood requests:  34%|███▍      | 103/300 [00:49<01:44,  1.89it/s]Running loglikelihood requests:  35%|███▍      | 104/300 [00:50<01:51,  1.76it/s]Running loglikelihood requests:  35%|███▌      | 105/300 [00:50<01:57,  1.67it/s]Running loglikelihood requests:  36%|███▌      | 107/300 [00:51<01:34,  2.04it/s]Running loglikelihood requests:  36%|███▌      | 108/300 [00:52<01:43,  1.86it/s]Running loglikelihood requests:  36%|███▋      | 109/300 [00:52<01:49,  1.74it/s]Running loglikelihood requests:  37%|███▋      | 111/300 [00:53<01:30,  2.09it/s]Running loglikelihood requests:  38%|███▊      | 113/300 [00:54<01:20,  2.34it/s]Running loglikelihood requests:  38%|███▊      | 115/300 [00:54<01:13,  2.51it/s]Running loglikelihood requests:  39%|███▉      | 117/300 [00:55<01:09,  2.64it/s]Running loglikelihood requests:  40%|███▉      | 119/300 [00:56<01:06,  2.72it/s]Running loglikelihood requests:  40%|████      | 121/300 [00:56<01:04,  2.78it/s]Running loglikelihood requests:  41%|████      | 122/300 [00:57<01:15,  2.36it/s]Running loglikelihood requests:  41%|████▏     | 124/300 [00:58<01:09,  2.53it/s]Running loglikelihood requests:  42%|████▏     | 125/300 [00:59<01:19,  2.21it/s]Running loglikelihood requests:  42%|████▏     | 126/300 [00:59<01:27,  1.99it/s]Running loglikelihood requests:  42%|████▏     | 127/300 [01:00<01:34,  1.83it/s]Running loglikelihood requests:  43%|████▎     | 129/300 [01:01<01:18,  2.17it/s]Running loglikelihood requests:  44%|████▎     | 131/300 [01:01<01:10,  2.40it/s]Running loglikelihood requests:  48%|████▊     | 145/300 [01:02<00:16,  9.53it/s]Running loglikelihood requests:  49%|████▉     | 147/300 [01:03<00:27,  5.54it/s]Running loglikelihood requests:  50%|████▉     | 149/300 [01:04<00:30,  4.90it/s]Running loglikelihood requests:  50%|█████     | 150/300 [01:04<00:37,  3.98it/s]Running loglikelihood requests:  51%|█████     | 152/300 [01:05<00:39,  3.73it/s]Running loglikelihood requests:  51%|█████▏    | 154/300 [01:05<00:41,  3.55it/s]Running loglikelihood requests:  52%|█████▏    | 155/300 [01:06<00:48,  2.97it/s]Running loglikelihood requests:  52%|█████▏    | 157/300 [01:07<00:47,  3.00it/s]Running loglikelihood requests:  53%|█████▎    | 159/300 [01:07<00:46,  3.03it/s]Running loglikelihood requests:  54%|█████▎    | 161/300 [01:08<00:45,  3.06it/s]Running loglikelihood requests:  54%|█████▍    | 162/300 [01:09<00:52,  2.61it/s]Running loglikelihood requests:  55%|█████▍    | 164/300 [01:09<00:49,  2.76it/s]Running loglikelihood requests:  55%|█████▌    | 165/300 [01:10<00:56,  2.41it/s]Running loglikelihood requests:  56%|█████▌    | 167/300 [01:11<00:50,  2.62it/s]Running loglikelihood requests:  56%|█████▋    | 169/300 [01:11<00:47,  2.78it/s]Running loglikelihood requests:  57%|█████▋    | 170/300 [01:12<00:53,  2.42it/s]Running loglikelihood requests:  57%|█████▋    | 171/300 [01:13<00:59,  2.17it/s]Running loglikelihood requests:  57%|█████▋    | 172/300 [01:13<01:04,  2.00it/s]Running loglikelihood requests:  58%|█████▊    | 173/300 [01:14<01:07,  1.87it/s]Running loglikelihood requests:  58%|█████▊    | 175/300 [01:14<00:55,  2.26it/s]Running loglikelihood requests:  59%|█████▉    | 177/300 [01:15<00:48,  2.54it/s]Running loglikelihood requests:  60%|█████▉    | 179/300 [01:16<00:44,  2.74it/s]Running loglikelihood requests:  60%|██████    | 180/300 [01:16<00:50,  2.40it/s]Running loglikelihood requests:  60%|██████    | 181/300 [01:17<00:55,  2.16it/s]Running loglikelihood requests:  61%|██████    | 183/300 [01:18<00:47,  2.48it/s]Running loglikelihood requests:  62%|██████▏   | 185/300 [01:18<00:42,  2.70it/s]Running loglikelihood requests:  62%|██████▏   | 186/300 [01:19<00:47,  2.38it/s]Running loglikelihood requests:  63%|██████▎   | 188/300 [01:19<00:42,  2.64it/s]Running loglikelihood requests:  63%|██████▎   | 190/300 [01:20<00:38,  2.83it/s]Running loglikelihood requests:  64%|██████▎   | 191/300 [01:21<00:44,  2.47it/s]Running loglikelihood requests:  64%|██████▍   | 192/300 [01:21<00:48,  2.22it/s]Running loglikelihood requests:  65%|██████▍   | 194/300 [01:22<00:41,  2.54it/s]Running loglikelihood requests:  65%|██████▌   | 195/300 [01:22<00:46,  2.27it/s]Running loglikelihood requests:  65%|██████▌   | 196/300 [01:23<00:49,  2.08it/s]Running loglikelihood requests:  66%|██████▌   | 198/300 [01:24<00:41,  2.45it/s]Running loglikelihood requests:  66%|██████▋   | 199/300 [01:24<00:45,  2.21it/s]Running loglikelihood requests:  67%|██████▋   | 200/300 [01:25<00:48,  2.05it/s]Running loglikelihood requests:  67%|██████▋   | 201/300 [01:25<00:51,  1.93it/s]Running loglikelihood requests:  68%|██████▊   | 203/300 [01:26<00:41,  2.35it/s]Running loglikelihood requests:  68%|██████▊   | 205/300 [01:27<00:35,  2.64it/s]Running loglikelihood requests:  69%|██████▊   | 206/300 [01:27<00:40,  2.35it/s]Running loglikelihood requests:  69%|██████▉   | 208/300 [01:28<00:34,  2.64it/s]Running loglikelihood requests:  70%|███████   | 210/300 [01:28<00:31,  2.85it/s]Running loglikelihood requests:  71%|███████   | 212/300 [01:29<00:29,  3.00it/s]Running loglikelihood requests:  71%|███████   | 213/300 [01:30<00:33,  2.61it/s]Running loglikelihood requests:  72%|███████▏  | 215/300 [01:30<00:29,  2.83it/s]Running loglikelihood requests:  72%|███████▏  | 216/300 [01:31<00:33,  2.49it/s]Running loglikelihood requests:  72%|███████▏  | 217/300 [01:31<00:36,  2.25it/s]Running loglikelihood requests:  73%|███████▎  | 219/300 [01:32<00:31,  2.59it/s]Running loglikelihood requests:  74%|███████▎  | 221/300 [01:33<00:27,  2.83it/s]Running loglikelihood requests:  74%|███████▍  | 222/300 [01:33<00:31,  2.49it/s]Running loglikelihood requests:  75%|███████▍  | 224/300 [01:34<00:27,  2.76it/s]Running loglikelihood requests:  75%|███████▌  | 225/300 [01:34<00:30,  2.45it/s]Running loglikelihood requests:  76%|███████▌  | 227/300 [01:35<00:26,  2.74it/s]Running loglikelihood requests:  76%|███████▌  | 228/300 [01:36<00:29,  2.43it/s]Running loglikelihood requests:  76%|███████▋  | 229/300 [01:36<00:31,  2.22it/s]Running loglikelihood requests:  77%|███████▋  | 231/300 [01:37<00:26,  2.58it/s]Running loglikelihood requests:  77%|███████▋  | 232/300 [01:37<00:29,  2.33it/s]Running loglikelihood requests:  78%|███████▊  | 233/300 [01:38<00:31,  2.15it/s]Running loglikelihood requests:  78%|███████▊  | 234/300 [01:38<00:32,  2.02it/s]Running loglikelihood requests:  78%|███████▊  | 235/300 [01:39<00:33,  1.94it/s]Running loglikelihood requests:  79%|███████▊  | 236/300 [01:40<00:34,  1.87it/s]Running loglikelihood requests:  79%|███████▉  | 237/300 [01:40<00:34,  1.81it/s]Running loglikelihood requests:  79%|███████▉  | 238/300 [01:41<00:34,  1.80it/s]Running loglikelihood requests:  80%|████████  | 240/300 [01:41<00:25,  2.31it/s]Running loglikelihood requests:  81%|████████  | 242/300 [01:42<00:22,  2.61it/s]Running loglikelihood requests:  81%|████████▏ | 244/300 [01:42<00:19,  2.87it/s]Running loglikelihood requests:  82%|████████▏ | 246/300 [01:43<00:17,  3.06it/s]Running loglikelihood requests:  83%|████████▎ | 248/300 [01:44<00:16,  3.20it/s]Running loglikelihood requests:  83%|████████▎ | 250/300 [01:44<00:15,  3.26it/s]Running loglikelihood requests:  84%|████████▍ | 252/300 [01:45<00:14,  3.33it/s]Running loglikelihood requests:  85%|████████▍ | 254/300 [01:45<00:13,  3.39it/s]Running loglikelihood requests:  85%|████████▌ | 256/300 [01:46<00:12,  3.43it/s]Running loglikelihood requests:  86%|████████▌ | 257/300 [01:46<00:14,  2.95it/s]Running loglikelihood requests:  86%|████████▌ | 258/300 [01:47<00:16,  2.61it/s]Running loglikelihood requests:  87%|████████▋ | 260/300 [01:48<00:13,  2.90it/s]Running loglikelihood requests:  87%|████████▋ | 261/300 [01:48<00:15,  2.58it/s]Running loglikelihood requests:  88%|████████▊ | 263/300 [01:49<00:12,  2.89it/s]Running loglikelihood requests:  88%|████████▊ | 264/300 [01:49<00:14,  2.56it/s]Running loglikelihood requests:  88%|████████▊ | 265/300 [01:50<00:14,  2.34it/s]Running loglikelihood requests:  89%|████████▊ | 266/300 [01:50<00:15,  2.19it/s]Running loglikelihood requests:  89%|████████▉ | 268/300 [01:51<00:12,  2.62it/s]Running loglikelihood requests:  90%|█████████ | 270/300 [01:51<00:10,  2.93it/s]Running loglikelihood requests:  91%|█████████ | 272/300 [01:52<00:08,  3.16it/s]Running loglikelihood requests:  91%|█████████▏| 274/300 [01:53<00:07,  3.32it/s]Running loglikelihood requests:  92%|█████████▏| 275/300 [01:53<00:08,  2.90it/s]Running loglikelihood requests:  92%|█████████▏| 277/300 [01:54<00:07,  3.15it/s]Running loglikelihood requests:  93%|█████████▎| 278/300 [01:54<00:07,  2.77it/s]Running loglikelihood requests:  93%|█████████▎| 279/300 [01:55<00:08,  2.51it/s]Running loglikelihood requests:  94%|█████████▎| 281/300 [01:55<00:06,  2.89it/s]Running loglikelihood requests:  94%|█████████▍| 282/300 [01:56<00:06,  2.59it/s]Running loglikelihood requests:  94%|█████████▍| 283/300 [01:56<00:07,  2.39it/s]Running loglikelihood requests:  95%|█████████▍| 284/300 [01:57<00:07,  2.24it/s]Running loglikelihood requests:  95%|█████████▌| 286/300 [01:57<00:05,  2.71it/s]Running loglikelihood requests:  96%|█████████▌| 288/300 [01:58<00:03,  3.04it/s]Running loglikelihood requests:  97%|█████████▋| 290/300 [01:58<00:03,  3.29it/s]Running loglikelihood requests:  97%|█████████▋| 292/300 [01:59<00:02,  3.48it/s]Running loglikelihood requests:  98%|█████████▊| 293/300 [01:59<00:02,  3.03it/s]Running loglikelihood requests:  98%|█████████▊| 294/300 [02:00<00:02,  2.72it/s]Running loglikelihood requests:  99%|█████████▊| 296/300 [02:00<00:01,  3.09it/s]Running loglikelihood requests:  99%|█████████▉| 298/300 [02:01<00:00,  3.39it/s]Running loglikelihood requests: 100%|█████████▉| 299/300 [02:01<00:00,  3.01it/s]Running loglikelihood requests: 100%|██████████| 300/300 [02:01<00:00,  2.46it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:7'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:7'}
full model:
{'mnli': {'alias': 'mnli', 'acc,none': 0.4, 'acc_stderr,none': 0.0492365963917331}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8171409999433525
0.8739893758684915
0.8333413990263023
0.8782828649633461
0.7602933913725962
0.7047335470608914
0.9813668198386444
0.7850949920891616
0.748983595161589
0.6557873189175887
0.7043210867322975
0.9420132312730142
0.9548348739467002
0.8459964595909506
0.6932219150662173
0.8804855383939617
0.868566987197908
0.8437399449827557
0.9153450822718411
0.8433082326287293
0.910397090611448
0.7626694638419581
0.752151649412453
0.8346986395359313
0.9530736745854924
0.8635918585764104
0.8647192950921815
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[4, 5, 6, 1, 7, 2, 3, 0]
tensor([4, 5, 6, 1, 7, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 4, 6, 2, 5, 1, 3, 0]
tensor([7, 4, 6, 2, 5, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 7, 3, 5, 1, 4, 0]
tensor([6, 2, 7, 3, 5, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 2, 5, 4, 6, 1, 3, 0]
tensor([7, 2, 5, 4, 6, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 5, 3, 1, 2, 4, 0]
tensor([0, 1, 5, 3, 1, 2, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 1, 2, 2, 3, 0]
tensor([0, 3, 1, 1, 2, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 2, 3, 1, 2, 0, 3, 1]
tensor([0, 2, 3, 1, 2, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 3, 1, 2, 2, 1, 3, 0]
tensor([0, 3, 1, 2, 2, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1.0, 1]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 4 to 8
done!
Normal merging for layer 9
tensor([0, 7])
tensor(0)
tensor([1, 4])
tensor(1)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 10 to 20
done!
Normal merging for layer 21
tensor([0, 7])
tensor(0)
tensor([2, 3])
tensor(2)
tensor([4, 5])
tensor(4)
tensor([1, 6])
tensor(1)
done!
Normal merging for layer 22
tensor([0, 5])
tensor(0)
tensor([3, 7])
tensor(3)
tensor([1, 4])
tensor(1)
tensor([2, 6])
tensor(2)
done!
Normal merging for layer 23
tensor([0, 7])
tensor(0)
tensor([2, 5])
tensor(2)
tensor([3, 4])
tensor(3)
tensor([1, 6])
tensor(1)
done!
Cross-layer merge completed for layers 24 to 25
done!
Normal merging for layer 26
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 27 to 31
done!
all done!
Model size: 12.5127 GB
254
cuda:7
sciq
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:37<00:37, 37.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:50<00:00, 23.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:50<00:00, 25.32s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/sciq HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/sciq HTTP/1.1" 200 1240
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/sciq/sciq.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/sciq HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/sciq HTTP/1.1" 200 1240
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/sciq/resolve/2c94ad3e1aafab77146f384e23536f97a4849815/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/sciq/resolve/2c94ad3e1aafab77146f384e23536f97a4849815/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/sciq/revision/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 111
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/sciq/revision/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 1240
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/sciq/tree/2c94ad3e1aafab77146f384e23536f97a4849815?recursive=False&expand=False HTTP/1.1" 307 136
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/sciq/tree/2c94ad3e1aafab77146f384e23536f97a4849815?recursive=False&expand=False HTTP/1.1" 200 291
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/sciq/tree/2c94ad3e1aafab77146f384e23536f97a4849815/data?recursive=False&expand=False HTTP/1.1" 307 141
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/sciq/tree/2c94ad3e1aafab77146f384e23536f97a4849815/data?recursive=False&expand=False HTTP/1.1" 200 358
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/sciq/revision/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 111
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/sciq/revision/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 1240
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/sciq/resolve/2c94ad3e1aafab77146f384e23536f97a4849815/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/sciq/resolve/2c94ad3e1aafab77146f384e23536f97a4849815/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140326447795760 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_sciq_default_0.0.0_2c94ad3e1aafab77146f384e23536f97a4849815.lock
DEBUG:filelock:Lock 140326447795760 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_sciq_default_0.0.0_2c94ad3e1aafab77146f384e23536f97a4849815.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815/dataset_info.json
DEBUG:filelock:Attempting to release lock 140326447795760 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_sciq_default_0.0.0_2c94ad3e1aafab77146f384e23536f97a4849815.lock
DEBUG:filelock:Lock 140326447795760 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_sciq_default_0.0.0_2c94ad3e1aafab77146f384e23536f97a4849815.lock
DEBUG:filelock:Attempting to acquire lock 140326447795760 on /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815_builder.lock
DEBUG:filelock:Lock 140326447795760 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815/dataset_info.json
DEBUG:filelock:Attempting to release lock 140326447795760 on /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815_builder.lock
DEBUG:filelock:Lock 140326447795760 released on /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of sciq from None to 0
INFO:lm_eval.api.task:Building contexts for sciq on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s] 96%|█████████▌| 96/100 [00:00<00:00, 950.25it/s]100%|██████████| 100/100 [00:00<00:00, 948.78it/s]
DEBUG:lm_eval.evaluator:Task: sciq; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:04<28:13,  4.24s/it]Running loglikelihood requests:   0%|          | 2/400 [00:08<26:16,  3.96s/it]Running loglikelihood requests:   1%|          | 3/400 [00:11<25:36,  3.87s/it]Running loglikelihood requests:   1%|          | 4/400 [00:15<25:17,  3.83s/it]Running loglikelihood requests:   1%|▏         | 5/400 [00:19<24:33,  3.73s/it]Running loglikelihood requests:   2%|▏         | 7/400 [00:20<13:40,  2.09s/it]Running loglikelihood requests:   2%|▏         | 8/400 [00:23<16:12,  2.48s/it]Running loglikelihood requests:   2%|▏         | 9/400 [00:26<17:27,  2.68s/it]Running loglikelihood requests:   2%|▎         | 10/400 [00:30<18:24,  2.83s/it]Running loglikelihood requests:   3%|▎         | 11/400 [00:33<19:04,  2.94s/it]Running loglikelihood requests:   3%|▎         | 12/400 [00:36<19:28,  3.01s/it]Running loglikelihood requests:   3%|▎         | 13/400 [00:39<19:15,  2.99s/it]Running loglikelihood requests:   4%|▎         | 14/400 [00:42<19:07,  2.97s/it]Running loglikelihood requests:   4%|▍         | 15/400 [00:45<18:59,  2.96s/it]Running loglikelihood requests:   4%|▍         | 16/400 [00:48<18:51,  2.95s/it]Running loglikelihood requests:   4%|▍         | 17/400 [00:50<18:33,  2.91s/it]Running loglikelihood requests:   4%|▍         | 18/400 [00:53<18:21,  2.88s/it]Running loglikelihood requests:   5%|▍         | 19/400 [00:56<18:08,  2.86s/it]Running loglikelihood requests:   5%|▌         | 20/400 [00:59<17:55,  2.83s/it]Running loglikelihood requests:   5%|▌         | 21/400 [01:02<17:42,  2.80s/it]Running loglikelihood requests:   6%|▌         | 22/400 [01:04<17:33,  2.79s/it]Running loglikelihood requests:   6%|▌         | 23/400 [01:07<17:24,  2.77s/it]Running loglikelihood requests:   6%|▌         | 24/400 [01:10<17:17,  2.76s/it]Running loglikelihood requests:   6%|▋         | 25/400 [01:12<16:52,  2.70s/it]Running loglikelihood requests:   6%|▋         | 26/400 [01:15<16:32,  2.65s/it]Running loglikelihood requests:   7%|▋         | 27/400 [01:17<16:16,  2.62s/it]Running loglikelihood requests:   7%|▋         | 28/400 [01:20<16:05,  2.60s/it]Running loglikelihood requests:   8%|▊         | 31/400 [01:20<07:31,  1.22s/it]Running loglikelihood requests:   8%|▊         | 32/400 [01:23<08:54,  1.45s/it]Running loglikelihood requests:   8%|▊         | 33/400 [01:25<10:00,  1.64s/it]Running loglikelihood requests:   8%|▊         | 34/400 [01:27<10:52,  1.78s/it]Running loglikelihood requests:   9%|▉         | 35/400 [01:29<11:26,  1.88s/it]Running loglikelihood requests:   9%|▉         | 36/400 [01:32<11:51,  1.96s/it]Running loglikelihood requests:   9%|▉         | 37/400 [01:34<12:10,  2.01s/it]Running loglikelihood requests:  10%|▉         | 38/400 [01:36<12:23,  2.05s/it]Running loglikelihood requests:  10%|█         | 40/400 [01:38<09:40,  1.61s/it]Running loglikelihood requests:  10%|█         | 41/400 [01:40<10:10,  1.70s/it]Running loglikelihood requests:  10%|█         | 42/400 [01:42<10:33,  1.77s/it]Running loglikelihood requests:  11%|█         | 43/400 [01:44<10:51,  1.82s/it]Running loglikelihood requests:  11%|█         | 44/400 [01:46<11:03,  1.86s/it]Running loglikelihood requests:  11%|█▏        | 45/400 [01:48<11:04,  1.87s/it]Running loglikelihood requests:  12%|█▏        | 46/400 [01:50<11:04,  1.88s/it]Running loglikelihood requests:  12%|█▏        | 47/400 [01:52<11:03,  1.88s/it]Running loglikelihood requests:  12%|█▏        | 48/400 [01:53<11:01,  1.88s/it]Running loglikelihood requests:  12%|█▏        | 49/400 [01:55<10:59,  1.88s/it]Running loglikelihood requests:  12%|█▎        | 50/400 [01:57<10:56,  1.88s/it]Running loglikelihood requests:  13%|█▎        | 51/400 [01:59<10:54,  1.88s/it]Running loglikelihood requests:  13%|█▎        | 52/400 [02:01<10:52,  1.87s/it]Running loglikelihood requests:  13%|█▎        | 53/400 [02:03<10:31,  1.82s/it]Running loglikelihood requests:  14%|█▎        | 54/400 [02:04<10:14,  1.78s/it]Running loglikelihood requests:  14%|█▍        | 55/400 [02:06<10:02,  1.75s/it]Running loglikelihood requests:  14%|█▍        | 57/400 [02:08<07:30,  1.31s/it]Running loglikelihood requests:  14%|█▍        | 58/400 [02:09<07:54,  1.39s/it]Running loglikelihood requests:  15%|█▍        | 59/400 [02:11<08:12,  1.44s/it]Running loglikelihood requests:  16%|█▌        | 62/400 [02:12<05:25,  1.04it/s]Running loglikelihood requests:  16%|█▌        | 63/400 [02:14<06:07,  1.09s/it]Running loglikelihood requests:  16%|█▌        | 64/400 [02:16<06:45,  1.21s/it]Running loglikelihood requests:  16%|█▋        | 65/400 [02:17<07:11,  1.29s/it]Running loglikelihood requests:  16%|█▋        | 66/400 [02:19<07:32,  1.35s/it]Running loglikelihood requests:  17%|█▋        | 67/400 [02:20<07:48,  1.41s/it]Running loglikelihood requests:  17%|█▋        | 68/400 [02:22<07:59,  1.45s/it]Running loglikelihood requests:  18%|█▊        | 72/400 [02:22<03:17,  1.66it/s]Running loglikelihood requests:  18%|█▊        | 73/400 [02:24<04:10,  1.31it/s]Running loglikelihood requests:  18%|█▊        | 74/400 [02:25<04:58,  1.09it/s]Running loglikelihood requests:  19%|█▉        | 77/400 [02:27<03:51,  1.39it/s]Running loglikelihood requests:  20%|█▉        | 78/400 [02:28<04:34,  1.17it/s]Running loglikelihood requests:  20%|█▉        | 79/400 [02:30<05:13,  1.02it/s]Running loglikelihood requests:  20%|██        | 81/400 [02:31<04:35,  1.16it/s]Running loglikelihood requests:  20%|██        | 82/400 [02:32<05:05,  1.04it/s]Running loglikelihood requests:  21%|██        | 83/400 [02:33<05:30,  1.04s/it]Running loglikelihood requests:  21%|██        | 84/400 [02:35<05:51,  1.11s/it]Running loglikelihood requests:  21%|██▏       | 85/400 [02:36<06:06,  1.16s/it]Running loglikelihood requests:  22%|██▏       | 86/400 [02:37<06:17,  1.20s/it]Running loglikelihood requests:  22%|██▏       | 89/400 [02:39<04:05,  1.27it/s]Running loglikelihood requests:  22%|██▎       | 90/400 [02:40<04:36,  1.12it/s]Running loglikelihood requests:  23%|██▎       | 91/400 [02:41<05:03,  1.02it/s]Running loglikelihood requests:  23%|██▎       | 92/400 [02:43<05:25,  1.06s/it]Running loglikelihood requests:  23%|██▎       | 93/400 [02:44<05:43,  1.12s/it]Running loglikelihood requests:  24%|██▍       | 97/400 [02:45<03:14,  1.56it/s]Running loglikelihood requests:  24%|██▍       | 98/400 [02:46<03:47,  1.33it/s]Running loglikelihood requests:  25%|██▍       | 99/400 [02:48<04:17,  1.17it/s]Running loglikelihood requests:  25%|██▌       | 100/400 [02:49<04:44,  1.06it/s]Running loglikelihood requests:  25%|██▌       | 101/400 [02:50<05:06,  1.02s/it]Running loglikelihood requests:  26%|██▌       | 102/400 [02:52<05:23,  1.08s/it]Running loglikelihood requests:  26%|██▌       | 103/400 [02:53<05:35,  1.13s/it]Running loglikelihood requests:  26%|██▌       | 104/400 [02:54<05:45,  1.17s/it]Running loglikelihood requests:  26%|██▋       | 105/400 [02:55<05:50,  1.19s/it]Running loglikelihood requests:  26%|██▋       | 106/400 [02:57<05:54,  1.20s/it]Running loglikelihood requests:  27%|██▋       | 107/400 [02:58<05:55,  1.21s/it]Running loglikelihood requests:  27%|██▋       | 108/400 [02:59<05:56,  1.22s/it]Running loglikelihood requests:  27%|██▋       | 109/400 [03:00<05:55,  1.22s/it]Running loglikelihood requests:  28%|██▊       | 110/400 [03:01<05:54,  1.22s/it]Running loglikelihood requests:  28%|██▊       | 111/400 [03:03<05:52,  1.22s/it]Running loglikelihood requests:  28%|██▊       | 112/400 [03:04<05:51,  1.22s/it]Running loglikelihood requests:  28%|██▊       | 113/400 [03:05<05:48,  1.22s/it]Running loglikelihood requests:  28%|██▊       | 114/400 [03:06<05:46,  1.21s/it]Running loglikelihood requests:  29%|██▉       | 115/400 [03:07<05:44,  1.21s/it]Running loglikelihood requests:  29%|██▉       | 116/400 [03:09<05:42,  1.20s/it]Running loglikelihood requests:  29%|██▉       | 117/400 [03:10<05:38,  1.20s/it]Running loglikelihood requests:  30%|██▉       | 118/400 [03:11<05:34,  1.19s/it]Running loglikelihood requests:  30%|██▉       | 119/400 [03:12<05:31,  1.18s/it]Running loglikelihood requests:  30%|███       | 120/400 [03:13<05:29,  1.18s/it]Running loglikelihood requests:  30%|███       | 121/400 [03:15<05:27,  1.17s/it]Running loglikelihood requests:  30%|███       | 122/400 [03:16<05:25,  1.17s/it]Running loglikelihood requests:  31%|███       | 123/400 [03:17<05:23,  1.17s/it]Running loglikelihood requests:  31%|███       | 124/400 [03:18<05:22,  1.17s/it]Running loglikelihood requests:  31%|███▏      | 125/400 [03:19<05:19,  1.16s/it]Running loglikelihood requests:  32%|███▏      | 126/400 [03:20<05:16,  1.15s/it]Running loglikelihood requests:  32%|███▏      | 127/400 [03:21<05:13,  1.15s/it]Running loglikelihood requests:  32%|███▏      | 128/400 [03:23<05:11,  1.15s/it]Running loglikelihood requests:  34%|███▍      | 136/400 [03:23<01:16,  3.45it/s]Running loglikelihood requests:  34%|███▍      | 137/400 [03:24<01:42,  2.56it/s]Running loglikelihood requests:  34%|███▍      | 138/400 [03:25<02:10,  2.01it/s]Running loglikelihood requests:  35%|███▍      | 139/400 [03:26<02:37,  1.65it/s]Running loglikelihood requests:  36%|███▌      | 142/400 [03:27<02:10,  1.98it/s]Running loglikelihood requests:  36%|███▌      | 143/400 [03:28<02:36,  1.65it/s]Running loglikelihood requests:  36%|███▌      | 144/400 [03:30<03:00,  1.42it/s]Running loglikelihood requests:  36%|███▋      | 145/400 [03:31<03:22,  1.26it/s]Running loglikelihood requests:  36%|███▋      | 146/400 [03:32<03:39,  1.15it/s]Running loglikelihood requests:  37%|███▋      | 147/400 [03:33<03:54,  1.08it/s]Running loglikelihood requests:  37%|███▋      | 148/400 [03:34<04:05,  1.03it/s]Running loglikelihood requests:  37%|███▋      | 149/400 [03:35<04:12,  1.01s/it]Running loglikelihood requests:  38%|███▊      | 150/400 [03:36<04:17,  1.03s/it]Running loglikelihood requests:  38%|███▊      | 151/400 [03:37<04:20,  1.05s/it]Running loglikelihood requests:  38%|███▊      | 152/400 [03:38<04:22,  1.06s/it]Running loglikelihood requests:  38%|███▊      | 153/400 [03:39<04:23,  1.07s/it]Running loglikelihood requests:  38%|███▊      | 154/400 [03:41<04:23,  1.07s/it]Running loglikelihood requests:  39%|███▉      | 155/400 [03:42<04:23,  1.07s/it]Running loglikelihood requests:  39%|███▉      | 156/400 [03:43<04:22,  1.08s/it]Running loglikelihood requests:  39%|███▉      | 157/400 [03:44<04:21,  1.07s/it]Running loglikelihood requests:  40%|███▉      | 158/400 [03:45<04:20,  1.07s/it]Running loglikelihood requests:  40%|███▉      | 159/400 [03:46<04:19,  1.07s/it]Running loglikelihood requests:  40%|████      | 160/400 [03:47<04:17,  1.07s/it]Running loglikelihood requests:  40%|████      | 161/400 [03:48<04:16,  1.07s/it]Running loglikelihood requests:  40%|████      | 162/400 [03:49<04:14,  1.07s/it]Running loglikelihood requests:  41%|████      | 163/400 [03:50<04:13,  1.07s/it]Running loglikelihood requests:  41%|████      | 164/400 [03:51<04:11,  1.07s/it]Running loglikelihood requests:  41%|████▏     | 165/400 [03:52<04:09,  1.06s/it]Running loglikelihood requests:  42%|████▏     | 166/400 [03:53<04:07,  1.06s/it]Running loglikelihood requests:  42%|████▏     | 167/400 [03:54<04:05,  1.05s/it]Running loglikelihood requests:  42%|████▏     | 168/400 [03:55<04:04,  1.05s/it]Running loglikelihood requests:  42%|████▏     | 169/400 [03:56<04:02,  1.05s/it]Running loglikelihood requests:  42%|████▎     | 170/400 [03:58<04:00,  1.05s/it]Running loglikelihood requests:  43%|████▎     | 172/400 [03:59<03:02,  1.25it/s]Running loglikelihood requests:  43%|████▎     | 173/400 [04:00<03:13,  1.17it/s]Running loglikelihood requests:  44%|████▎     | 174/400 [04:01<03:21,  1.12it/s]Running loglikelihood requests:  44%|████▍     | 177/400 [04:02<02:12,  1.68it/s]Running loglikelihood requests:  44%|████▍     | 178/400 [04:03<02:30,  1.48it/s]Running loglikelihood requests:  45%|████▍     | 179/400 [04:04<02:45,  1.34it/s]Running loglikelihood requests:  45%|████▌     | 180/400 [04:05<02:57,  1.24it/s]Running loglikelihood requests:  45%|████▌     | 181/400 [04:06<03:06,  1.17it/s]Running loglikelihood requests:  46%|████▌     | 182/400 [04:07<03:12,  1.13it/s]Running loglikelihood requests:  46%|████▌     | 183/400 [04:07<03:17,  1.10it/s]Running loglikelihood requests:  46%|████▌     | 184/400 [04:08<03:20,  1.08it/s]Running loglikelihood requests:  46%|████▋     | 185/400 [04:09<03:22,  1.06it/s]Running loglikelihood requests:  46%|████▋     | 186/400 [04:10<03:22,  1.06it/s]Running loglikelihood requests:  47%|████▋     | 187/400 [04:11<03:22,  1.05it/s]Running loglikelihood requests:  47%|████▋     | 188/400 [04:12<03:22,  1.05it/s]Running loglikelihood requests:  47%|████▋     | 189/400 [04:13<03:21,  1.05it/s]Running loglikelihood requests:  48%|████▊     | 190/400 [04:14<03:20,  1.05it/s]Running loglikelihood requests:  48%|████▊     | 191/400 [04:15<03:19,  1.05it/s]Running loglikelihood requests:  48%|████▊     | 192/400 [04:16<03:17,  1.05it/s]Running loglikelihood requests:  48%|████▊     | 193/400 [04:17<03:15,  1.06it/s]Running loglikelihood requests:  48%|████▊     | 194/400 [04:18<03:13,  1.06it/s]Running loglikelihood requests:  49%|████▉     | 195/400 [04:19<03:12,  1.06it/s]Running loglikelihood requests:  49%|████▉     | 196/400 [04:20<03:10,  1.07it/s]Running loglikelihood requests:  49%|████▉     | 197/400 [04:21<03:09,  1.07it/s]Running loglikelihood requests:  50%|████▉     | 198/400 [04:22<03:07,  1.08it/s]Running loglikelihood requests:  50%|████▉     | 199/400 [04:23<03:05,  1.08it/s]Running loglikelihood requests:  50%|█████     | 200/400 [04:24<03:04,  1.08it/s]Running loglikelihood requests:  52%|█████▏    | 207/400 [04:24<00:53,  3.58it/s]Running loglikelihood requests:  52%|█████▏    | 208/400 [04:25<01:09,  2.75it/s]Running loglikelihood requests:  52%|█████▏    | 209/400 [04:26<01:26,  2.21it/s]Running loglikelihood requests:  53%|█████▎    | 212/400 [04:27<01:13,  2.56it/s]Running loglikelihood requests:  53%|█████▎    | 213/400 [04:28<01:28,  2.12it/s]Running loglikelihood requests:  54%|█████▎    | 214/400 [04:28<01:42,  1.82it/s]Running loglikelihood requests:  54%|█████▍    | 215/400 [04:29<01:54,  1.61it/s]Running loglikelihood requests:  54%|█████▍    | 216/400 [04:30<02:05,  1.47it/s]Running loglikelihood requests:  54%|█████▍    | 217/400 [04:31<02:13,  1.37it/s]Running loglikelihood requests:  55%|█████▍    | 218/400 [04:32<02:20,  1.30it/s]Running loglikelihood requests:  55%|█████▍    | 219/400 [04:33<02:25,  1.25it/s]Running loglikelihood requests:  55%|█████▌    | 220/400 [04:34<02:28,  1.21it/s]Running loglikelihood requests:  55%|█████▌    | 221/400 [04:35<02:30,  1.19it/s]Running loglikelihood requests:  56%|█████▌    | 222/400 [04:36<02:31,  1.18it/s]Running loglikelihood requests:  56%|█████▌    | 223/400 [04:36<02:31,  1.17it/s]Running loglikelihood requests:  56%|█████▌    | 224/400 [04:37<02:31,  1.16it/s]Running loglikelihood requests:  56%|█████▋    | 225/400 [04:38<02:30,  1.16it/s]Running loglikelihood requests:  56%|█████▋    | 226/400 [04:39<02:30,  1.16it/s]Running loglikelihood requests:  57%|█████▋    | 227/400 [04:40<02:29,  1.16it/s]Running loglikelihood requests:  57%|█████▋    | 228/400 [04:41<02:28,  1.16it/s]Running loglikelihood requests:  57%|█████▋    | 229/400 [04:42<02:28,  1.15it/s]Running loglikelihood requests:  57%|█████▊    | 230/400 [04:42<02:27,  1.15it/s]Running loglikelihood requests:  58%|█████▊    | 231/400 [04:43<02:26,  1.15it/s]Running loglikelihood requests:  58%|█████▊    | 232/400 [04:44<02:25,  1.15it/s]Running loglikelihood requests:  58%|█████▊    | 233/400 [04:45<02:24,  1.16it/s]Running loglikelihood requests:  58%|█████▊    | 234/400 [04:46<02:23,  1.16it/s]Running loglikelihood requests:  59%|█████▉    | 235/400 [04:47<02:22,  1.16it/s]Running loglikelihood requests:  59%|█████▉    | 236/400 [04:48<02:21,  1.16it/s]Running loglikelihood requests:  59%|█████▉    | 237/400 [04:49<02:20,  1.16it/s]Running loglikelihood requests:  60%|██████    | 240/400 [04:49<01:25,  1.87it/s]Running loglikelihood requests:  60%|██████    | 241/400 [04:50<01:35,  1.67it/s]Running loglikelihood requests:  60%|██████    | 242/400 [04:51<01:43,  1.53it/s]Running loglikelihood requests:  61%|██████    | 243/400 [04:52<01:49,  1.43it/s]Running loglikelihood requests:  61%|██████    | 244/400 [04:53<01:54,  1.36it/s]Running loglikelihood requests:  61%|██████▏   | 245/400 [04:54<01:57,  1.32it/s]Running loglikelihood requests:  62%|██████▏   | 246/400 [04:54<01:59,  1.29it/s]Running loglikelihood requests:  62%|██████▏   | 247/400 [04:55<02:00,  1.27it/s]Running loglikelihood requests:  62%|██████▏   | 248/400 [04:56<02:01,  1.25it/s]Running loglikelihood requests:  62%|██████▏   | 249/400 [04:57<02:01,  1.24it/s]Running loglikelihood requests:  62%|██████▎   | 250/400 [04:58<02:01,  1.24it/s]Running loglikelihood requests:  63%|██████▎   | 251/400 [04:58<02:00,  1.23it/s]Running loglikelihood requests:  63%|██████▎   | 252/400 [04:59<02:00,  1.23it/s]Running loglikelihood requests:  63%|██████▎   | 253/400 [05:00<01:59,  1.23it/s]Running loglikelihood requests:  64%|██████▎   | 254/400 [05:01<01:58,  1.23it/s]Running loglikelihood requests:  64%|██████▍   | 255/400 [05:02<01:57,  1.23it/s]Running loglikelihood requests:  64%|██████▍   | 256/400 [05:03<01:57,  1.23it/s]Running loglikelihood requests:  64%|██████▍   | 257/400 [05:03<01:56,  1.23it/s]Running loglikelihood requests:  64%|██████▍   | 258/400 [05:04<01:55,  1.23it/s]Running loglikelihood requests:  65%|██████▍   | 259/400 [05:05<01:53,  1.24it/s]Running loglikelihood requests:  65%|██████▌   | 260/400 [05:06<01:52,  1.24it/s]Running loglikelihood requests:  65%|██████▌   | 261/400 [05:06<01:50,  1.26it/s]Running loglikelihood requests:  66%|██████▌   | 262/400 [05:07<01:47,  1.28it/s]Running loglikelihood requests:  66%|██████▌   | 263/400 [05:08<01:46,  1.29it/s]Running loglikelihood requests:  66%|██████▌   | 264/400 [05:09<01:44,  1.30it/s]Running loglikelihood requests:  66%|██████▋   | 265/400 [05:09<01:41,  1.33it/s]Running loglikelihood requests:  66%|██████▋   | 266/400 [05:10<01:39,  1.35it/s]Running loglikelihood requests:  67%|██████▋   | 267/400 [05:11<01:37,  1.37it/s]Running loglikelihood requests:  67%|██████▋   | 268/400 [05:12<01:35,  1.38it/s]Running loglikelihood requests:  67%|██████▋   | 269/400 [05:12<01:33,  1.39it/s]Running loglikelihood requests:  68%|██████▊   | 270/400 [05:13<01:32,  1.41it/s]Running loglikelihood requests:  68%|██████▊   | 271/400 [05:14<01:30,  1.42it/s]Running loglikelihood requests:  68%|██████▊   | 272/400 [05:14<01:29,  1.43it/s]Running loglikelihood requests:  68%|██████▊   | 273/400 [05:15<01:27,  1.45it/s]Running loglikelihood requests:  68%|██████▊   | 274/400 [05:16<01:25,  1.48it/s]Running loglikelihood requests:  69%|██████▉   | 275/400 [05:16<01:23,  1.50it/s]Running loglikelihood requests:  69%|██████▉   | 276/400 [05:17<01:22,  1.51it/s]Running loglikelihood requests:  69%|██████▉   | 277/400 [05:18<01:20,  1.52it/s]Running loglikelihood requests:  70%|██████▉   | 278/400 [05:18<01:19,  1.53it/s]Running loglikelihood requests:  70%|██████▉   | 279/400 [05:19<01:18,  1.54it/s]Running loglikelihood requests:  70%|███████   | 280/400 [05:20<01:17,  1.54it/s]Running loglikelihood requests:  70%|███████   | 281/400 [05:20<01:16,  1.55it/s]Running loglikelihood requests:  70%|███████   | 282/400 [05:21<01:15,  1.56it/s]Running loglikelihood requests:  71%|███████   | 283/400 [05:21<01:14,  1.56it/s]Running loglikelihood requests:  71%|███████   | 284/400 [05:22<01:14,  1.57it/s]Running loglikelihood requests:  71%|███████▏  | 285/400 [05:23<01:13,  1.57it/s]Running loglikelihood requests:  72%|███████▏  | 286/400 [05:23<01:12,  1.57it/s]Running loglikelihood requests:  72%|███████▏  | 287/400 [05:24<01:11,  1.58it/s]Running loglikelihood requests:  72%|███████▏  | 288/400 [05:25<01:10,  1.58it/s]Running loglikelihood requests:  74%|███████▍  | 298/400 [05:25<00:14,  7.16it/s]Running loglikelihood requests:  75%|███████▌  | 300/400 [05:26<00:23,  4.24it/s]Running loglikelihood requests:  75%|███████▌  | 301/400 [05:27<00:28,  3.51it/s]Running loglikelihood requests:  76%|███████▌  | 302/400 [05:27<00:33,  2.94it/s]Running loglikelihood requests:  76%|███████▌  | 303/400 [05:28<00:37,  2.59it/s]Running loglikelihood requests:  76%|███████▌  | 304/400 [05:29<00:41,  2.32it/s]Running loglikelihood requests:  76%|███████▋  | 305/400 [05:29<00:44,  2.12it/s]Running loglikelihood requests:  76%|███████▋  | 306/400 [05:30<00:47,  1.98it/s]Running loglikelihood requests:  77%|███████▋  | 307/400 [05:31<00:49,  1.88it/s]Running loglikelihood requests:  78%|███████▊  | 310/400 [05:31<00:32,  2.77it/s]Running loglikelihood requests:  78%|███████▊  | 311/400 [05:32<00:36,  2.44it/s]Running loglikelihood requests:  78%|███████▊  | 312/400 [05:32<00:39,  2.20it/s]Running loglikelihood requests:  78%|███████▊  | 313/400 [05:33<00:42,  2.04it/s]Running loglikelihood requests:  78%|███████▊  | 314/400 [05:34<00:44,  1.92it/s]Running loglikelihood requests:  79%|███████▉  | 315/400 [05:34<00:46,  1.83it/s]Running loglikelihood requests:  79%|███████▉  | 316/400 [05:35<00:47,  1.78it/s]Running loglikelihood requests:  79%|███████▉  | 317/400 [05:35<00:47,  1.75it/s]Running loglikelihood requests:  80%|███████▉  | 318/400 [05:36<00:47,  1.73it/s]Running loglikelihood requests:  80%|███████▉  | 319/400 [05:37<00:46,  1.72it/s]Running loglikelihood requests:  80%|████████  | 321/400 [05:37<00:35,  2.23it/s]Running loglikelihood requests:  80%|████████  | 322/400 [05:38<00:37,  2.08it/s]Running loglikelihood requests:  81%|████████  | 323/400 [05:38<00:38,  1.98it/s]Running loglikelihood requests:  81%|████████  | 324/400 [05:39<00:39,  1.90it/s]Running loglikelihood requests:  81%|████████▏ | 325/400 [05:40<00:40,  1.85it/s]Running loglikelihood requests:  82%|████████▏ | 326/400 [05:40<00:40,  1.82it/s]Running loglikelihood requests:  82%|████████▏ | 327/400 [05:41<00:40,  1.80it/s]Running loglikelihood requests:  82%|████████▏ | 328/400 [05:41<00:40,  1.79it/s]Running loglikelihood requests:  82%|████████▏ | 329/400 [05:42<00:39,  1.78it/s]Running loglikelihood requests:  82%|████████▎ | 330/400 [05:42<00:39,  1.78it/s]Running loglikelihood requests:  83%|████████▎ | 331/400 [05:43<00:38,  1.77it/s]Running loglikelihood requests:  83%|████████▎ | 332/400 [05:43<00:38,  1.77it/s]Running loglikelihood requests:  83%|████████▎ | 333/400 [05:44<00:37,  1.78it/s]Running loglikelihood requests:  84%|████████▎ | 334/400 [05:45<00:36,  1.79it/s]Running loglikelihood requests:  84%|████████▍ | 335/400 [05:45<00:36,  1.80it/s]Running loglikelihood requests:  84%|████████▍ | 336/400 [05:46<00:35,  1.80it/s]Running loglikelihood requests:  84%|████████▍ | 337/400 [05:46<00:34,  1.81it/s]Running loglikelihood requests:  84%|████████▍ | 338/400 [05:47<00:34,  1.82it/s]Running loglikelihood requests:  85%|████████▍ | 339/400 [05:47<00:33,  1.82it/s]Running loglikelihood requests:  85%|████████▌ | 340/400 [05:48<00:33,  1.77it/s]Running loglikelihood requests:  85%|████████▌ | 341/400 [05:48<00:33,  1.78it/s]Running loglikelihood requests:  86%|████████▌ | 342/400 [05:49<00:32,  1.79it/s]Running loglikelihood requests:  86%|████████▌ | 343/400 [05:50<00:31,  1.80it/s]Running loglikelihood requests:  86%|████████▌ | 344/400 [05:50<00:31,  1.81it/s]Running loglikelihood requests:  86%|████████▋ | 345/400 [05:51<00:30,  1.82it/s]Running loglikelihood requests:  86%|████████▋ | 346/400 [05:51<00:29,  1.82it/s]Running loglikelihood requests:  87%|████████▋ | 347/400 [05:52<00:29,  1.83it/s]Running loglikelihood requests:  87%|████████▋ | 348/400 [05:52<00:28,  1.83it/s]Running loglikelihood requests:  87%|████████▋ | 349/400 [05:53<00:27,  1.83it/s]Running loglikelihood requests:  88%|████████▊ | 350/400 [05:53<00:27,  1.83it/s]Running loglikelihood requests:  88%|████████▊ | 351/400 [05:54<00:26,  1.84it/s]Running loglikelihood requests:  88%|████████▊ | 352/400 [05:54<00:26,  1.84it/s]Running loglikelihood requests:  88%|████████▊ | 353/400 [05:55<00:25,  1.84it/s]Running loglikelihood requests:  88%|████████▊ | 354/400 [05:56<00:24,  1.85it/s]Running loglikelihood requests:  89%|████████▉ | 355/400 [05:56<00:24,  1.85it/s]Running loglikelihood requests:  89%|████████▉ | 356/400 [05:57<00:23,  1.86it/s]Running loglikelihood requests:  89%|████████▉ | 357/400 [05:57<00:23,  1.86it/s]Running loglikelihood requests:  90%|████████▉ | 358/400 [05:58<00:22,  1.86it/s]Running loglikelihood requests:  90%|████████▉ | 359/400 [05:58<00:21,  1.87it/s]Running loglikelihood requests:  90%|█████████ | 360/400 [05:59<00:21,  1.87it/s]Running loglikelihood requests:  90%|█████████ | 361/400 [05:59<00:20,  1.87it/s]Running loglikelihood requests:  90%|█████████ | 362/400 [06:00<00:20,  1.87it/s]Running loglikelihood requests:  91%|█████████ | 363/400 [06:00<00:19,  1.88it/s]Running loglikelihood requests:  91%|█████████ | 364/400 [06:01<00:19,  1.88it/s]Running loglikelihood requests:  91%|█████████▏| 365/400 [06:01<00:18,  1.89it/s]Running loglikelihood requests:  92%|█████████▏| 366/400 [06:02<00:17,  1.89it/s]Running loglikelihood requests:  92%|█████████▏| 368/400 [06:02<00:13,  2.46it/s]Running loglikelihood requests:  92%|█████████▏| 369/400 [06:03<00:13,  2.29it/s]Running loglikelihood requests:  92%|█████████▎| 370/400 [06:04<00:13,  2.17it/s]Running loglikelihood requests:  93%|█████████▎| 371/400 [06:04<00:13,  2.09it/s]Running loglikelihood requests:  93%|█████████▎| 373/400 [06:05<00:10,  2.60it/s]Running loglikelihood requests:  94%|█████████▎| 374/400 [06:05<00:10,  2.39it/s]Running loglikelihood requests:  94%|█████████▍| 375/400 [06:06<00:11,  2.25it/s]Running loglikelihood requests:  94%|█████████▍| 376/400 [06:06<00:11,  2.16it/s]Running loglikelihood requests:  94%|█████████▍| 377/400 [06:07<00:10,  2.09it/s]Running loglikelihood requests:  94%|█████████▍| 378/400 [06:07<00:10,  2.06it/s]Running loglikelihood requests:  95%|█████████▍| 379/400 [06:08<00:10,  2.03it/s]Running loglikelihood requests:  95%|█████████▌| 380/400 [06:08<00:09,  2.01it/s]Running loglikelihood requests:  95%|█████████▌| 381/400 [06:09<00:09,  2.00it/s]Running loglikelihood requests:  96%|█████████▌| 382/400 [06:09<00:09,  2.00it/s]Running loglikelihood requests:  96%|█████████▌| 383/400 [06:10<00:08,  2.00it/s]Running loglikelihood requests:  96%|█████████▌| 384/400 [06:10<00:07,  2.00it/s]Running loglikelihood requests:  96%|█████████▋| 385/400 [06:11<00:07,  2.00it/s]Running loglikelihood requests:  96%|█████████▋| 386/400 [06:11<00:06,  2.01it/s]Running loglikelihood requests:  97%|█████████▋| 387/400 [06:12<00:06,  2.01it/s]Running loglikelihood requests:  97%|█████████▋| 388/400 [06:12<00:05,  2.02it/s]Running loglikelihood requests:  97%|█████████▋| 389/400 [06:13<00:05,  2.02it/s]Running loglikelihood requests:  98%|█████████▊| 390/400 [06:13<00:04,  2.02it/s]Running loglikelihood requests:  98%|█████████▊| 391/400 [06:14<00:04,  2.03it/s]Running loglikelihood requests:  98%|█████████▊| 392/400 [06:14<00:03,  2.03it/s]Running loglikelihood requests:  98%|█████████▊| 393/400 [06:15<00:03,  2.04it/s]Running loglikelihood requests:  98%|█████████▊| 394/400 [06:15<00:02,  2.04it/s]Running loglikelihood requests:  99%|█████████▉| 395/400 [06:16<00:02,  2.05it/s]Running loglikelihood requests:  99%|█████████▉| 396/400 [06:16<00:01,  2.05it/s]Running loglikelihood requests:  99%|█████████▉| 397/400 [06:17<00:01,  2.06it/s]Running loglikelihood requests: 100%|█████████▉| 398/400 [06:17<00:00,  2.06it/s]Running loglikelihood requests: 100%|█████████▉| 399/400 [06:18<00:00,  2.07it/s]Running loglikelihood requests: 100%|██████████| 400/400 [06:18<00:00,  2.07it/s]Running loglikelihood requests: 100%|██████████| 400/400 [06:18<00:00,  1.06it/s]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:0'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}
full model:
{'sciq': {'alias': 'sciq', 'acc,none': 0.94, 'acc_stderr,none': 0.023868325657594204, 'acc_norm,none': 0.91, 'acc_norm_stderr,none': 0.028762349126466136}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.969062788859705
0.9024924890572922
0.7706109127217512
0.8221264026535647
0.9190490061886575
0.9866654579796295
0.6586322754204971
0.7962110384246164
0.8195614021629236
0.7124178311176441
0.787697814339696
0.7034455022322618
0.8136386046534271
0.8174990104652458
0.6784276389594894
0.8698440245672888
0.8886492811850213
0.6541737276411673
0.6560861559753316
0.8139845219953913
0.6714741870309046
0.6164364868717988
0.8331581872497299
0.9065420049234512
0.9246185715568276
0.7477515960551026
0.574165362968651
0.8586446364199891
0.8889771415746612
Total groups 70 exceeded the threshold, stopping comparison.
The group tensor is
[7, 3, 4, 2, 6, 1, 5, 0]
tensor([7, 3, 4, 2, 6, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 5, 3, 4, 0, 7, 1]
tensor([6, 2, 5, 3, 4, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 3, 6, 2, 7, 1, 4, 0]
tensor([5, 3, 6, 2, 7, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 4, 2, 1, 3, 5, 1]
tensor([0, 0, 4, 2, 1, 3, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 3, 4, 5, 0, 1, 1]
tensor([0, 2, 3, 4, 5, 0, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 0, 2, 2, 3, 1]
tensor([0, 3, 1, 0, 2, 2, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 3, 1, 1, 2, 2, 3, 0]
tensor([0, 3, 1, 1, 2, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 1, 1.0, 1.0, 1.0, 1.0, 1]
tensor([0, 0, 1, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 2 to 4
done!
Normal merging for layer 5
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 6 to 9
done!
Normal merging for layer 10
tensor([0, 1])
tensor(0)
tensor([4, 7])
tensor(4)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 11 to 12
done!
Normal merging for layer 13
tensor([0, 5])
tensor(0)
tensor([6, 7])
tensor(6)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 14 to 19
done!
Normal merging for layer 20
tensor([0, 3])
tensor(0)
tensor([2, 7])
tensor(2)
tensor([4, 5])
tensor(4)
tensor([1, 6])
tensor(1)
done!
Normal merging for layer 21
tensor([0, 7])
tensor(0)
tensor([2, 3])
tensor(2)
tensor([4, 5])
tensor(4)
tensor([1, 6])
tensor(1)
done!
Cross-layer merge completed for layers 22 to 23
done!
Normal merging for layer 24
tensor([0, 1])
tensor(0)
tensor([2, 3, 4, 5, 6, 7])
tensor(2)
done!
Cross-layer merge completed for layers 25 to 31
done!
all done!
Model size: 12.2608 GB
253
cuda:0
mastermind_24_easy
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:36<00:36, 36.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:49<00:00, 22.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:49<00:00, 24.94s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_24_mcq_random HTTP/1.1" 200 779
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/flair/mastermind_24_mcq_random/flair/mastermind_24_mcq_random.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_24_mcq_random HTTP/1.1" 200 779
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/flair/mastermind_24_mcq_random/resolve/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/flair/mastermind_24_mcq_random/resolve/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_24_mcq_random/paths-info/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5 HTTP/1.1" 200 218
DEBUG:filelock:Attempting to acquire lock 140321890744752 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_24_mcq_random_default_0.0.0_cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5.lock
DEBUG:filelock:Lock 140321890744752 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_24_mcq_random_default_0.0.0_cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321890744752 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_24_mcq_random_default_0.0.0_cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5.lock
DEBUG:filelock:Lock 140321890744752 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_24_mcq_random_default_0.0.0_cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5.lock
DEBUG:filelock:Attempting to acquire lock 140321499044832 on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5_builder.lock
DEBUG:filelock:Lock 140321499044832 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321499044832 on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5_builder.lock
DEBUG:filelock:Lock 140321499044832 released on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_24_mcq_random/default/0.0.0/cdc1332d34ece1ecdd2453e227ebdc3837b4a0c5_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of mastermind_24_easy from None to 0
INFO:lm_eval.api.task:Building contexts for mastermind_24_easy on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1388.16it/s]
DEBUG:lm_eval.evaluator:Task: mastermind_24_easy; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:01<11:16,  1.70s/it]Running loglikelihood requests:   0%|          | 2/400 [00:02<09:09,  1.38s/it]Running loglikelihood requests:   1%|          | 3/400 [00:04<08:27,  1.28s/it]Running loglikelihood requests:   1%|          | 4/400 [00:05<08:07,  1.23s/it]Running loglikelihood requests:   1%|▏         | 5/400 [00:06<07:53,  1.20s/it]Running loglikelihood requests:   2%|▏         | 6/400 [00:07<07:44,  1.18s/it]Running loglikelihood requests:   2%|▏         | 7/400 [00:08<07:38,  1.17s/it]Running loglikelihood requests:   2%|▏         | 8/400 [00:09<07:34,  1.16s/it]Running loglikelihood requests:   2%|▏         | 9/400 [00:10<07:31,  1.15s/it]Running loglikelihood requests:   2%|▎         | 10/400 [00:12<07:28,  1.15s/it]Running loglikelihood requests:   3%|▎         | 12/400 [00:13<05:40,  1.14it/s]Running loglikelihood requests:   3%|▎         | 13/400 [00:14<06:04,  1.06it/s]Running loglikelihood requests:   4%|▎         | 14/400 [00:15<06:23,  1.01it/s]Running loglikelihood requests:   4%|▍         | 16/400 [00:16<05:11,  1.23it/s]Running loglikelihood requests:   4%|▍         | 17/400 [00:17<05:39,  1.13it/s]Running loglikelihood requests:   4%|▍         | 18/400 [00:18<06:02,  1.05it/s]Running loglikelihood requests:   5%|▍         | 19/400 [00:19<06:20,  1.00it/s]Running loglikelihood requests:   5%|▌         | 20/400 [00:21<06:34,  1.04s/it]Running loglikelihood requests:   5%|▌         | 21/400 [00:22<06:43,  1.06s/it]Running loglikelihood requests:   6%|▌         | 22/400 [00:23<06:50,  1.09s/it]Running loglikelihood requests:   6%|▌         | 23/400 [00:24<06:54,  1.10s/it]Running loglikelihood requests:   6%|▋         | 25/400 [00:25<05:21,  1.17it/s]Running loglikelihood requests:   8%|▊         | 32/400 [00:26<02:00,  3.06it/s]Running loglikelihood requests:   8%|▊         | 34/400 [00:27<02:19,  2.63it/s]Running loglikelihood requests:   9%|▉         | 35/400 [00:28<02:57,  2.06it/s]Running loglikelihood requests:  10%|▉         | 38/400 [00:29<02:43,  2.22it/s]Running loglikelihood requests:  10%|▉         | 39/400 [00:30<03:20,  1.80it/s]Running loglikelihood requests:  10%|█         | 40/400 [00:32<03:58,  1.51it/s]Running loglikelihood requests:  10%|█         | 41/400 [00:33<04:33,  1.31it/s]Running loglikelihood requests:  10%|█         | 42/400 [00:34<05:04,  1.18it/s]Running loglikelihood requests:  11%|█         | 43/400 [00:35<05:30,  1.08it/s]Running loglikelihood requests:  11%|█         | 44/400 [00:36<05:50,  1.02it/s]Running loglikelihood requests:  11%|█▏        | 45/400 [00:37<06:06,  1.03s/it]Running loglikelihood requests:  12%|█▏        | 47/400 [00:39<04:54,  1.20it/s]Running loglikelihood requests:  12%|█▏        | 48/400 [00:40<05:20,  1.10it/s]Running loglikelihood requests:  12%|█▏        | 49/400 [00:41<05:41,  1.03it/s]Running loglikelihood requests:  12%|█▎        | 50/400 [00:42<05:57,  1.02s/it]Running loglikelihood requests:  13%|█▎        | 52/400 [00:43<04:48,  1.21it/s]Running loglikelihood requests:  13%|█▎        | 53/400 [00:44<05:14,  1.10it/s]Running loglikelihood requests:  14%|█▎        | 54/400 [00:45<05:33,  1.04it/s]Running loglikelihood requests:  14%|█▍        | 56/400 [00:47<04:31,  1.27it/s]Running loglikelihood requests:  14%|█▍        | 57/400 [00:48<04:55,  1.16it/s]Running loglikelihood requests:  14%|█▍        | 58/400 [00:49<05:14,  1.09it/s]Running loglikelihood requests:  15%|█▍        | 59/400 [00:50<05:28,  1.04it/s]Running loglikelihood requests:  15%|█▌        | 60/400 [00:51<05:39,  1.00it/s]Running loglikelihood requests:  15%|█▌        | 61/400 [00:52<05:47,  1.02s/it]Running loglikelihood requests:  16%|█▌        | 62/400 [00:53<05:52,  1.04s/it]Running loglikelihood requests:  16%|█▌        | 63/400 [00:54<05:56,  1.06s/it]Running loglikelihood requests:  16%|█▌        | 64/400 [00:55<05:58,  1.07s/it]Running loglikelihood requests:  16%|█▋        | 65/400 [00:56<05:59,  1.07s/it]Running loglikelihood requests:  16%|█▋        | 66/400 [00:57<06:00,  1.08s/it]Running loglikelihood requests:  17%|█▋        | 67/400 [00:59<06:00,  1.08s/it]Running loglikelihood requests:  17%|█▋        | 68/400 [01:00<05:59,  1.08s/it]Running loglikelihood requests:  17%|█▋        | 69/400 [01:01<05:58,  1.08s/it]Running loglikelihood requests:  18%|█▊        | 70/400 [01:02<05:57,  1.08s/it]Running loglikelihood requests:  18%|█▊        | 71/400 [01:03<05:56,  1.08s/it]Running loglikelihood requests:  18%|█▊        | 72/400 [01:04<05:55,  1.08s/it]Running loglikelihood requests:  18%|█▊        | 73/400 [01:05<05:54,  1.08s/it]Running loglikelihood requests:  18%|█▊        | 74/400 [01:06<05:53,  1.08s/it]Running loglikelihood requests:  19%|█▉        | 76/400 [01:07<04:30,  1.20it/s]Running loglikelihood requests:  19%|█▉        | 77/400 [01:08<04:49,  1.12it/s]Running loglikelihood requests:  20%|█▉        | 78/400 [01:09<05:04,  1.06it/s]Running loglikelihood requests:  20%|█▉        | 79/400 [01:10<05:15,  1.02it/s]Running loglikelihood requests:  20%|██        | 80/400 [01:12<05:23,  1.01s/it]Running loglikelihood requests:  20%|██        | 81/400 [01:13<05:28,  1.03s/it]Running loglikelihood requests:  20%|██        | 82/400 [01:14<05:31,  1.04s/it]Running loglikelihood requests:  21%|██        | 83/400 [01:15<05:33,  1.05s/it]Running loglikelihood requests:  21%|██        | 84/400 [01:16<05:34,  1.06s/it]Running loglikelihood requests:  21%|██▏       | 85/400 [01:17<05:35,  1.07s/it]Running loglikelihood requests:  22%|██▏       | 87/400 [01:18<04:17,  1.21it/s]Running loglikelihood requests:  22%|██▏       | 88/400 [01:19<04:36,  1.13it/s]Running loglikelihood requests:  22%|██▏       | 89/400 [01:20<04:50,  1.07it/s]Running loglikelihood requests:  22%|██▎       | 90/400 [01:21<05:01,  1.03it/s]Running loglikelihood requests:  23%|██▎       | 91/400 [01:22<05:09,  1.00s/it]Running loglikelihood requests:  23%|██▎       | 92/400 [01:23<05:14,  1.02s/it]Running loglikelihood requests:  24%|██▎       | 94/400 [01:24<04:06,  1.24it/s]Running loglikelihood requests:  24%|██▍       | 95/400 [01:26<04:24,  1.15it/s]Running loglikelihood requests:  26%|██▌       | 104/400 [01:26<01:11,  4.16it/s]Running loglikelihood requests:  27%|██▋       | 107/400 [01:27<01:21,  3.61it/s]Running loglikelihood requests:  27%|██▋       | 108/400 [01:28<01:49,  2.68it/s]Running loglikelihood requests:  27%|██▋       | 109/400 [01:29<02:19,  2.09it/s]Running loglikelihood requests:  28%|██▊       | 110/400 [01:30<02:50,  1.70it/s]Running loglikelihood requests:  28%|██▊       | 111/400 [01:32<03:19,  1.45it/s]Running loglikelihood requests:  28%|██▊       | 112/400 [01:33<03:47,  1.26it/s]Running loglikelihood requests:  28%|██▊       | 113/400 [01:34<04:05,  1.17it/s]Running loglikelihood requests:  28%|██▊       | 114/400 [01:35<04:19,  1.10it/s]Running loglikelihood requests:  29%|██▉       | 116/400 [01:36<03:32,  1.33it/s]Running loglikelihood requests:  29%|██▉       | 117/400 [01:37<03:52,  1.22it/s]Running loglikelihood requests:  30%|██▉       | 118/400 [01:38<04:07,  1.14it/s]Running loglikelihood requests:  30%|███       | 121/400 [01:39<02:49,  1.64it/s]Running loglikelihood requests:  30%|███       | 122/400 [01:40<03:13,  1.44it/s]Running loglikelihood requests:  31%|███       | 123/400 [01:41<03:34,  1.29it/s]Running loglikelihood requests:  31%|███       | 124/400 [01:42<03:52,  1.19it/s]Running loglikelihood requests:  31%|███▏      | 125/400 [01:43<04:05,  1.12it/s]Running loglikelihood requests:  32%|███▏      | 126/400 [01:44<04:14,  1.08it/s]Running loglikelihood requests:  32%|███▏      | 127/400 [01:45<04:20,  1.05it/s]Running loglikelihood requests:  32%|███▏      | 128/400 [01:46<04:25,  1.03it/s]Running loglikelihood requests:  32%|███▎      | 130/400 [01:47<03:27,  1.30it/s]Running loglikelihood requests:  33%|███▎      | 132/400 [01:48<02:59,  1.50it/s]Running loglikelihood requests:  33%|███▎      | 133/400 [01:49<03:19,  1.34it/s]Running loglikelihood requests:  34%|███▎      | 134/400 [01:50<03:35,  1.23it/s]Running loglikelihood requests:  34%|███▍      | 135/400 [01:51<03:49,  1.16it/s]Running loglikelihood requests:  34%|███▍      | 136/400 [01:53<03:59,  1.10it/s]Running loglikelihood requests:  34%|███▍      | 137/400 [01:54<04:06,  1.07it/s]Running loglikelihood requests:  34%|███▍      | 138/400 [01:55<04:11,  1.04it/s]Running loglikelihood requests:  35%|███▍      | 139/400 [01:56<04:15,  1.02it/s]Running loglikelihood requests:  35%|███▌      | 140/400 [01:57<04:17,  1.01it/s]Running loglikelihood requests:  35%|███▌      | 141/400 [01:58<04:18,  1.00it/s]Running loglikelihood requests:  36%|███▌      | 142/400 [01:59<04:18,  1.00s/it]Running loglikelihood requests:  36%|███▌      | 143/400 [02:00<04:18,  1.01s/it]Running loglikelihood requests:  36%|███▋      | 145/400 [02:01<03:18,  1.29it/s]Running loglikelihood requests:  36%|███▋      | 146/400 [02:02<03:32,  1.19it/s]Running loglikelihood requests:  37%|███▋      | 147/400 [02:03<03:43,  1.13it/s]Running loglikelihood requests:  37%|███▋      | 148/400 [02:04<03:51,  1.09it/s]Running loglikelihood requests:  37%|███▋      | 149/400 [02:05<03:57,  1.06it/s]Running loglikelihood requests:  38%|███▊      | 150/400 [02:06<04:01,  1.03it/s]Running loglikelihood requests:  38%|███▊      | 152/400 [02:07<03:08,  1.32it/s]Running loglikelihood requests:  38%|███▊      | 153/400 [02:08<03:23,  1.22it/s]Running loglikelihood requests:  38%|███▊      | 154/400 [02:09<03:34,  1.15it/s]Running loglikelihood requests:  39%|███▉      | 155/400 [02:10<03:43,  1.10it/s]Running loglikelihood requests:  39%|███▉      | 157/400 [02:11<02:58,  1.36it/s]Running loglikelihood requests:  40%|███▉      | 158/400 [02:12<03:14,  1.25it/s]Running loglikelihood requests:  40%|████      | 160/400 [02:13<02:43,  1.47it/s]Running loglikelihood requests:  40%|████      | 161/400 [02:14<03:00,  1.32it/s]Running loglikelihood requests:  40%|████      | 162/400 [02:15<03:14,  1.22it/s]Running loglikelihood requests:  41%|████      | 163/400 [02:16<03:25,  1.15it/s]Running loglikelihood requests:  41%|████▏     | 165/400 [02:17<02:47,  1.40it/s]Running loglikelihood requests:  42%|████▏     | 166/400 [02:18<03:03,  1.28it/s]Running loglikelihood requests:  42%|████▏     | 167/400 [02:19<03:15,  1.19it/s]Running loglikelihood requests:  42%|████▏     | 168/400 [02:20<03:25,  1.13it/s]Running loglikelihood requests:  42%|████▎     | 170/400 [02:21<02:46,  1.38it/s]Running loglikelihood requests:  43%|████▎     | 171/400 [02:22<03:01,  1.26it/s]Running loglikelihood requests:  43%|████▎     | 172/400 [02:23<03:12,  1.18it/s]Running loglikelihood requests:  43%|████▎     | 173/400 [02:24<03:22,  1.12it/s]Running loglikelihood requests:  44%|████▎     | 174/400 [02:25<03:28,  1.08it/s]Running loglikelihood requests:  44%|████▍     | 175/400 [02:26<03:33,  1.06it/s]Running loglikelihood requests:  44%|████▍     | 176/400 [02:27<03:36,  1.04it/s]Running loglikelihood requests:  47%|████▋     | 187/400 [02:28<00:52,  4.04it/s]Running loglikelihood requests:  47%|████▋     | 188/400 [02:29<01:08,  3.12it/s]Running loglikelihood requests:  47%|████▋     | 189/400 [02:30<01:25,  2.48it/s]Running loglikelihood requests:  48%|████▊     | 190/400 [02:31<01:43,  2.03it/s]Running loglikelihood requests:  48%|████▊     | 191/400 [02:32<02:01,  1.72it/s]Running loglikelihood requests:  48%|████▊     | 192/400 [02:33<02:18,  1.50it/s]Running loglikelihood requests:  48%|████▊     | 193/400 [02:34<02:33,  1.35it/s]Running loglikelihood requests:  48%|████▊     | 194/400 [02:35<02:46,  1.24it/s]Running loglikelihood requests:  49%|████▉     | 196/400 [02:36<02:19,  1.46it/s]Running loglikelihood requests:  49%|████▉     | 197/400 [02:37<02:33,  1.32it/s]Running loglikelihood requests:  50%|████▉     | 198/400 [02:38<02:45,  1.22it/s]Running loglikelihood requests:  50%|████▉     | 199/400 [02:39<02:54,  1.15it/s]Running loglikelihood requests:  50%|█████     | 200/400 [02:40<03:01,  1.10it/s]Running loglikelihood requests:  50%|█████     | 202/400 [02:41<02:24,  1.37it/s]Running loglikelihood requests:  51%|█████     | 203/400 [02:42<02:37,  1.25it/s]Running loglikelihood requests:  51%|█████     | 204/400 [02:43<02:46,  1.18it/s]Running loglikelihood requests:  51%|█████▏    | 205/400 [02:44<02:54,  1.12it/s]Running loglikelihood requests:  52%|█████▏    | 206/400 [02:45<02:59,  1.08it/s]Running loglikelihood requests:  52%|█████▏    | 207/400 [02:46<03:03,  1.05it/s]Running loglikelihood requests:  52%|█████▎    | 210/400 [02:47<01:56,  1.63it/s]Running loglikelihood requests:  53%|█████▎    | 212/400 [02:48<01:48,  1.73it/s]Running loglikelihood requests:  53%|█████▎    | 213/400 [02:49<02:03,  1.51it/s]Running loglikelihood requests:  54%|█████▍    | 215/400 [02:50<01:51,  1.65it/s]Running loglikelihood requests:  54%|█████▍    | 216/400 [02:51<02:06,  1.46it/s]Running loglikelihood requests:  54%|█████▍    | 217/400 [02:52<02:18,  1.32it/s]Running loglikelihood requests:  55%|█████▍    | 219/400 [02:53<01:59,  1.52it/s]Running loglikelihood requests:  55%|█████▌    | 220/400 [02:54<02:12,  1.36it/s]Running loglikelihood requests:  55%|█████▌    | 221/400 [02:55<02:23,  1.25it/s]Running loglikelihood requests:  56%|█████▌    | 222/400 [02:56<02:31,  1.17it/s]Running loglikelihood requests:  56%|█████▌    | 223/400 [02:57<02:38,  1.12it/s]Running loglikelihood requests:  56%|█████▌    | 224/400 [02:58<02:42,  1.08it/s]Running loglikelihood requests:  56%|█████▋    | 225/400 [02:59<02:45,  1.06it/s]Running loglikelihood requests:  56%|█████▋    | 226/400 [03:00<02:47,  1.04it/s]Running loglikelihood requests:  57%|█████▋    | 227/400 [03:01<02:48,  1.03it/s]Running loglikelihood requests:  57%|█████▋    | 228/400 [03:02<02:49,  1.02it/s]Running loglikelihood requests:  57%|█████▋    | 229/400 [03:03<02:49,  1.01it/s]Running loglikelihood requests:  57%|█████▊    | 230/400 [03:04<02:49,  1.01it/s]Running loglikelihood requests:  58%|█████▊    | 231/400 [03:05<02:48,  1.00it/s]Running loglikelihood requests:  58%|█████▊    | 232/400 [03:06<02:47,  1.00it/s]Running loglikelihood requests:  58%|█████▊    | 233/400 [03:07<02:47,  1.00s/it]Running loglikelihood requests:  59%|█████▉    | 235/400 [03:08<02:07,  1.30it/s]Running loglikelihood requests:  59%|█████▉    | 236/400 [03:09<02:15,  1.21it/s]Running loglikelihood requests:  59%|█████▉    | 237/400 [03:10<02:22,  1.14it/s]Running loglikelihood requests:  60%|█████▉    | 238/400 [03:11<02:27,  1.10it/s]Running loglikelihood requests:  60%|█████▉    | 239/400 [03:12<02:30,  1.07it/s]Running loglikelihood requests:  60%|██████    | 240/400 [03:13<02:32,  1.05it/s]Running loglikelihood requests:  60%|██████    | 242/400 [03:14<01:58,  1.33it/s]Running loglikelihood requests:  61%|██████    | 243/400 [03:15<02:07,  1.23it/s]Running loglikelihood requests:  61%|██████    | 244/400 [03:16<02:14,  1.16it/s]Running loglikelihood requests:  61%|██████▏   | 245/400 [03:17<02:19,  1.11it/s]Running loglikelihood requests:  62%|██████▏   | 246/400 [03:18<02:22,  1.08it/s]Running loglikelihood requests:  62%|██████▏   | 247/400 [03:19<02:25,  1.05it/s]Running loglikelihood requests:  62%|██████▏   | 249/400 [03:20<01:52,  1.34it/s]Running loglikelihood requests:  62%|██████▎   | 250/400 [03:21<02:01,  1.24it/s]Running loglikelihood requests:  63%|██████▎   | 251/400 [03:22<02:07,  1.17it/s]Running loglikelihood requests:  63%|██████▎   | 252/400 [03:23<02:12,  1.12it/s]Running loglikelihood requests:  63%|██████▎   | 253/400 [03:24<02:15,  1.08it/s]Running loglikelihood requests:  64%|██████▎   | 254/400 [03:25<02:18,  1.06it/s]Running loglikelihood requests:  64%|██████▍   | 255/400 [03:26<02:19,  1.04it/s]Running loglikelihood requests:  64%|██████▍   | 256/400 [03:27<02:20,  1.03it/s]Running loglikelihood requests:  64%|██████▍   | 257/400 [03:28<02:20,  1.02it/s]Running loglikelihood requests:  67%|██████▋   | 267/400 [03:29<00:36,  3.64it/s]Running loglikelihood requests:  67%|██████▋   | 268/400 [03:30<00:46,  2.86it/s]Running loglikelihood requests:  67%|██████▋   | 269/400 [03:31<00:56,  2.31it/s]Running loglikelihood requests:  68%|██████▊   | 270/400 [03:32<01:07,  1.92it/s]Running loglikelihood requests:  68%|██████▊   | 271/400 [03:33<01:18,  1.65it/s]Running loglikelihood requests:  68%|██████▊   | 272/400 [03:34<01:27,  1.46it/s]Running loglikelihood requests:  68%|██████▊   | 273/400 [03:35<01:36,  1.32it/s]Running loglikelihood requests:  68%|██████▊   | 274/400 [03:36<01:42,  1.23it/s]Running loglikelihood requests:  69%|██████▉   | 277/400 [03:37<01:09,  1.77it/s]Running loglikelihood requests:  70%|██████▉   | 279/400 [03:38<01:05,  1.84it/s]Running loglikelihood requests:  70%|███████   | 281/400 [03:39<01:02,  1.89it/s]Running loglikelihood requests:  71%|███████   | 284/400 [03:40<00:51,  2.23it/s]Running loglikelihood requests:  71%|███████▏  | 285/400 [03:41<01:01,  1.87it/s]Running loglikelihood requests:  72%|███████▏  | 286/400 [03:42<01:10,  1.61it/s]Running loglikelihood requests:  72%|███████▏  | 287/400 [03:43<01:19,  1.43it/s]Running loglikelihood requests:  72%|███████▏  | 288/400 [03:44<01:25,  1.30it/s]Running loglikelihood requests:  72%|███████▏  | 289/400 [03:45<01:31,  1.21it/s]Running loglikelihood requests:  72%|███████▎  | 290/400 [03:46<01:35,  1.15it/s]Running loglikelihood requests:  73%|███████▎  | 291/400 [03:47<01:38,  1.11it/s]Running loglikelihood requests:  73%|███████▎  | 293/400 [03:48<01:17,  1.38it/s]Running loglikelihood requests:  74%|███████▎  | 294/400 [03:49<01:23,  1.27it/s]Running loglikelihood requests:  74%|███████▍  | 295/400 [03:50<01:27,  1.19it/s]Running loglikelihood requests:  74%|███████▍  | 296/400 [03:51<01:31,  1.14it/s]Running loglikelihood requests:  74%|███████▍  | 297/400 [03:52<01:33,  1.10it/s]Running loglikelihood requests:  75%|███████▍  | 299/400 [03:53<01:13,  1.38it/s]Running loglikelihood requests:  75%|███████▌  | 300/400 [03:54<01:18,  1.27it/s]Running loglikelihood requests:  75%|███████▌  | 301/400 [03:55<01:23,  1.19it/s]Running loglikelihood requests:  76%|███████▌  | 302/400 [03:56<01:26,  1.14it/s]Running loglikelihood requests:  76%|███████▌  | 304/400 [03:57<01:08,  1.40it/s]Running loglikelihood requests:  76%|███████▋  | 305/400 [03:58<01:13,  1.29it/s]Running loglikelihood requests:  77%|███████▋  | 307/400 [03:59<01:01,  1.51it/s]Running loglikelihood requests:  77%|███████▋  | 309/400 [04:00<00:54,  1.67it/s]Running loglikelihood requests:  78%|███████▊  | 310/400 [04:01<01:01,  1.47it/s]Running loglikelihood requests:  78%|███████▊  | 311/400 [04:02<01:06,  1.34it/s]Running loglikelihood requests:  78%|███████▊  | 312/400 [04:03<01:10,  1.24it/s]Running loglikelihood requests:  78%|███████▊  | 313/400 [04:04<01:14,  1.18it/s]Running loglikelihood requests:  79%|███████▉  | 315/400 [04:05<00:59,  1.43it/s]Running loglikelihood requests:  79%|███████▉  | 317/400 [04:06<00:51,  1.61it/s]Running loglikelihood requests:  80%|███████▉  | 318/400 [04:07<00:57,  1.44it/s]Running loglikelihood requests:  80%|███████▉  | 319/400 [04:08<01:01,  1.31it/s]Running loglikelihood requests:  80%|████████  | 321/400 [04:09<00:51,  1.53it/s]Running loglikelihood requests:  81%|████████  | 323/400 [04:10<00:45,  1.68it/s]Running loglikelihood requests:  82%|████████▏ | 326/400 [04:11<00:35,  2.09it/s]Running loglikelihood requests:  82%|████████▏ | 327/400 [04:12<00:41,  1.77it/s]Running loglikelihood requests:  82%|████████▏ | 328/400 [04:13<00:46,  1.55it/s]Running loglikelihood requests:  82%|████████▏ | 329/400 [04:14<00:51,  1.39it/s]Running loglikelihood requests:  82%|████████▎ | 330/400 [04:15<00:54,  1.28it/s]Running loglikelihood requests:  83%|████████▎ | 332/400 [04:16<00:45,  1.51it/s]Running loglikelihood requests:  83%|████████▎ | 333/400 [04:17<00:49,  1.36it/s]Running loglikelihood requests:  84%|████████▎ | 334/400 [04:18<00:52,  1.26it/s]Running loglikelihood requests:  84%|████████▍ | 335/400 [04:19<00:54,  1.19it/s]Running loglikelihood requests:  84%|████████▍ | 336/400 [04:20<00:56,  1.14it/s]Running loglikelihood requests:  84%|████████▍ | 337/400 [04:21<00:56,  1.11it/s]Running loglikelihood requests:  84%|████████▍ | 338/400 [04:22<00:57,  1.08it/s]Running loglikelihood requests:  85%|████████▍ | 339/400 [04:23<00:57,  1.06it/s]Running loglikelihood requests:  85%|████████▌ | 340/400 [04:24<00:56,  1.05it/s]Running loglikelihood requests:  85%|████████▌ | 341/400 [04:24<00:55,  1.07it/s]Running loglikelihood requests:  86%|████████▌ | 342/400 [04:25<00:53,  1.08it/s]Running loglikelihood requests:  86%|████████▌ | 343/400 [04:26<00:52,  1.10it/s]Running loglikelihood requests:  86%|████████▋ | 346/400 [04:27<00:30,  1.76it/s]Running loglikelihood requests:  87%|████████▋ | 348/400 [04:28<00:27,  1.90it/s]Running loglikelihood requests:  87%|████████▋ | 349/400 [04:29<00:30,  1.67it/s]Running loglikelihood requests:  90%|████████▉ | 358/400 [04:30<00:09,  4.53it/s]Running loglikelihood requests:  90%|████████▉ | 359/400 [04:30<00:11,  3.46it/s]Running loglikelihood requests:  90%|█████████ | 360/400 [04:31<00:14,  2.74it/s]Running loglikelihood requests:  90%|█████████ | 361/400 [04:32<00:17,  2.25it/s]Running loglikelihood requests:  90%|█████████ | 362/400 [04:33<00:19,  1.91it/s]Running loglikelihood requests:  91%|█████████ | 364/400 [04:34<00:17,  2.01it/s]Running loglikelihood requests:  91%|█████████▏| 365/400 [04:35<00:20,  1.75it/s]Running loglikelihood requests:  92%|█████████▏| 367/400 [04:36<00:17,  1.90it/s]Running loglikelihood requests:  92%|█████████▏| 368/400 [04:37<00:19,  1.67it/s]Running loglikelihood requests:  92%|█████████▏| 369/400 [04:38<00:20,  1.51it/s]Running loglikelihood requests:  92%|█████████▎| 370/400 [04:38<00:21,  1.39it/s]Running loglikelihood requests:  93%|█████████▎| 371/400 [04:39<00:22,  1.31it/s]Running loglikelihood requests:  93%|█████████▎| 373/400 [04:40<00:16,  1.60it/s]Running loglikelihood requests:  94%|█████████▍| 375/400 [04:41<00:13,  1.80it/s]Running loglikelihood requests:  94%|█████████▍| 376/400 [04:42<00:15,  1.60it/s]Running loglikelihood requests:  95%|█████████▍| 379/400 [04:43<00:09,  2.14it/s]Running loglikelihood requests:  95%|█████████▌| 381/400 [04:44<00:08,  2.18it/s]Running loglikelihood requests:  96%|█████████▌| 382/400 [04:45<00:09,  1.87it/s]Running loglikelihood requests:  96%|█████████▌| 384/400 [04:46<00:08,  1.99it/s]Running loglikelihood requests:  96%|█████████▋| 386/400 [04:46<00:06,  2.07it/s]Running loglikelihood requests:  97%|█████████▋| 387/400 [04:47<00:07,  1.79it/s]Running loglikelihood requests:  97%|█████████▋| 389/400 [04:48<00:05,  1.93it/s]Running loglikelihood requests:  98%|█████████▊| 391/400 [04:49<00:04,  2.03it/s]Running loglikelihood requests:  98%|█████████▊| 393/400 [04:50<00:03,  2.10it/s]Running loglikelihood requests:  99%|█████████▉| 395/400 [04:51<00:02,  2.16it/s]Running loglikelihood requests:  99%|█████████▉| 397/400 [04:52<00:01,  2.19it/s]Running loglikelihood requests: 100%|█████████▉| 398/400 [04:53<00:01,  1.88it/s]Running loglikelihood requests: 100%|██████████| 400/400 [04:53<00:00,  1.99it/s]Running loglikelihood requests: 100%|██████████| 400/400 [04:53<00:00,  1.36it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:1'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:1'}
full model:
{'mastermind_24_easy': {'alias': 'mastermind_24_easy', 'acc,none': 0.33, 'acc_stderr,none': 0.04725815626252609}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9705388716727775
0.9880227828623074
0.982119607496995
0.9941469535767887
0.984917378949356
0.9883609317461776
0.9752442385272352
0.9858149677985593
0.9966001900863091
0.9954965780195978
0.9982374916142641
0.987839947084121
0.9740425263242771
0.9802324544963317
0.9965978314947238
0.9890806289155061
0.971808392501808
0.9767123038440666
0.9820534501654181
0.9672299205809463
0.9753579231968917
0.9973837437819523
0.9951994747100236
0.9465102818948206
0.9757933184251313
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[7, 0, 1, 3, 6, 5, 4, 2]
tensor([7, 0, 1, 3, 6, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 0, 1, 3, 7, 5, 4, 2]
tensor([6, 0, 1, 3, 7, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 0, 1, 3, 6, 5, 4, 2]
tensor([7, 0, 1, 3, 6, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 0, 1, 3, 7, 5, 4, 2]
tensor([6, 0, 1, 3, 7, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 0, 1, 3, 6, 5, 4, 2]
tensor([7, 0, 1, 3, 6, 5, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 0, 2, 3, 6, 5, 4, 1]
tensor([7, 0, 2, 3, 6, 5, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 3
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Normal merging for layer 4
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 5
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 6 to 31
done!
all done!
Model size: 12.0718 GB
175
cuda:1
wnli
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:37<00:37, 37.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:50<00:00, 23.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:50<00:00, 25.32s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140321891561408 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321891561408 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321891561408 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321891561408 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140324836912512 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140324836912512 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140324836912512 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140324836912512 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2408.08it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:01<03:24,  1.45s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:02<01:35,  1.46it/s]Running loglikelihood requests:   4%|▎         | 5/142 [00:03<01:14,  1.83it/s]Running loglikelihood requests:   5%|▍         | 7/142 [00:03<01:06,  2.04it/s]Running loglikelihood requests:   6%|▋         | 9/142 [00:04<01:00,  2.19it/s]Running loglikelihood requests:   8%|▊         | 11/142 [00:05<00:56,  2.31it/s]Running loglikelihood requests:   9%|▉         | 13/142 [00:06<00:53,  2.39it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:07<00:51,  2.48it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:07<00:48,  2.56it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:08<00:46,  2.65it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:09<00:44,  2.72it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:09<00:42,  2.80it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:10<00:40,  2.86it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:11<00:39,  2.91it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:11<00:38,  2.94it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:12<00:37,  2.98it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:13<00:36,  3.00it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:13<00:35,  3.02it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:14<00:34,  3.03it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:15<00:33,  3.04it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:15<00:33,  3.05it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:16<00:32,  3.07it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:17<00:31,  3.10it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:17<00:30,  3.12it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:18<00:29,  3.16it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:18<00:28,  3.20it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:19<00:27,  3.23it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:20<00:26,  3.26it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:20<00:25,  3.28it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:21<00:25,  3.29it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:21<00:24,  3.30it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:22<00:23,  3.31it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:23<00:23,  3.32it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:23<00:22,  3.33it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:24<00:21,  3.34it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:24<00:21,  3.35it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:25<00:20,  3.37it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:26<00:19,  3.38it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [00:26<00:19,  3.39it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [00:27<00:18,  3.41it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [00:27<00:17,  3.42it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [00:28<00:17,  3.42it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [00:28<00:16,  3.43it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [00:29<00:16,  3.44it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [00:30<00:15,  3.44it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [00:30<00:14,  3.44it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [00:31<00:14,  3.46it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [00:31<00:13,  3.46it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [00:32<00:12,  3.47it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [00:32<00:12,  3.48it/s]Running loglikelihood requests:  71%|███████   | 101/142 [00:33<00:11,  3.49it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [00:34<00:11,  3.50it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [00:34<00:10,  3.51it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [00:35<00:10,  3.46it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [00:35<00:09,  3.47it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [00:36<00:08,  3.49it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [00:36<00:08,  3.50it/s]Running loglikelihood requests:  81%|████████  | 115/142 [00:37<00:07,  3.49it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [00:38<00:07,  3.51it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [00:38<00:06,  3.54it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [00:39<00:05,  3.56it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [00:39<00:05,  3.59it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [00:40<00:04,  3.61it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [00:40<00:04,  3.62it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [00:41<00:03,  3.64it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [00:41<00:03,  3.66it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [00:42<00:02,  3.68it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [00:43<00:01,  3.71it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [00:43<00:01,  3.73it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [00:44<00:00,  3.75it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [00:44<00:00,  3.79it/s]Running loglikelihood requests: 100%|██████████| 142/142 [00:44<00:00,  3.19it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:2'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:2'}
full model:
{'wnli': {'alias': 'wnli', 'acc,none': 0.5352112676056338, 'acc_stderr,none': 0.0596130578497224}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.31190044950254875
0.7306280553786256
0.6985925052465822
0.5843516732063875
0.8124235844589114
0.820438203921003
0.6253838264581936
0.7600037006947045
0.8225872978677496
0.708398461303215
0.8646089269479391
0.8239362351853257
0.7608099850331435
0.6657423857513638
0.7943257460202938
0.7511476003698512
0.9073696655228775
0.8741838353767599
0.7945799099309127
0.9323691001541556
0.865243808509542
0.8176606226311932
0.6785099625983169
0.9579534328203848
0.788928884938056
0.9833718962298513
0.5933012307657521
0.7829988799240639
0.7823073743206628
0.31190044950254875
0.7306280553786256
0.6985925052465822
0.5843516732063875
0.8124235844589114
0.820438203921003
0.6253838264581936
0.7600037006947045
0.8225872978677496
0.708398461303215
0.8646089269479391
0.8239362351853257
0.7608099850331435
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[7, 4, 5, 1, 6, 3, 2, 0]
tensor([7, 4, 5, 1, 6, 3, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 2, 4, 5, 6, 1, 0, 3]
tensor([7, 2, 4, 5, 6, 1, 0, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 5, 4, 1, 2, 0]
tensor([6, 3, 7, 5, 4, 1, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 6, 4, 7, 1, 2, 0]
tensor([5, 3, 6, 4, 7, 1, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 3, 6, 5, 7, 2, 1, 0]
tensor([4, 3, 6, 5, 7, 2, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1, 2, 3, 2, 3, 0]
tensor([0, 1, 1, 2, 3, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 1, 2, 2, 3, 0, 3, 1]
tensor([0, 1, 2, 2, 3, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 1, 1, 1.0, 1.0, 1.0, 1.0]
tensor([0, 0, 1, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 4
tensor([7])
tensor(7)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 5 to 15
done!
Normal merging for layer 16
tensor([0, 7])
tensor(0)
tensor([1, 2])
tensor(1)
tensor([3, 5])
tensor(3)
tensor([4, 6])
tensor(4)
done!
Normal merging for layer 17
tensor([0, 5])
tensor(0)
tensor([1, 7])
tensor(1)
tensor([2, 3])
tensor(2)
tensor([4, 6])
tensor(4)
done!
Cross-layer merge completed for layers 18 to 30
done!
Normal merging for layer 31
tensor([0, 1])
tensor(0)
tensor([2, 3, 4, 5, 6, 7])
tensor(2)
done!
all done!
Model size: 12.2608 GB
48
cuda:2
wnli
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:43<00:43, 43.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 22.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 25.76s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140321622945824 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321622945824 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321622945824 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321622945824 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140321081430272 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321081430272 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321081430272 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321081430272 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2437.01it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:01<03:22,  1.44s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:02<01:35,  1.46it/s]Running loglikelihood requests:   4%|▎         | 5/142 [00:03<01:15,  1.82it/s]Running loglikelihood requests:   5%|▍         | 7/142 [00:03<01:06,  2.03it/s]Running loglikelihood requests:   6%|▋         | 9/142 [00:04<01:01,  2.17it/s]Running loglikelihood requests:   8%|▊         | 11/142 [00:05<00:57,  2.29it/s]Running loglikelihood requests:   9%|▉         | 13/142 [00:06<00:54,  2.38it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:07<00:51,  2.47it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:07<00:49,  2.55it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:08<00:46,  2.64it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:09<00:44,  2.71it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:09<00:42,  2.78it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:10<00:41,  2.84it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:11<00:39,  2.89it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:11<00:38,  2.93it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:12<00:37,  2.96it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:13<00:36,  2.98it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:13<00:35,  3.00it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:14<00:34,  3.01it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:15<00:34,  3.02it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:15<00:33,  3.04it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:16<00:32,  3.05it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:17<00:31,  3.08it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:17<00:30,  3.12it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:18<00:29,  3.15it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:18<00:28,  3.19it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:19<00:27,  3.22it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:20<00:26,  3.25it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:20<00:26,  3.27it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:21<00:25,  3.28it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:21<00:24,  3.29it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:22<00:23,  3.30it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:23<00:23,  3.32it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:23<00:22,  3.33it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:24<00:21,  3.34it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:24<00:21,  3.35it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:25<00:20,  3.37it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:26<00:19,  3.38it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [00:26<00:19,  3.38it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [00:27<00:18,  3.40it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [00:27<00:17,  3.41it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [00:28<00:17,  3.41it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [00:29<00:16,  3.42it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [00:29<00:16,  3.43it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [00:30<00:15,  3.44it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [00:30<00:14,  3.44it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [00:31<00:14,  3.45it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [00:31<00:13,  3.46it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [00:32<00:12,  3.47it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [00:33<00:12,  3.48it/s]Running loglikelihood requests:  71%|███████   | 101/142 [00:33<00:11,  3.48it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [00:34<00:11,  3.50it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [00:34<00:10,  3.51it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [00:35<00:09,  3.52it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [00:35<00:09,  3.53it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [00:36<00:08,  3.54it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [00:37<00:08,  3.55it/s]Running loglikelihood requests:  81%|████████  | 115/142 [00:37<00:07,  3.56it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [00:38<00:07,  3.56it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [00:38<00:06,  3.58it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [00:39<00:05,  3.59it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [00:39<00:05,  3.60it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [00:40<00:04,  3.61it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [00:40<00:04,  3.62it/s]Running loglikelihood requests: 100%|██████████| 142/142 [00:38<00:00,  3.68it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:3'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:3'}
full model:
{'wnli': {'alias': 'wnli', 'acc,none': 0.5352112676056338, 'acc_stderr,none': 0.0596130578497224}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.31190044950254875
0.7306280553786256
0.6985925052465822
0.5843516732063875
0.8124235844589114
0.820438203921003
0.6253838264581936
0.7600037006947045
0.8225872978677496
0.708398461303215
0.8646089269479391
0.8239362351853257
0.7608099850331435
0.6657423857513638
0.7943257460202938
0.7511476003698512
0.9073696655228775
0.8741838353767599
0.7945799099309127
0.9323691001541556
0.865243808509542
0.8176606226311932
0.6785099625983169
0.9579534328203848
0.788928884938056
0.9833718962298513
0.5933012307657521
0.7829988799240639
0.7823073743206628
0.31190044950254875
0.7306280553786256
0.6985925052465822
0.5843516732063875
0.8124235844589114
0.820438203921003
0.6253838264581936
0.7600037006947045
0.8225872978677496
0.708398461303215
0.8646089269479391
0.8239362351853257
0.7608099850331435
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[7, 4, 5, 1, 6, 3, 2, 0]
tensor([7, 4, 5, 1, 6, 3, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 2, 4, 5, 6, 1, 0, 3]
tensor([7, 2, 4, 5, 6, 1, 0, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 5, 4, 1, 2, 0]
tensor([6, 3, 7, 5, 4, 1, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 6, 4, 7, 1, 2, 0]
tensor([5, 3, 6, 4, 7, 1, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 3, 6, 5, 7, 2, 1, 0]
tensor([4, 3, 6, 5, 7, 2, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1, 2, 3, 2, 3, 0]
tensor([0, 1, 1, 2, 3, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 1, 2, 2, 3, 0, 3, 1]
tensor([0, 1, 2, 2, 3, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 1, 1, 1.0, 1.0, 1.0, 1.0]
tensor([0, 0, 1, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 4
tensor([7])
tensor(7)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 5 to 15
done!
Normal merging for layer 16
tensor([0, 7])
tensor(0)
tensor([1, 2])
tensor(1)
tensor([3, 5])
tensor(3)
tensor([4, 6])
tensor(4)
done!
Normal merging for layer 17
tensor([0, 5])
tensor(0)
tensor([1, 7])
tensor(1)
tensor([2, 3])
tensor(2)
tensor([4, 6])
tensor(4)
done!
Cross-layer merge completed for layers 18 to 30
done!
Normal merging for layer 31
tensor([0, 1])
tensor(0)
tensor([2, 3, 4, 5, 6, 7])
tensor(2)
done!
all done!
Model size: 12.2608 GB
66
cuda:3
qnli
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:36<00:36, 36.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:49<00:00, 22.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:49<00:00, 24.95s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: qnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: qnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140320955917056 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320955917056 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320955917056 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320955917056 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140321081599344 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321081599344 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321081599344 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321081599344 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of qnli from None to 0
INFO:lm_eval.api.task:Building contexts for qnli on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1807.38it/s]
DEBUG:lm_eval.evaluator:Task: qnli; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<05:42,  1.72s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:56,  1.11it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:04<02:21,  1.38it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:05<02:04,  1.55it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:06<01:55,  1.66it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:07<01:48,  1.75it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:08<01:41,  1.84it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:09<01:37,  1.91it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:10<01:31,  1.99it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:10<01:27,  2.07it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:11<01:24,  2.13it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:12<01:21,  2.17it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:13<01:19,  2.20it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:14<01:17,  2.23it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:15<01:16,  2.25it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:16<01:14,  2.26it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:17<01:13,  2.28it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:17<01:11,  2.30it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:18<01:10,  2.31it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:19<01:09,  2.32it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:20<01:08,  2.32it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:21<01:07,  2.34it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:22<01:05,  2.36it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:22<01:04,  2.38it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:23<01:02,  2.40it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:24<01:01,  2.41it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:25<01:00,  2.42it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:26<00:59,  2.43it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:27<00:58,  2.44it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:27<00:57,  2.45it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:28<00:56,  2.45it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:29<00:55,  2.46it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:30<00:54,  2.47it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:31<00:53,  2.50it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:31<00:52,  2.52it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:32<00:50,  2.53it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:33<00:49,  2.55it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:34<00:48,  2.57it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:34<00:47,  2.59it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:35<00:46,  2.61it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:36<00:45,  2.62it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:37<00:44,  2.64it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:37<00:43,  2.65it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:38<00:42,  2.66it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:39<00:41,  2.67it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:40<00:40,  2.68it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:40<00:39,  2.69it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:41<00:38,  2.69it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:42<00:38,  2.71it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:43<00:37,  2.72it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:43<00:36,  2.73it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:44<00:35,  2.74it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:45<00:34,  2.75it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:45<00:07, 10.57it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:46<00:08,  8.31it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:47<00:10,  6.71it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:47<00:12,  5.58it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:48<00:13,  4.80it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:49<00:15,  4.25it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:50<00:16,  3.87it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:50<00:16,  3.60it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:51<00:17,  3.42it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:52<00:17,  3.31it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [00:52<00:17,  3.22it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [00:53<00:16,  3.16it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [00:53<00:16,  3.12it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [00:54<00:15,  3.09it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [00:55<00:15,  3.07it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [00:55<00:14,  3.05it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [00:56<00:14,  3.05it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [00:57<00:13,  3.05it/s]Running loglikelihood requests:  80%|████████  | 161/200 [00:57<00:12,  3.05it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [00:58<00:12,  3.06it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [00:59<00:11,  3.07it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [00:59<00:10,  3.08it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:00<00:10,  3.09it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:01<00:09,  3.12it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:01<00:08,  3.14it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:02<00:07,  3.16it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:03<00:07,  3.17it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:03<00:06,  3.18it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:04<00:05,  3.19it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:04<00:05,  3.20it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:05<00:04,  3.22it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:06<00:03,  3.26it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:06<00:03,  3.30it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:07<00:02,  3.32it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:07<00:02,  3.35it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:08<00:01,  3.39it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:09<00:00,  3.42it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:09<00:00,  3.45it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:09<00:00,  2.87it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:4'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:4'}
full model:
{'qnli': {'alias': 'qnli', 'acc,none': 0.46, 'acc_stderr,none': 0.05009082659620332}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9846426504484882
0.5824664507839472
0.519615692575773
0.501155542927289
0.7945367166753022
0.308422053695068
0.8640142063914085
0.9138139850456612
0.4633856064809356
0.6590614315196575
0.6979080802987999
0.7199917897264415
0.7749024201981475
0.6702564595367945
0.48800861061165646
0.4420522818267443
0.9519161971847797
0.7484302940574024
0.5133259423607681
0.5588784666822361
0.6658633104669237
0.7569182042181317
0.5308683195195316
0.8457203515884096
0.2160432078364904
0.9025653665852331
0.9196489243365332
0.6742217686289
0.8668007010904994
0.9846426504484882
0.5824664507839472
0.519615692575773
0.501155542927289
0.7945367166753022
0.308422053695068
0.8640142063914085
0.9138139850456612
0.4633856064809356
0.6590614315196575
0.6979080802987999
0.7199917897264415
0.7749024201981475
0.6702564595367945
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[7, 5, 4, 2, 6, 1, 3, 0]
tensor([7, 5, 4, 2, 6, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 5, 1, 3, 6, 2, 7, 0]
tensor([4, 5, 1, 3, 6, 2, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 3, 4, 1, 2, 5, 0]
tensor([0, 1, 3, 4, 1, 2, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 1, 5, 2, 4, 1, 3, 0]
tensor([0, 1, 5, 2, 4, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 4, 1, 2, 1, 3, 5, 0]
tensor([0, 4, 1, 2, 1, 3, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[2, 4, 3, 0, 5, 1, 1, 0]
tensor([2, 4, 3, 0, 5, 1, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 3, 4, 2, 1, 1, 5, 0]
tensor([0, 3, 4, 2, 1, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 1, 3, 2, 0, 3, 1]
tensor([0, 2, 1, 3, 2, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 2 to 8
done!
Normal merging for layer 9
tensor([0, 7])
tensor(0)
tensor([1, 4])
tensor(1)
tensor([5])
tensor(5)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
done!
Normal merging for layer 10
tensor([0, 7])
tensor(0)
tensor([1, 5])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
done!
Normal merging for layer 11
tensor([0, 7])
tensor(0)
tensor([2, 4])
tensor(2)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
done!
Normal merging for layer 12
tensor([3, 7])
tensor(3)
tensor([5, 6])
tensor(5)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
done!
Normal merging for layer 13
tensor([0, 7])
tensor(0)
tensor([4, 5])
tensor(4)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 14 to 15
done!
Normal merging for layer 16
tensor([0, 5])
tensor(0)
tensor([2, 7])
tensor(2)
tensor([1, 4])
tensor(1)
tensor([3, 6])
tensor(3)
done!
Cross-layer merge completed for layers 17 to 31
done!
all done!
Model size: 12.3238 GB
238
cuda:4
qqp
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:36<00:36, 36.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:50<00:00, 23.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:50<00:00, 25.40s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: qqp] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: qqp] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
WARNING:lm_eval.api.task:[Task: qqp] metric f1 is defined, but aggregation is not. using default aggregation=f1
WARNING:lm_eval.api.task:[Task: qqp] metric f1 is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/qqp?recursive=False&expand=False HTTP/1.1" 307 140
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/qqp?recursive=False&expand=False HTTP/1.1" 200 355
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140321885151616 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qqp_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321885151616 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qqp_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321885151616 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qqp_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321885151616 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qqp_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140321499049440 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321499049440 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321499049440 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321499049440 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of qqp from None to 0
INFO:lm_eval.api.task:Building contexts for qqp on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2264.41it/s]
DEBUG:lm_eval.evaluator:Task: qqp; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<05:20,  1.61s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:32,  1.29it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:03<01:56,  1.68it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:04<01:41,  1.91it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:05<01:31,  2.08it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:05<01:26,  2.19it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:06<01:21,  2.29it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:07<01:18,  2.37it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:08<01:15,  2.43it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:09<01:13,  2.48it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:09<01:10,  2.53it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:10<01:08,  2.57it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:11<01:07,  2.61it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:12<01:05,  2.63it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:12<01:04,  2.65it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:13<01:03,  2.66it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:14<01:02,  2.68it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:14<01:01,  2.68it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:15<01:00,  2.70it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:16<00:59,  2.71it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:17<00:58,  2.71it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:17<00:57,  2.72it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:18<00:56,  2.73it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:19<00:55,  2.74it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:20<00:54,  2.75it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:20<00:53,  2.77it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:21<00:52,  2.78it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:22<00:51,  2.79it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:22<00:51,  2.80it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:23<00:50,  2.82it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:24<00:49,  2.83it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:25<00:48,  2.84it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:25<00:47,  2.85it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:26<00:46,  2.86it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:27<00:45,  2.86it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:27<00:44,  2.87it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:28<00:44,  2.87it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:29<00:43,  2.87it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:29<00:42,  2.88it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:30<00:41,  2.89it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:31<00:41,  2.90it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:31<00:40,  2.91it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:32<00:39,  2.91it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:33<00:38,  2.92it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:33<00:37,  2.93it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:34<00:37,  2.93it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:35<00:36,  2.95it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:36<00:35,  2.96it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:36<00:34,  2.97it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:37<00:33,  2.98it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:38<00:33,  2.98it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:38<00:32,  2.99it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:39<00:31,  3.00it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:39<00:30,  3.02it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:40<00:30,  3.03it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:41<00:29,  3.04it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:41<00:28,  3.05it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:42<00:27,  3.05it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:43<00:27,  3.06it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:43<00:26,  3.07it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:44<00:25,  3.08it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:45<00:24,  3.08it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:45<00:24,  3.09it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:46<00:23,  3.10it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:47<00:22,  3.11it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:47<00:22,  3.12it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:48<00:21,  3.13it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:49<00:20,  3.13it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:49<00:20,  3.14it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:50<00:19,  3.14it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:50<00:18,  3.15it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:51<00:18,  3.16it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [00:52<00:17,  3.16it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [00:52<00:16,  3.18it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [00:53<00:16,  3.18it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [00:54<00:15,  3.20it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [00:54<00:14,  3.21it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [00:55<00:13,  3.21it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [00:55<00:13,  3.23it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [00:56<00:12,  3.24it/s]Running loglikelihood requests:  80%|████████  | 161/200 [00:57<00:12,  3.25it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [00:57<00:11,  3.25it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [00:58<00:10,  3.25it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [00:58<00:10,  3.26it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [00:59<00:09,  3.27it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:00<00:00, 13.72it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:00<00:00, 10.61it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:01<00:00,  8.46it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:01<00:00,  6.98it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:02<00:00,  5.97it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:02<00:00,  3.20it/s]
bootstrapping for stddev (sequential): f1_score
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:01<02:09,  1.31s/it]  2%|▏         | 2/100 [00:02<02:08,  1.31s/it]  3%|▎         | 3/100 [00:03<02:06,  1.31s/it]  4%|▍         | 4/100 [00:05<02:05,  1.31s/it]  5%|▌         | 5/100 [00:06<02:04,  1.31s/it]  6%|▌         | 6/100 [00:07<02:02,  1.31s/it]  7%|▋         | 7/100 [00:09<02:01,  1.31s/it]  8%|▊         | 8/100 [00:10<02:00,  1.31s/it]  9%|▉         | 9/100 [00:11<01:58,  1.31s/it] 10%|█         | 10/100 [00:13<01:57,  1.31s/it] 11%|█         | 11/100 [00:14<01:56,  1.31s/it] 12%|█▏        | 12/100 [00:15<01:54,  1.31s/it] 13%|█▎        | 13/100 [00:16<01:53,  1.31s/it] 14%|█▍        | 14/100 [00:18<01:52,  1.31s/it] 15%|█▌        | 15/100 [00:19<01:51,  1.31s/it] 16%|█▌        | 16/100 [00:20<01:49,  1.31s/it] 17%|█▋        | 17/100 [00:22<01:48,  1.31s/it] 18%|█▊        | 18/100 [00:23<01:47,  1.31s/it] 19%|█▉        | 19/100 [00:24<01:46,  1.31s/it] 20%|██        | 20/100 [00:26<01:45,  1.32s/it] 21%|██        | 21/100 [00:27<01:44,  1.32s/it] 22%|██▏       | 22/100 [00:28<01:43,  1.32s/it] 23%|██▎       | 23/100 [00:30<01:41,  1.32s/it] 24%|██▍       | 24/100 [00:31<01:40,  1.32s/it] 25%|██▌       | 25/100 [00:32<01:38,  1.32s/it] 26%|██▌       | 26/100 [00:34<01:38,  1.33s/it] 27%|██▋       | 27/100 [00:35<01:36,  1.32s/it] 28%|██▊       | 28/100 [00:36<01:35,  1.32s/it] 29%|██▉       | 29/100 [00:38<01:33,  1.32s/it] 30%|███       | 30/100 [00:39<01:32,  1.32s/it] 31%|███       | 31/100 [00:40<01:30,  1.32s/it] 32%|███▏      | 32/100 [00:42<01:29,  1.31s/it] 33%|███▎      | 33/100 [00:43<01:27,  1.31s/it] 34%|███▍      | 34/100 [00:44<01:26,  1.31s/it] 35%|███▌      | 35/100 [00:45<01:25,  1.31s/it] 36%|███▌      | 36/100 [00:47<01:23,  1.31s/it] 37%|███▋      | 37/100 [00:48<01:22,  1.31s/it] 38%|███▊      | 38/100 [00:49<01:21,  1.31s/it] 39%|███▉      | 39/100 [00:51<01:19,  1.31s/it] 40%|████      | 40/100 [00:52<01:18,  1.31s/it] 41%|████      | 41/100 [00:53<01:17,  1.31s/it] 42%|████▏     | 42/100 [00:55<01:15,  1.31s/it] 43%|████▎     | 43/100 [00:56<01:14,  1.31s/it] 44%|████▍     | 44/100 [00:57<01:13,  1.31s/it] 49%|████▉     | 49/100 [00:58<00:24,  2.09it/s] 50%|█████     | 50/100 [00:59<00:30,  1.63it/s] 51%|█████     | 51/100 [01:00<00:36,  1.34it/s] 52%|█████▏    | 52/100 [01:02<00:41,  1.16it/s] 53%|█████▎    | 53/100 [01:03<00:45,  1.03it/s] 54%|█████▍    | 54/100 [01:04<00:48,  1.05s/it] 55%|█████▌    | 55/100 [01:06<00:50,  1.12s/it] 56%|█████▌    | 56/100 [01:07<00:51,  1.17s/it] 57%|█████▋    | 57/100 [01:08<00:52,  1.21s/it] 58%|█████▊    | 58/100 [01:09<00:51,  1.24s/it] 59%|█████▉    | 59/100 [01:11<00:51,  1.26s/it] 60%|██████    | 60/100 [01:12<00:50,  1.27s/it] 61%|██████    | 61/100 [01:13<00:50,  1.28s/it] 62%|██████▏   | 62/100 [01:15<00:49,  1.29s/it] 63%|██████▎   | 63/100 [01:16<00:47,  1.30s/it] 64%|██████▍   | 64/100 [01:17<00:46,  1.30s/it] 65%|██████▌   | 65/100 [01:19<00:45,  1.31s/it] 66%|██████▌   | 66/100 [01:20<00:44,  1.31s/it] 67%|██████▋   | 67/100 [01:21<00:43,  1.31s/it] 68%|██████▊   | 68/100 [01:23<00:41,  1.31s/it] 69%|██████▉   | 69/100 [01:24<00:40,  1.31s/it] 70%|███████   | 70/100 [01:25<00:39,  1.31s/it] 71%|███████   | 71/100 [01:26<00:38,  1.31s/it] 72%|███████▏  | 72/100 [01:28<00:36,  1.31s/it] 73%|███████▎  | 73/100 [01:29<00:35,  1.31s/it] 74%|███████▍  | 74/100 [01:30<00:34,  1.31s/it] 75%|███████▌  | 75/100 [01:32<00:32,  1.31s/it] 76%|███████▌  | 76/100 [01:33<00:31,  1.31s/it] 77%|███████▋  | 77/100 [01:34<00:30,  1.31s/it] 78%|███████▊  | 78/100 [01:36<00:28,  1.31s/it] 79%|███████▉  | 79/100 [01:37<00:27,  1.31s/it] 80%|████████  | 80/100 [01:38<00:26,  1.32s/it] 81%|████████  | 81/100 [01:40<00:25,  1.33s/it] 82%|████████▏ | 82/100 [01:41<00:23,  1.32s/it] 83%|████████▎ | 83/100 [01:42<00:22,  1.32s/it] 84%|████████▍ | 84/100 [01:44<00:21,  1.32s/it] 85%|████████▌ | 85/100 [01:45<00:19,  1.32s/it] 86%|████████▌ | 86/100 [01:46<00:18,  1.31s/it] 87%|████████▋ | 87/100 [01:48<00:17,  1.31s/it] 88%|████████▊ | 88/100 [01:49<00:15,  1.31s/it] 89%|████████▉ | 89/100 [01:50<00:14,  1.33s/it] 90%|█████████ | 90/100 [01:52<00:13,  1.32s/it] 91%|█████████ | 91/100 [01:53<00:11,  1.32s/it] 92%|█████████▏| 92/100 [01:54<00:10,  1.32s/it] 93%|█████████▎| 93/100 [01:56<00:09,  1.32s/it] 94%|█████████▍| 94/100 [01:57<00:07,  1.32s/it] 95%|█████████▌| 95/100 [01:58<00:06,  1.32s/it]100%|██████████| 100/100 [01:59<00:00,  2.07it/s]100%|██████████| 100/100 [01:59<00:00,  1.19s/it]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:5'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:5'}
full model:
{'qqp': {'alias': 'qqp', 'acc,none': 0.3, 'acc_stderr,none': 0.04605661864718382, 'f1,none': np.float64(0.46153846153846156), 'f1_stderr,none': 0.05409397997448054}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7956670939612598
0.9434665619842647
0.7711450328862919
0.8003111158987146
0.8660685706927677
0.6448594355779524
0.8450704433196731
0.7858702319299253
0.44571740342828275
0.8279983705061201
0.5649013775668057
0.5764430622159564
0.7600312367627909
0.4667905570890965
0.6441885742762831
0.5956490746171949
0.7561037067659027
0.6832036593513438
0.703034342774608
0.741950763385666
0.9094352412525417
0.3828457019607829
0.592762753179094
0.5983336442242687
0.5414593978662989
0.3299447012562784
0.4799857918750434
0.8524920468933548
0.713165518996795
0.7956670939612598
0.9434665619842647
0.7711450328862919
0.8003111158987146
0.8660685706927677
0.6448594355779524
0.8450704433196731
0.7858702319299253
0.44571740342828275
0.8279983705061201
0.5649013775668057
0.5764430622159564
0.7600312367627909
0.4667905570890965
0.6441885742762831
0.5956490746171949
0.7561037067659027
0.6832036593513438
0.703034342774608
0.741950763385666
0.9094352412525417
0.3828457019607829
0.592762753179094
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[5, 4, 6, 2, 7, 1, 3, 0]
tensor([5, 4, 6, 2, 7, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 4, 5, 2, 6, 1, 3, 0]
tensor([7, 4, 5, 2, 6, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 2, 7, 1, 3, 0]
tensor([5, 4, 6, 2, 7, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 4, 5, 2, 7, 3, 0, 1]
tensor([6, 4, 5, 2, 7, 3, 0, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 7, 4, 2, 5, 0, 1, 3]
tensor([6, 7, 4, 2, 5, 0, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 6, 4, 3, 7, 1, 0, 2]
tensor([5, 6, 4, 3, 7, 1, 0, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 3 to 4
done!
Normal merging for layer 5
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Normal merging for layer 6
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
done!
Normal merging for layer 7
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 8 to 31
done!
all done!
Model size: 12.1348 GB
57
cuda:5
openbookqa
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:38<00:38, 38.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 23.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 25.78s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa HTTP/1.1" 307 67
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa HTTP/1.1" 200 1409
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/openbookqa/openbookqa.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa HTTP/1.1" 307 67
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa HTTP/1.1" 200 1409
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/openbookqa/resolve/388097ea7776314e93a529163e0fea805b8a6454/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/openbookqa/resolve/388097ea7776314e93a529163e0fea805b8a6454/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/openbookqa/resolve/388097ea7776314e93a529163e0fea805b8a6454/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/openbookqa/resolve/388097ea7776314e93a529163e0fea805b8a6454/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140321218032608 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_openbookqa_main_0.0.0_388097ea7776314e93a529163e0fea805b8a6454.lock
DEBUG:filelock:Lock 140321218032608 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_openbookqa_main_0.0.0_388097ea7776314e93a529163e0fea805b8a6454.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321218032608 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_openbookqa_main_0.0.0_388097ea7776314e93a529163e0fea805b8a6454.lock
DEBUG:filelock:Lock 140321218032608 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_openbookqa_main_0.0.0_388097ea7776314e93a529163e0fea805b8a6454.lock
DEBUG:filelock:Attempting to acquire lock 140321084593008 on /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454_builder.lock
DEBUG:filelock:Lock 140321084593008 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321084593008 on /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454_builder.lock
DEBUG:filelock:Lock 140321084593008 released on /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of openbookqa from None to 0
INFO:lm_eval.api.task:Building contexts for openbookqa on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2544.10it/s]
DEBUG:lm_eval.evaluator:Task: openbookqa; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:01<08:44,  1.31s/it]Running loglikelihood requests:   0%|          | 2/400 [00:02<06:44,  1.02s/it]Running loglikelihood requests:   1%|          | 3/400 [00:02<06:03,  1.09it/s]Running loglikelihood requests:   1%|          | 4/400 [00:03<05:41,  1.16it/s]Running loglikelihood requests:   1%|▏         | 5/400 [00:04<05:09,  1.28it/s]Running loglikelihood requests:   2%|▏         | 6/400 [00:04<04:48,  1.36it/s]Running loglikelihood requests:   2%|▏         | 7/400 [00:05<04:35,  1.43it/s]Running loglikelihood requests:   2%|▏         | 8/400 [00:06<04:24,  1.48it/s]Running loglikelihood requests:   2%|▏         | 9/400 [00:06<04:16,  1.52it/s]Running loglikelihood requests:   2%|▎         | 10/400 [00:07<04:10,  1.55it/s]Running loglikelihood requests:   3%|▎         | 11/400 [00:08<04:07,  1.57it/s]Running loglikelihood requests:   3%|▎         | 12/400 [00:08<04:03,  1.59it/s]Running loglikelihood requests:   3%|▎         | 13/400 [00:09<04:00,  1.61it/s]Running loglikelihood requests:   4%|▍         | 15/400 [00:09<03:02,  2.10it/s]Running loglikelihood requests:   4%|▍         | 16/400 [00:10<03:15,  1.97it/s]Running loglikelihood requests:   4%|▍         | 17/400 [00:11<03:24,  1.88it/s]Running loglikelihood requests:   4%|▍         | 18/400 [00:11<03:29,  1.82it/s]Running loglikelihood requests:   5%|▍         | 19/400 [00:12<03:34,  1.78it/s]Running loglikelihood requests:   5%|▌         | 20/400 [00:12<03:36,  1.76it/s]Running loglikelihood requests:   5%|▌         | 21/400 [00:13<03:37,  1.74it/s]Running loglikelihood requests:   6%|▌         | 22/400 [00:14<03:37,  1.74it/s]Running loglikelihood requests:   6%|▌         | 23/400 [00:14<03:37,  1.73it/s]Running loglikelihood requests:  10%|▉         | 38/400 [00:14<00:36, 10.05it/s]Running loglikelihood requests:  10%|█         | 40/400 [00:16<00:57,  6.27it/s]Running loglikelihood requests:  10%|█         | 42/400 [00:17<01:18,  4.55it/s]Running loglikelihood requests:  11%|█         | 43/400 [00:17<01:29,  3.97it/s]Running loglikelihood requests:  11%|█         | 44/400 [00:18<01:42,  3.48it/s]Running loglikelihood requests:  11%|█▏        | 45/400 [00:18<01:55,  3.08it/s]Running loglikelihood requests:  12%|█▏        | 46/400 [00:19<02:07,  2.77it/s]Running loglikelihood requests:  12%|█▏        | 47/400 [00:19<02:18,  2.54it/s]Running loglikelihood requests:  12%|█▏        | 48/400 [00:20<02:28,  2.38it/s]Running loglikelihood requests:  12%|█▏        | 49/400 [00:20<02:35,  2.25it/s]Running loglikelihood requests:  12%|█▎        | 50/400 [00:21<02:41,  2.16it/s]Running loglikelihood requests:  13%|█▎        | 51/400 [00:21<02:46,  2.10it/s]Running loglikelihood requests:  13%|█▎        | 52/400 [00:22<02:49,  2.05it/s]Running loglikelihood requests:  13%|█▎        | 53/400 [00:22<02:51,  2.03it/s]Running loglikelihood requests:  14%|█▎        | 54/400 [00:23<02:52,  2.01it/s]Running loglikelihood requests:  14%|█▍        | 55/400 [00:23<02:52,  1.99it/s]Running loglikelihood requests:  14%|█▍        | 57/400 [00:24<02:13,  2.58it/s]Running loglikelihood requests:  15%|█▌        | 60/400 [00:24<01:35,  3.57it/s]Running loglikelihood requests:  15%|█▌        | 61/400 [00:25<01:49,  3.10it/s]Running loglikelihood requests:  16%|█▌        | 62/400 [00:25<02:02,  2.77it/s]Running loglikelihood requests:  16%|█▌        | 63/400 [00:26<02:13,  2.53it/s]Running loglikelihood requests:  16%|█▌        | 64/400 [00:26<02:21,  2.37it/s]Running loglikelihood requests:  16%|█▋        | 65/400 [00:27<02:28,  2.25it/s]Running loglikelihood requests:  16%|█▋        | 66/400 [00:27<02:33,  2.17it/s]Running loglikelihood requests:  17%|█▋        | 67/400 [00:28<02:37,  2.12it/s]Running loglikelihood requests:  17%|█▋        | 68/400 [00:28<02:39,  2.08it/s]Running loglikelihood requests:  17%|█▋        | 69/400 [00:29<02:40,  2.06it/s]Running loglikelihood requests:  18%|█▊        | 70/400 [00:29<02:41,  2.05it/s]Running loglikelihood requests:  18%|█▊        | 71/400 [00:30<02:41,  2.03it/s]Running loglikelihood requests:  18%|█▊        | 72/400 [00:30<02:41,  2.03it/s]Running loglikelihood requests:  18%|█▊        | 73/400 [00:31<02:41,  2.02it/s]Running loglikelihood requests:  18%|█▊        | 74/400 [00:31<02:41,  2.02it/s]Running loglikelihood requests:  19%|█▉        | 75/400 [00:32<02:41,  2.01it/s]Running loglikelihood requests:  19%|█▉        | 76/400 [00:32<02:41,  2.01it/s]Running loglikelihood requests:  19%|█▉        | 77/400 [00:33<02:40,  2.01it/s]Running loglikelihood requests:  20%|█▉        | 78/400 [00:33<02:40,  2.01it/s]Running loglikelihood requests:  20%|█▉        | 79/400 [00:34<02:39,  2.01it/s]Running loglikelihood requests:  20%|██        | 81/400 [00:34<02:01,  2.62it/s]Running loglikelihood requests:  20%|██        | 82/400 [00:35<02:10,  2.44it/s]Running loglikelihood requests:  21%|██        | 83/400 [00:35<02:16,  2.32it/s]Running loglikelihood requests:  21%|██        | 84/400 [00:36<02:21,  2.23it/s]Running loglikelihood requests:  22%|██▏       | 86/400 [00:36<01:52,  2.78it/s]Running loglikelihood requests:  22%|██▏       | 87/400 [00:37<02:02,  2.56it/s]Running loglikelihood requests:  22%|██▏       | 88/400 [00:37<02:10,  2.40it/s]Running loglikelihood requests:  22%|██▏       | 89/400 [00:38<02:15,  2.29it/s]Running loglikelihood requests:  22%|██▎       | 90/400 [00:38<02:19,  2.22it/s]Running loglikelihood requests:  23%|██▎       | 91/400 [00:39<02:23,  2.16it/s]Running loglikelihood requests:  23%|██▎       | 92/400 [00:39<02:25,  2.12it/s]Running loglikelihood requests:  23%|██▎       | 93/400 [00:40<02:26,  2.10it/s]Running loglikelihood requests:  24%|██▎       | 94/400 [00:40<02:27,  2.08it/s]Running loglikelihood requests:  24%|██▍       | 95/400 [00:41<02:27,  2.06it/s]Running loglikelihood requests:  24%|██▍       | 96/400 [00:41<02:28,  2.05it/s]Running loglikelihood requests:  24%|██▍       | 97/400 [00:42<02:28,  2.05it/s]Running loglikelihood requests:  24%|██▍       | 98/400 [00:42<02:27,  2.04it/s]Running loglikelihood requests:  25%|██▍       | 99/400 [00:43<02:27,  2.04it/s]Running loglikelihood requests:  25%|██▌       | 100/400 [00:43<02:27,  2.03it/s]Running loglikelihood requests:  25%|██▌       | 101/400 [00:44<02:27,  2.03it/s]Running loglikelihood requests:  26%|██▌       | 102/400 [00:44<02:26,  2.04it/s]Running loglikelihood requests:  26%|██▌       | 103/400 [00:45<02:25,  2.05it/s]Running loglikelihood requests:  26%|██▋       | 106/400 [00:45<01:29,  3.28it/s]Running loglikelihood requests:  27%|██▋       | 107/400 [00:46<01:40,  2.92it/s]Running loglikelihood requests:  27%|██▋       | 108/400 [00:46<01:49,  2.65it/s]Running loglikelihood requests:  27%|██▋       | 109/400 [00:47<01:57,  2.47it/s]Running loglikelihood requests:  28%|██▊       | 110/400 [00:47<02:03,  2.34it/s]Running loglikelihood requests:  28%|██▊       | 111/400 [00:48<02:08,  2.25it/s]Running loglikelihood requests:  28%|██▊       | 112/400 [00:48<02:11,  2.19it/s]Running loglikelihood requests:  28%|██▊       | 113/400 [00:49<02:13,  2.15it/s]Running loglikelihood requests:  28%|██▊       | 114/400 [00:49<02:15,  2.12it/s]Running loglikelihood requests:  29%|██▉       | 115/400 [00:49<02:15,  2.11it/s]Running loglikelihood requests:  29%|██▉       | 116/400 [00:50<02:14,  2.11it/s]Running loglikelihood requests:  29%|██▉       | 117/400 [00:50<02:14,  2.10it/s]Running loglikelihood requests:  30%|██▉       | 118/400 [00:51<02:14,  2.10it/s]Running loglikelihood requests:  30%|██▉       | 119/400 [00:51<02:13,  2.10it/s]Running loglikelihood requests:  30%|███       | 120/400 [00:52<02:14,  2.09it/s]Running loglikelihood requests:  30%|███       | 121/400 [00:52<02:13,  2.09it/s]Running loglikelihood requests:  30%|███       | 122/400 [00:53<02:13,  2.08it/s]Running loglikelihood requests:  31%|███       | 123/400 [00:53<02:13,  2.08it/s]Running loglikelihood requests:  31%|███       | 124/400 [00:54<02:12,  2.08it/s]Running loglikelihood requests:  31%|███▏      | 125/400 [00:54<02:12,  2.07it/s]Running loglikelihood requests:  32%|███▏      | 126/400 [00:55<02:14,  2.04it/s]Running loglikelihood requests:  32%|███▏      | 127/400 [00:55<02:14,  2.03it/s]Running loglikelihood requests:  32%|███▏      | 128/400 [00:56<02:13,  2.04it/s]Running loglikelihood requests:  32%|███▏      | 129/400 [00:56<02:12,  2.04it/s]Running loglikelihood requests:  32%|███▎      | 130/400 [00:57<02:11,  2.06it/s]Running loglikelihood requests:  33%|███▎      | 131/400 [00:57<02:10,  2.07it/s]Running loglikelihood requests:  33%|███▎      | 132/400 [00:58<02:08,  2.08it/s]Running loglikelihood requests:  33%|███▎      | 133/400 [00:58<02:08,  2.08it/s]Running loglikelihood requests:  34%|███▎      | 134/400 [00:59<02:07,  2.09it/s]Running loglikelihood requests:  34%|███▍      | 138/400 [00:59<01:06,  3.97it/s]Running loglikelihood requests:  35%|███▍      | 139/400 [01:00<01:16,  3.40it/s]Running loglikelihood requests:  35%|███▌      | 140/400 [01:00<01:29,  2.91it/s]Running loglikelihood requests:  35%|███▌      | 141/400 [01:01<01:37,  2.67it/s]Running loglikelihood requests:  36%|███▌      | 142/400 [01:01<01:43,  2.49it/s]Running loglikelihood requests:  36%|███▌      | 143/400 [01:02<01:48,  2.37it/s]Running loglikelihood requests:  36%|███▌      | 144/400 [01:02<01:51,  2.29it/s]Running loglikelihood requests:  36%|███▋      | 145/400 [01:03<01:54,  2.23it/s]Running loglikelihood requests:  37%|███▋      | 147/400 [01:03<01:29,  2.82it/s]Running loglikelihood requests:  37%|███▋      | 148/400 [01:04<01:37,  2.58it/s]Running loglikelihood requests:  37%|███▋      | 149/400 [01:04<01:42,  2.46it/s]Running loglikelihood requests:  38%|███▊      | 151/400 [01:04<01:22,  3.00it/s]Running loglikelihood requests:  38%|███▊      | 152/400 [01:05<01:30,  2.75it/s]Running loglikelihood requests:  38%|███▊      | 153/400 [01:05<01:35,  2.57it/s]Running loglikelihood requests:  38%|███▊      | 154/400 [01:06<01:40,  2.45it/s]Running loglikelihood requests:  39%|███▉      | 155/400 [01:06<01:43,  2.36it/s]Running loglikelihood requests:  39%|███▉      | 156/400 [01:07<01:45,  2.31it/s]Running loglikelihood requests:  39%|███▉      | 157/400 [01:07<01:47,  2.26it/s]Running loglikelihood requests:  40%|███▉      | 158/400 [01:08<01:48,  2.23it/s]Running loglikelihood requests:  40%|███▉      | 159/400 [01:08<01:48,  2.22it/s]Running loglikelihood requests:  40%|████      | 160/400 [01:09<01:51,  2.16it/s]Running loglikelihood requests:  40%|████      | 161/400 [01:09<01:51,  2.14it/s]Running loglikelihood requests:  40%|████      | 162/400 [01:10<01:51,  2.13it/s]Running loglikelihood requests:  41%|████      | 163/400 [01:10<01:51,  2.13it/s]Running loglikelihood requests:  42%|████▏     | 166/400 [01:11<01:08,  3.40it/s]Running loglikelihood requests:  42%|████▏     | 167/400 [01:11<01:17,  3.02it/s]Running loglikelihood requests:  43%|████▎     | 171/400 [01:11<00:49,  4.66it/s]Running loglikelihood requests:  43%|████▎     | 172/400 [01:12<00:58,  3.91it/s]Running loglikelihood requests:  43%|████▎     | 173/400 [01:12<01:07,  3.37it/s]Running loglikelihood requests:  44%|████▎     | 174/400 [01:13<01:15,  3.00it/s]Running loglikelihood requests:  44%|████▍     | 175/400 [01:13<01:21,  2.75it/s]Running loglikelihood requests:  44%|████▍     | 176/400 [01:14<01:27,  2.57it/s]Running loglikelihood requests:  44%|████▍     | 177/400 [01:14<01:31,  2.44it/s]Running loglikelihood requests:  44%|████▍     | 178/400 [01:15<01:34,  2.36it/s]Running loglikelihood requests:  45%|████▍     | 179/400 [01:15<01:36,  2.29it/s]Running loglikelihood requests:  49%|████▉     | 195/400 [01:16<00:16, 12.69it/s]Running loglikelihood requests:  49%|████▉     | 197/400 [01:16<00:26,  7.68it/s]Running loglikelihood requests:  50%|████▉     | 199/400 [01:17<00:29,  6.82it/s]Running loglikelihood requests:  50%|█████     | 202/400 [01:17<00:29,  6.76it/s]Running loglikelihood requests:  51%|█████     | 203/400 [01:18<00:35,  5.52it/s]Running loglikelihood requests:  51%|█████     | 204/400 [01:18<00:42,  4.61it/s]Running loglikelihood requests:  51%|█████▏    | 205/400 [01:19<00:49,  3.94it/s]Running loglikelihood requests:  52%|█████▏    | 206/400 [01:19<00:56,  3.43it/s]Running loglikelihood requests:  52%|█████▏    | 207/400 [01:20<01:02,  3.07it/s]Running loglikelihood requests:  52%|█████▏    | 209/400 [01:20<00:54,  3.48it/s]Running loglikelihood requests:  52%|█████▎    | 210/400 [01:21<01:01,  3.10it/s]Running loglikelihood requests:  53%|█████▎    | 211/400 [01:21<01:06,  2.83it/s]Running loglikelihood requests:  53%|█████▎    | 212/400 [01:21<01:11,  2.63it/s]Running loglikelihood requests:  53%|█████▎    | 213/400 [01:22<01:15,  2.49it/s]Running loglikelihood requests:  54%|█████▎    | 214/400 [01:22<01:17,  2.40it/s]Running loglikelihood requests:  54%|█████▍    | 215/400 [01:23<01:19,  2.33it/s]Running loglikelihood requests:  54%|█████▍    | 216/400 [01:23<01:19,  2.30it/s]Running loglikelihood requests:  54%|█████▍    | 217/400 [01:24<01:20,  2.28it/s]Running loglikelihood requests:  55%|█████▍    | 218/400 [01:24<01:20,  2.26it/s]Running loglikelihood requests:  55%|█████▍    | 219/400 [01:25<01:20,  2.25it/s]Running loglikelihood requests:  55%|█████▌    | 220/400 [01:25<01:20,  2.23it/s]Running loglikelihood requests:  55%|█████▌    | 221/400 [01:26<01:20,  2.23it/s]Running loglikelihood requests:  56%|█████▌    | 222/400 [01:26<01:20,  2.22it/s]Running loglikelihood requests:  56%|█████▌    | 223/400 [01:26<01:19,  2.22it/s]Running loglikelihood requests:  56%|█████▌    | 224/400 [01:27<01:18,  2.23it/s]Running loglikelihood requests:  56%|█████▋    | 225/400 [01:27<01:18,  2.23it/s]Running loglikelihood requests:  56%|█████▋    | 226/400 [01:28<01:17,  2.23it/s]Running loglikelihood requests:  57%|█████▋    | 227/400 [01:28<01:17,  2.23it/s]Running loglikelihood requests:  57%|█████▋    | 228/400 [01:29<01:16,  2.25it/s]Running loglikelihood requests:  57%|█████▋    | 229/400 [01:29<01:15,  2.25it/s]Running loglikelihood requests:  57%|█████▊    | 230/400 [01:30<01:15,  2.24it/s]Running loglikelihood requests:  58%|█████▊    | 231/400 [01:30<01:15,  2.24it/s]Running loglikelihood requests:  58%|█████▊    | 232/400 [01:30<01:14,  2.24it/s]Running loglikelihood requests:  58%|█████▊    | 233/400 [01:31<01:14,  2.25it/s]Running loglikelihood requests:  58%|█████▊    | 234/400 [01:31<01:13,  2.25it/s]Running loglikelihood requests:  59%|█████▉    | 235/400 [01:32<01:13,  2.25it/s]Running loglikelihood requests:  59%|█████▉    | 237/400 [01:32<00:55,  2.93it/s]Running loglikelihood requests:  60%|█████▉    | 239/400 [01:33<00:47,  3.42it/s]Running loglikelihood requests:  60%|██████    | 240/400 [01:33<00:52,  3.07it/s]Running loglikelihood requests:  60%|██████    | 241/400 [01:34<00:56,  2.83it/s]Running loglikelihood requests:  60%|██████    | 242/400 [01:34<00:59,  2.66it/s]Running loglikelihood requests:  61%|██████    | 243/400 [01:34<01:02,  2.53it/s]Running loglikelihood requests:  61%|██████    | 244/400 [01:35<01:03,  2.44it/s]Running loglikelihood requests:  61%|██████▏   | 245/400 [01:35<01:05,  2.38it/s]Running loglikelihood requests:  62%|██████▏   | 246/400 [01:36<01:05,  2.33it/s]Running loglikelihood requests:  62%|██████▏   | 247/400 [01:36<01:06,  2.30it/s]Running loglikelihood requests:  62%|██████▏   | 248/400 [01:37<01:06,  2.29it/s]Running loglikelihood requests:  62%|██████▏   | 249/400 [01:37<01:06,  2.28it/s]Running loglikelihood requests:  62%|██████▎   | 250/400 [01:38<01:06,  2.27it/s]Running loglikelihood requests:  63%|██████▎   | 251/400 [01:38<01:06,  2.25it/s]Running loglikelihood requests:  63%|██████▎   | 252/400 [01:38<01:05,  2.25it/s]Running loglikelihood requests:  63%|██████▎   | 253/400 [01:39<01:05,  2.25it/s]Running loglikelihood requests:  64%|██████▎   | 254/400 [01:39<01:04,  2.26it/s]Running loglikelihood requests:  64%|██████▍   | 255/400 [01:40<01:03,  2.27it/s]Running loglikelihood requests:  64%|██████▍   | 256/400 [01:40<01:03,  2.28it/s]Running loglikelihood requests:  64%|██████▍   | 257/400 [01:41<01:02,  2.28it/s]Running loglikelihood requests:  64%|██████▍   | 258/400 [01:41<01:02,  2.28it/s]Running loglikelihood requests:  65%|██████▍   | 259/400 [01:42<01:02,  2.27it/s]Running loglikelihood requests:  65%|██████▌   | 260/400 [01:42<01:01,  2.27it/s]Running loglikelihood requests:  65%|██████▌   | 261/400 [01:42<01:01,  2.27it/s]Running loglikelihood requests:  66%|██████▌   | 262/400 [01:43<01:00,  2.27it/s]Running loglikelihood requests:  66%|██████▌   | 263/400 [01:43<00:59,  2.29it/s]Running loglikelihood requests:  66%|██████▋   | 265/400 [01:44<00:45,  3.00it/s]Running loglikelihood requests:  66%|██████▋   | 266/400 [01:44<00:47,  2.80it/s]Running loglikelihood requests:  67%|██████▋   | 267/400 [01:45<00:50,  2.64it/s]Running loglikelihood requests:  67%|██████▋   | 268/400 [01:45<00:52,  2.54it/s]Running loglikelihood requests:  67%|██████▋   | 269/400 [01:45<00:53,  2.47it/s]Running loglikelihood requests:  68%|██████▊   | 270/400 [01:46<00:53,  2.44it/s]Running loglikelihood requests:  68%|██████▊   | 271/400 [01:46<00:53,  2.41it/s]Running loglikelihood requests:  68%|██████▊   | 272/400 [01:47<00:53,  2.38it/s]Running loglikelihood requests:  68%|██████▊   | 273/400 [01:47<00:53,  2.38it/s]Running loglikelihood requests:  68%|██████▊   | 274/400 [01:48<00:53,  2.34it/s]Running loglikelihood requests:  69%|██████▉   | 275/400 [01:48<00:53,  2.32it/s]Running loglikelihood requests:  69%|██████▉   | 276/400 [01:49<00:53,  2.31it/s]Running loglikelihood requests:  69%|██████▉   | 277/400 [01:49<00:53,  2.30it/s]Running loglikelihood requests:  70%|██████▉   | 278/400 [01:49<00:52,  2.30it/s]Running loglikelihood requests:  70%|██████▉   | 279/400 [01:50<00:52,  2.29it/s]Running loglikelihood requests:  70%|███████   | 280/400 [01:50<00:52,  2.29it/s]Running loglikelihood requests:  70%|███████   | 281/400 [01:51<00:51,  2.31it/s]Running loglikelihood requests:  70%|███████   | 282/400 [01:51<00:51,  2.31it/s]Running loglikelihood requests:  71%|███████   | 283/400 [01:52<00:50,  2.33it/s]Running loglikelihood requests:  71%|███████   | 284/400 [01:52<00:49,  2.33it/s]Running loglikelihood requests:  71%|███████▏  | 285/400 [01:52<00:49,  2.34it/s]Running loglikelihood requests:  72%|███████▏  | 286/400 [01:53<00:48,  2.35it/s]Running loglikelihood requests:  72%|███████▏  | 287/400 [01:53<00:48,  2.34it/s]Running loglikelihood requests:  72%|███████▏  | 288/400 [01:54<00:47,  2.35it/s]Running loglikelihood requests:  72%|███████▏  | 289/400 [01:54<00:47,  2.36it/s]Running loglikelihood requests:  72%|███████▎  | 290/400 [01:54<00:46,  2.38it/s]Running loglikelihood requests:  73%|███████▎  | 291/400 [01:55<00:45,  2.38it/s]Running loglikelihood requests:  73%|███████▎  | 292/400 [01:55<00:45,  2.37it/s]Running loglikelihood requests:  73%|███████▎  | 293/400 [01:56<00:45,  2.36it/s]Running loglikelihood requests:  74%|███████▎  | 294/400 [01:56<00:44,  2.36it/s]Running loglikelihood requests:  74%|███████▍  | 297/400 [01:57<00:27,  3.81it/s]Running loglikelihood requests:  74%|███████▍  | 298/400 [01:57<00:30,  3.34it/s]Running loglikelihood requests:  75%|███████▍  | 299/400 [01:57<00:33,  3.04it/s]Running loglikelihood requests:  75%|███████▌  | 300/400 [01:58<00:35,  2.83it/s]Running loglikelihood requests:  75%|███████▌  | 301/400 [01:58<00:36,  2.68it/s]Running loglikelihood requests:  76%|███████▌  | 302/400 [01:59<00:38,  2.58it/s]Running loglikelihood requests:  76%|███████▌  | 303/400 [01:59<00:39,  2.48it/s]Running loglikelihood requests:  77%|███████▋  | 307/400 [02:00<00:20,  4.53it/s]Running loglikelihood requests:  77%|███████▋  | 308/400 [02:00<00:23,  3.87it/s]Running loglikelihood requests:  77%|███████▋  | 309/400 [02:00<00:26,  3.43it/s]Running loglikelihood requests:  78%|███████▊  | 310/400 [02:01<00:29,  3.10it/s]Running loglikelihood requests:  78%|███████▊  | 311/400 [02:01<00:30,  2.88it/s]Running loglikelihood requests:  78%|███████▊  | 312/400 [02:02<00:32,  2.71it/s]Running loglikelihood requests:  78%|███████▊  | 313/400 [02:02<00:33,  2.59it/s]Running loglikelihood requests:  78%|███████▊  | 314/400 [02:03<00:34,  2.51it/s]Running loglikelihood requests:  79%|███████▉  | 315/400 [02:03<00:34,  2.47it/s]Running loglikelihood requests:  79%|███████▉  | 316/400 [02:03<00:34,  2.45it/s]Running loglikelihood requests:  79%|███████▉  | 317/400 [02:04<00:34,  2.44it/s]Running loglikelihood requests:  80%|███████▉  | 318/400 [02:04<00:34,  2.41it/s]Running loglikelihood requests:  80%|████████  | 321/400 [02:05<00:20,  3.85it/s]Running loglikelihood requests:  80%|████████  | 322/400 [02:05<00:22,  3.41it/s]Running loglikelihood requests:  81%|████████  | 323/400 [02:06<00:24,  3.12it/s]Running loglikelihood requests:  81%|████████  | 324/400 [02:06<00:25,  2.92it/s]Running loglikelihood requests:  81%|████████▏ | 325/400 [02:06<00:27,  2.76it/s]Running loglikelihood requests:  82%|████████▏ | 326/400 [02:07<00:27,  2.66it/s]Running loglikelihood requests:  82%|████████▏ | 327/400 [02:07<00:28,  2.58it/s]Running loglikelihood requests:  82%|████████▏ | 328/400 [02:08<00:28,  2.54it/s]Running loglikelihood requests:  82%|████████▏ | 329/400 [02:08<00:28,  2.49it/s]Running loglikelihood requests:  82%|████████▎ | 330/400 [02:08<00:28,  2.45it/s]Running loglikelihood requests:  83%|████████▎ | 331/400 [02:09<00:28,  2.46it/s]Running loglikelihood requests:  83%|████████▎ | 332/400 [02:09<00:27,  2.43it/s]Running loglikelihood requests:  83%|████████▎ | 333/400 [02:10<00:27,  2.47it/s]Running loglikelihood requests:  84%|████████▎ | 334/400 [02:10<00:27,  2.43it/s]Running loglikelihood requests:  84%|████████▍ | 336/400 [02:11<00:20,  3.16it/s]Running loglikelihood requests:  84%|████████▍ | 337/400 [02:11<00:21,  2.93it/s]Running loglikelihood requests:  84%|████████▍ | 338/400 [02:11<00:22,  2.78it/s]Running loglikelihood requests:  85%|████████▍ | 339/400 [02:12<00:22,  2.66it/s]Running loglikelihood requests:  85%|████████▌ | 340/400 [02:12<00:23,  2.58it/s]Running loglikelihood requests:  85%|████████▌ | 341/400 [02:13<00:23,  2.52it/s]Running loglikelihood requests:  86%|████████▌ | 342/400 [02:13<00:23,  2.48it/s]Running loglikelihood requests:  86%|████████▌ | 343/400 [02:13<00:23,  2.43it/s]Running loglikelihood requests:  86%|████████▌ | 344/400 [02:14<00:23,  2.42it/s]Running loglikelihood requests:  86%|████████▋ | 345/400 [02:14<00:22,  2.43it/s]Running loglikelihood requests:  86%|████████▋ | 346/400 [02:15<00:21,  2.50it/s]Running loglikelihood requests:  87%|████████▋ | 347/400 [02:15<00:21,  2.47it/s]Running loglikelihood requests:  87%|████████▋ | 348/400 [02:15<00:21,  2.45it/s]Running loglikelihood requests:  87%|████████▋ | 349/400 [02:16<00:20,  2.45it/s]Running loglikelihood requests:  92%|█████████▏| 367/400 [02:16<00:02, 16.49it/s]Running loglikelihood requests:  92%|█████████▎| 370/400 [02:17<00:03,  8.37it/s]Running loglikelihood requests:  93%|█████████▎| 372/400 [02:18<00:04,  6.40it/s]Running loglikelihood requests:  94%|█████████▎| 374/400 [02:19<00:05,  5.13it/s]Running loglikelihood requests:  94%|█████████▍| 377/400 [02:19<00:04,  5.71it/s]Running loglikelihood requests:  94%|█████████▍| 378/400 [02:20<00:04,  5.07it/s]Running loglikelihood requests:  95%|█████████▍| 379/400 [02:20<00:04,  4.49it/s]Running loglikelihood requests:  95%|█████████▌| 380/400 [02:20<00:04,  4.06it/s]Running loglikelihood requests:  95%|█████████▌| 381/400 [02:21<00:05,  3.73it/s]Running loglikelihood requests:  96%|█████████▌| 382/400 [02:21<00:05,  3.46it/s]Running loglikelihood requests:  96%|█████████▌| 383/400 [02:21<00:05,  3.24it/s]Running loglikelihood requests:  96%|█████████▌| 384/400 [02:22<00:05,  3.09it/s]Running loglikelihood requests:  97%|█████████▋| 387/400 [02:22<00:02,  4.55it/s]Running loglikelihood requests:  97%|█████████▋| 388/400 [02:23<00:02,  4.02it/s]Running loglikelihood requests:  98%|█████████▊| 390/400 [02:23<00:02,  4.38it/s]Running loglikelihood requests:  98%|█████████▊| 391/400 [02:23<00:02,  3.93it/s]Running loglikelihood requests:  98%|█████████▊| 392/400 [02:24<00:02,  3.61it/s]Running loglikelihood requests:  98%|█████████▊| 393/400 [02:24<00:02,  3.45it/s]Running loglikelihood requests:  98%|█████████▊| 394/400 [02:24<00:01,  3.23it/s]Running loglikelihood requests:  99%|█████████▉| 395/400 [02:25<00:01,  3.12it/s]Running loglikelihood requests:  99%|█████████▉| 396/400 [02:25<00:01,  3.07it/s]Running loglikelihood requests: 100%|█████████▉| 398/400 [02:25<00:00,  3.93it/s]Running loglikelihood requests: 100%|█████████▉| 399/400 [02:26<00:00,  3.86it/s]Running loglikelihood requests: 100%|██████████| 400/400 [02:26<00:00,  2.74it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:6'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:6'}
full model:
{'openbookqa': {'alias': 'openbookqa', 'acc,none': 0.25, 'acc_stderr,none': 0.04351941398892446, 'acc_norm,none': 0.39, 'acc_norm_stderr,none': 0.04902071300001973}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7187319796021299
0.7542307644255121
0.9218102409329888
0.9507846064925026
0.9244473932111009
0.9697851265705995
0.9088369054586506
0.9555908644563589
0.6065244449438403
0.8380982140772407
0.9227293008726757
0.940339967265193
0.8763319017557598
0.8956221731459367
0.8537768995934096
0.9486125995141542
0.9673324054839537
0.9137514724447725
0.9379677722073356
0.9822722776537973
0.9198472199854921
0.886666948473494
0.9000260607563779
0.9759633945879138
0.9869744580755335
Total groups 76 exceeded the threshold, stopping comparison.
The group tensor is
[6, 2, 3, 1, 5, 4, 7, 0]
tensor([6, 2, 3, 1, 5, 4, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 3, 1, 4, 5, 7, 0]
tensor([6, 2, 3, 1, 4, 5, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 3, 1, 4, 5, 7, 0]
tensor([6, 2, 3, 1, 4, 5, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 5, 2, 4, 3, 7, 0]
tensor([6, 1, 5, 2, 4, 3, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 1, 3, 2, 5, 4, 6, 0]
tensor([7, 1, 3, 2, 5, 4, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 3, 1, 5, 4, 7, 0]
tensor([6, 2, 3, 1, 5, 4, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1, 1.0, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 1, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Normal merging for layer 4
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
done!
Normal merging for layer 5
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 6 to 29
done!
Normal merging for layer 30
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
Normal merging for layer 31
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
all done!
Model size: 12.3238 GB
133
cuda:6
qqp
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:35<00:35, 35.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:48<00:00, 22.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:48<00:00, 24.39s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: qqp] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: qqp] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
WARNING:lm_eval.api.task:[Task: qqp] metric f1 is defined, but aggregation is not. using default aggregation=f1
WARNING:lm_eval.api.task:[Task: qqp] metric f1 is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140320945978592 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qqp_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320945978592 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qqp_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320945978592 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qqp_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320945978592 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qqp_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140321486054576 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321486054576 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321486054576 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321486054576 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of qqp from None to 0
INFO:lm_eval.api.task:Building contexts for qqp on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2292.14it/s]
DEBUG:lm_eval.evaluator:Task: qqp; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<05:00,  1.51s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:25,  1.36it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:03<01:51,  1.75it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:04<01:37,  1.98it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:04<01:28,  2.17it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:05<01:22,  2.29it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:06<01:18,  2.38it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:07<01:15,  2.44it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:07<01:13,  2.49it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:08<01:11,  2.53it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:09<01:09,  2.57it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:10<01:07,  2.61it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:10<01:06,  2.65it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:11<01:04,  2.68it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:12<01:03,  2.70it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:13<01:02,  2.72it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:13<01:00,  2.74it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:14<00:59,  2.75it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:15<00:58,  2.76it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:16<00:58,  2.77it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:16<00:57,  2.78it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:17<00:56,  2.79it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:18<00:55,  2.80it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:18<00:54,  2.81it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:19<00:53,  2.82it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:20<00:52,  2.84it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:20<00:51,  2.85it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:21<00:50,  2.86it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:22<00:49,  2.87it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:23<00:49,  2.84it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:23<00:48,  2.85it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:24<00:48,  2.85it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:25<00:47,  2.86it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:25<00:46,  2.87it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:26<00:45,  2.88it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:27<00:44,  2.88it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:27<00:44,  2.89it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:28<00:43,  2.88it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:29<00:42,  2.87it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:29<00:41,  2.90it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:30<00:40,  2.92it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:31<00:39,  2.93it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:31<00:39,  2.95it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:32<00:38,  2.96it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:33<00:37,  2.97it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:33<00:36,  2.98it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:34<00:35,  2.99it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:35<00:34,  3.01it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:35<00:34,  3.02it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:36<00:33,  3.03it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:37<00:32,  3.03it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:37<00:31,  3.04it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:38<00:31,  3.01it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:39<00:30,  3.02it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:39<00:29,  3.05it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:40<00:28,  3.07it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:41<00:28,  3.09it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:41<00:27,  3.11it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:42<00:26,  3.11it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:43<00:25,  3.13it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:43<00:25,  3.14it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:44<00:24,  3.15it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:44<00:23,  3.15it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:45<00:23,  3.16it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:46<00:22,  3.16it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:46<00:21,  3.18it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:47<00:20,  3.19it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:48<00:20,  3.19it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:48<00:19,  3.20it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:49<00:19,  3.21it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:49<00:18,  3.22it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:50<00:17,  3.23it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [00:51<00:17,  3.23it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [00:51<00:16,  3.25it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [00:52<00:15,  3.25it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [00:53<00:15,  3.27it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [00:53<00:14,  3.28it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [00:54<00:13,  3.28it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [00:54<00:13,  3.30it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [00:55<00:12,  3.30it/s]Running loglikelihood requests:  80%|████████  | 161/200 [00:56<00:11,  3.31it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [00:56<00:11,  3.32it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [00:57<00:10,  3.32it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [00:57<00:09,  3.33it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [00:58<00:09,  3.34it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [00:59<00:08,  3.34it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [00:59<00:08,  3.35it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:00<00:07,  3.36it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:00<00:06,  3.37it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:01<00:06,  3.33it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:02<00:05,  3.34it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:02<00:05,  3.34it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:03<00:04,  3.31it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:03<00:03,  3.33it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:04<00:03,  3.36it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:05<00:02,  3.39it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:05<00:02,  3.41it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:01<00:00,  3.27it/s]
bootstrapping for stddev (sequential): f1_score
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:01<02:09,  1.31s/it]  2%|▏         | 2/100 [00:02<02:08,  1.31s/it]  3%|▎         | 3/100 [00:03<02:06,  1.31s/it]  4%|▍         | 4/100 [00:05<02:05,  1.31s/it]  5%|▌         | 5/100 [00:06<02:03,  1.31s/it]  6%|▌         | 6/100 [00:07<02:02,  1.31s/it]  7%|▋         | 7/100 [00:09<02:01,  1.31s/it]  8%|▊         | 8/100 [00:10<02:01,  1.32s/it]  9%|▉         | 9/100 [00:11<01:59,  1.32s/it] 10%|█         | 10/100 [00:13<01:58,  1.32s/it] 11%|█         | 11/100 [00:14<01:56,  1.31s/it] 12%|█▏        | 12/100 [00:15<01:55,  1.31s/it] 13%|█▎        | 13/100 [00:17<01:53,  1.31s/it] 14%|█▍        | 14/100 [00:18<01:52,  1.31s/it] 15%|█▌        | 15/100 [00:19<01:51,  1.31s/it] 16%|█▌        | 16/100 [00:20<01:49,  1.31s/it] 17%|█▋        | 17/100 [00:22<01:48,  1.31s/it] 18%|█▊        | 18/100 [00:23<01:47,  1.31s/it] 19%|█▉        | 19/100 [00:24<01:45,  1.31s/it] 20%|██        | 20/100 [00:26<01:44,  1.31s/it] 21%|██        | 21/100 [00:27<01:43,  1.31s/it] 22%|██▏       | 22/100 [00:28<01:41,  1.31s/it] 23%|██▎       | 23/100 [00:30<01:40,  1.31s/it] 24%|██▍       | 24/100 [00:31<01:39,  1.31s/it] 25%|██▌       | 25/100 [00:32<01:37,  1.31s/it] 26%|██▌       | 26/100 [00:34<01:36,  1.31s/it] 27%|██▋       | 27/100 [00:35<01:35,  1.31s/it] 28%|██▊       | 28/100 [00:36<01:34,  1.31s/it] 29%|██▉       | 29/100 [00:37<01:32,  1.31s/it] 30%|███       | 30/100 [00:39<01:31,  1.31s/it] 31%|███       | 31/100 [00:40<01:30,  1.31s/it] 32%|███▏      | 32/100 [00:41<01:28,  1.31s/it] 33%|███▎      | 33/100 [00:43<01:27,  1.31s/it] 34%|███▍      | 34/100 [00:44<01:26,  1.31s/it] 35%|███▌      | 35/100 [00:45<01:24,  1.31s/it] 36%|███▌      | 36/100 [00:47<01:23,  1.31s/it] 37%|███▋      | 37/100 [00:48<01:22,  1.31s/it] 38%|███▊      | 38/100 [00:49<01:21,  1.31s/it] 39%|███▉      | 39/100 [00:51<01:19,  1.31s/it] 40%|████      | 40/100 [00:52<01:18,  1.31s/it] 41%|████      | 41/100 [00:53<01:17,  1.31s/it] 42%|████▏     | 42/100 [00:54<01:15,  1.31s/it] 43%|████▎     | 43/100 [00:56<01:14,  1.31s/it] 44%|████▍     | 44/100 [00:57<01:13,  1.31s/it] 45%|████▌     | 45/100 [00:58<01:11,  1.31s/it] 46%|████▌     | 46/100 [01:00<01:10,  1.31s/it] 47%|████▋     | 47/100 [01:01<01:09,  1.31s/it] 48%|████▊     | 48/100 [01:02<01:07,  1.31s/it] 49%|████▉     | 49/100 [01:04<01:06,  1.31s/it] 50%|█████     | 50/100 [01:05<01:05,  1.31s/it] 55%|█████▌    | 55/100 [01:05<00:20,  2.21it/s] 56%|█████▌    | 56/100 [01:06<00:26,  1.69it/s] 57%|█████▋    | 57/100 [01:08<00:31,  1.37it/s] 58%|█████▊    | 58/100 [01:09<00:35,  1.17it/s] 59%|█████▉    | 59/100 [01:10<00:39,  1.04it/s] 60%|██████    | 60/100 [01:12<00:41,  1.05s/it] 61%|██████    | 61/100 [01:13<00:43,  1.11s/it] 62%|██████▏   | 62/100 [01:14<00:44,  1.17s/it] 63%|██████▎   | 63/100 [01:16<00:44,  1.21s/it] 64%|██████▍   | 64/100 [01:17<00:44,  1.24s/it] 65%|██████▌   | 65/100 [01:18<00:43,  1.26s/it] 66%|██████▌   | 66/100 [01:20<00:43,  1.27s/it] 67%|██████▋   | 67/100 [01:21<00:42,  1.28s/it] 68%|██████▊   | 68/100 [01:22<00:41,  1.29s/it] 69%|██████▉   | 69/100 [01:23<00:40,  1.29s/it] 70%|███████   | 70/100 [01:25<00:38,  1.30s/it] 71%|███████   | 71/100 [01:26<00:37,  1.30s/it] 72%|███████▏  | 72/100 [01:27<00:36,  1.30s/it] 73%|███████▎  | 73/100 [01:29<00:35,  1.31s/it] 74%|███████▍  | 74/100 [01:30<00:33,  1.31s/it] 75%|███████▌  | 75/100 [01:31<00:32,  1.31s/it] 76%|███████▌  | 76/100 [01:33<00:31,  1.31s/it] 77%|███████▋  | 77/100 [01:34<00:30,  1.31s/it] 78%|███████▊  | 78/100 [01:35<00:28,  1.31s/it] 79%|███████▉  | 79/100 [01:37<00:27,  1.31s/it] 80%|████████  | 80/100 [01:38<00:26,  1.31s/it] 81%|████████  | 81/100 [01:39<00:24,  1.31s/it] 82%|████████▏ | 82/100 [01:41<00:23,  1.32s/it] 83%|████████▎ | 83/100 [01:42<00:22,  1.32s/it] 84%|████████▍ | 84/100 [01:43<00:21,  1.32s/it] 85%|████████▌ | 85/100 [01:44<00:19,  1.32s/it] 86%|████████▌ | 86/100 [01:46<00:18,  1.32s/it] 87%|████████▋ | 87/100 [01:47<00:17,  1.32s/it] 88%|████████▊ | 88/100 [01:48<00:15,  1.32s/it] 89%|████████▉ | 89/100 [01:50<00:14,  1.32s/it] 90%|█████████ | 90/100 [01:51<00:13,  1.32s/it] 91%|█████████ | 91/100 [01:52<00:11,  1.32s/it] 92%|█████████▏| 92/100 [01:54<00:10,  1.32s/it] 93%|█████████▎| 93/100 [01:55<00:09,  1.32s/it] 94%|█████████▍| 94/100 [01:56<00:07,  1.32s/it] 95%|█████████▌| 95/100 [01:58<00:06,  1.32s/it] 96%|█████████▌| 96/100 [01:59<00:05,  1.32s/it] 97%|█████████▋| 97/100 [02:00<00:03,  1.32s/it] 98%|█████████▊| 98/100 [02:02<00:02,  1.32s/it] 99%|█████████▉| 99/100 [02:03<00:01,  1.32s/it]100%|██████████| 100/100 [02:04<00:00,  1.32s/it]100%|██████████| 100/100 [02:04<00:00,  1.25s/it]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:7'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:7'}
full model:
{'qqp': {'alias': 'qqp', 'acc,none': 0.3, 'acc_stderr,none': 0.04605661864718382, 'f1,none': np.float64(0.46153846153846156), 'f1_stderr,none': 0.05409397997448054}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7956670939612598
0.9434665619842647
0.7711450328862919
0.8003111158987146
0.8660685706927677
0.6448594355779524
0.8450704433196731
0.7858702319299253
0.44571740342828275
0.8279983705061201
0.5649013775668057
0.5764430622159564
0.7600312367627909
0.4667905570890965
0.6441885742762831
0.5956490746171949
0.7561037067659027
0.6832036593513438
0.703034342774608
0.741950763385666
0.9094352412525417
0.3828457019607829
0.592762753179094
0.5983336442242687
0.5414593978662989
0.3299447012562784
0.4799857918750434
0.8524920468933548
0.713165518996795
0.7956670939612598
0.9434665619842647
0.7711450328862919
0.8003111158987146
0.8660685706927677
0.6448594355779524
0.8450704433196731
0.7858702319299253
0.44571740342828275
0.8279983705061201
0.5649013775668057
0.5764430622159564
0.7600312367627909
0.4667905570890965
0.6441885742762831
0.5956490746171949
0.7561037067659027
0.6832036593513438
0.703034342774608
0.741950763385666
0.9094352412525417
0.3828457019607829
0.592762753179094
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[5, 4, 6, 2, 7, 1, 3, 0]
tensor([5, 4, 6, 2, 7, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 4, 5, 2, 6, 1, 3, 0]
tensor([7, 4, 5, 2, 6, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 2, 7, 1, 3, 0]
tensor([5, 4, 6, 2, 7, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 4, 5, 2, 7, 3, 0, 1]
tensor([6, 4, 5, 2, 7, 3, 0, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 7, 4, 2, 5, 0, 1, 3]
tensor([6, 7, 4, 2, 5, 0, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 6, 4, 3, 7, 1, 0, 2]
tensor([5, 6, 4, 3, 7, 1, 0, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 3 to 4
done!
Normal merging for layer 5
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Normal merging for layer 6
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
done!
Normal merging for layer 7
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 8 to 31
done!
all done!
Model size: 12.1348 GB
103
cuda:7
winogrande
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:37<00:37, 37.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:50<00:00, 23.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:50<00:00, 25.24s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/winogrande HTTP/1.1" 307 67
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/winogrande HTTP/1.1" 200 1036
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/winogrande/winogrande.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): datasets-server.hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://datasets-server.hf-mirror.com:443 "GET /parquet?dataset=winogrande HTTP/1.1" 302 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET / HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/winogrande/winogrande.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/winogrande/resolve/main/winogrande.py HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/winogrande/resolve/main/winogrande.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/winogrande/resolve/main/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/winogrande/resolve/main/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/winogrande/resolve/main/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/winogrande/resolve/main/README.md HTTP/1.1" 200 0
DEBUG:filelock:Attempting to acquire lock 140321081180528 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_winogrande_winogrande_xl_1.1.0_a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2.lock
DEBUG:filelock:Lock 140321081180528 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_winogrande_winogrande_xl_1.1.0_a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321081180528 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_winogrande_winogrande_xl_1.1.0_a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2.lock
DEBUG:filelock:Lock 140321081180528 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_winogrande_winogrande_xl_1.1.0_a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2.lock
DEBUG:filelock:Attempting to acquire lock 140321483978032 on /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2_builder.lock
DEBUG:filelock:Lock 140321483978032 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321483978032 on /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2_builder.lock
DEBUG:filelock:Lock 140321483978032 released on /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
DEBUG:lm_eval.api.task:doc_to_text returned an int. Assuming multiple inputs.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of winogrande from None to 0
INFO:lm_eval.api.task:Building contexts for winogrande on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 131359.35it/s]
DEBUG:lm_eval.evaluator:Task: winogrande; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<03:43,  1.12s/it]Running loglikelihood requests:   1%|          | 2/200 [00:01<02:41,  1.23it/s]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:17,  1.43it/s]Running loglikelihood requests:   2%|▏         | 4/200 [00:02<02:06,  1.55it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:03<01:59,  1.64it/s]Running loglikelihood requests:   3%|▎         | 6/200 [00:03<01:54,  1.70it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:04<01:51,  1.73it/s]Running loglikelihood requests:   4%|▍         | 8/200 [00:05<01:49,  1.76it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:05<01:47,  1.78it/s]Running loglikelihood requests:   5%|▌         | 10/200 [00:06<01:45,  1.80it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:06<01:44,  1.81it/s]Running loglikelihood requests:   6%|▌         | 12/200 [00:07<01:43,  1.82it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:07<01:42,  1.83it/s]Running loglikelihood requests:   7%|▋         | 14/200 [00:08<01:41,  1.84it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:08<01:40,  1.84it/s]Running loglikelihood requests:   8%|▊         | 16/200 [00:09<01:39,  1.85it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:09<01:38,  1.85it/s]Running loglikelihood requests:   9%|▉         | 18/200 [00:10<01:38,  1.85it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:10<01:37,  1.85it/s]Running loglikelihood requests:  10%|█         | 20/200 [00:11<01:37,  1.85it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:12<01:36,  1.85it/s]Running loglikelihood requests:  11%|█         | 22/200 [00:12<01:35,  1.86it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:13<01:35,  1.86it/s]Running loglikelihood requests:  12%|█▏        | 24/200 [00:13<01:34,  1.87it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:14<01:33,  1.87it/s]Running loglikelihood requests:  13%|█▎        | 26/200 [00:14<01:33,  1.87it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:15<01:32,  1.87it/s]Running loglikelihood requests:  14%|█▍        | 28/200 [00:15<01:31,  1.87it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:16<01:31,  1.88it/s]Running loglikelihood requests:  15%|█▌        | 30/200 [00:16<01:30,  1.88it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:17<01:29,  1.88it/s]Running loglikelihood requests:  16%|█▌        | 32/200 [00:17<01:29,  1.89it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:18<01:28,  1.89it/s]Running loglikelihood requests:  17%|█▋        | 34/200 [00:18<01:27,  1.89it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:19<01:27,  1.89it/s]Running loglikelihood requests:  18%|█▊        | 36/200 [00:20<01:26,  1.89it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:20<01:26,  1.89it/s]Running loglikelihood requests:  19%|█▉        | 38/200 [00:21<01:25,  1.90it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:21<01:24,  1.90it/s]Running loglikelihood requests:  20%|██        | 40/200 [00:22<01:23,  1.91it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:22<01:23,  1.91it/s]Running loglikelihood requests:  21%|██        | 42/200 [00:23<01:22,  1.91it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:23<01:22,  1.91it/s]Running loglikelihood requests:  22%|██▏       | 44/200 [00:24<01:21,  1.91it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:24<01:21,  1.91it/s]Running loglikelihood requests:  23%|██▎       | 46/200 [00:25<01:20,  1.91it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:25<01:20,  1.91it/s]Running loglikelihood requests:  24%|██▍       | 48/200 [00:26<01:19,  1.91it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:26<01:19,  1.91it/s]Running loglikelihood requests:  25%|██▌       | 50/200 [00:27<01:18,  1.91it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:27<01:17,  1.91it/s]Running loglikelihood requests:  26%|██▌       | 52/200 [00:28<01:17,  1.91it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:28<01:16,  1.91it/s]Running loglikelihood requests:  27%|██▋       | 54/200 [00:29<01:16,  1.91it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:29<01:15,  1.92it/s]Running loglikelihood requests:  28%|██▊       | 56/200 [00:30<01:15,  1.92it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:31<01:14,  1.92it/s]Running loglikelihood requests:  29%|██▉       | 58/200 [00:31<01:13,  1.92it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:32<01:13,  1.93it/s]Running loglikelihood requests:  30%|███       | 60/200 [00:32<01:12,  1.93it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:33<01:12,  1.93it/s]Running loglikelihood requests:  31%|███       | 62/200 [00:33<01:11,  1.93it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:34<01:10,  1.93it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:34<00:13,  9.54it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:35<00:21,  5.79it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:36<00:28,  4.20it/s]Running loglikelihood requests:  40%|████      | 80/200 [00:36<00:32,  3.69it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:37<00:36,  3.26it/s]Running loglikelihood requests:  41%|████      | 82/200 [00:38<00:40,  2.92it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:38<00:43,  2.66it/s]Running loglikelihood requests:  42%|████▏     | 84/200 [00:39<00:46,  2.47it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:39<00:49,  2.33it/s]Running loglikelihood requests:  43%|████▎     | 86/200 [00:40<00:51,  2.23it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:40<00:52,  2.15it/s]Running loglikelihood requests:  44%|████▍     | 88/200 [00:41<00:53,  2.10it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:41<00:53,  2.06it/s]Running loglikelihood requests:  45%|████▌     | 90/200 [00:42<00:54,  2.03it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:42<00:54,  2.00it/s]Running loglikelihood requests:  46%|████▌     | 92/200 [00:43<00:54,  1.99it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:43<00:53,  1.99it/s]Running loglikelihood requests:  47%|████▋     | 94/200 [00:44<00:53,  1.98it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:44<00:53,  1.98it/s]Running loglikelihood requests:  48%|████▊     | 96/200 [00:45<00:52,  1.97it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:45<00:52,  1.97it/s]Running loglikelihood requests:  49%|████▉     | 98/200 [00:46<00:51,  1.97it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:46<00:51,  1.97it/s]Running loglikelihood requests:  50%|█████     | 100/200 [00:47<00:50,  1.97it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:47<00:50,  1.98it/s]Running loglikelihood requests:  51%|█████     | 102/200 [00:48<00:49,  1.98it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:48<00:48,  1.99it/s]Running loglikelihood requests:  52%|█████▏    | 104/200 [00:49<00:48,  1.99it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:49<00:47,  1.99it/s]Running loglikelihood requests:  53%|█████▎    | 106/200 [00:50<00:47,  2.00it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:50<00:46,  2.00it/s]Running loglikelihood requests:  54%|█████▍    | 108/200 [00:51<00:46,  2.00it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:51<00:45,  2.00it/s]Running loglikelihood requests:  55%|█████▌    | 110/200 [00:52<00:45,  1.99it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:52<00:44,  1.99it/s]Running loglikelihood requests:  56%|█████▌    | 112/200 [00:53<00:44,  1.99it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:53<00:43,  1.99it/s]Running loglikelihood requests:  57%|█████▋    | 114/200 [00:54<00:43,  2.00it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:54<00:42,  2.00it/s]Running loglikelihood requests:  58%|█████▊    | 116/200 [00:55<00:41,  2.01it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:55<00:41,  2.01it/s]Running loglikelihood requests:  59%|█████▉    | 118/200 [00:56<00:40,  2.01it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:56<00:40,  2.01it/s]Running loglikelihood requests:  60%|██████    | 120/200 [00:57<00:39,  2.01it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:57<00:39,  2.01it/s]Running loglikelihood requests:  61%|██████    | 122/200 [00:58<00:38,  2.01it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:58<00:38,  2.01it/s]Running loglikelihood requests:  62%|██████▏   | 124/200 [00:59<00:37,  2.01it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:59<00:37,  2.01it/s]Running loglikelihood requests:  63%|██████▎   | 126/200 [01:00<00:36,  2.01it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [01:00<00:36,  2.01it/s]Running loglikelihood requests:  64%|██████▍   | 128/200 [01:01<00:35,  2.01it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [01:01<00:35,  2.01it/s]Running loglikelihood requests:  65%|██████▌   | 130/200 [01:02<00:34,  2.01it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [01:02<00:34,  2.02it/s]Running loglikelihood requests:  66%|██████▌   | 132/200 [01:03<00:33,  2.02it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [01:03<00:33,  2.03it/s]Running loglikelihood requests:  67%|██████▋   | 134/200 [01:04<00:32,  2.03it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [01:04<00:32,  2.03it/s]Running loglikelihood requests:  68%|██████▊   | 136/200 [01:05<00:31,  2.03it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [01:05<00:31,  2.03it/s]Running loglikelihood requests:  69%|██████▉   | 138/200 [01:06<00:30,  2.03it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [01:06<00:30,  2.03it/s]Running loglikelihood requests:  70%|███████   | 140/200 [01:07<00:29,  2.03it/s]Running loglikelihood requests:  70%|███████   | 141/200 [01:07<00:29,  2.03it/s]Running loglikelihood requests:  71%|███████   | 142/200 [01:08<00:28,  2.03it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [01:08<00:28,  2.03it/s]Running loglikelihood requests:  72%|███████▏  | 144/200 [01:09<00:27,  2.03it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:09<00:27,  2.04it/s]Running loglikelihood requests:  73%|███████▎  | 146/200 [01:10<00:26,  2.04it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [01:10<00:26,  2.04it/s]Running loglikelihood requests:  74%|███████▍  | 148/200 [01:10<00:25,  2.04it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:11<00:24,  2.05it/s]Running loglikelihood requests:  75%|███████▌  | 150/200 [01:11<00:24,  2.05it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:12<00:23,  2.05it/s]Running loglikelihood requests:  76%|███████▌  | 152/200 [01:12<00:23,  2.05it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:13<00:22,  2.06it/s]Running loglikelihood requests:  77%|███████▋  | 154/200 [01:13<00:22,  2.06it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:14<00:21,  2.06it/s]Running loglikelihood requests:  78%|███████▊  | 156/200 [01:14<00:21,  2.07it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:15<00:20,  2.06it/s]Running loglikelihood requests:  79%|███████▉  | 158/200 [01:15<00:20,  2.06it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:16<00:19,  2.06it/s]Running loglikelihood requests:  80%|████████  | 160/200 [01:16<00:19,  2.06it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:17<00:18,  2.05it/s]Running loglikelihood requests:  81%|████████  | 162/200 [01:17<00:18,  2.05it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:18<00:18,  2.01it/s]Running loglikelihood requests:  82%|████████▏ | 164/200 [01:18<00:17,  2.02it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:19<00:17,  2.03it/s]Running loglikelihood requests:  83%|████████▎ | 166/200 [01:19<00:16,  2.04it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:20<00:16,  2.04it/s]Running loglikelihood requests:  84%|████████▍ | 168/200 [01:20<00:15,  2.04it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:21<00:15,  2.03it/s]Running loglikelihood requests:  85%|████████▌ | 170/200 [01:21<00:14,  2.04it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:22<00:14,  2.04it/s]Running loglikelihood requests:  86%|████████▌ | 172/200 [01:22<00:13,  2.05it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:23<00:13,  2.05it/s]Running loglikelihood requests:  87%|████████▋ | 174/200 [01:23<00:12,  2.05it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:24<00:12,  2.05it/s]Running loglikelihood requests:  88%|████████▊ | 176/200 [01:24<00:11,  2.05it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:25<00:11,  2.05it/s]Running loglikelihood requests:  89%|████████▉ | 178/200 [01:25<00:10,  2.05it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:26<00:10,  2.06it/s]Running loglikelihood requests:  90%|█████████ | 180/200 [01:26<00:09,  2.07it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:27<00:09,  2.08it/s]Running loglikelihood requests:  91%|█████████ | 182/200 [01:27<00:08,  2.09it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:28<00:08,  2.09it/s]Running loglikelihood requests:  92%|█████████▏| 184/200 [01:28<00:07,  2.09it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:28<00:07,  2.10it/s]Running loglikelihood requests:  93%|█████████▎| 186/200 [01:29<00:06,  2.11it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:29<00:06,  2.12it/s]Running loglikelihood requests:  94%|█████████▍| 188/200 [01:30<00:05,  2.12it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:30<00:05,  2.13it/s]Running loglikelihood requests:  95%|█████████▌| 190/200 [01:31<00:04,  2.15it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:31<00:04,  2.17it/s]Running loglikelihood requests:  96%|█████████▌| 192/200 [01:32<00:03,  2.17it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:32<00:03,  2.18it/s]Running loglikelihood requests:  97%|█████████▋| 194/200 [01:33<00:02,  2.18it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:33<00:02,  2.18it/s]Running loglikelihood requests:  98%|█████████▊| 196/200 [01:34<00:01,  2.18it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:34<00:01,  2.18it/s]Running loglikelihood requests:  99%|█████████▉| 198/200 [01:34<00:00,  2.18it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:35<00:00,  2.18it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:29<00:00,  2.23it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 928
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140325646617584 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140325646617584 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140325646617584 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140325646617584 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140321093267728 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321093267728 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321093267728 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321093267728 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
full model:
{'winogrande': {'alias': 'winogrande', 'acc,none': 0.69, 'acc_stderr,none': 0.046482319871173176}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9565474555896152
0.9256436871263655
0.919777800952949
0.8502516827829548
0.9788217808920475
0.7842184683361061
0.5847175538207384
0.7741211354638314
0.8734705119073022
0.9731317123565076
0.9046875380381674
0.8858535931048256
0.9314298098713261
0.8442403312485581
0.7166029053748131
0.7061450250233585
0.7563059483803782
0.6877563559653158
0.8738989590810852
0.784285727893607
0.8409178900131131
0.8425380927376759
0.6782655591765769
0.7228968829640015
0.8418686487797186
0.9141168065903919
0.8562481461255738
0.6139553840231665
0.934088538459943
Total groups 66 exceeded the threshold, stopping comparison.
The group tensor is
[6, 2, 4, 3, 5, 1, 7, 0]
tensor([6, 2, 4, 3, 5, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 3, 1, 5, 4, 7, 0]
tensor([6, 2, 3, 1, 5, 4, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 4, 1, 0, 3, 0, 1, 2]
tensor([5, 4, 1, 0, 3, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 3, 1, 4, 5, 1, 0]
tensor([0, 2, 3, 1, 4, 5, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[5, 1, 2, 3, 4, 0, 1, 0]
tensor([5, 1, 2, 3, 4, 0, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 2, 1, 1, 2, 3, 3, 0]
tensor([0, 2, 1, 1, 2, 3, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1, 1.0, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 0, 1.0, 1.0, 1.0, 1]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 2 to 7
done!
Normal merging for layer 8
tensor([3, 5])
tensor(3)
tensor([2, 6])
tensor(2)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 9 to 13
done!
Normal merging for layer 14
tensor([0, 7])
tensor(0)
tensor([3, 6])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
done!
Normal merging for layer 15
tensor([5, 7])
tensor(5)
tensor([1, 6])
tensor(1)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 16
tensor([0, 7])
tensor(0)
tensor([2, 3])
tensor(2)
tensor([1, 4])
tensor(1)
tensor([5, 6])
tensor(5)
done!
Cross-layer merge completed for layers 17 to 23
done!
Normal merging for layer 24
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
Normal merging for layer 25
tensor([0, 3])
tensor(0)
tensor([1, 2, 4, 5, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 26 to 31
done!
all done!
Model size: 11.8828 GB
[47] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2346.31it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:48,  3.32s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:48,  1.64s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:02,  1.33s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:42,  1.20s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:37,  1.18s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:26,  1.12s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:18,  1.07s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:10,  1.03s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<02:04,  1.00it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<01:58,  1.04it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:23<01:56,  1.04it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<01:50,  1.08it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:26<01:44,  1.12it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:28<01:40,  1.15it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:28<00:37,  2.83it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:30<00:46,  2.28it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:31<00:52,  1.95it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:33<00:58,  1.72it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:35<01:02,  1.57it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:36<01:09,  1.40it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:38<01:09,  1.38it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:39<01:08,  1.36it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:41<01:06,  1.36it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:42<01:05,  1.36it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:44<01:03,  1.36it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:45<01:02,  1.36it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:47<01:03,  1.32it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:48<01:00,  1.34it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:50<00:58,  1.36it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:51<00:56,  1.37it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:53<00:54,  1.38it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:54<00:52,  1.39it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:51,  1.39it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<01:03,  1.09it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:00<00:57,  1.17it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:52,  1.23it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:03<00:49,  1.29it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:04<00:46,  1.32it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:05<00:43,  1.35it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:08<00:51,  1.10it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:09<00:46,  1.19it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:11<00:42,  1.26it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:12<00:38,  1.31it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:14<00:36,  1.35it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:15<00:33,  1.39it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:16<00:32,  1.37it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:18<00:30,  1.40it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:19<00:28,  1.42it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:20<00:27,  1.44it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:22<00:25,  1.43it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:24<00:26,  1.31it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:25<00:24,  1.37it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:26<00:21,  1.42it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:30<00:28,  1.02it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:30<00:06,  3.13it/s]Running loglikelihood requests:  89%|████████▊ | 126/142 [01:31<00:05,  2.81it/s]Running loglikelihood requests:  90%|█████████ | 128/142 [01:32<00:05,  2.48it/s]Running loglikelihood requests:  92%|█████████▏| 130/142 [01:34<00:05,  2.24it/s]Running loglikelihood requests:  93%|█████████▎| 132/142 [01:35<00:04,  2.07it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:36<00:05,  1.72it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:37<00:04,  1.70it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:38<00:02,  1.70it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:40<00:01,  1.69it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:41<00:00,  1.65it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:41<00:00,  1.40it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-8): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-30): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-8): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-30): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/47.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140320965430592 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320965430592 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320965430592 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320965430592 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140321890730976 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321890730976 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321890730976 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321890730976 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[47] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.891975958338049
0.3982001908036579
0.26652204443251004
0.14111026067874816
0.11763525057572936
0.8635004180455561
0.8741394655456793
0.5304527321557103
0.44617634270319845
0.7007455592903002
0.6799150362476898
0.3511365594568724
0.47145939463701925
0.4744673317050211
0.905435493752087
0.903612941640167
0.8893763849001028
0.6821933807976941
0.6796182015499347
0.7774502436639344
0.8164788896198145
0.9011095591650676
0.8564639597370403
0.9528866120532432
0.4784452972921274
0.48852492817861076
0.8397355586186119
0.7433051149723976
0.5457687574271932
0.891975958338049
0.3982001908036579
0.26652204443251004
0.14111026067874816
0.11763525057572936
0.8635004180455561
0.8741394655456793
0.5304527321557103
0.44617634270319845
0.7007455592903002
0.6799150362476898
0.3511365594568724
0.47145939463701925
0.4744673317050211
0.905435493752087
0.903612941640167
0.8893763849001028
0.6821933807976941
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[6, 5, 4, 3, 2, 1, 7, 0]
tensor([6, 5, 4, 3, 2, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 7, 4, 1, 0, 6, 3]
tensor([5, 2, 7, 4, 1, 0, 6, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 6, 5, 2, 4, 0, 7, 1]
tensor([3, 6, 5, 2, 4, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 0, 2, 1, 7, 3]
tensor([5, 4, 6, 0, 2, 1, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 0, 7, 3, 5, 1, 6, 2]
tensor([4, 0, 7, 3, 5, 1, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 5, 1, 3, 2, 0, 1, 4]
tensor([0, 5, 1, 3, 2, 0, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 0, 0, 1, 1.0, 1.0, 1.0, 1.0]
tensor([1, 0, 0, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[1, 0, 1, 1.0, 1.0, 0, 1.0, 1.0]
tensor([1, 0, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 1.0, 0, 1, 1.0, 1.0]
tensor([0, 1, 1, 1, 0, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/47.pt
[122] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2398.21it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:18,  3.54s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:57,  1.71s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:07,  1.37s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:10<03:02,  1.35s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:12<02:43,  1.23s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:14<02:30,  1.15s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:32,  1.18s/it]Running loglikelihood requests:  13%|█▎        | 19/142 [00:17<01:07,  1.83it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:19<01:14,  1.62it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:20<01:19,  1.50it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:22<01:22,  1.42it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:24<01:32,  1.24it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:26<01:31,  1.24it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:27<01:29,  1.24it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:29<01:27,  1.24it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:31<01:26,  1.24it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:32<01:24,  1.24it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:37<02:05,  1.22s/it]Running loglikelihood requests:  29%|██▉       | 41/142 [00:38<01:50,  1.10s/it]Running loglikelihood requests:  30%|███       | 43/142 [00:40<01:39,  1.01s/it]Running loglikelihood requests:  32%|███▏      | 45/142 [00:41<01:30,  1.07it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:43<01:25,  1.11it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:45<01:25,  1.08it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:47<01:18,  1.15it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:48<01:13,  1.21it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:49<01:09,  1.26it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:51<01:05,  1.29it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:52<01:02,  1.32it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:54<01:07,  1.19it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:56<01:03,  1.25it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:57<00:59,  1.29it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:59<00:56,  1.33it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [01:00<00:53,  1.36it/s]Running loglikelihood requests:  50%|█████     | 71/142 [01:01<00:51,  1.37it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:03<00:49,  1.39it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:05<00:51,  1.29it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:06<00:48,  1.33it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:07<00:46,  1.37it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:09<00:43,  1.39it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:10<00:41,  1.41it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:12<00:40,  1.42it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:13<00:38,  1.44it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:15<00:39,  1.35it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:16<00:36,  1.38it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:17<00:34,  1.40it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:18<00:10,  3.58it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:19<00:12,  2.94it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:21<00:13,  2.51it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:22<00:14,  2.21it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:23<00:15,  2.02it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:25<00:15,  1.88it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:26<00:15,  1.79it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:27<00:14,  1.72it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:29<00:14,  1.61it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:30<00:13,  1.59it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:31<00:11,  1.59it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:32<00:10,  1.59it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:34<00:09,  1.59it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:35<00:08,  1.60it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:36<00:06,  1.60it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:37<00:05,  1.61it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:39<00:04,  1.42it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:40<00:03,  1.49it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:42<00:01,  1.53it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:43<00:00,  1.58it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:43<00:00,  1.38it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-8): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-30): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-8): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-30): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/122.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140320947652704 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320947652704 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320947652704 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320947652704 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140321499038352 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321499038352 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321499038352 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321499038352 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[122] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.891975958338049
0.3982001908036579
0.26652204443251004
0.14111026067874816
0.11763525057572936
0.8635004180455561
0.8741394655456793
0.5304527321557103
0.44617634270319845
0.7007455592903002
0.6799150362476898
0.3511365594568724
0.47145939463701925
0.4744673317050211
0.905435493752087
0.903612941640167
0.8893763849001028
0.6821933807976941
0.6796182015499347
0.7774502436639344
0.8164788896198145
0.9011095591650676
0.8564639597370403
0.9528866120532432
0.4784452972921274
0.48852492817861076
0.8397355586186119
0.7433051149723976
0.5457687574271932
0.891975958338049
0.3982001908036579
0.26652204443251004
0.14111026067874816
0.11763525057572936
0.8635004180455561
0.8741394655456793
0.5304527321557103
0.44617634270319845
0.7007455592903002
0.6799150362476898
0.3511365594568724
0.47145939463701925
0.4744673317050211
0.905435493752087
0.903612941640167
0.8893763849001028
0.6821933807976941
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[6, 5, 4, 3, 2, 1, 7, 0]
tensor([6, 5, 4, 3, 2, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 7, 4, 1, 0, 6, 3]
tensor([5, 2, 7, 4, 1, 0, 6, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 6, 5, 2, 4, 0, 7, 1]
tensor([3, 6, 5, 2, 4, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 0, 2, 1, 7, 3]
tensor([5, 4, 6, 0, 2, 1, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 0, 7, 3, 5, 1, 6, 2]
tensor([4, 0, 7, 3, 5, 1, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 5, 1, 3, 2, 0, 1, 4]
tensor([0, 5, 1, 3, 2, 0, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 0, 0, 1, 1.0, 1.0, 1.0, 1.0]
tensor([1, 0, 0, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[1, 0, 1, 1.0, 1.0, 0, 1.0, 1.0]
tensor([1, 0, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 1.0, 0, 1, 1.0, 1.0]
tensor([0, 1, 1, 1, 0, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/122.pt
[56] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2341.92it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<07:00,  2.98s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:36,  1.56s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:15,  1.42s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:50,  1.26s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:36,  1.17s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:25,  1.11s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:17,  1.07s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:13,  1.05s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<02:06,  1.01s/it]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<01:58,  1.04it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:23<01:52,  1.08it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<01:46,  1.12it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:26<01:41,  1.15it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:28<01:51,  1.03it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:30<01:43,  1.09it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:31<01:37,  1.14it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:33<01:33,  1.17it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:35<01:29,  1.20it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:36<01:26,  1.22it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:38<01:28,  1.17it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:40<01:24,  1.19it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:41<01:21,  1.22it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:43<01:17,  1.25it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:44<01:14,  1.28it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:46<01:11,  1.31it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:47<01:08,  1.34it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:49<01:09,  1.28it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:50<01:05,  1.32it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:52<01:03,  1.35it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:53<01:00,  1.37it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:54<00:58,  1.38it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:57<01:07,  1.17it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:59<01:14,  1.03it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [01:01<01:06,  1.13it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [01:02<01:00,  1.21it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:04<00:26,  2.42it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:06<00:27,  2.20it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:07<00:29,  2.01it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:08<00:30,  1.87it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:10<00:30,  1.78it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:11<00:31,  1.71it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:13<00:33,  1.50it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:14<00:32,  1.51it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:15<00:30,  1.52it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:16<00:29,  1.53it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:18<00:27,  1.54it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:19<00:26,  1.55it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:20<00:25,  1.56it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:22<00:23,  1.56it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:26<00:38,  1.09s/it]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:27<00:31,  1.05it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:28<00:26,  1.17it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:30<00:22,  1.27it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:31<00:20,  1.35it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:32<00:17,  1.41it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:34<00:17,  1.29it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:35<00:15,  1.37it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:37<00:13,  1.44it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:38<00:11,  1.50it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:39<00:09,  1.54it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:40<00:08,  1.58it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:41<00:06,  1.60it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:43<00:05,  1.63it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:48<00:08,  1.22s/it]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:49<00:05,  1.03s/it]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:50<00:02,  1.12it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:51<00:00,  1.26it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:51<00:00,  1.27it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-5): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7-9): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-26): 16 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28-30): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-5): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7-9): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-26): 16 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28-30): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/56.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140320960071440 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320960071440 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320960071440 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320960071440 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140321499045984 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321499045984 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321499045984 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321499045984 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[56] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7169249479288816
0.6183111372245332
0.46095560668163316
0.45412629074361927
0.5961385422474115
0.8890533054728373
0.9122749865545955
0.7722412965326397
0.28598717020359266
0.16135518374824295
0.6405733513488978
0.34881239744915987
0.02163114281968652
0.8586809120820075
0.5280864903169471
0.5549214872848108
0.8809884720847853
0.8184475126773784
0.7411818478320343
0.9204128494945557
0.8828541622108917
0.8520796846320295
0.9938665155729866
0.8532228506983649
0.8363754351071634
0.7452449391867536
0.8461045724576085
0.3693743162395131
0.3582309316878524
0.7169249479288816
0.6183111372245332
0.46095560668163316
0.45412629074361927
0.5961385422474115
0.8890533054728373
0.9122749865545955
0.7722412965326397
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[5, 4, 6, 3, 2, 0, 7, 1]
tensor([5, 4, 6, 3, 2, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 7, 2, 4, 0, 5, 3]
tensor([6, 1, 7, 2, 4, 0, 5, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 6, 3, 1, 0, 7, 5]
tensor([2, 4, 6, 3, 1, 0, 7, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 0, 6, 4, 3, 1, 5, 7]
tensor([2, 0, 6, 4, 3, 1, 5, 7], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[3, 1, 0, 1, 2, 0, 2, 3]
tensor([3, 1, 0, 1, 2, 0, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 2, 1, 0, 2, 3, 3]
tensor([0, 1, 2, 1, 0, 2, 3, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[3, 1, 0, 1, 2, 0, 3, 2]
tensor([3, 1, 0, 1, 2, 0, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 1, 2, 2, 3, 0, 3, 1]
tensor([0, 1, 2, 2, 3, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[3, 0, 1, 2, 1, 2, 3, 0]
tensor([3, 0, 1, 2, 1, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/56.pt
[226] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2353.45it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:25,  2.73s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:04<03:35,  1.55s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:56,  1.29s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:39,  1.18s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:29,  1.12s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:20,  1.08s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:17,  1.07s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:10,  1.02s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<02:03,  1.01it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:56,  1.06it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:50,  1.09it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<01:52,  1.06it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:25<01:45,  1.11it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:27<01:39,  1.15it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:29<01:35,  1.19it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:30<01:31,  1.21it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:32<01:28,  1.23it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:33<01:26,  1.24it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:36<01:38,  1.07it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:37<01:31,  1.12it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:39<01:26,  1.17it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:41<01:22,  1.20it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:42<01:18,  1.24it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:43<01:14,  1.27it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:45<01:17,  1.20it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:47<01:12,  1.26it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:48<01:08,  1.30it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:50<01:05,  1.33it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:50<00:22,  3.48it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:51<00:27,  2.77it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:53<00:32,  2.32it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:54<00:35,  2.04it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:55<00:38,  1.84it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:57<00:40,  1.72it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:58<00:40,  1.65it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:42,  1.54it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:01<00:40,  1.54it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:02<00:39,  1.53it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:03<00:38,  1.54it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:05<00:37,  1.53it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:06<00:35,  1.54it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:07<00:34,  1.55it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:09<00:32,  1.55it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:10<00:33,  1.47it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:11<00:31,  1.50it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:13<00:29,  1.51it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:14<00:28,  1.53it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:15<00:26,  1.53it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:17<00:25,  1.55it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:18<00:23,  1.55it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:19<00:23,  1.48it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:21<00:21,  1.51it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:22<00:20,  1.54it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:23<00:18,  1.56it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:24<00:17,  1.57it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:26<00:15,  1.58it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:27<00:14,  1.60it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:28<00:13,  1.60it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:29<00:12,  1.54it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:31<00:10,  1.57it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:32<00:09,  1.60it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:33<00:08,  1.62it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:34<00:06,  1.64it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:35<00:05,  1.67it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:37<00:04,  1.69it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:38<00:02,  1.71it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:39<00:01,  1.72it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:40<00:00,  1.55it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:40<00:00,  1.41it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-8): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-30): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-8): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-30): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/226.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140321486053760 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321486053760 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321486053760 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321486053760 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140321891279136 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321891279136 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321891279136 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321891279136 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[226] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.891975958338049
0.3982001908036579
0.26652204443251004
0.14111026067874816
0.11763525057572936
0.8635004180455561
0.8741394655456793
0.5304527321557103
0.44617634270319845
0.7007455592903002
0.6799150362476898
0.3511365594568724
0.47145939463701925
0.4744673317050211
0.905435493752087
0.903612941640167
0.8893763849001028
0.6821933807976941
0.6796182015499347
0.7774502436639344
0.8164788896198145
0.9011095591650676
0.8564639597370403
0.9528866120532432
0.4784452972921274
0.48852492817861076
0.8397355586186119
0.7433051149723976
0.5457687574271932
0.891975958338049
0.3982001908036579
0.26652204443251004
0.14111026067874816
0.11763525057572936
0.8635004180455561
0.8741394655456793
0.5304527321557103
0.44617634270319845
0.7007455592903002
0.6799150362476898
0.3511365594568724
0.47145939463701925
0.4744673317050211
0.905435493752087
0.903612941640167
0.8893763849001028
0.6821933807976941
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[6, 5, 4, 3, 2, 1, 7, 0]
tensor([6, 5, 4, 3, 2, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 7, 4, 1, 0, 6, 3]
tensor([5, 2, 7, 4, 1, 0, 6, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 6, 5, 2, 4, 0, 7, 1]
tensor([3, 6, 5, 2, 4, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 0, 2, 1, 7, 3]
tensor([5, 4, 6, 0, 2, 1, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 0, 7, 3, 5, 1, 6, 2]
tensor([4, 0, 7, 3, 5, 1, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 5, 1, 3, 2, 0, 1, 4]
tensor([0, 5, 1, 3, 2, 0, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 0, 0, 1, 1.0, 1.0, 1.0, 1.0]
tensor([1, 0, 0, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[1, 0, 1, 1.0, 1.0, 0, 1.0, 1.0]
tensor([1, 0, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 1.0, 0, 1, 1.0, 1.0]
tensor([0, 1, 1, 1, 0, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/226.pt
[87] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2393.74it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:14,  3.08s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:40,  1.59s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:09,  1.38s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:45,  1.23s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:32,  1.15s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:23,  1.09s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:15,  1.05s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:12,  1.05s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<02:06,  1.01s/it]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<01:58,  1.03it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:52,  1.07it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<01:47,  1.11it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:26<01:42,  1.14it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:28<01:44,  1.10it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:29<01:39,  1.13it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:31<01:35,  1.16it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:33<01:32,  1.18it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:34<01:29,  1.20it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:36<01:26,  1.21it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:38<01:27,  1.18it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:39<01:24,  1.20it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:41<01:21,  1.22it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:42<01:18,  1.24it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:43<00:28,  3.14it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:44<00:34,  2.55it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:45<00:39,  2.18it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:47<00:43,  1.93it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:48<00:46,  1.76it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:50<00:47,  1.65it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:51<00:51,  1.50it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:53<00:51,  1.47it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:54<00:50,  1.46it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:48,  1.45it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:57<00:47,  1.44it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:58<00:46,  1.43it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:45,  1.42it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:47,  1.33it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:47,  1.29it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:05<00:43,  1.34it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:40,  1.39it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:38,  1.43it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:09<00:36,  1.46it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:10<00:34,  1.48it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:11<00:32,  1.50it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:13<00:33,  1.41it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:14<00:31,  1.44it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:15<00:29,  1.47it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:17<00:27,  1.50it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:18<00:25,  1.51it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:19<00:24,  1.53it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:21<00:22,  1.54it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:22<00:22,  1.44it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:23<00:20,  1.48it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:25<00:19,  1.51it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:26<00:17,  1.53it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:27<00:16,  1.54it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:28<00:14,  1.55it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:30<00:13,  1.57it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:31<00:12,  1.58it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:32<00:11,  1.54it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:34<00:09,  1.56it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:35<00:08,  1.58it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:36<00:06,  1.60it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:37<00:05,  1.62it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:38<00:04,  1.63it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:40<00:03,  1.66it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:41<00:01,  1.67it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:42<00:00,  1.71it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:42<00:00,  1.39it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-8): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (12): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-15): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-31): 15 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-8): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (12): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-15): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-31): 15 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/87.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140321352729040 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321352729040 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321352729040 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321352729040 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140321482098880 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321482098880 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321482098880 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321482098880 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[87] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.43661971830985913, 'acc_stderr,none': 0.05927935558412972}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8109168010046441
0.49072556218914853
0.5566403100849345
0.44024201071133245
0.36782307445393897
0.7670018188163468
0.7843128732802526
0.6988690221064
0.5981195337895844
0.6376244172053548
0.2853091375727497
0.5515546649264043
0.5557505676171792
0.7972024130534298
0.4603750431482902
0.5520483059922646
0.6822463474556734
0.5190759382405471
0.4088145199157014
0.2043407724351867
0.7513222512225379
0.25132541955681237
0.9558801903797878
0.30340608378143175
0.4127519712321019
0.5614277962750809
0.663812026146247
0.6254515974988584
0.8729079397851264
0.8109168010046441
0.49072556218914853
0.5566403100849345
0.44024201071133245
0.36782307445393897
0.7670018188163468
0.7843128732802526
0.6988690221064
0.5981195337895844
0.6376244172053548
0.2853091375727497
0.5515546649264043
0.5557505676171792
0.7972024130534298
0.4603750431482902
0.5520483059922646
0.6822463474556734
0.5190759382405471
0.4088145199157014
0.2043407724351867
0.7513222512225379
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[7, 4, 6, 3, 2, 1, 5, 0]
tensor([7, 4, 6, 3, 2, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 5, 4, 2, 6, 0, 7, 1]
tensor([3, 5, 4, 2, 6, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 6, 3, 7, 2, 1, 4, 0]
tensor([5, 6, 3, 7, 2, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 6, 2, 3, 4, 1, 5, 0]
tensor([7, 6, 2, 3, 4, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 7, 3, 6, 0, 4, 2, 1]
tensor([5, 7, 3, 6, 0, 4, 2, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 5, 1, 6, 0, 2, 7, 4]
tensor([3, 5, 1, 6, 0, 2, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/87.pt
[197] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2363.25it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:19,  3.11s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:41,  1.59s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:00,  1.31s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:41,  1.20s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:34,  1.16s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:24,  1.10s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:16,  1.06s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:09,  1.02s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<02:02,  1.02it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<02:00,  1.02it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:53,  1.07it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<01:47,  1.11it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:26<01:41,  1.15it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:27<01:37,  1.18it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:29<01:33,  1.21it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:31<01:40,  1.11it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:32<01:34,  1.15it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:34<01:29,  1.19it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:36<01:26,  1.22it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:37<01:22,  1.24it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:39<01:20,  1.26it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:40<01:22,  1.20it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:42<01:17,  1.24it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:43<01:14,  1.28it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:45<01:10,  1.31it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:46<01:07,  1.35it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:48<01:05,  1.36it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:48<00:26,  3.04it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:50<00:31,  2.53it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:51<00:35,  2.19it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:53<00:38,  1.97it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:54<00:40,  1.82it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:44,  1.59it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:57<00:43,  1.57it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:58<00:43,  1.55it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:42,  1.53it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:01<00:41,  1.53it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:02<00:41,  1.48it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:39,  1.49it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:05<00:39,  1.45it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:43,  1.28it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:08<00:39,  1.35it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:10<00:36,  1.40it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:11<00:34,  1.44it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:12<00:31,  1.47it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:14<00:30,  1.50it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:15<00:28,  1.52it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:17<00:33,  1.21it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:19<00:29,  1.31it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:20<00:26,  1.39it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:21<00:24,  1.45it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:22<00:22,  1.49it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:23<00:20,  1.53it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:25<00:18,  1.57it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:26<00:17,  1.51it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:27<00:16,  1.54it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:29<00:14,  1.57it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:30<00:13,  1.59it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:31<00:11,  1.61it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:32<00:10,  1.62it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:33<00:09,  1.64it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:35<00:07,  1.66it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:36<00:07,  1.52it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:37<00:05,  1.58it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:38<00:04,  1.62it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:40<00:03,  1.66it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:42<00:02,  1.25it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:43<00:00,  1.37it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:43<00:00,  1.37it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-8): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-30): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-8): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-30): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/197.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140321486354912 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321486354912 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321486354912 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321486354912 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140321484476912 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321484476912 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321484476912 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321484476912 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[197] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.891975958338049
0.3982001908036579
0.26652204443251004
0.14111026067874816
0.11763525057572936
0.8635004180455561
0.8741394655456793
0.5304527321557103
0.44617634270319845
0.7007455592903002
0.6799150362476898
0.3511365594568724
0.47145939463701925
0.4744673317050211
0.905435493752087
0.903612941640167
0.8893763849001028
0.6821933807976941
0.6796182015499347
0.7774502436639344
0.8164788896198145
0.9011095591650676
0.8564639597370403
0.9528866120532432
0.4784452972921274
0.48852492817861076
0.8397355586186119
0.7433051149723976
0.5457687574271932
0.891975958338049
0.3982001908036579
0.26652204443251004
0.14111026067874816
0.11763525057572936
0.8635004180455561
0.8741394655456793
0.5304527321557103
0.44617634270319845
0.7007455592903002
0.6799150362476898
0.3511365594568724
0.47145939463701925
0.4744673317050211
0.905435493752087
0.903612941640167
0.8893763849001028
0.6821933807976941
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[6, 5, 4, 3, 2, 1, 7, 0]
tensor([6, 5, 4, 3, 2, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 7, 4, 1, 0, 6, 3]
tensor([5, 2, 7, 4, 1, 0, 6, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 6, 5, 2, 4, 0, 7, 1]
tensor([3, 6, 5, 2, 4, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 0, 2, 1, 7, 3]
tensor([5, 4, 6, 0, 2, 1, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 0, 7, 3, 5, 1, 6, 2]
tensor([4, 0, 7, 3, 5, 1, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 5, 1, 3, 2, 0, 1, 4]
tensor([0, 5, 1, 3, 2, 0, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 0, 0, 1, 1.0, 1.0, 1.0, 1.0]
tensor([1, 0, 0, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[1, 0, 1, 1.0, 1.0, 0, 1.0, 1.0]
tensor([1, 0, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 1.0, 0, 1, 1.0, 1.0]
tensor([0, 1, 1, 1, 0, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/197.pt
[134] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2378.20it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:17,  3.10s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:35,  1.55s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:53,  1.27s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:40,  1.19s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:28,  1.11s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:18,  1.06s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:10,  1.01s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:16<02:03,  1.03it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:57,  1.06it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:55,  1.07it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:48,  1.11it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:42,  1.16it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:25<01:37,  1.20it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:33,  1.23it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:29,  1.26it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:31,  1.21it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:31<01:28,  1.23it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:33<01:25,  1.25it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:34<01:23,  1.26it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:20,  1.28it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:37<01:18,  1.29it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:17,  1.27it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:40<01:15,  1.29it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:42<01:12,  1.31it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:43<01:09,  1.34it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:45<01:06,  1.36it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:46<01:04,  1.38it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<01:02,  1.39it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:48<00:23,  3.31it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:49<00:28,  2.70it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:51<00:32,  2.31it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:52<00:35,  2.06it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:53<00:40,  1.77it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:55<00:40,  1.69it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:56<00:41,  1.63it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [00:57<00:40,  1.60it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [00:59<00:40,  1.57it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:00<00:39,  1.55it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:01<00:38,  1.54it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:03<00:37,  1.53it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:04<00:37,  1.47it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:06<00:35,  1.49it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:07<00:33,  1.51it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:08<00:31,  1.53it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:09<00:30,  1.55it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:11<00:28,  1.56it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:12<00:27,  1.57it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:13<00:26,  1.57it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:15<00:26,  1.49it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:16<00:24,  1.52it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:17<00:22,  1.54it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:18<00:21,  1.56it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:20<00:19,  1.58it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:21<00:18,  1.59it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:22<00:16,  1.60it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:23<00:15,  1.60it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:25<00:15,  1.52it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:26<00:13,  1.55it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:27<00:11,  1.59it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:28<00:10,  1.61it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:30<00:09,  1.63it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:31<00:07,  1.65it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:32<00:06,  1.66it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:33<00:05,  1.68it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:35<00:04,  1.57it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:36<00:03,  1.62it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:37<00:01,  1.67it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:38<00:00,  1.72it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:38<00:00,  1.44it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-11): 8 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (12): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-31): 18 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-11): 8 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (12): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-31): 18 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/134.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140321885150896 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321885150896 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321885150896 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321885150896 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140320812321392 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320812321392 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320812321392 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320812321392 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[134] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4788732394366197, 'acc_stderr,none': 0.05970805879899505}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.6890224269714
0.6519987454814186
0.44009979043140846
0.5066155659876163
0.7217003976797293
0.12899593918836094
0.4620116191334168
0.3468065225687064
0.10107939599729882
0.5600103662507451
0.577132167051305
0.6748045019607131
0.4004557650420357
0.700349463047757
0.9095160148001629
0.6131975246160634
0.5678386109581075
0.8437866782628755
0.3536670789186643
0.44960924435079147
0.5862896519346712
0.8246697057441258
0.5484674172302575
0.3288911762125467
0.6478473442183775
0.571596663207821
0.7179565348178579
0.38406312361402845
0.42349247998810474
0.6890224269714
0.6519987454814186
0.44009979043140846
0.5066155659876163
0.7217003976797293
0.12899593918836094
0.4620116191334168
0.3468065225687064
0.10107939599729882
0.5600103662507451
0.577132167051305
0.6748045019607131
0.4004557650420357
0.700349463047757
0.9095160148001629
0.6131975246160634
0.5678386109581075
0.8437866782628755
0.3536670789186643
0.44960924435079147
0.5862896519346712
0.8246697057441258
0.5484674172302575
0.3288911762125467
0.6478473442183775
0.571596663207821
Total groups 70 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 6, 2, 1, 4, 0]
tensor([5, 3, 7, 6, 2, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 5, 1, 3, 4, 0, 6, 2]
tensor([7, 5, 1, 3, 4, 0, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 2, 1, 0, 4, 5]
tensor([6, 3, 7, 2, 1, 0, 4, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 5, 2, 1, 4, 0]
tensor([6, 3, 7, 5, 2, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 3, 2, 5, 1, 0, 4, 6]
tensor([7, 3, 2, 5, 1, 0, 4, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 2, 3, 0, 3, 2, 1]
tensor([0, 1, 2, 3, 0, 3, 2, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/134.pt
[49] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2389.09it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:45,  2.88s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:04<03:32,  1.53s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:55,  1.28s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:43,  1.21s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:30,  1.13s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:21,  1.08s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:14,  1.04s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:07,  1.01s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<02:04,  1.01it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:56,  1.06it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:50,  1.10it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:44,  1.14it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:25<01:39,  1.17it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:27<01:35,  1.21it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:35,  1.19it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:30<01:31,  1.22it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:31<01:28,  1.24it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:33<01:25,  1.25it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:35<01:22,  1.27it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:20,  1.28it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:38<01:18,  1.29it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:42<02:02,  1.24s/it]Running loglikelihood requests:  32%|███▏      | 45/142 [00:44<01:45,  1.09s/it]Running loglikelihood requests:  33%|███▎      | 47/142 [00:45<01:32,  1.02it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:47<01:23,  1.11it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:48<01:19,  1.15it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:50<01:16,  1.17it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:52<01:17,  1.12it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:53<01:10,  1.20it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:55<01:05,  1.27it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:56<01:01,  1.32it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:57<00:58,  1.35it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:21,  3.20it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:00<00:24,  2.71it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:27,  2.36it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:03<00:30,  2.04it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:04<00:32,  1.89it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:05<00:32,  1.79it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:33,  1.70it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:08<00:33,  1.65it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:09<00:32,  1.61it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:10<00:32,  1.59it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:12<00:30,  1.59it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:14<00:35,  1.32it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:15<00:32,  1.38it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:16<00:30,  1.43it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:18<00:27,  1.47it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:19<00:25,  1.50it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:20<00:24,  1.53it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:21<00:22,  1.55it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:23<00:21,  1.51it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:24<00:20,  1.54it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:25<00:18,  1.57it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:27<00:17,  1.59it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:28<00:15,  1.59it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:29<00:14,  1.61it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:30<00:12,  1.62it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:31<00:11,  1.63it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:33<00:10,  1.64it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:34<00:09,  1.53it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:35<00:08,  1.57it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:36<00:06,  1.61it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:38<00:05,  1.64it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:39<00:04,  1.67it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:40<00:02,  1.70it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:41<00:01,  1.72it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:42<00:00,  1.75it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:42<00:00,  1.38it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-8): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-22): 13 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (23): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-31): 8 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-8): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-22): 13 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (23): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-31): 8 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/49.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140321891276160 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321891276160 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321891276160 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321891276160 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140320945976864 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320945976864 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320945976864 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320945976864 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[49] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4647887323943662, 'acc_stderr,none': 0.05961305784972239}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8670569790937798
0.5488271932172035
0.673013746499191
0.7739554714966184
0.626122351153327
0.7480239804573661
0.8404062667282521
0.585845593784257
0.5486089643527782
0.295628094796439
0.8187255304150794
0.4183228390778105
0.5130578032019412
0.8769017151034573
0.9364820135353734
0.9916222006713932
0.9862354083632185
0.9582978800582982
0.8980649992917061
0.8669816466709701
0.9008509099634621
0.9321426254796342
0.8646919425811247
0.6505326186022521
0.6612551300691957
0.8835733690017037
0.7234157945938043
0.7342253877992871
0.5683390151095499
0.8670569790937798
0.5488271932172035
0.673013746499191
0.7739554714966184
0.626122351153327
0.7480239804573661
0.8404062667282521
0.585845593784257
0.5486089643527782
0.295628094796439
0.8187255304150794
0.4183228390778105
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 6, 2, 4, 0, 7, 1]
tensor([5, 3, 6, 2, 4, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 4, 6, 7, 2, 0, 5, 1]
tensor([3, 4, 6, 7, 2, 0, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 6, 2, 1, 3, 0, 7, 5]
tensor([4, 6, 2, 1, 3, 0, 7, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 0, 1, 2, 7, 3]
tensor([5, 4, 6, 0, 1, 2, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 2, 6, 5, 4, 0, 7, 1]
tensor([3, 2, 6, 5, 4, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 2, 5, 6, 3, 0, 7, 1]
tensor([4, 2, 5, 6, 3, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/49.pt
[86] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2355.05it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:17,  3.10s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:42,  1.60s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:01,  1.32s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:43,  1.21s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:32,  1.15s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:27,  1.13s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:18,  1.08s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:11,  1.04s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<02:05,  1.00s/it]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<01:58,  1.04it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:52,  1.07it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:25<01:56,  1.02it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:26<01:49,  1.07it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:28<01:43,  1.11it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:30<01:38,  1.14it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:31<01:34,  1.17it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:33<01:32,  1.18it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:35<01:32,  1.15it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:36<01:29,  1.17it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:38<01:26,  1.19it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:39<01:23,  1.21it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:41<01:21,  1.22it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:43<01:18,  1.24it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:44<01:17,  1.23it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:46<01:13,  1.27it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:47<01:10,  1.30it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:49<01:07,  1.32it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:52<01:24,  1.03it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:53<01:16,  1.12it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:26,  2.84it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:29,  2.45it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:32,  2.15it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:36,  1.88it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:00<00:38,  1.75it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:38,  1.67it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:03<00:39,  1.61it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:04<00:39,  1.56it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:05<00:38,  1.53it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:07<00:38,  1.50it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:08<00:38,  1.42it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:10<00:37,  1.43it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:11<00:35,  1.44it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:12<00:33,  1.45it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:14<00:32,  1.46it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:15<00:30,  1.47it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:16<00:29,  1.47it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:18<00:27,  1.47it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:20<00:32,  1.21it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:21<00:28,  1.28it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:23<00:26,  1.34it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:24<00:23,  1.39it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:25<00:21,  1.42it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:27<00:19,  1.46it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:28<00:18,  1.48it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:30<00:18,  1.38it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:31<00:16,  1.43it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:32<00:14,  1.46it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:34<00:12,  1.49it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:35<00:11,  1.51it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:36<00:09,  1.53it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:37<00:08,  1.55it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:39<00:07,  1.51it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:40<00:05,  1.54it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:41<00:04,  1.58it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:42<00:03,  1.60it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:44<00:01,  1.62it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:45<00:00,  1.65it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:45<00:00,  1.35it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-15): 11 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (18-30): 13 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-15): 11 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (18-30): 13 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/86.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140321891273328 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321891273328 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321891273328 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321891273328 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140321492940416 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321492940416 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321492940416 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321492940416 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[86] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8980271762152415
0.2639787465429745
0.5625332696290335
0.793649776762635
0.3255906663223904
0.0484338095439226
0.14923347971103265
0.5710137578745996
0.4024845012023718
0.40192324238029414
0.7421347168699821
0.33528288680973795
0.3799695309319889
0.38260979896102654
0.8288488026343434
0.5741428622514854
0.32292256805357755
0.2608862087917634
0.5815302784189874
0.6526878745506514
0.23164180573632143
0.7209107763819704
0.641400211099821
0.36378810546943974
0.5869502091248732
0.1447672816588167
0.22624580082754567
0.43042437260655503
0.5802248606876254
0.8980271762152415
0.2639787465429745
0.5625332696290335
0.793649776762635
0.3255906663223904
0.0484338095439226
0.14923347971103265
0.5710137578745996
0.4024845012023718
0.40192324238029414
0.7421347168699821
0.33528288680973795
0.3799695309319889
0.38260979896102654
0.8288488026343434
0.5741428622514854
0.32292256805357755
0.2608862087917634
0.5815302784189874
0.6526878745506514
0.23164180573632143
0.7209107763819704
0.641400211099821
0.36378810546943974
0.5869502091248732
0.1447672816588167
0.22624580082754567
0.43042437260655503
Total groups 73 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 4, 1, 0, 6, 2]
tensor([5, 3, 7, 4, 1, 0, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 5, 1, 2, 4, 0, 7, 3]
tensor([6, 5, 1, 2, 4, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 4, 3, 1, 5, 0, 7, 2]
tensor([6, 4, 3, 1, 5, 0, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 3, 6, 0, 4, 2, 5, 1]
tensor([7, 3, 6, 0, 4, 2, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 4, 1, 2, 0, 3, 5, 1]
tensor([0, 4, 1, 2, 0, 3, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[2, 0, 1, 3, 1, 0, 2, 3]
tensor([2, 0, 1, 3, 1, 0, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 1, 1, 3, 0, 2, 3, 2]
tensor([0, 1, 1, 3, 0, 2, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
The group tensor is
[1, 0, 1, 1.0, 1.0, 0, 1.0, 1.0]
tensor([1, 0, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/86.pt
[141] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2391.47it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:04<10:19,  4.39s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:06<04:27,  1.92s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:08<03:32,  1.55s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:10<02:59,  1.33s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:12<02:40,  1.21s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:14<02:28,  1.13s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:18,  1.08s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:18<02:13,  1.05s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:20<02:06,  1.01s/it]Running loglikelihood requests:  13%|█▎        | 19/142 [00:22<01:58,  1.04it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:24<01:52,  1.08it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:25<01:46,  1.12it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:27<01:42,  1.15it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:29<01:44,  1.10it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:31<01:39,  1.14it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:32<01:34,  1.17it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:34<01:31,  1.19it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:36<01:31,  1.17it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:37<01:27,  1.19it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:39<01:28,  1.17it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:41<01:24,  1.20it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:42<01:21,  1.22it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:44<01:17,  1.25it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:45<01:14,  1.27it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:47<01:11,  1.30it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:48<01:08,  1.32it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:49<00:27,  2.99it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:51<00:31,  2.54it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:52<00:34,  2.22it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:39,  1.88it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:41,  1.76it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:42,  1.68it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:42,  1.62it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:42,  1.57it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:42,  1.54it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:41,  1.52it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:04<00:49,  1.24it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:06<00:45,  1.29it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:07<00:42,  1.34it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:08<00:40,  1.37it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:10<00:37,  1.41it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:11<00:35,  1.43it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:13<00:33,  1.45it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:14<00:33,  1.41it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:15<00:31,  1.45it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:17<00:29,  1.48it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:18<00:27,  1.50it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:19<00:25,  1.53it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:20<00:24,  1.54it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:22<00:22,  1.55it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:23<00:21,  1.56it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:24<00:21,  1.47it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:26<00:19,  1.50it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:27<00:17,  1.53it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:28<00:16,  1.54it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:30<00:14,  1.55it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:31<00:13,  1.56it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:32<00:12,  1.58it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:34<00:12,  1.36it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:35<00:10,  1.43it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:36<00:08,  1.48it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:38<00:07,  1.53it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:39<00:05,  1.57it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:40<00:04,  1.60it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:41<00:03,  1.64it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:42<00:01,  1.67it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:43<00:00,  1.69it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:43<00:00,  1.37it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-7): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9-13): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-15): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-23): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-25): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26-31): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-7): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9-13): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-15): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-23): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-25): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26-31): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/141.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140320678132816 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320678132816 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320678132816 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320678132816 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140320823509072 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320823509072 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320823509072 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320823509072 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[141] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.4006974550645674
0.553274626438677
0.7114020264319938
0.6984504610379519
0.8708568984836823
0.6210210193880212
0.7638777300395475
0.4562187856309214
0.27778416422139596
0.7174549442708792
0.8874047516186899
0.7197371862802024
0.9973953807217149
0.8197334014617479
0.9421229754064668
0.9500023700992883
0.6312941692561447
0.47890039407555335
0.4338007663161437
0.9443928308635141
0.5752272779462101
0.5023096877772295
0.7917678755247893
0.42255885423676204
0.5830463630592772
0.884102890932443
0.7935069002678894
0.6274856483424485
0.841637545221562
0.4006974550645674
0.553274626438677
0.7114020264319938
0.6984504610379519
0.8708568984836823
0.6210210193880212
0.7638777300395475
0.4562187856309214
0.27778416422139596
0.7174549442708792
0.8874047516186899
0.7197371862802024
0.9973953807217149
0.8197334014617479
0.9421229754064668
0.9500023700992883
0.6312941692561447
0.47890039407555335
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[4, 2, 5, 1, 6, 3, 7, 0]
tensor([4, 2, 5, 1, 6, 3, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 4, 7, 5, 1, 0, 6, 2]
tensor([3, 4, 7, 5, 1, 0, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[2, 5, 6, 4, 3, 1, 7, 0]
tensor([2, 5, 6, 4, 3, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 1, 4, 3, 6, 0, 5, 2]
tensor([7, 1, 4, 3, 6, 0, 5, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[0, 5, 3, 1, 1, 0, 2, 4]
tensor([0, 5, 3, 1, 1, 0, 2, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[2, 5, 1, 0, 1, 0, 4, 3]
tensor([2, 5, 1, 0, 1, 0, 4, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[4, 0, 3, 0, 2, 1, 5, 1]
tensor([4, 0, 3, 0, 2, 1, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/141.pt
[178] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2379.53it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:31,  3.63s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<04:00,  1.73s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:08<03:17,  1.44s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:10<02:50,  1.26s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:12<02:34,  1.16s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:14<02:24,  1.10s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:15,  1.05s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:11,  1.03s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<02:03,  1.01it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<01:55,  1.06it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:23<01:49,  1.10it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<01:43,  1.14it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:26<01:39,  1.18it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:28<01:39,  1.15it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:29<01:35,  1.19it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:31<01:31,  1.21it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:32<01:28,  1.23it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:34<01:25,  1.25it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:35<01:23,  1.26it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:37<01:24,  1.21it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:39<01:21,  1.24it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:40<01:18,  1.26it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:42<01:15,  1.28it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:43<01:12,  1.31it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:45<01:09,  1.33it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:46<01:06,  1.36it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:48<01:08,  1.29it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:49<00:24,  3.19it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:50<00:28,  2.68it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:52<00:34,  2.19it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:53<00:36,  2.00it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:54<00:38,  1.85it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:56<00:39,  1.75it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:57<00:39,  1.68it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [00:58<00:39,  1.63it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:00<00:39,  1.60it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:01<00:41,  1.49it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:03<00:39,  1.49it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:04<00:37,  1.50it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:05<00:36,  1.51it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:07<00:34,  1.52it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:08<00:33,  1.53it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:09<00:31,  1.54it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:10<00:30,  1.56it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:12<00:31,  1.45it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:13<00:28,  1.49it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:14<00:27,  1.51it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:16<00:25,  1.53it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:17<00:23,  1.55it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:18<00:22,  1.56it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:20<00:20,  1.58it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:21<00:19,  1.59it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:22<00:18,  1.53it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:23<00:17,  1.55it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:25<00:16,  1.56it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:26<00:14,  1.58it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:27<00:13,  1.59it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:28<00:11,  1.61it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:30<00:10,  1.62it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:31<00:09,  1.63it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:32<00:08,  1.54it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:33<00:06,  1.58it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:35<00:05,  1.62it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:36<00:04,  1.66it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:37<00:02,  1.69it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:38<00:01,  1.71it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:39<00:00,  1.74it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:39<00:00,  1.43it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-6): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-12): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15-18): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (19): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (20-23): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (25-31): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-6): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-12): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15-18): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (19): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (20-23): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (25-31): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/178.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140321091051184 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321091051184 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321091051184 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321091051184 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140321085068576 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321085068576 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321085068576 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321085068576 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[178] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.43661971830985913, 'acc_stderr,none': 0.05927935558412972}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.45379462613562144
0.5121987218538359
0.3805605062852953
0.9250105662738365
0.5393046917775384
0.6260326331104198
0.8626372247282749
0.4859614961709456
0.13108090074681872
0.540361134548941
0.3691796883542366
0.8419863957417434
0.3738225663436576
0.7939340566193666
0.6916928670672426
0.661840354699148
0.5510051512470766
0.5066100211995253
0.5208949147180995
0.7283079951082674
0.8857784175671931
0.553624027417834
0.6922319498836209
0.6063680148478464
0.9220941595054846
0.9172099247528117
0.5762720309944068
0.05484791873187908
0.6210078066443784
0.45379462613562144
0.5121987218538359
0.3805605062852953
0.9250105662738365
0.5393046917775384
0.6260326331104198
0.8626372247282749
0.4859614961709456
0.13108090074681872
0.540361134548941
0.3691796883542366
0.8419863957417434
0.3738225663436576
0.7939340566193666
0.6916928670672426
0.661840354699148
0.5510051512470766
0.5066100211995253
0.5208949147180995
0.7283079951082674
0.8857784175671931
0.553624027417834
Total groups 72 exceeded the threshold, stopping comparison.
The group tensor is
[1, 2, 6, 3, 5, 0, 7, 4]
tensor([1, 2, 6, 3, 5, 0, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 3, 2, 0, 6, 7, 1]
tensor([5, 4, 3, 2, 0, 6, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 4, 6, 2, 5, 0, 7, 1]
tensor([3, 4, 6, 2, 5, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 2, 4, 6, 1, 0, 3, 5]
tensor([7, 2, 4, 6, 1, 0, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 0, 7, 4, 3, 5, 6, 1]
tensor([2, 0, 7, 4, 3, 5, 6, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[3, 0, 5, 4, 1, 0, 1, 2]
tensor([3, 0, 5, 4, 1, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/178.pt
[233] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2369.98it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:44,  2.87s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<04:07,  1.78s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:13,  1.41s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:48,  1.25s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:34,  1.16s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:14<02:33,  1.17s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:22,  1.11s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:18<02:13,  1.05s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<02:06,  1.01s/it]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<01:59,  1.03it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:23<01:53,  1.07it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:25<01:51,  1.07it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:26<01:45,  1.11it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:28<01:41,  1.14it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:30<01:37,  1.16it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:31<01:34,  1.17it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:33<01:31,  1.19it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:35<01:31,  1.17it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:36<01:28,  1.18it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:38<01:26,  1.20it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:40<01:23,  1.21it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:41<01:21,  1.22it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:43<01:18,  1.23it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:45<01:21,  1.17it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:46<01:16,  1.22it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:48<01:12,  1.26it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:49<01:10,  1.27it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:51<01:06,  1.30it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:52<01:04,  1.32it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:54<01:01,  1.34it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:55<01:03,  1.27it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:57<01:00,  1.30it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:21,  3.23it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:26,  2.54it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:29,  2.21it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:31,  1.98it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:04<00:33,  1.82it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:05<00:34,  1.70it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:35,  1.62it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:08<00:35,  1.56it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:10<00:36,  1.44it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:11<00:35,  1.43it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:12<00:34,  1.44it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:14<00:32,  1.45it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:15<00:31,  1.45it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:16<00:29,  1.46it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:18<00:27,  1.47it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:20<00:30,  1.27it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:21<00:27,  1.33it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:23<00:25,  1.38it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:24<00:23,  1.41it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:25<00:21,  1.44it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:27<00:19,  1.46it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:28<00:18,  1.48it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:30<00:18,  1.32it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:31<00:16,  1.38it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:32<00:14,  1.43it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:34<00:12,  1.47it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:35<00:11,  1.50it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:36<00:09,  1.52it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:37<00:08,  1.55it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:39<00:07,  1.56it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:40<00:06,  1.47it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:41<00:04,  1.51it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:43<00:03,  1.55it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:44<00:01,  1.59it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:45<00:00,  1.63it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:45<00:00,  1.35it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-8): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (12): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-15): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-31): 15 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-8): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (12): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-15): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-31): 15 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/233.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140320830996800 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320830996800 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320830996800 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320830996800 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140321885865360 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321885865360 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321885865360 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321885865360 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[233] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.43661971830985913, 'acc_stderr,none': 0.05927935558412972}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8109168010046441
0.49072556218914853
0.5566403100849345
0.44024201071133245
0.36782307445393897
0.7670018188163468
0.7843128732802526
0.6988690221064
0.5981195337895844
0.6376244172053548
0.2853091375727497
0.5515546649264043
0.5557505676171792
0.7972024130534298
0.4603750431482902
0.5520483059922646
0.6822463474556734
0.5190759382405471
0.4088145199157014
0.2043407724351867
0.7513222512225379
0.25132541955681237
0.9558801903797878
0.30340608378143175
0.4127519712321019
0.5614277962750809
0.663812026146247
0.6254515974988584
0.8729079397851264
0.8109168010046441
0.49072556218914853
0.5566403100849345
0.44024201071133245
0.36782307445393897
0.7670018188163468
0.7843128732802526
0.6988690221064
0.5981195337895844
0.6376244172053548
0.2853091375727497
0.5515546649264043
0.5557505676171792
0.7972024130534298
0.4603750431482902
0.5520483059922646
0.6822463474556734
0.5190759382405471
0.4088145199157014
0.2043407724351867
0.7513222512225379
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[7, 4, 6, 3, 2, 1, 5, 0]
tensor([7, 4, 6, 3, 2, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 5, 4, 2, 6, 0, 7, 1]
tensor([3, 5, 4, 2, 6, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 6, 3, 7, 2, 1, 4, 0]
tensor([5, 6, 3, 7, 2, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 6, 2, 3, 4, 1, 5, 0]
tensor([7, 6, 2, 3, 4, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 7, 3, 6, 0, 4, 2, 1]
tensor([5, 7, 3, 6, 0, 4, 2, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 5, 1, 6, 0, 2, 7, 4]
tensor([3, 5, 1, 6, 0, 2, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/233.pt
[31] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2364.08it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:36,  3.24s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:47,  1.64s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:04,  1.35s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:10<02:59,  1.33s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:12<02:42,  1.22s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:14<02:30,  1.15s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:23,  1.12s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:18<02:21,  1.11s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:20<02:15,  1.08s/it]Running loglikelihood requests:  13%|█▎        | 19/142 [00:22<02:04,  1.01s/it]Running loglikelihood requests:  15%|█▍        | 21/142 [00:24<01:56,  1.04it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:25<01:49,  1.09it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:27<01:43,  1.13it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:28<01:38,  1.16it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:30<01:38,  1.15it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:32<01:34,  1.18it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:33<01:30,  1.20it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:35<01:35,  1.12it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:37<01:30,  1.16it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:39<01:26,  1.19it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:43<02:00,  1.20s/it]Running loglikelihood requests:  36%|███▌      | 51/142 [00:44<00:44,  2.05it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:46<00:46,  1.90it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<00:48,  1.79it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<00:50,  1.70it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:50<00:51,  1.62it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:51<00:51,  1.56it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:53<00:51,  1.53it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:54<00:52,  1.46it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:56<00:57,  1.30it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:58<00:54,  1.34it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:59<00:51,  1.37it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:00<00:49,  1.40it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:02<00:47,  1.42it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:03<00:45,  1.43it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:05<00:48,  1.29it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:06<00:45,  1.34it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:08<00:42,  1.38it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:09<00:40,  1.42it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:10<00:37,  1.45it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:12<00:35,  1.48it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:13<00:34,  1.50it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:14<00:32,  1.51it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:16<00:31,  1.47it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:17<00:30,  1.50it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:18<00:28,  1.52it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:19<00:26,  1.53it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:21<00:25,  1.53it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:22<00:23,  1.54it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:23<00:22,  1.56it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:25<00:21,  1.57it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:26<00:20,  1.50it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:27<00:18,  1.53it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:28<00:17,  1.56it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:30<00:15,  1.57it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:31<00:14,  1.59it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:32<00:13,  1.60it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:33<00:11,  1.61it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:35<00:10,  1.62it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:36<00:10,  1.46it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:38<00:08,  1.51it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:39<00:07,  1.56it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:40<00:05,  1.59it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:41<00:04,  1.63it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:42<00:03,  1.65it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:43<00:01,  1.68it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:45<00:00,  1.70it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:45<00:00,  1.35it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-5): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7-9): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-26): 16 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28-30): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-5): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7-9): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-26): 16 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28-30): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/31.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140321483438704 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321483438704 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321483438704 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321483438704 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140320821942496 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320821942496 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320821942496 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320821942496 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[31] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7169249479288816
0.6183111372245332
0.46095560668163316
0.45412629074361927
0.5961385422474115
0.8890533054728373
0.9122749865545955
0.7722412965326397
0.28598717020359266
0.16135518374824295
0.6405733513488978
0.34881239744915987
0.02163114281968652
0.8586809120820075
0.5280864903169471
0.5549214872848108
0.8809884720847853
0.8184475126773784
0.7411818478320343
0.9204128494945557
0.8828541622108917
0.8520796846320295
0.9938665155729866
0.8532228506983649
0.8363754351071634
0.7452449391867536
0.8461045724576085
0.3693743162395131
0.3582309316878524
0.7169249479288816
0.6183111372245332
0.46095560668163316
0.45412629074361927
0.5961385422474115
0.8890533054728373
0.9122749865545955
0.7722412965326397
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[5, 4, 6, 3, 2, 0, 7, 1]
tensor([5, 4, 6, 3, 2, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 7, 2, 4, 0, 5, 3]
tensor([6, 1, 7, 2, 4, 0, 5, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 6, 3, 1, 0, 7, 5]
tensor([2, 4, 6, 3, 1, 0, 7, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 0, 6, 4, 3, 1, 5, 7]
tensor([2, 0, 6, 4, 3, 1, 5, 7], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[3, 1, 0, 1, 2, 0, 2, 3]
tensor([3, 1, 0, 1, 2, 0, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 2, 1, 0, 2, 3, 3]
tensor([0, 1, 2, 1, 0, 2, 3, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[3, 1, 0, 1, 2, 0, 3, 2]
tensor([3, 1, 0, 1, 2, 0, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 1, 2, 2, 3, 0, 3, 1]
tensor([0, 1, 2, 2, 3, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[3, 0, 1, 2, 1, 2, 3, 0]
tensor([3, 0, 1, 2, 1, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/31.pt
[138] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2359.75it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:28,  2.76s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:48,  1.64s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:04,  1.34s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:53,  1.29s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:37,  1.18s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:26,  1.12s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:17,  1.07s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:10,  1.03s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<02:09,  1.03s/it]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<02:00,  1.02it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:23<01:54,  1.06it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<01:47,  1.10it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:26<01:42,  1.14it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:28<01:38,  1.17it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:29<01:39,  1.14it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:31<01:35,  1.17it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:33<01:31,  1.19it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:34<01:28,  1.21it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:36<01:25,  1.22it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:37<01:23,  1.24it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:39<01:24,  1.20it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:41<01:20,  1.23it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:42<01:17,  1.26it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:44<00:30,  2.89it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:45<00:34,  2.49it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:46<00:37,  2.19it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:48<00:41,  1.97it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:49<00:43,  1.81it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:51<00:45,  1.70it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:53<00:59,  1.26it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:56,  1.29it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:53,  1.33it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:57<00:50,  1.37it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:48,  1.39it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:45,  1.42it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:43,  1.45it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:45,  1.34it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:05<00:42,  1.39it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:39,  1.43it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:37,  1.46it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:08<00:35,  1.48it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:10<00:34,  1.50it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:11<00:32,  1.51it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:12<00:30,  1.53it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:14<00:30,  1.48it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:15<00:28,  1.50it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:16<00:27,  1.52it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:18<00:25,  1.53it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:19<00:24,  1.54it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:20<00:22,  1.55it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:21<00:21,  1.56it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:23<00:19,  1.57it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:25<00:21,  1.38it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:26<00:18,  1.43it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:27<00:16,  1.47it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:28<00:15,  1.51it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:30<00:13,  1.53it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:31<00:12,  1.56it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:32<00:10,  1.57it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:33<00:09,  1.59it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:37<00:12,  1.02it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:38<00:09,  1.16it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:39<00:07,  1.28it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:40<00:05,  1.38it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:42<00:03,  1.48it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:43<00:01,  1.54it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:44<00:00,  1.54it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:44<00:00,  1.36it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-3): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-6): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-15): 8 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-21): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (23-27): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (29-30): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-3): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-6): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-15): 8 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-21): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (23-27): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (29-30): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/138.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140320812453232 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320812453232 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320812453232 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320812453232 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140321495728000 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321495728000 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321495728000 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321495728000 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[138] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.43661971830985913, 'acc_stderr,none': 0.05927935558412972}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7988766190696382
0.433785810695937
0.6525315214782026
0.871250377966373
0.7790022985921221
0.6581533546839238
0.6948782189696622
0.4773368074709654
0.7967725763717977
0.6899017375628973
0.8876656342513365
0.741048035098914
0.7711231987685904
0.658722026565161
0.8415498496492992
0.7230018765402745
0.5047290951856622
0.7541475774993723
0.5858483604351846
0.9316615143976832
0.7753381144708663
0.5505312367470351
0.97693166402346
0.9600861235134092
0.9818242386505116
0.9256028941559552
0.44553043946224297
0.3299453067275913
0.5544144775455869
0.7988766190696382
0.433785810695937
0.6525315214782026
0.871250377966373
0.7790022985921221
0.6581533546839238
0.6948782189696622
0.4773368074709654
0.7967725763717977
0.6899017375628973
0.8876656342513365
0.741048035098914
0.7711231987685904
0.658722026565161
0.8415498496492992
0.7230018765402745
0.5047290951856622
0.7541475774993723
Total groups 72 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 1, 2, 0, 6, 4]
tensor([5, 3, 7, 1, 2, 0, 6, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 4, 5, 2, 0, 7, 1]
tensor([6, 3, 4, 5, 2, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 4, 7, 2, 5, 1, 6, 0]
tensor([3, 4, 7, 2, 5, 1, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 4, 7, 1, 3, 0, 5, 2]
tensor([6, 4, 7, 1, 3, 0, 5, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 6, 2, 0, 1, 7, 4]
tensor([5, 3, 6, 2, 0, 1, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[3, 5, 0, 2, 4, 0, 1, 1]
tensor([3, 5, 0, 2, 4, 0, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/138.pt
[4] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2372.99it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:58,  3.39s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:50,  1.66s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:03,  1.34s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:43,  1.21s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:37,  1.19s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:19<04:25,  2.03s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:21<03:39,  1.70s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:23<03:04,  1.45s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:24<02:39,  1.28s/it]Running loglikelihood requests:  13%|█▎        | 19/142 [00:26<02:20,  1.14s/it]Running loglikelihood requests:  15%|█▍        | 21/142 [00:28<02:06,  1.05s/it]Running loglikelihood requests:  16%|█▌        | 23/142 [00:29<01:56,  1.02it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:31<01:50,  1.06it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:33<01:43,  1.12it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:34<01:37,  1.16it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:36<01:33,  1.19it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:37<00:45,  2.31it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:38<00:52,  1.98it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:40<00:57,  1.76it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:41<01:01,  1.60it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:43<01:03,  1.52it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:44<01:05,  1.46it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:46<01:11,  1.29it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:48<01:09,  1.32it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:49<01:06,  1.33it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:51<01:04,  1.35it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:52<01:01,  1.37it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:53<00:59,  1.39it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:55<00:57,  1.41it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:56<00:59,  1.33it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:58<00:56,  1.37it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:59<00:53,  1.40it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [01:01<00:51,  1.42it/s]Running loglikelihood requests:  50%|█████     | 71/142 [01:02<00:49,  1.44it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:03<00:47,  1.46it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:05<00:45,  1.47it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:06<00:45,  1.43it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:08<00:48,  1.29it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:09<00:45,  1.35it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:11<00:42,  1.39it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:14<01:01,  1.08s/it]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:16<00:54,  1.01it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:17<00:47,  1.13it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:19<00:41,  1.22it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:20<00:37,  1.30it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:21<00:34,  1.37it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:22<00:31,  1.42it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:24<00:29,  1.46it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:25<00:27,  1.49it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:26<00:26,  1.46it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:28<00:24,  1.49it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:29<00:23,  1.51it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:30<00:21,  1.53it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:32<00:20,  1.55it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:33<00:18,  1.56it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:34<00:17,  1.57it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:35<00:15,  1.57it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:37<00:15,  1.49it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:38<00:02,  3.80it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:39<00:02,  3.21it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:41<00:02,  2.48it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:42<00:02,  2.28it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:43<00:01,  2.12it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:44<00:00,  2.02it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:44<00:00,  1.35it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/4.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140321495714512 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321495714512 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321495714512 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321495714512 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140320828373328 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320828373328 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320828373328 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320828373328 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[4] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9416968404415913
0.7938815868474761
0.8191819727516524
0.8917669687772857
0.90570342985793
0.3043907333176312
0.4952220736758006
0.29899990072660143
0.7758711420345523
0.838301482150715
0.8086122742620726
0.9126741954079025
0.6304407611776705
0.6595297618724628
0.176689009460142
0.4424440684940646
0.43152501031897406
0.48316840881543294
0.43857258209632005
0.7021905687600761
0.2402003641072479
0.27348823957209967
0.6649171314730306
0.25117416182112623
0.9114030338716436
0.9328254651349717
0.9096786197852272
0.9107213191843687
0.8764776988845487
0.9416968404415913
0.7938815868474761
0.8191819727516524
0.8917669687772857
0.90570342985793
0.3043907333176312
0.4952220736758006
0.29899990072660143
0.7758711420345523
0.838301482150715
0.8086122742620726
0.9126741954079025
0.6304407611776705
0.6595297618724628
0.176689009460142
0.4424440684940646
0.43152501031897406
Total groups 76 exceeded the threshold, stopping comparison.
The group tensor is
[4, 3, 7, 2, 5, 1, 6, 0]
tensor([4, 3, 7, 2, 5, 1, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 5, 7, 4, 3, 0, 6, 1]
tensor([2, 5, 7, 4, 3, 0, 6, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 5, 4, 0, 1, 1, 2]
tensor([0, 3, 5, 4, 0, 1, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[4, 3, 0, 2, 5, 0, 1, 1]
tensor([4, 3, 0, 2, 5, 0, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 0, 5, 1, 4, 2, 1, 3]
tensor([0, 0, 5, 1, 4, 2, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[3, 4, 0, 1, 2, 0, 5, 1]
tensor([3, 4, 0, 1, 2, 0, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[5, 0, 4, 3, 2, 0, 1, 1]
tensor([5, 0, 4, 3, 2, 0, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 3, 4, 0, 1, 2, 1, 5]
tensor([0, 3, 4, 0, 1, 2, 1, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/4.pt
[183] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2391.07it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:33,  3.22s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:44,  1.61s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:00,  1.32s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:42,  1.20s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:30,  1.13s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:28,  1.13s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:18,  1.07s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:18<02:23,  1.13s/it]Running loglikelihood requests:  16%|█▌        | 23/142 [00:19<01:01,  1.94it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:21<01:06,  1.76it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:22<01:10,  1.62it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:24<01:14,  1.51it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:26<01:17,  1.44it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:28<01:35,  1.14it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:30<01:32,  1.16it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:32<01:29,  1.17it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:34<01:30,  1.14it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:35<01:26,  1.16it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:37<01:27,  1.13it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:39<01:21,  1.19it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:40<01:16,  1.23it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:42<01:17,  1.20it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:44<01:22,  1.10it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:45<01:15,  1.18it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<01:13,  1.18it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<01:08,  1.25it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:50<01:04,  1.30it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:51<01:00,  1.33it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:53<00:57,  1.37it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:54<00:55,  1.39it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:55<00:53,  1.40it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:57<00:54,  1.35it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:58<00:51,  1.38it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:00<00:48,  1.42it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:01<00:46,  1.44it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:02<00:44,  1.46it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:04<00:42,  1.48it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:05<00:40,  1.49it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:06<00:39,  1.49it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:08<00:40,  1.40it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:09<00:38,  1.44it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:11<00:36,  1.46it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:12<00:34,  1.48it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:13<00:32,  1.50it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:14<00:31,  1.51it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:16<00:29,  1.51it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:17<00:28,  1.52it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:19<00:27,  1.47it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:20<00:07,  3.82it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:21<00:08,  3.01it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:23<00:09,  2.59it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:24<00:09,  2.31it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:25<00:10,  2.10it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:26<00:10,  1.89it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:28<00:09,  1.81it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:29<00:08,  1.75it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:30<00:07,  1.72it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:32<00:06,  1.59it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:33<00:05,  1.61it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:34<00:04,  1.63it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:35<00:03,  1.65it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:36<00:01,  1.66it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:38<00:00,  1.70it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:38<00:00,  1.45it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/183.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140320279971600 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320279971600 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320279971600 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320279971600 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140320824704224 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320824704224 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320824704224 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320824704224 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[183] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.4249710358556562
0.875077076525312
0.8377657430496294
0.7631146361001728
0.8061134194824199
0.597293936853132
0.8993038548124084
0.40672319990150296
0.4427306135620195
0.6697754068966065
0.560869697816391
0.6365238158372074
0.7977251509942833
0.5774168099619936
0.7636367981130562
0.45204149170439595
0.2621700055892588
0.8796773875905071
0.8519712476811715
0.8067705781994788
0.5994711637356425
0.8334667984344581
0.8573465838873401
0.8381586594258955
0.8329727759300379
0.8464402476627775
0.7864641899632313
0.6699089018877085
0.7340555031678195
0.4249710358556562
0.875077076525312
0.8377657430496294
0.7631146361001728
0.8061134194824199
0.597293936853132
0.8993038548124084
0.40672319990150296
0.4427306135620195
0.6697754068966065
0.560869697816391
0.6365238158372074
0.7977251509942833
0.5774168099619936
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[2, 1, 7, 5, 6, 0, 4, 3]
tensor([2, 1, 7, 5, 6, 0, 4, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 6, 7, 1, 3, 0, 5, 4]
tensor([2, 6, 7, 1, 3, 0, 5, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 1, 7, 4, 3, 0, 5, 6]
tensor([2, 1, 7, 4, 3, 0, 5, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 7, 1, 4, 0, 6, 2]
tensor([5, 3, 7, 1, 4, 0, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 1, 3, 0, 7, 2]
tensor([5, 4, 6, 1, 3, 0, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[2, 5, 0, 4, 1, 3, 1, 0]
tensor([2, 5, 0, 4, 1, 3, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 3, 1, 1, 2, 0, 3, 2]
tensor([0, 3, 1, 1, 2, 0, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/183.pt
[204] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2347.88it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:44,  3.29s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:45,  1.62s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:59,  1.31s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:41,  1.19s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:29,  1.12s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:24,  1.10s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:15,  1.05s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:08,  1.01s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<02:01,  1.03it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:19<00:48,  2.40it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:21<00:55,  2.06it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:23<01:02,  1.81it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:24<01:07,  1.64it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:26<01:15,  1.45it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:28<01:23,  1.27it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:30<01:22,  1.27it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:31<01:20,  1.27it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:33<01:18,  1.28it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:34<01:17,  1.28it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:37<01:22,  1.18it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:38<01:17,  1.23it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:39<01:12,  1.28it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:41<01:08,  1.32it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:42<01:05,  1.36it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:44<01:02,  1.39it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:45<01:00,  1.41it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:46<01:01,  1.36it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:48<00:58,  1.38it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:49<00:56,  1.40it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:51<00:54,  1.42it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:52<00:52,  1.43it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:53<00:50,  1.45it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:55<00:48,  1.46it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:56<00:46,  1.47it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:58<00:48,  1.37it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [00:59<00:46,  1.41it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:00<00:43,  1.44it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:02<00:41,  1.46it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:03<00:39,  1.48it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:04<00:38,  1.49it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:06<00:36,  1.50it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:07<00:37,  1.43it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:08<00:34,  1.46it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:10<00:33,  1.48it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:11<00:31,  1.51it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:12<00:29,  1.52it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:14<00:28,  1.53it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:15<00:26,  1.54it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:16<00:25,  1.55it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:18<00:24,  1.51it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:19<00:22,  1.53it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:20<00:21,  1.55it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:20<00:05,  4.50it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:22<00:06,  3.34it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:23<00:06,  2.79it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:24<00:07,  2.43it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:25<00:06,  2.18it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:27<00:06,  2.02it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:28<00:05,  1.92it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:29<00:04,  1.85it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:30<00:03,  1.82it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:31<00:02,  1.79it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:33<00:02,  1.44it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:34<00:00,  1.53it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:34<00:00,  1.50it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-29): 24 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (30-31): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-29): 24 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (30-31): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/204.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140320821937408 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320821937408 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320821937408 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320821937408 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140320949912880 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320949912880 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320949912880 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320949912880 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[204] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4084507042253521, 'acc_stderr,none': 0.058751136942575236}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9179357562486347
0.7304758827874278
0.9028361165350093
0.9177386064031547
0.736264115539795
0.7821589386855158
0.7475448337241333
0.6622221680289488
0.7583296634655016
0.33645990832698236
0.21982307453464583
0.17345324326533693
0.35977004925853934
0.8808900006571698
0.6260065234155616
0.8246960552033293
0.6144641592309815
0.6394144929277611
0.9223063206094516
0.7309208925817541
0.5091734316973643
0.7222829855220914
0.24969394334306674
0.508931879405537
0.8097279042671086
0.5258809052220982
0.4071243454156871
0.9510154626938295
0.3256867622893698
0.9179357562486347
0.7304758827874278
0.9028361165350093
0.9177386064031547
0.736264115539795
0.7821589386855158
0.7475448337241333
0.6622221680289488
0.7583296634655016
0.33645990832698236
0.21982307453464583
0.17345324326533693
0.35977004925853934
0.8808900006571698
0.6260065234155616
0.8246960552033293
0.6144641592309815
0.6394144929277611
0.9223063206094516
0.7309208925817541
0.5091734316973643
0.7222829855220914
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[1, 4, 3, 6, 2, 0, 7, 5]
tensor([1, 4, 3, 6, 2, 0, 7, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 1, 4, 7, 2, 0, 6, 5]
tensor([3, 1, 4, 7, 2, 0, 6, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 6, 7, 0, 3, 2, 4, 1]
tensor([5, 6, 7, 0, 3, 2, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 6, 3, 5, 0, 1, 7, 2]
tensor([4, 6, 3, 5, 0, 1, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 5, 0, 4, 1, 1, 3]
tensor([0, 2, 5, 0, 4, 1, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 2, 3, 0, 2, 3, 1]
tensor([0, 1, 2, 3, 0, 2, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[2, 1, 0, 1, 2, 0, 3, 3]
tensor([2, 1, 0, 1, 2, 0, 3, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 2, 1, 3, 0, 1, 2, 3]
tensor([0, 2, 1, 3, 0, 1, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/204.pt
[155] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2384.10it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:08,  3.04s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<04:08,  1.79s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:13,  1.41s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:49,  1.26s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:35,  1.17s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:25,  1.11s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:20,  1.09s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:12,  1.04s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<02:05,  1.00s/it]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<01:57,  1.05it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:22<00:45,  2.52it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:23<00:53,  2.11it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:25<01:00,  1.83it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:27<01:06,  1.65it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:28<01:14,  1.44it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:30<01:16,  1.38it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:32<01:16,  1.35it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:33<01:16,  1.33it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:35<01:14,  1.32it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:36<01:13,  1.33it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:38<01:11,  1.33it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:40<01:13,  1.27it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:41<01:09,  1.31it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:42<01:06,  1.34it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:44<01:03,  1.37it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:45<01:01,  1.39it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:46<00:59,  1.40it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:48<00:57,  1.41it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:49<00:57,  1.38it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:51<00:54,  1.40it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:52<00:52,  1.42it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:53<00:50,  1.44it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:55<00:48,  1.45it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:56<00:47,  1.47it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:58<00:46,  1.44it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [00:59<00:46,  1.41it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:00<00:43,  1.44it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:02<00:41,  1.47it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:03<00:39,  1.48it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:04<00:38,  1.50it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:06<00:36,  1.51it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:07<00:34,  1.52it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:08<00:33,  1.52it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:10<00:35,  1.38it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:11<00:32,  1.43it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:13<00:30,  1.47it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:14<00:28,  1.50it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:15<00:26,  1.52it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:16<00:25,  1.54it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:18<00:23,  1.55it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:19<00:22,  1.56it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:21<00:23,  1.38it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:22<00:21,  1.44it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:22<00:04,  4.30it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:24<00:05,  3.26it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:25<00:06,  2.72it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:26<00:06,  2.38it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:27<00:06,  2.15it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:28<00:05,  2.00it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:30<00:04,  1.90it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:31<00:03,  1.84it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:32<00:02,  1.80it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:33<00:01,  1.78it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:35<00:00,  1.69it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:35<00:00,  1.49it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-3): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-6): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-15): 8 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-21): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (23-27): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (29-30): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-3): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-6): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-15): 8 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-21): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (23-27): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (29-30): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/155.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140320824696688 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320824696688 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320824696688 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320824696688 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140320818868496 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320818868496 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320818868496 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320818868496 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[155] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.43661971830985913, 'acc_stderr,none': 0.05927935558412972}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7988766190696382
0.433785810695937
0.6525315214782026
0.871250377966373
0.7790022985921221
0.6581533546839238
0.6948782189696622
0.4773368074709654
0.7967725763717977
0.6899017375628973
0.8876656342513365
0.741048035098914
0.7711231987685904
0.658722026565161
0.8415498496492992
0.7230018765402745
0.5047290951856622
0.7541475774993723
0.5858483604351846
0.9316615143976832
0.7753381144708663
0.5505312367470351
0.97693166402346
0.9600861235134092
0.9818242386505116
0.9256028941559552
0.44553043946224297
0.3299453067275913
0.5544144775455869
0.7988766190696382
0.433785810695937
0.6525315214782026
0.871250377966373
0.7790022985921221
0.6581533546839238
0.6948782189696622
0.4773368074709654
0.7967725763717977
0.6899017375628973
0.8876656342513365
0.741048035098914
0.7711231987685904
0.658722026565161
0.8415498496492992
0.7230018765402745
0.5047290951856622
0.7541475774993723
Total groups 72 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 1, 2, 0, 6, 4]
tensor([5, 3, 7, 1, 2, 0, 6, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 4, 5, 2, 0, 7, 1]
tensor([6, 3, 4, 5, 2, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 4, 7, 2, 5, 1, 6, 0]
tensor([3, 4, 7, 2, 5, 1, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 4, 7, 1, 3, 0, 5, 2]
tensor([6, 4, 7, 1, 3, 0, 5, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 6, 2, 0, 1, 7, 4]
tensor([5, 3, 6, 2, 0, 1, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[3, 5, 0, 2, 4, 0, 1, 1]
tensor([3, 5, 0, 2, 4, 0, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/155.pt
[36] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2369.61it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:53,  2.93s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:46,  1.63s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:04,  1.35s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:45,  1.23s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:34,  1.16s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:14<02:33,  1.17s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:23,  1.11s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:15,  1.07s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<02:08,  1.03s/it]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<02:00,  1.02it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:22<00:49,  2.30it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:24<00:57,  1.98it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:25<01:03,  1.74it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:27<01:09,  1.57it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:29<01:20,  1.32it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:31<01:20,  1.31it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:33<01:19,  1.30it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:34<01:18,  1.29it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:36<01:16,  1.29it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:37<01:14,  1.30it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:41<01:49,  1.16s/it]Running loglikelihood requests:  35%|███▍      | 49/142 [00:43<01:36,  1.04s/it]Running loglikelihood requests:  36%|███▌      | 51/142 [00:44<01:25,  1.06it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:46<01:18,  1.14it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<01:12,  1.20it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:49<01:09,  1.22it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:50<01:05,  1.27it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:52<01:01,  1.31it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:53<00:58,  1.35it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:54<00:56,  1.37it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:56<00:53,  1.39it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:57<00:51,  1.41it/s]Running loglikelihood requests:  50%|█████     | 71/142 [01:00<01:03,  1.11it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:01<00:57,  1.20it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:02<00:52,  1.27it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:04<00:48,  1.33it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:05<00:45,  1.38it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:07<00:43,  1.41it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:08<00:41,  1.44it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:09<00:41,  1.39it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:11<00:38,  1.42it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:12<00:36,  1.45it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:13<00:34,  1.46it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:15<00:33,  1.47it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:16<00:31,  1.48it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:18<00:37,  1.20it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:20<00:34,  1.24it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:21<00:31,  1.31it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:22<00:11,  2.99it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:24<00:13,  2.36it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:25<00:13,  2.12it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:26<00:13,  1.96it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:27<00:13,  1.83it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:29<00:13,  1.76it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:30<00:12,  1.70it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:31<00:11,  1.67it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:33<00:10,  1.64it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:34<00:09,  1.53it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:35<00:08,  1.56it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:37<00:06,  1.58it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:38<00:05,  1.60it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:39<00:04,  1.62it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:40<00:03,  1.64it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:41<00:01,  1.66it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:42<00:00,  1.68it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:42<00:00,  1.38it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-5): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7-9): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-26): 16 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28-30): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-5): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7-9): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-26): 16 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28-30): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/36.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140321222457920 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321222457920 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321222457920 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321222457920 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140320949926320 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320949926320 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320949926320 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320949926320 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[36] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7169249479288816
0.6183111372245332
0.46095560668163316
0.45412629074361927
0.5961385422474115
0.8890533054728373
0.9122749865545955
0.7722412965326397
0.28598717020359266
0.16135518374824295
0.6405733513488978
0.34881239744915987
0.02163114281968652
0.8586809120820075
0.5280864903169471
0.5549214872848108
0.8809884720847853
0.8184475126773784
0.7411818478320343
0.9204128494945557
0.8828541622108917
0.8520796846320295
0.9938665155729866
0.8532228506983649
0.8363754351071634
0.7452449391867536
0.8461045724576085
0.3693743162395131
0.3582309316878524
0.7169249479288816
0.6183111372245332
0.46095560668163316
0.45412629074361927
0.5961385422474115
0.8890533054728373
0.9122749865545955
0.7722412965326397
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[5, 4, 6, 3, 2, 0, 7, 1]
tensor([5, 4, 6, 3, 2, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 7, 2, 4, 0, 5, 3]
tensor([6, 1, 7, 2, 4, 0, 5, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 6, 3, 1, 0, 7, 5]
tensor([2, 4, 6, 3, 1, 0, 7, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 0, 6, 4, 3, 1, 5, 7]
tensor([2, 0, 6, 4, 3, 1, 5, 7], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[3, 1, 0, 1, 2, 0, 2, 3]
tensor([3, 1, 0, 1, 2, 0, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 2, 1, 0, 2, 3, 3]
tensor([0, 1, 2, 1, 0, 2, 3, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[3, 1, 0, 1, 2, 0, 3, 2]
tensor([3, 1, 0, 1, 2, 0, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 1, 2, 2, 3, 0, 3, 1]
tensor([0, 1, 2, 2, 3, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[3, 0, 1, 2, 1, 2, 3, 0]
tensor([3, 0, 1, 2, 1, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/36.pt
[101] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2418.90it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:37,  3.67s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:07<05:13,  2.26s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:09<03:45,  1.64s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:12<03:24,  1.52s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:14<02:57,  1.34s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<01:27,  1.48it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:16<01:34,  1.34it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:39,  1.26it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:39,  1.24it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:41,  1.19it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:38,  1.20it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:35,  1.22it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:32,  1.24it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:30,  1.25it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:27,  1.26it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:31<01:26,  1.26it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:32<01:30,  1.18it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:34<01:26,  1.21it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:23,  1.24it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:38<01:30,  1.12it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:24,  1.18it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:41<01:18,  1.23it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:42<01:19,  1.20it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:44<01:13,  1.26it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:45<01:09,  1.31it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:47<01:05,  1.35it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:48<01:02,  1.38it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:51<01:15,  1.12it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:52<01:11,  1.16it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:54<01:05,  1.23it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:55<01:01,  1.29it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:56<00:57,  1.35it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:58<00:54,  1.38it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:59<00:51,  1.42it/s]Running loglikelihood requests:  50%|█████     | 71/142 [01:00<00:49,  1.44it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:02<00:50,  1.37it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:03<00:47,  1.41it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:05<00:45,  1.43it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:06<00:43,  1.46it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:07<00:41,  1.48it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:09<00:39,  1.48it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:10<00:38,  1.49it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:11<00:36,  1.50it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:13<00:37,  1.42it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:14<00:34,  1.46it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:15<00:32,  1.49it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:17<00:31,  1.51it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:17<00:09,  3.80it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:19<00:11,  3.12it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:20<00:12,  2.64it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:21<00:13,  2.33it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:22<00:13,  2.12it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:24<00:13,  1.97it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:25<00:13,  1.87it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:26<00:12,  1.80it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:28<00:13,  1.58it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:29<00:11,  1.61it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:30<00:10,  1.62it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:31<00:09,  1.64it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:32<00:07,  1.65it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:34<00:06,  1.65it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:35<00:05,  1.67it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:36<00:04,  1.68it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:37<00:03,  1.63it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:38<00:01,  1.66it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:40<00:00,  1.70it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:40<00:00,  1.42it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-25): 18 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27-31): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-25): 18 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27-31): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/101.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140320678621648 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320678621648 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320678621648 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320678621648 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140319886481680 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140319886481680 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140319886481680 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140319886481680 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[101] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.4211049002945932
0.6717558353282359
0.7474911564899941
0.6826176878179091
0.7797142635267413
0.8231499933202435
0.9112841579744179
0.9329117857417774
0.3884156651377716
0.7116719161574963
0.3163186199985964
0.7579641143181478
0.7487852880300513
0.644901206148365
0.6252899485594469
0.8300709483875389
0.9686405093585702
0.4827230969630938
0.4401250454367978
0.846541236433719
0.9874080634814415
0.6954384705363965
0.33457229698994645
0.3090774332022817
0.9161115178604439
0.6466213503285708
0.719815146262638
0.8765685188625579
0.30061586460742784
0.4211049002945932
0.6717558353282359
0.7474911564899941
0.6826176878179091
0.7797142635267413
0.8231499933202435
0.9112841579744179
0.9329117857417774
0.3884156651377716
0.7116719161574963
0.3163186199985964
0.7579641143181478
0.7487852880300513
0.644901206148365
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[2, 5, 6, 1, 4, 0, 7, 3]
tensor([2, 5, 6, 1, 4, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 6, 1, 5, 0, 7, 3]
tensor([2, 4, 6, 1, 5, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 2, 7, 0, 1, 3, 6, 5]
tensor([4, 2, 7, 0, 1, 3, 6, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 3, 7, 1, 2, 0, 5, 4]
tensor([6, 3, 7, 1, 2, 0, 5, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 5, 0, 0, 2, 3, 1, 1]
tensor([4, 5, 0, 0, 2, 3, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 4, 1, 2, 1, 0, 3, 5]
tensor([0, 4, 1, 2, 1, 0, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 5, 0, 2, 1, 1, 4]
tensor([0, 3, 5, 0, 2, 1, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/101.pt
[217] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2394.16it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:48,  2.89s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:33,  1.54s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:56,  1.29s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:07<00:56,  2.32it/s]Running loglikelihood requests:   9%|▉         | 13/142 [00:09<01:12,  1.77it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:11<01:23,  1.52it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:13<01:37,  1.28it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:15<01:38,  1.25it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:16<01:38,  1.23it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:18<01:36,  1.23it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:20<01:34,  1.24it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:21<01:32,  1.25it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:23<01:33,  1.21it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:24<01:29,  1.23it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:26<01:27,  1.25it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:27<01:24,  1.26it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:29<01:22,  1.27it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:31<01:20,  1.28it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:33<01:33,  1.08it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:35<01:27,  1.13it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:36<01:21,  1.19it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:38<01:16,  1.24it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:39<01:12,  1.28it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:40<01:08,  1.32it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:42<01:05,  1.36it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:44<01:07,  1.29it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:45<01:03,  1.33it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:46<01:01,  1.36it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:48<00:58,  1.39it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:49<00:56,  1.40it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:50<00:54,  1.42it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:52<00:52,  1.44it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:53<00:53,  1.37it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:55<00:50,  1.40it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:56<00:48,  1.43it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:57<00:46,  1.45it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [00:59<00:44,  1.46it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:00<00:42,  1.48it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:01<00:41,  1.48it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:07<01:19,  1.34s/it]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:09<01:05,  1.14s/it]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:09<00:19,  2.42it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:11<00:20,  2.21it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:12<00:20,  2.05it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:13<00:21,  1.92it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:14<00:21,  1.83it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:16<00:21,  1.76it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:17<00:21,  1.66it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:19<00:20,  1.61it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:20<00:19,  1.62it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:21<00:17,  1.61it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:22<00:16,  1.61it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:23<00:15,  1.61it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:25<00:14,  1.62it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:26<00:12,  1.62it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:27<00:11,  1.63it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:29<00:10,  1.55it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:30<00:09,  1.56it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:31<00:08,  1.58it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:32<00:06,  1.60it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:33<00:05,  1.63it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:35<00:04,  1.66it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:36<00:02,  1.69it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:37<00:01,  1.70it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:39<00:00,  1.52it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:39<00:00,  1.43it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-8): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-30): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-8): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-30): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/217.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140319886483648 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140319886483648 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140319886483648 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140319886483648 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140321352693856 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321352693856 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321352693856 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321352693856 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[217] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.891975958338049
0.3982001908036579
0.26652204443251004
0.14111026067874816
0.11763525057572936
0.8635004180455561
0.8741394655456793
0.5304527321557103
0.44617634270319845
0.7007455592903002
0.6799150362476898
0.3511365594568724
0.47145939463701925
0.4744673317050211
0.905435493752087
0.903612941640167
0.8893763849001028
0.6821933807976941
0.6796182015499347
0.7774502436639344
0.8164788896198145
0.9011095591650676
0.8564639597370403
0.9528866120532432
0.4784452972921274
0.48852492817861076
0.8397355586186119
0.7433051149723976
0.5457687574271932
0.891975958338049
0.3982001908036579
0.26652204443251004
0.14111026067874816
0.11763525057572936
0.8635004180455561
0.8741394655456793
0.5304527321557103
0.44617634270319845
0.7007455592903002
0.6799150362476898
0.3511365594568724
0.47145939463701925
0.4744673317050211
0.905435493752087
0.903612941640167
0.8893763849001028
0.6821933807976941
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[6, 5, 4, 3, 2, 1, 7, 0]
tensor([6, 5, 4, 3, 2, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 7, 4, 1, 0, 6, 3]
tensor([5, 2, 7, 4, 1, 0, 6, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 6, 5, 2, 4, 0, 7, 1]
tensor([3, 6, 5, 2, 4, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 0, 2, 1, 7, 3]
tensor([5, 4, 6, 0, 2, 1, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 0, 7, 3, 5, 1, 6, 2]
tensor([4, 0, 7, 3, 5, 1, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 5, 1, 3, 2, 0, 1, 4]
tensor([0, 5, 1, 3, 2, 0, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 0, 0, 1, 1.0, 1.0, 1.0, 1.0]
tensor([1, 0, 0, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[1, 0, 1, 1.0, 1.0, 0, 1.0, 1.0]
tensor([1, 0, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 1.0, 0, 1, 1.0, 1.0]
tensor([0, 1, 1, 1, 0, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/217.pt
[209] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2352.35it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:13,  3.07s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:03<00:50,  2.70it/s]Running loglikelihood requests:   6%|▋         | 9/142 [00:05<01:15,  1.76it/s]Running loglikelihood requests:   8%|▊         | 11/142 [00:07<01:38,  1.32it/s]Running loglikelihood requests:   9%|▉         | 13/142 [00:09<01:45,  1.22it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:11<01:48,  1.17it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:13<01:49,  1.15it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:15<01:46,  1.15it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:16<01:44,  1.16it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:18<01:45,  1.13it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:20<01:41,  1.16it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:22<01:37,  1.18it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:23<01:33,  1.20it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:25<01:30,  1.22it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:26<01:28,  1.23it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:29<01:36,  1.11it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:30<01:31,  1.14it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:32<01:27,  1.17it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:33<01:24,  1.20it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:35<01:21,  1.21it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:37<01:25,  1.14it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:39<01:23,  1.13it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:40<01:18,  1.19it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:42<01:13,  1.24it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:43<01:09,  1.28it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:45<01:06,  1.31it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:46<01:03,  1.33it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:48<01:05,  1.27it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:49<01:01,  1.31it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:51<00:59,  1.34it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:52<00:56,  1.37it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:53<00:54,  1.38it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:52,  1.40it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:50,  1.41it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:48,  1.42it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:49,  1.36it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:46,  1.39it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:44,  1.42it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:42,  1.44it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:05<00:40,  1.45it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:05<00:12,  4.02it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:06<00:14,  3.17it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:08<00:17,  2.63it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:09<00:18,  2.28it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:10<00:20,  2.04it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:12<00:20,  1.88it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:13<00:22,  1.68it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:14<00:21,  1.64it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:16<00:20,  1.61it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:17<00:19,  1.59it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:18<00:18,  1.58it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:20<00:17,  1.58it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:21<00:15,  1.57it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:22<00:15,  1.51it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:24<00:13,  1.53it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:25<00:12,  1.54it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:26<00:10,  1.55it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:27<00:09,  1.57it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:29<00:08,  1.58it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:30<00:07,  1.40it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:32<00:06,  1.46it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:33<00:04,  1.47it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:34<00:03,  1.53it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:35<00:01,  1.58it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:36<00:00,  1.63it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:36<00:00,  1.46it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-15): 11 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (18-30): 13 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-15): 11 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (18-30): 13 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/209.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140320427164832 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320427164832 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320427164832 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320427164832 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140320678617328 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320678617328 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320678617328 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320678617328 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[209] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8980271762152415
0.2639787465429745
0.5625332696290335
0.793649776762635
0.3255906663223904
0.0484338095439226
0.14923347971103265
0.5710137578745996
0.4024845012023718
0.40192324238029414
0.7421347168699821
0.33528288680973795
0.3799695309319889
0.38260979896102654
0.8288488026343434
0.5741428622514854
0.32292256805357755
0.2608862087917634
0.5815302784189874
0.6526878745506514
0.23164180573632143
0.7209107763819704
0.641400211099821
0.36378810546943974
0.5869502091248732
0.1447672816588167
0.22624580082754567
0.43042437260655503
0.5802248606876254
0.8980271762152415
0.2639787465429745
0.5625332696290335
0.793649776762635
0.3255906663223904
0.0484338095439226
0.14923347971103265
0.5710137578745996
0.4024845012023718
0.40192324238029414
0.7421347168699821
0.33528288680973795
0.3799695309319889
0.38260979896102654
0.8288488026343434
0.5741428622514854
0.32292256805357755
0.2608862087917634
0.5815302784189874
0.6526878745506514
0.23164180573632143
0.7209107763819704
0.641400211099821
0.36378810546943974
0.5869502091248732
0.1447672816588167
0.22624580082754567
0.43042437260655503
Total groups 73 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 4, 1, 0, 6, 2]
tensor([5, 3, 7, 4, 1, 0, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 5, 1, 2, 4, 0, 7, 3]
tensor([6, 5, 1, 2, 4, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 4, 3, 1, 5, 0, 7, 2]
tensor([6, 4, 3, 1, 5, 0, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 3, 6, 0, 4, 2, 5, 1]
tensor([7, 3, 6, 0, 4, 2, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 4, 1, 2, 0, 3, 5, 1]
tensor([0, 4, 1, 2, 0, 3, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[2, 0, 1, 3, 1, 0, 2, 3]
tensor([2, 0, 1, 3, 1, 0, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 1, 1, 3, 0, 2, 3, 2]
tensor([0, 1, 1, 3, 0, 2, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
The group tensor is
[1, 0, 1, 1.0, 1.0, 0, 1.0, 1.0]
tensor([1, 0, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/209.pt
[165] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2399.76it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:33,  3.64s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<04:00,  1.73s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:09,  1.39s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:10<02:51,  1.27s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:12<02:37,  1.19s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:14<02:27,  1.12s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:18,  1.07s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:18<02:10,  1.03s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:20<02:09,  1.03s/it]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<02:00,  1.02it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:23<01:54,  1.06it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:25<01:48,  1.10it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:26<01:43,  1.13it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:28<01:39,  1.16it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:30<01:38,  1.15it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:32<01:37,  1.14it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:33<01:33,  1.16it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:35<01:30,  1.18it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:37<01:27,  1.20it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:38<01:24,  1.21it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:40<01:25,  1.19it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:41<01:22,  1.20it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:43<01:18,  1.23it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:45<01:15,  1.25it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:46<01:13,  1.27it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:48<01:09,  1.30it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:49<01:07,  1.32it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:51<01:08,  1.28it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:52<01:04,  1.31it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:54<01:02,  1.34it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:55<00:59,  1.35it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:56<00:57,  1.37it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:22,  3.11it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:25,  2.60it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:28,  2.25it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:31,  2.01it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:33,  1.84it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:05<00:35,  1.66it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:35,  1.59it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:08<00:35,  1.56it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:09<00:34,  1.54it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:10<00:33,  1.52it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:12<00:32,  1.52it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:13<00:31,  1.51it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:14<00:30,  1.46it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:16<00:29,  1.46it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:17<00:27,  1.47it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:18<00:26,  1.49it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:20<00:24,  1.49it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:21<00:23,  1.50it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:23<00:25,  1.31it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:24<00:22,  1.38it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:27<00:24,  1.19it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:28<00:21,  1.28it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:29<00:18,  1.34it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:30<00:16,  1.40it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:32<00:14,  1.44it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:33<00:12,  1.47it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:34<00:11,  1.51it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:36<00:10,  1.44it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:37<00:08,  1.49it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:38<00:07,  1.53it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:40<00:05,  1.55it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:41<00:04,  1.59it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:42<00:03,  1.61it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:43<00:01,  1.64it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:44<00:00,  1.67it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:44<00:00,  1.36it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-8): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-20): 11 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (21): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22-23): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-25): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27-31): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-8): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-20): 11 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (21): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22-23): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-25): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27-31): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/165.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140320689334624 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320689334624 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320689334624 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320689334624 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140319886489264 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140319886489264 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140319886489264 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140319886489264 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[165] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.43661971830985913, 'acc_stderr,none': 0.05927935558412972}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8745617680355892
0.4870053329545456
0.7039635907698274
0.13683136283338845
0.45491781836309914
0.7394007339170455
0.7100810563328822
0.5089938007827006
0.6148932447522303
0.6097500975530942
0.2557945445996536
0.6585024343395449
0.3077968033302773
0.5696663704302732
0.4357302352587323
0.8508165638532892
0.2937402676601578
0.5546720889398816
0.456887590217546
0.8515148368106646
0.6351128178266827
0.587082322906938
0.4014945886309545
0.6816643170727646
0.7647343638504926
0.15578350583970074
0.7065213224778255
0.687952127174302
0.881700230921547
0.8745617680355892
0.4870053329545456
0.7039635907698274
0.13683136283338845
0.45491781836309914
0.7394007339170455
0.7100810563328822
0.5089938007827006
0.6148932447522303
0.6097500975530942
0.2557945445996536
0.6585024343395449
0.3077968033302773
0.5696663704302732
0.4357302352587323
0.8508165638532892
0.2937402676601578
0.5546720889398816
0.456887590217546
0.8515148368106646
0.6351128178266827
0.587082322906938
0.4014945886309545
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 6, 1, 0, 4, 2]
tensor([5, 3, 7, 6, 1, 0, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 7, 1, 5, 6, 0, 4, 2]
tensor([3, 7, 1, 5, 6, 0, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 3, 2, 4, 5, 1, 7, 0]
tensor([6, 3, 2, 4, 5, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 7, 5, 1, 3, 6, 0]
tensor([2, 4, 7, 5, 1, 3, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[0, 2, 4, 6, 5, 1, 7, 3]
tensor([0, 2, 4, 6, 5, 1, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 2, 5, 3, 6, 0, 7, 1]
tensor([4, 2, 5, 3, 6, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/165.pt
[254] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2410.36it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:02,  3.42s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:51,  1.67s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:04,  1.34s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:43,  1.21s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:31,  1.14s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:14<02:30,  1.15s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:20,  1.09s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:12,  1.04s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<02:05,  1.01s/it]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<01:58,  1.04it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:23<01:55,  1.05it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<01:48,  1.10it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:26<01:43,  1.13it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:28<01:39,  1.16it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:29<01:35,  1.18it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:31<01:32,  1.20it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:33<01:45,  1.04it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:35<01:37,  1.09it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:37<01:32,  1.14it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:38<01:27,  1.17it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:40<01:24,  1.20it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:41<01:20,  1.22it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:43<01:22,  1.18it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:45<01:17,  1.22it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:46<01:13,  1.26it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:48<01:10,  1.30it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:49<01:07,  1.32it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:51<01:04,  1.35it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:52<01:02,  1.36it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:54<01:02,  1.33it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:55<00:59,  1.35it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:56<00:57,  1.38it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:58<00:55,  1.40it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:59<00:53,  1.41it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [01:00<00:51,  1.43it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:01<00:26,  2.52it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:03<00:29,  2.21it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:04<00:31,  1.99it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:05<00:33,  1.84it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:07<00:33,  1.74it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:08<00:35,  1.61it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:10<00:34,  1.57it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:11<00:34,  1.55it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:12<00:33,  1.54it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:14<00:31,  1.53it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:15<00:30,  1.53it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:16<00:29,  1.53it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:18<00:30,  1.42it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:19<00:28,  1.45it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:20<00:26,  1.48it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:22<00:24,  1.49it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:23<00:23,  1.50it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:24<00:22,  1.49it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:26<00:20,  1.51it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:27<00:19,  1.53it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:28<00:18,  1.49it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:30<00:16,  1.52it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:31<00:14,  1.55it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:32<00:13,  1.57it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:33<00:11,  1.60it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:35<00:10,  1.61it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:36<00:09,  1.63it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:37<00:07,  1.64it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:38<00:06,  1.58it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:39<00:05,  1.61it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:41<00:04,  1.64it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:42<00:03,  1.64it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:43<00:01,  1.67it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:44<00:00,  1.70it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:44<00:00,  1.36it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-4): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-9): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-12): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-19): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (20-21): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22-23): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (25-31): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-4): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-9): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-12): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-19): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (20-21): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22-23): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (25-31): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/254.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140320678615360 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320678615360 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320678615360 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320678615360 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140321095711008 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321095711008 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321095711008 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140321095711008 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[254] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.43661971830985913, 'acc_stderr,none': 0.05927935558412972}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8840011732877091
0.9530187867791682
0.922962504956303
0.7839472459406365
0.8981387177104178
0.9267264969548299
0.5821108615798573
0.2837945650872556
0.6077879555368297
0.40197237439213185
0.8904940347912964
0.7509530386402397
0.901875691813653
0.9122390055224058
0.7316046462179062
0.7968435583929763
0.548007144578676
0.5393492220671232
0.9101317167988644
0.5489553505889563
0.4310648048222707
0.3687557715800523
0.8114931082909914
0.5012706843250611
0.2095243909347347
0.3518983916697559
0.9676050865236582
0.8840253404453832
0.6389046419601837
0.8840011732877091
0.9530187867791682
0.922962504956303
0.7839472459406365
0.8981387177104178
0.9267264969548299
0.5821108615798573
0.2837945650872556
0.6077879555368297
0.40197237439213185
0.8904940347912964
0.7509530386402397
0.901875691813653
0.9122390055224058
0.7316046462179062
0.7968435583929763
0.548007144578676
0.5393492220671232
0.9101317167988644
0.5489553505889563
Total groups 72 exceeded the threshold, stopping comparison.
The group tensor is
[1, 2, 7, 4, 3, 0, 6, 5]
tensor([1, 2, 7, 4, 3, 0, 6, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 5, 4, 1, 0, 7, 2]
tensor([6, 3, 5, 4, 1, 0, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 3, 6, 4, 2, 0, 7, 5]
tensor([1, 3, 6, 4, 2, 0, 7, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 3, 6, 4, 0, 1, 7, 2]
tensor([5, 3, 6, 4, 0, 1, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 6, 5, 3, 0, 7, 1]
tensor([2, 4, 6, 5, 3, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 5, 2, 4, 0, 1, 3]
tensor([0, 1, 5, 2, 4, 0, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/254.pt
[253] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2382.31it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:04<10:48,  4.60s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:07<04:51,  2.10s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:09<03:32,  1.55s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:11<03:00,  1.34s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:13<02:41,  1.21s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:15<02:34,  1.18s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:17<02:22,  1.10s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:19<02:13,  1.05s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:21<02:04,  1.00it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:22<01:57,  1.05it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:24<01:50,  1.09it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:26<01:49,  1.09it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:27<01:43,  1.13it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:29<01:38,  1.17it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:30<01:34,  1.20it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:32<01:30,  1.22it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:34<01:28,  1.24it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:36<01:45,  1.01it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:38<01:37,  1.08it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:40<01:30,  1.14it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:41<01:25,  1.18it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:43<01:21,  1.21it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:44<01:17,  1.25it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:46<01:17,  1.22it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:47<01:13,  1.27it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:49<01:09,  1.31it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:50<01:06,  1.35it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:51<01:03,  1.37it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:53<01:00,  1.39it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:54<00:58,  1.41it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:56<01:02,  1.30it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:57<00:58,  1.34it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:59<00:55,  1.38it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [01:00<00:53,  1.40it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [01:01<00:51,  1.43it/s]Running loglikelihood requests:  50%|█████     | 71/142 [01:03<00:49,  1.44it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:04<00:47,  1.46it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:04<00:16,  3.80it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:06<00:19,  2.98it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:07<00:22,  2.49it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:08<00:25,  2.18it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:10<00:28,  1.89it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:11<00:28,  1.78it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:12<00:28,  1.71it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:14<00:28,  1.62it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:15<00:28,  1.60it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:16<00:27,  1.58it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:18<00:26,  1.57it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:19<00:24,  1.57it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:20<00:24,  1.51it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:22<00:22,  1.53it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:23<00:21,  1.55it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:24<00:19,  1.57it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:25<00:18,  1.58it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:27<00:16,  1.59it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:28<00:15,  1.60it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:29<00:14,  1.61it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:31<00:14,  1.45it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:32<00:12,  1.50it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:33<00:11,  1.54it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:34<00:09,  1.57it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:36<00:08,  1.60it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:37<00:06,  1.62it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:38<00:05,  1.64it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:39<00:04,  1.66it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:41<00:03,  1.52it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:42<00:01,  1.58it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:43<00:00,  1.65it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:43<00:00,  1.37it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/253.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140321095705248 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321095705248 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140321095705248 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140321095705248 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140320427076608 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320427076608 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320427076608 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320427076608 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[253] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.4249710358556562
0.875077076525312
0.8377657430496294
0.7631146361001728
0.8061134194824199
0.597293936853132
0.8993038548124084
0.40672319990150296
0.4427306135620195
0.6697754068966065
0.560869697816391
0.6365238158372074
0.7977251509942833
0.5774168099619936
0.7636367981130562
0.45204149170439595
0.2621700055892588
0.8796773875905071
0.8519712476811715
0.8067705781994788
0.5994711637356425
0.8334667984344581
0.8573465838873401
0.8381586594258955
0.8329727759300379
0.8464402476627775
0.7864641899632313
0.6699089018877085
0.7340555031678195
0.4249710358556562
0.875077076525312
0.8377657430496294
0.7631146361001728
0.8061134194824199
0.597293936853132
0.8993038548124084
0.40672319990150296
0.4427306135620195
0.6697754068966065
0.560869697816391
0.6365238158372074
0.7977251509942833
0.5774168099619936
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[2, 1, 7, 5, 6, 0, 4, 3]
tensor([2, 1, 7, 5, 6, 0, 4, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 6, 7, 1, 3, 0, 5, 4]
tensor([2, 6, 7, 1, 3, 0, 5, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 1, 7, 4, 3, 0, 5, 6]
tensor([2, 1, 7, 4, 3, 0, 5, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 7, 1, 4, 0, 6, 2]
tensor([5, 3, 7, 1, 4, 0, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 1, 3, 0, 7, 2]
tensor([5, 4, 6, 1, 3, 0, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[2, 5, 0, 4, 1, 3, 1, 0]
tensor([2, 5, 0, 4, 1, 3, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 3, 1, 1, 2, 0, 3, 2]
tensor([0, 3, 1, 1, 2, 0, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/253.pt
[175] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2365.30it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:22,  3.56s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:55,  1.70s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:05,  1.36s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:44,  1.22s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:12<02:42,  1.22s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:14<02:29,  1.14s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:19,  1.08s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:10,  1.03s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<02:04,  1.01it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<01:56,  1.05it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:23<02:03,  1.02s/it]Running loglikelihood requests:  16%|█▌        | 23/142 [00:25<01:54,  1.04it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:27<01:47,  1.09it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:28<01:41,  1.14it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:30<01:36,  1.17it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:32<01:36,  1.14it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:33<01:33,  1.17it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:35<01:30,  1.19it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:37<01:27,  1.20it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:38<01:24,  1.22it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:40<01:22,  1.23it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:41<01:20,  1.23it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:43<01:19,  1.23it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:44<01:15,  1.25it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:46<01:12,  1.29it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:47<01:09,  1.31it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:49<01:06,  1.34it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:50<01:04,  1.35it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:52<01:02,  1.37it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:54<01:10,  1.17it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:55<01:05,  1.24it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:57<01:01,  1.29it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:58<00:57,  1.33it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [01:00<00:55,  1.36it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [01:01<00:52,  1.39it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:02<00:22,  2.86it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:03<00:25,  2.45it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:05<00:28,  2.15it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:06<00:31,  1.87it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:08<00:32,  1.75it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:09<00:32,  1.68it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:10<00:32,  1.61it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:12<00:33,  1.54it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:13<00:32,  1.52it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:14<00:31,  1.51it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:16<00:29,  1.51it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:18<00:32,  1.33it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:19<00:29,  1.38it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:20<00:27,  1.42it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:22<00:25,  1.45it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:23<00:23,  1.48it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:24<00:22,  1.50it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:26<00:20,  1.51it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:27<00:19,  1.47it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:28<00:18,  1.49it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:30<00:16,  1.51it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:31<00:15,  1.53it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:32<00:13,  1.53it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:33<00:12,  1.55it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:35<00:10,  1.56it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:36<00:09,  1.57it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:41<00:15,  1.16s/it]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:42<00:11,  1.00s/it]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:43<00:07,  1.13it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:44<00:05,  1.25it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:46<00:03,  1.35it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:47<00:02,  1.44it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:48<00:00,  1.44it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:48<00:00,  1.31it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-15): 11 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (18-30): 13 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-15): 11 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (18-30): 13 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/175.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140319745497408 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140319745497408 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140319745497408 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140319745497408 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140320690776752 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320690776752 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320690776752 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320690776752 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[175] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8980271762152415
0.2639787465429745
0.5625332696290335
0.793649776762635
0.3255906663223904
0.0484338095439226
0.14923347971103265
0.5710137578745996
0.4024845012023718
0.40192324238029414
0.7421347168699821
0.33528288680973795
0.3799695309319889
0.38260979896102654
0.8288488026343434
0.5741428622514854
0.32292256805357755
0.2608862087917634
0.5815302784189874
0.6526878745506514
0.23164180573632143
0.7209107763819704
0.641400211099821
0.36378810546943974
0.5869502091248732
0.1447672816588167
0.22624580082754567
0.43042437260655503
0.5802248606876254
0.8980271762152415
0.2639787465429745
0.5625332696290335
0.793649776762635
0.3255906663223904
0.0484338095439226
0.14923347971103265
0.5710137578745996
0.4024845012023718
0.40192324238029414
0.7421347168699821
0.33528288680973795
0.3799695309319889
0.38260979896102654
0.8288488026343434
0.5741428622514854
0.32292256805357755
0.2608862087917634
0.5815302784189874
0.6526878745506514
0.23164180573632143
0.7209107763819704
0.641400211099821
0.36378810546943974
0.5869502091248732
0.1447672816588167
0.22624580082754567
0.43042437260655503
Total groups 73 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 4, 1, 0, 6, 2]
tensor([5, 3, 7, 4, 1, 0, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 5, 1, 2, 4, 0, 7, 3]
tensor([6, 5, 1, 2, 4, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 4, 3, 1, 5, 0, 7, 2]
tensor([6, 4, 3, 1, 5, 0, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 3, 6, 0, 4, 2, 5, 1]
tensor([7, 3, 6, 0, 4, 2, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 4, 1, 2, 0, 3, 5, 1]
tensor([0, 4, 1, 2, 0, 3, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[2, 0, 1, 3, 1, 0, 2, 3]
tensor([2, 0, 1, 3, 1, 0, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 1, 1, 3, 0, 2, 3, 2]
tensor([0, 1, 1, 3, 0, 2, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
The group tensor is
[1, 0, 1, 1.0, 1.0, 0, 1.0, 1.0]
tensor([1, 0, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/175.pt
[48] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2374.69it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:04<09:44,  4.14s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:06<04:18,  1.86s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:08<03:17,  1.44s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:10<02:54,  1.29s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:12<02:40,  1.21s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:14<02:29,  1.14s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:19,  1.08s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:18<02:11,  1.04s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:20<02:05,  1.00s/it]Running loglikelihood requests:  13%|█▎        | 19/142 [00:22<02:11,  1.07s/it]Running loglikelihood requests:  15%|█▍        | 21/142 [00:24<02:01,  1.01s/it]Running loglikelihood requests:  16%|█▌        | 23/142 [00:26<01:53,  1.05it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:27<01:46,  1.10it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:29<01:40,  1.14it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:31<01:38,  1.15it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:32<01:36,  1.14it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:34<01:32,  1.17it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:36<01:29,  1.20it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:37<01:26,  1.21it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:39<01:23,  1.23it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:40<01:21,  1.24it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:42<01:23,  1.18it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:44<00:32,  2.74it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:45<00:37,  2.35it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:47<00:42,  2.01it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:48<00:45,  1.84it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:50<00:47,  1.71it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:51<00:48,  1.62it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:52<00:49,  1.56it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:49,  1.52it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:48,  1.50it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:49,  1.42it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:48,  1.42it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:00<00:47,  1.42it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:45,  1.42it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:44,  1.43it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:04<00:42,  1.43it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:05<00:41,  1.43it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:07<00:41,  1.39it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:08<00:38,  1.42it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:09<00:36,  1.44it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:11<00:35,  1.46it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:12<00:33,  1.47it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:13<00:31,  1.49it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:15<00:29,  1.50it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:16<00:29,  1.44it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:17<00:27,  1.47it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:19<00:26,  1.49it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:20<00:24,  1.51it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:21<00:22,  1.53it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:23<00:21,  1.54it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:24<00:20,  1.55it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:25<00:18,  1.56it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:27<00:18,  1.44it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:28<00:17,  1.44it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:29<00:15,  1.48it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:31<00:13,  1.51it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:32<00:12,  1.53it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:33<00:10,  1.55it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:34<00:09,  1.58it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:36<00:08,  1.59it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:37<00:07,  1.54it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:38<00:05,  1.56it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:40<00:04,  1.58it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:41<00:03,  1.61it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:42<00:01,  1.63it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:43<00:00,  1.66it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:43<00:00,  1.37it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-15): 11 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (18-30): 13 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-15): 11 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (18-30): 13 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/48.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140319616434144 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140319616434144 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140319616434144 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140319616434144 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140320427165360 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320427165360 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320427165360 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320427165360 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[48] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8980271762152415
0.2639787465429745
0.5625332696290335
0.793649776762635
0.3255906663223904
0.0484338095439226
0.14923347971103265
0.5710137578745996
0.4024845012023718
0.40192324238029414
0.7421347168699821
0.33528288680973795
0.3799695309319889
0.38260979896102654
0.8288488026343434
0.5741428622514854
0.32292256805357755
0.2608862087917634
0.5815302784189874
0.6526878745506514
0.23164180573632143
0.7209107763819704
0.641400211099821
0.36378810546943974
0.5869502091248732
0.1447672816588167
0.22624580082754567
0.43042437260655503
0.5802248606876254
0.8980271762152415
0.2639787465429745
0.5625332696290335
0.793649776762635
0.3255906663223904
0.0484338095439226
0.14923347971103265
0.5710137578745996
0.4024845012023718
0.40192324238029414
0.7421347168699821
0.33528288680973795
0.3799695309319889
0.38260979896102654
0.8288488026343434
0.5741428622514854
0.32292256805357755
0.2608862087917634
0.5815302784189874
0.6526878745506514
0.23164180573632143
0.7209107763819704
0.641400211099821
0.36378810546943974
0.5869502091248732
0.1447672816588167
0.22624580082754567
0.43042437260655503
Total groups 73 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 4, 1, 0, 6, 2]
tensor([5, 3, 7, 4, 1, 0, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 5, 1, 2, 4, 0, 7, 3]
tensor([6, 5, 1, 2, 4, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 4, 3, 1, 5, 0, 7, 2]
tensor([6, 4, 3, 1, 5, 0, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 3, 6, 0, 4, 2, 5, 1]
tensor([7, 3, 6, 0, 4, 2, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 4, 1, 2, 0, 3, 5, 1]
tensor([0, 4, 1, 2, 0, 3, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[2, 0, 1, 3, 1, 0, 2, 3]
tensor([2, 0, 1, 3, 1, 0, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 1, 1, 3, 0, 2, 3, 2]
tensor([0, 1, 1, 3, 0, 2, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
The group tensor is
[1, 0, 1, 1.0, 1.0, 0, 1.0, 1.0]
tensor([1, 0, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/48.pt
[66] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2363.44it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:09,  3.04s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:39,  1.58s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:15,  1.43s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:50,  1.26s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:35,  1.17s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:25,  1.11s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:22,  1.11s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:13,  1.05s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<02:07,  1.02s/it]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<01:59,  1.03it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:23<01:53,  1.06it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<01:47,  1.10it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:26<01:47,  1.09it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:27<00:43,  2.52it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:29<00:50,  2.12it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:31<00:59,  1.77it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:32<01:03,  1.62it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:34<01:06,  1.51it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:35<01:08,  1.44it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:37<01:09,  1.40it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:38<01:08,  1.38it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:40<01:07,  1.37it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:42<01:11,  1.28it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:43<01:08,  1.31it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:45<01:05,  1.33it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:46<01:02,  1.35it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:47<01:00,  1.36it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:49<00:59,  1.37it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:50<00:57,  1.37it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:52<00:57,  1.35it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:53<00:55,  1.36it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:53,  1.38it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:51,  1.39it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:57<00:49,  1.40it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:47,  1.40it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:46,  1.41it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:50,  1.25it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:04<00:46,  1.31it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:05<00:43,  1.35it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:41,  1.38it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:08<00:39,  1.41it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:09<00:37,  1.43it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:11<00:35,  1.44it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:12<00:35,  1.37it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:13<00:33,  1.41it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:15<00:31,  1.43it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:16<00:29,  1.46it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:17<00:27,  1.47it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:19<00:26,  1.48it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:20<00:24,  1.49it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:22<00:24,  1.42it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:23<00:22,  1.45it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:24<00:21,  1.47it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:26<00:19,  1.49it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:27<00:17,  1.51it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:28<00:16,  1.51it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:29<00:03,  4.13it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:30<00:03,  3.29it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:31<00:04,  2.75it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:32<00:03,  2.40it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:34<00:03,  2.16it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:35<00:02,  2.02it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:36<00:01,  1.85it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:37<00:00,  1.81it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:37<00:00,  1.45it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-8): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (12): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-15): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-31): 15 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-8): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (12): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-15): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-31): 15 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/66.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140320812893248 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320812893248 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320812893248 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320812893248 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140320689381472 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320689381472 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320689381472 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320689381472 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[66] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.43661971830985913, 'acc_stderr,none': 0.05927935558412972}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8109168010046441
0.49072556218914853
0.5566403100849345
0.44024201071133245
0.36782307445393897
0.7670018188163468
0.7843128732802526
0.6988690221064
0.5981195337895844
0.6376244172053548
0.2853091375727497
0.5515546649264043
0.5557505676171792
0.7972024130534298
0.4603750431482902
0.5520483059922646
0.6822463474556734
0.5190759382405471
0.4088145199157014
0.2043407724351867
0.7513222512225379
0.25132541955681237
0.9558801903797878
0.30340608378143175
0.4127519712321019
0.5614277962750809
0.663812026146247
0.6254515974988584
0.8729079397851264
0.8109168010046441
0.49072556218914853
0.5566403100849345
0.44024201071133245
0.36782307445393897
0.7670018188163468
0.7843128732802526
0.6988690221064
0.5981195337895844
0.6376244172053548
0.2853091375727497
0.5515546649264043
0.5557505676171792
0.7972024130534298
0.4603750431482902
0.5520483059922646
0.6822463474556734
0.5190759382405471
0.4088145199157014
0.2043407724351867
0.7513222512225379
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[7, 4, 6, 3, 2, 1, 5, 0]
tensor([7, 4, 6, 3, 2, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 5, 4, 2, 6, 0, 7, 1]
tensor([3, 5, 4, 2, 6, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 6, 3, 7, 2, 1, 4, 0]
tensor([5, 6, 3, 7, 2, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 6, 2, 3, 4, 1, 5, 0]
tensor([7, 6, 2, 3, 4, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 7, 3, 6, 0, 4, 2, 1]
tensor([5, 7, 3, 6, 0, 4, 2, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 5, 1, 6, 0, 2, 7, 4]
tensor([3, 5, 1, 6, 0, 2, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/66.pt
[238] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2398.85it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:04<09:50,  4.19s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:06<04:22,  1.89s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:08<03:20,  1.46s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:10<02:58,  1.33s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:12<02:41,  1.22s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:14<02:29,  1.14s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:19,  1.09s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:18<02:11,  1.03s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:20<02:04,  1.00it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:22<02:00,  1.02it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:22<00:44,  2.60it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:24<00:51,  2.18it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:26<01:00,  1.82it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:27<01:06,  1.65it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:29<01:09,  1.54it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:30<01:11,  1.46it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:32<01:13,  1.41it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:33<01:13,  1.37it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:35<01:17,  1.27it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:37<01:14,  1.30it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:38<01:12,  1.32it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:40<01:09,  1.34it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:41<01:06,  1.37it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:42<01:04,  1.39it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:44<01:02,  1.40it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:46<01:04,  1.32it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:47<01:01,  1.35it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:48<00:59,  1.37it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:50<00:56,  1.39it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:51<00:54,  1.41it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:52<00:52,  1.42it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:54<00:50,  1.43it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:52,  1.36it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:57<00:49,  1.38it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:58<00:47,  1.41it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:45,  1.44it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:01<00:43,  1.46it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:02<00:41,  1.49it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:03<00:39,  1.50it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:05<00:37,  1.51it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:11<01:17,  1.40s/it]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:12<01:02,  1.18s/it]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:14<00:52,  1.02s/it]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:15<00:44,  1.10it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:16<00:39,  1.18it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:18<00:35,  1.27it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:19<00:31,  1.35it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:20<00:29,  1.41it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:21<00:26,  1.46it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:23<00:24,  1.48it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:24<00:23,  1.51it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:24<00:05,  4.26it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:26<00:06,  3.39it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:27<00:07,  2.81it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:28<00:07,  2.45it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:29<00:07,  2.20it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:31<00:09,  1.62it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:33<00:07,  1.64it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:34<00:06,  1.65it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:35<00:05,  1.66it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:36<00:04,  1.68it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:37<00:02,  1.70it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:38<00:01,  1.71it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:40<00:00,  1.74it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:40<00:00,  1.42it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-31): 24 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-31): 24 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/238.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140320811969152 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320811969152 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320811969152 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320811969152 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140320822355072 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320822355072 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320822355072 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320822355072 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[238] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4647887323943662, 'acc_stderr,none': 0.05961305784972239}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8184046739906345
0.7274935575095154
0.7761510346641957
0.4952640991880176
0.6026187510593423
0.9057397954213379
0.8847150465408834
0.8902479029100969
0.8909574741962474
0.9547688146913799
0.8922781384249192
0.8543488371673005
0.875959532399785
0.5999975074231996
0.5166995128054149
0.9251070768460925
0.9548013456523431
0.895916642208903
0.816962777390054
0.8352073170783986
0.8879963837403804
0.7652959230128763
0.6541271822267725
0.6185613360581901
0.9254826611316034
0.6384241255415785
0.5206869695172345
0.6788316772896046
0.8168301435765506
Total groups 76 exceeded the threshold, stopping comparison.
The group tensor is
[3, 2, 6, 5, 1, 0, 7, 4]
tensor([3, 2, 6, 5, 1, 0, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 3, 6, 5, 2, 0, 7, 4]
tensor([1, 3, 6, 5, 2, 0, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 1, 4, 2, 5, 0, 7, 3]
tensor([6, 1, 4, 2, 5, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 0, 7, 4, 3, 1, 6, 5]
tensor([2, 0, 7, 4, 3, 1, 6, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 4, 1, 3, 1, 0, 5, 2]
tensor([0, 4, 1, 3, 1, 0, 5, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[3, 4, 0, 1, 1, 0, 2, 5]
tensor([3, 4, 0, 1, 1, 0, 2, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 3, 1, 2, 1, 2, 3]
tensor([0, 0, 3, 1, 2, 1, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 1, 1.0, 1.0, 1, 1.0, 1.0]
tensor([0, 0, 1, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[1, 0, 1, 1.0, 1.0, 0, 1.0, 1.0]
tensor([1, 0, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[1, 0, 1, 1.0, 1.0, 0, 1.0, 1.0]
tensor([1, 0, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/238.pt
[57] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2397.86it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:36,  3.66s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:06<04:16,  1.85s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:08<03:35,  1.57s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:10<03:02,  1.35s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:12<02:42,  1.22s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:14<02:30,  1.15s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:24,  1.12s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:18<02:14,  1.06s/it]Running loglikelihood requests:  16%|█▌        | 23/142 [00:19<00:53,  2.22it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:21<01:02,  1.88it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:23<01:07,  1.70it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:24<01:11,  1.58it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:26<01:14,  1.49it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:27<01:16,  1.42it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:29<01:17,  1.38it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:31<01:21,  1.28it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:32<01:20,  1.28it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:34<01:18,  1.28it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:35<01:17,  1.28it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:37<01:15,  1.29it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:38<01:12,  1.31it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:40<01:12,  1.28it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:42<01:09,  1.32it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:43<01:07,  1.33it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:44<01:04,  1.35it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:46<01:02,  1.37it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:47<01:00,  1.38it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:49<00:58,  1.39it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:50<00:56,  1.41it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:52<00:56,  1.36it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:53<00:54,  1.38it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:54<00:51,  1.41it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:50,  1.42it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:57<00:48,  1.44it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:46,  1.44it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:44,  1.45it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:48,  1.31it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:44,  1.36it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:42,  1.40it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:40,  1.42it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:38,  1.42it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:09<00:36,  1.45it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:10<00:34,  1.46it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:11<00:35,  1.38it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:13<00:33,  1.42it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:14<00:30,  1.45it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:15<00:29,  1.48it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:17<00:28,  1.45it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:18<00:26,  1.48it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:19<00:24,  1.49it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:20<00:07,  3.62it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:22<00:08,  2.97it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:23<00:09,  2.55it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:24<00:09,  2.26it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:26<00:09,  1.99it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:27<00:09,  1.88it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:28<00:08,  1.80it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:29<00:07,  1.75it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:31<00:06,  1.72it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:32<00:05,  1.70it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:33<00:04,  1.70it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:34<00:02,  1.70it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:35<00:01,  1.63it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:37<00:00,  1.67it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:37<00:00,  1.46it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-29): 24 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (30-31): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-29): 24 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (30-31): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/57.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140320689384592 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320689384592 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320689384592 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320689384592 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140320823029072 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320823029072 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320823029072 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320823029072 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[57] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4084507042253521, 'acc_stderr,none': 0.058751136942575236}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9179357562486347
0.7304758827874278
0.9028361165350093
0.9177386064031547
0.736264115539795
0.7821589386855158
0.7475448337241333
0.6622221680289488
0.7583296634655016
0.33645990832698236
0.21982307453464583
0.17345324326533693
0.35977004925853934
0.8808900006571698
0.6260065234155616
0.8246960552033293
0.6144641592309815
0.6394144929277611
0.9223063206094516
0.7309208925817541
0.5091734316973643
0.7222829855220914
0.24969394334306674
0.508931879405537
0.8097279042671086
0.5258809052220982
0.4071243454156871
0.9510154626938295
0.3256867622893698
0.9179357562486347
0.7304758827874278
0.9028361165350093
0.9177386064031547
0.736264115539795
0.7821589386855158
0.7475448337241333
0.6622221680289488
0.7583296634655016
0.33645990832698236
0.21982307453464583
0.17345324326533693
0.35977004925853934
0.8808900006571698
0.6260065234155616
0.8246960552033293
0.6144641592309815
0.6394144929277611
0.9223063206094516
0.7309208925817541
0.5091734316973643
0.7222829855220914
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[1, 4, 3, 6, 2, 0, 7, 5]
tensor([1, 4, 3, 6, 2, 0, 7, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 1, 4, 7, 2, 0, 6, 5]
tensor([3, 1, 4, 7, 2, 0, 6, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 6, 7, 0, 3, 2, 4, 1]
tensor([5, 6, 7, 0, 3, 2, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 6, 3, 5, 0, 1, 7, 2]
tensor([4, 6, 3, 5, 0, 1, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 5, 0, 4, 1, 1, 3]
tensor([0, 2, 5, 0, 4, 1, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 2, 3, 0, 2, 3, 1]
tensor([0, 1, 2, 3, 0, 2, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[2, 1, 0, 1, 2, 0, 3, 3]
tensor([2, 1, 0, 1, 2, 0, 3, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 2, 1, 3, 0, 1, 2, 3]
tensor([0, 2, 1, 3, 0, 1, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/57.pt
[133] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2387.46it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:52,  2.93s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:42,  1.60s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:00,  1.32s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:42,  1.20s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:30,  1.13s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:21,  1.08s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:18,  1.07s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:09,  1.02s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<02:03,  1.01it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:19<00:52,  2.27it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:20<01:00,  1.95it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:22<01:05,  1.75it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:23<01:10,  1.61it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:25<01:13,  1.51it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:27<01:15,  1.44it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:28<01:16,  1.40it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:30<01:20,  1.31it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:31<01:18,  1.31it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:33<01:17,  1.31it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:34<01:15,  1.31it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:36<01:13,  1.32it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:37<01:11,  1.33it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:39<01:09,  1.34it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:41<01:18,  1.16it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:43<01:11,  1.24it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:44<01:07,  1.30it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:45<01:03,  1.35it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:47<00:59,  1.39it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:48<00:57,  1.41it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:49<00:55,  1.43it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:51<00:55,  1.38it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:52<00:53,  1.41it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:54<00:50,  1.44it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:55<00:48,  1.46it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:56<00:46,  1.49it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:57<00:44,  1.49it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [00:59<00:43,  1.51it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:00<00:43,  1.46it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:02<00:41,  1.48it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:03<00:39,  1.50it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:04<00:37,  1.51it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:05<00:36,  1.52it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:07<00:34,  1.53it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:08<00:33,  1.54it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:09<00:31,  1.55it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:11<00:31,  1.50it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:12<00:29,  1.52it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:13<00:27,  1.54it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:14<00:26,  1.56it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:16<00:24,  1.57it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:17<00:23,  1.58it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:18<00:22,  1.59it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:19<00:20,  1.59it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:20<00:05,  4.28it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:21<00:06,  3.39it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:23<00:06,  2.75it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:24<00:07,  2.41it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:25<00:07,  1.96it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:27<00:06,  1.88it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:28<00:06,  1.83it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:29<00:05,  1.79it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:30<00:03,  1.78it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:31<00:02,  1.77it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:32<00:01,  1.77it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:33<00:00,  1.79it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:33<00:00,  1.51it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-31): 24 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-31): 24 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/133.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140320811962480 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320811962480 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320811962480 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140320811962480 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140320823024512 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320823024512 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140320823024512 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140320823024512 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[133] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4647887323943662, 'acc_stderr,none': 0.05961305784972239}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8184046739906345
0.7274935575095154
0.7761510346641957
0.4952640991880176
0.6026187510593423
0.9057397954213379
0.8847150465408834
0.8902479029100969
0.8909574741962474
0.9547688146913799
0.8922781384249192
0.8543488371673005
0.875959532399785
0.5999975074231996
0.5166995128054149
0.9251070768460925
0.9548013456523431
0.895916642208903
0.816962777390054
0.8352073170783986
0.8879963837403804
0.7652959230128763
0.6541271822267725
0.6185613360581901
0.9254826611316034
0.6384241255415785
0.5206869695172345
0.6788316772896046
0.8168301435765506
Total groups 76 exceeded the threshold, stopping comparison.
The group tensor is
[3, 2, 6, 5, 1, 0, 7, 4]
tensor([3, 2, 6, 5, 1, 0, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 3, 6, 5, 2, 0, 7, 4]
tensor([1, 3, 6, 5, 2, 0, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 1, 4, 2, 5, 0, 7, 3]
tensor([6, 1, 4, 2, 5, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 0, 7, 4, 3, 1, 6, 5]
tensor([2, 0, 7, 4, 3, 1, 6, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 4, 1, 3, 1, 0, 5, 2]
tensor([0, 4, 1, 3, 1, 0, 5, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[3, 4, 0, 1, 1, 0, 2, 5]
tensor([3, 4, 0, 1, 1, 0, 2, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 3, 1, 2, 1, 2, 3]
tensor([0, 0, 3, 1, 2, 1, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 1, 1.0, 1.0, 1, 1.0, 1.0]
tensor([0, 0, 1, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[1, 0, 1, 1.0, 1.0, 0, 1.0, 1.0]
tensor([1, 0, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[1, 0, 1, 1.0, 1.0, 0, 1.0, 1.0]
tensor([1, 0, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/133.pt
[103] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2379.55it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:06,  3.45s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<04:00,  1.73s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:07,  1.37s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:42,  1.21s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:35,  1.17s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:23,  1.10s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:15,  1.05s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:06,  1.00it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<02:00,  1.04it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<01:53,  1.08it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:51,  1.08it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<01:45,  1.13it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:26<01:39,  1.17it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:26<00:35,  3.11it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:27<00:43,  2.48it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:29<00:50,  2.09it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:31<00:59,  1.73it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:32<01:03,  1.60it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:34<01:05,  1.51it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:35<01:06,  1.46it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:37<01:11,  1.33it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:38<01:08,  1.35it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:40<01:06,  1.38it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:41<01:03,  1.39it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:43<01:01,  1.41it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:44<00:59,  1.43it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:45<00:57,  1.44it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:47<01:00,  1.34it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:48<00:57,  1.39it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:50<00:54,  1.42it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:51<00:51,  1.45it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:52<00:49,  1.47it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:54<00:47,  1.49it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:55<00:45,  1.50it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:56<00:47,  1.41it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [00:58<00:44,  1.45it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [00:59<00:42,  1.48it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:00<00:40,  1.51it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:02<00:38,  1.52it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:03<00:36,  1.54it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:04<00:35,  1.56it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:05<00:33,  1.57it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:07<00:33,  1.50it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:08<00:32,  1.52it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:09<00:30,  1.54it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:11<00:28,  1.55it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:12<00:27,  1.57it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:13<00:25,  1.58it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:14<00:24,  1.60it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:16<00:23,  1.60it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:17<00:22,  1.54it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:18<00:21,  1.56it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:19<00:19,  1.59it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:21<00:18,  1.61it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:22<00:16,  1.62it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:23<00:15,  1.62it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:24<00:14,  1.64it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:25<00:12,  1.65it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:27<00:12,  1.56it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:28<00:01,  4.17it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:29<00:01,  3.51it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:30<00:00,  3.03it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:31<00:00,  2.55it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:31<00:00,  1.55it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-7): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9-13): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-15): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-23): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-25): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26-31): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-7): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9-13): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-15): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-23): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-25): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26-31): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/103.pt
INFO:save_model:Node info successfully sent
